WARNING:tensorflow:

  TensorFlow's `tf-nightly` package will soon be updated to TensorFlow 2.0.

  Please upgrade your code to TensorFlow 2.0:
    * https://www.tensorflow.org/beta/guide/migration_guide

  Or install the latest stable TensorFlow 1.X release:
    * `pip install -U "tensorflow==1.*"`

  Otherwise your code may be broken by the change.

  
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/compat/v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
I0930 03:45:03.793235 139840756766528 model_imports.py:47] Importing lingvo.tasks.asr.params
I0930 03:45:03.862008 139840756766528 model_registry.py:124] Registering models from module: lingvo.tasks.asr.params.librispeech
I0930 03:45:03.868063 139840756766528 model_imports.py:47] Importing lingvo.tasks.car.params
I0930 03:45:03.949092 139840756766528 model_registry.py:124] Registering models from module: lingvo.tasks.car.params.kitti
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/waymo_open_dataset/metrics/ops/py_metrics_ops.py:24: The name tf.resource_loader.get_path_to_datafile is deprecated. Please use tf.compat.v1.resource_loader.get_path_to_datafile instead.

W0930 03:45:03.968616 139840756766528 module_wrapper.py:137] From /usr/local/lib/python3.6/dist-packages/waymo_open_dataset/metrics/ops/py_metrics_ops.py:24: The name tf.resource_loader.get_path_to_datafile is deprecated. Please use tf.compat.v1.resource_loader.get_path_to_datafile instead.

I0930 03:45:04.019523 139840756766528 model_registry.py:124] Registering models from module: lingvo.tasks.car.params.waymo
I0930 03:45:04.025103 139840756766528 model_imports.py:47] Importing lingvo.tasks.image.params
I0930 03:45:04.033692 139840756766528 model_registry.py:124] Registering models from module: lingvo.tasks.image.params.mnist
I0930 03:45:04.033822 139840756766528 model_imports.py:47] Importing lingvo.tasks.lm.params
I0930 03:45:04.039903 139840756766528 model_registry.py:124] Registering models from module: lingvo.tasks.lm.params.one_billion_wds
I0930 03:45:04.043138 139840756766528 model_imports.py:47] Importing lingvo.tasks.mt.params
I0930 03:45:04.058165 139840756766528 model_registry.py:124] Registering models from module: lingvo.tasks.mt.params.wmt14_en_de
I0930 03:45:04.066216 139840756766528 model_registry.py:124] Registering models from module: lingvo.tasks.mt.params.wmtm16_en_de
I0930 03:45:04.067293 139840756766528 model_imports.py:47] Importing lingvo.tasks.punctuator.params
I0930 03:45:04.074726 139840756766528 model_registry.py:124] Registering models from module: lingvo.tasks.punctuator.params.codelab
2019-09-30 03:45:04.075275: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-09-30 03:45:04.102813: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300050000 Hz
2019-09-30 03:45:04.104274: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x6b90a20 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2019-09-30 03:45:04.104300: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2019-09-30 03:45:04.110276: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2019-09-30 03:45:04.257346: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-30 03:45:04.258407: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x6c68100 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2019-09-30 03:45:04.258436: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-SXM2-16GB, Compute Capability 7.0
2019-09-30 03:45:04.259497: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-30 03:45:04.260388: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:00:1e.0
2019-09-30 03:45:04.264144: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2019-09-30 03:45:04.334460: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2019-09-30 03:45:04.368689: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2019-09-30 03:45:04.379302: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2019-09-30 03:45:04.460526: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2019-09-30 03:45:04.510837: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2019-09-30 03:45:04.654799: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2019-09-30 03:45:04.655026: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-30 03:45:04.656096: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-30 03:45:04.656970: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2019-09-30 03:45:04.658054: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2019-09-30 03:45:04.660561: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-09-30 03:45:04.660581: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2019-09-30 03:45:04.660589: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2019-09-30 03:45:04.661628: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-30 03:45:04.662586: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-30 03:45:04.663498: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:local/replica:0/task:0/device:GPU:0 with 15024 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1e.0, compute capability: 7.0)
2019-09-30 03:45:04.678272: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:258] Initialize GrpcChannelCache for job local -> {0 -> localhost:42866}
2019-09-30 03:45:04.682630: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:365] Started server with target: grpc://localhost:42866
I0930 03:45:04.683118 139840756766528 trainer.py:1501] Job controller start
I0930 03:45:04.790053 139840756766528 base_runner.py:57] ============================================================
I0930 03:45:04.807914 139840756766528 base_runner.py:59] allow_implicit_capture : NoneType
I0930 03:45:04.808199 139840756766528 base_runner.py:59] cls : type/lingvo.core.base_model/SingleTaskModel
I0930 03:45:04.808285 139840756766528 base_runner.py:59] cluster.add_summary : NoneType
I0930 03:45:04.808371 139840756766528 base_runner.py:59] cluster.cls : type/lingvo.core.cluster/_Cluster
I0930 03:45:04.808466 139840756766528 base_runner.py:59] cluster.controller.cpus_per_replica : 1
I0930 03:45:04.808542 139840756766528 base_runner.py:59] cluster.controller.devices_per_split : 1
I0930 03:45:04.808607 139840756766528 base_runner.py:59] cluster.controller.gpus_per_replica : 4
I0930 03:45:04.808685 139840756766528 base_runner.py:59] cluster.controller.name : '/job:local'
I0930 03:45:04.808762 139840756766528 base_runner.py:59] cluster.controller.num_tpu_hosts : 0
I0930 03:45:04.808841 139840756766528 base_runner.py:59] cluster.controller.replicas : 1
I0930 03:45:04.808913 139840756766528 base_runner.py:59] cluster.controller.targets : ''
I0930 03:45:04.808978 139840756766528 base_runner.py:59] cluster.controller.tpus_per_replica : 0
I0930 03:45:04.809054 139840756766528 base_runner.py:59] cluster.decoder.cpus_per_replica : 1
I0930 03:45:04.809132 139840756766528 base_runner.py:59] cluster.decoder.devices_per_split : 1
I0930 03:45:04.809197 139840756766528 base_runner.py:59] cluster.decoder.gpus_per_replica : 1
I0930 03:45:04.809269 139840756766528 base_runner.py:59] cluster.decoder.name : '/job:local'
I0930 03:45:04.809346 139840756766528 base_runner.py:59] cluster.decoder.num_tpu_hosts : 0
I0930 03:45:04.809419 139840756766528 base_runner.py:59] cluster.decoder.replicas : 1
I0930 03:45:04.809492 139840756766528 base_runner.py:59] cluster.decoder.targets : ''
I0930 03:45:04.809581 139840756766528 base_runner.py:59] cluster.decoder.tpus_per_replica : 0
I0930 03:45:04.809659 139840756766528 base_runner.py:59] cluster.evaler.cpus_per_replica : 1
I0930 03:45:04.809728 139840756766528 base_runner.py:59] cluster.evaler.devices_per_split : 1
I0930 03:45:04.809792 139840756766528 base_runner.py:59] cluster.evaler.gpus_per_replica : 1
I0930 03:45:04.809870 139840756766528 base_runner.py:59] cluster.evaler.name : '/job:local'
I0930 03:45:04.809935 139840756766528 base_runner.py:59] cluster.evaler.num_tpu_hosts : 0
I0930 03:45:04.810009 139840756766528 base_runner.py:59] cluster.evaler.replicas : 1
I0930 03:45:04.810081 139840756766528 base_runner.py:59] cluster.evaler.targets : ''
I0930 03:45:04.810156 139840756766528 base_runner.py:59] cluster.evaler.tpus_per_replica : 0
I0930 03:45:04.810221 139840756766528 base_runner.py:59] cluster.input.cpus_per_replica : 1
I0930 03:45:04.810293 139840756766528 base_runner.py:59] cluster.input.devices_per_split : 1
I0930 03:45:04.810367 139840756766528 base_runner.py:59] cluster.input.gpus_per_replica : 0
I0930 03:45:04.810433 139840756766528 base_runner.py:59] cluster.input.name : '/job:local'
I0930 03:45:04.810507 139840756766528 base_runner.py:59] cluster.input.num_tpu_hosts : 0
I0930 03:45:04.810582 139840756766528 base_runner.py:59] cluster.input.replicas : 0
I0930 03:45:04.810656 139840756766528 base_runner.py:59] cluster.input.targets : ''
I0930 03:45:04.810729 139840756766528 base_runner.py:59] cluster.input.tpus_per_replica : 0
I0930 03:45:04.810796 139840756766528 base_runner.py:59] cluster.job : 'controller'
I0930 03:45:04.810872 139840756766528 base_runner.py:59] cluster.logdir : ''
I0930 03:45:04.810935 139840756766528 base_runner.py:59] cluster.mode : 'sync'
I0930 03:45:04.811011 139840756766528 base_runner.py:59] cluster.ps.cpus_per_replica : 1
I0930 03:45:04.811084 139840756766528 base_runner.py:59] cluster.ps.devices_per_split : 1
I0930 03:45:04.811152 139840756766528 base_runner.py:59] cluster.ps.gpus_per_replica : 0
I0930 03:45:04.811225 139840756766528 base_runner.py:59] cluster.ps.name : '/job:local'
I0930 03:45:04.811295 139840756766528 base_runner.py:59] cluster.ps.num_tpu_hosts : 0
I0930 03:45:04.811361 139840756766528 base_runner.py:59] cluster.ps.replicas : 1
I0930 03:45:04.811440 139840756766528 base_runner.py:59] cluster.ps.targets : ''
I0930 03:45:04.811510 139840756766528 base_runner.py:59] cluster.ps.tpus_per_replica : 0
I0930 03:45:04.811583 139840756766528 base_runner.py:59] cluster.task : 0
I0930 03:45:04.811653 139840756766528 base_runner.py:59] cluster.worker.cpus_per_replica : 1
I0930 03:45:04.811730 139840756766528 base_runner.py:59] cluster.worker.devices_per_split : 4
I0930 03:45:04.811802 139840756766528 base_runner.py:59] cluster.worker.gpus_per_replica : 4
I0930 03:45:04.811877 139840756766528 base_runner.py:59] cluster.worker.name : '/job:local'
I0930 03:45:04.811949 139840756766528 base_runner.py:59] cluster.worker.num_tpu_hosts : 0
I0930 03:45:04.812013 139840756766528 base_runner.py:59] cluster.worker.replicas : 1
I0930 03:45:04.812089 139840756766528 base_runner.py:59] cluster.worker.targets : ''
I0930 03:45:04.812173 139840756766528 base_runner.py:59] cluster.worker.tpus_per_replica : 0
I0930 03:45:04.812240 139840756766528 base_runner.py:59] dtype : float32
I0930 03:45:04.812313 139840756766528 base_runner.py:59] fprop_dtype : NoneType
I0930 03:45:04.812382 139840756766528 base_runner.py:59] inference_driver_name : NoneType
I0930 03:45:04.812449 139840756766528 base_runner.py:59] input.allow_implicit_capture : NoneType
I0930 03:45:04.812525 139840756766528 base_runner.py:59] input.bucket_adjust_every_n : 0
I0930 03:45:04.812602 139840756766528 base_runner.py:59] input.bucket_batch_limit : [32]
I0930 03:45:04.812674 139840756766528 base_runner.py:59] input.bucket_upper_bound : [1024]
I0930 03:45:04.812735 139840756766528 base_runner.py:59] input.cls : type/lingvo.tasks.lm.input_generator/LmInput
I0930 03:45:04.812808 139840756766528 base_runner.py:59] input.dtype : float32
I0930 03:45:04.812883 139840756766528 base_runner.py:59] input.file_buffer_size : 10000000
I0930 03:45:04.812960 139840756766528 base_runner.py:59] input.file_datasource : NoneType
I0930 03:45:04.813029 139840756766528 base_runner.py:59] input.file_parallelism : 10
I0930 03:45:04.813096 139840756766528 base_runner.py:59] input.file_pattern : 'text:/tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en*'
I0930 03:45:04.813174 139840756766528 base_runner.py:59] input.file_random_seed : 301
I0930 03:45:04.813238 139840756766528 base_runner.py:59] input.fixed_input_shape : True
I0930 03:45:04.813308 139840756766528 base_runner.py:59] input.flush_every_n : 0
I0930 03:45:04.813383 139840756766528 base_runner.py:59] input.fprop_dtype : NoneType
I0930 03:45:04.813450 139840756766528 base_runner.py:59] input.inference_driver_name : NoneType
I0930 03:45:04.813549 139840756766528 base_runner.py:59] input.is_eval : NoneType
I0930 03:45:04.813619 139840756766528 base_runner.py:59] input.is_inference : NoneType
I0930 03:45:04.813683 139840756766528 base_runner.py:59] input.name : '1bwds_train_set'
I0930 03:45:04.813759 139840756766528 base_runner.py:59] input.num_batcher_threads : 16
I0930 03:45:04.813837 139840756766528 base_runner.py:59] input.num_samples : 0
I0930 03:45:04.813912 139840756766528 base_runner.py:59] input.pad_to_max_seq_length : False
I0930 03:45:04.813978 139840756766528 base_runner.py:59] input.params_init.method : 'xavier'
I0930 03:45:04.814042 139840756766528 base_runner.py:59] input.params_init.scale : 1.000001
I0930 03:45:04.814119 139840756766528 base_runner.py:59] input.params_init.seed : NoneType
I0930 03:45:04.814188 139840756766528 base_runner.py:59] input.random_seed : NoneType
I0930 03:45:04.814268 139840756766528 base_runner.py:59] input.remote.max_inflights_per_target : 32
I0930 03:45:04.814327 139840756766528 base_runner.py:59] input.remote.shardable_batch : False
I0930 03:45:04.814400 139840756766528 base_runner.py:59] input.require_sequential_order : False
I0930 03:45:04.814477 139840756766528 base_runner.py:59] input.skip_lp_regularization : NoneType
I0930 03:45:04.814544 139840756766528 base_runner.py:59] input.source_max_length : NoneType
I0930 03:45:04.814617 139840756766528 base_runner.py:59] input.target_max_length : 1024
I0930 03:45:04.814688 139840756766528 base_runner.py:59] input.tokenizer.allow_implicit_capture : NoneType
I0930 03:45:04.814764 139840756766528 base_runner.py:59] input.tokenizer.append_eos : True
I0930 03:45:04.814834 139840756766528 base_runner.py:59] input.tokenizer.cls : type/lingvo.core.tokenizers/AsciiTokenizer
I0930 03:45:04.814902 139840756766528 base_runner.py:59] input.tokenizer.dtype : float32
I0930 03:45:04.814980 139840756766528 base_runner.py:59] input.tokenizer.fprop_dtype : NoneType
I0930 03:45:04.815062 139840756766528 base_runner.py:59] input.tokenizer.inference_driver_name : NoneType
I0930 03:45:04.815131 139840756766528 base_runner.py:59] input.tokenizer.is_eval : NoneType
I0930 03:45:04.815204 139840756766528 base_runner.py:59] input.tokenizer.is_inference : NoneType
I0930 03:45:04.815270 139840756766528 base_runner.py:59] input.tokenizer.name : 'tokenizer'
I0930 03:45:04.815348 139840756766528 base_runner.py:59] input.tokenizer.pad_to_max_length : True
I0930 03:45:04.815411 139840756766528 base_runner.py:59] input.tokenizer.params_init.method : 'xavier'
I0930 03:45:04.815478 139840756766528 base_runner.py:59] input.tokenizer.params_init.scale : 1.000001
I0930 03:45:04.815553 139840756766528 base_runner.py:59] input.tokenizer.params_init.seed : NoneType
I0930 03:45:04.815625 139840756766528 base_runner.py:59] input.tokenizer.random_seed : NoneType
I0930 03:45:04.815697 139840756766528 base_runner.py:59] input.tokenizer.skip_lp_regularization : NoneType
I0930 03:45:04.815766 139840756766528 base_runner.py:59] input.tokenizer.target_eos_id : 2
I0930 03:45:04.815843 139840756766528 base_runner.py:59] input.tokenizer.target_sos_id : 1
I0930 03:45:04.815906 139840756766528 base_runner.py:59] input.tokenizer.target_unk_id : 0
I0930 03:45:04.815974 139840756766528 base_runner.py:59] input.tokenizer.vn.global_vn : False
I0930 03:45:04.816049 139840756766528 base_runner.py:59] input.tokenizer.vn.per_step_vn : False
I0930 03:45:04.816123 139840756766528 base_runner.py:59] input.tokenizer.vn.scale : NoneType
I0930 03:45:04.816199 139840756766528 base_runner.py:59] input.tokenizer.vn.seed : NoneType
I0930 03:45:04.816269 139840756766528 base_runner.py:59] input.tokenizer.vocab_size : 32000
I0930 03:45:04.816349 139840756766528 base_runner.py:59] input.tokenizer_dict : {}
I0930 03:45:04.816418 139840756766528 base_runner.py:59] input.tpu_infeed_parallelism : 1
I0930 03:45:04.816496 139840756766528 base_runner.py:59] input.use_chaining : False
I0930 03:45:04.816563 139840756766528 base_runner.py:59] input.use_per_host_infeed : False
I0930 03:45:04.816636 139840756766528 base_runner.py:59] input.use_within_batch_mixing : False
I0930 03:45:04.816707 139840756766528 base_runner.py:59] input.vn.global_vn : False
I0930 03:45:04.816782 139840756766528 base_runner.py:59] input.vn.per_step_vn : False
I0930 03:45:04.816857 139840756766528 base_runner.py:59] input.vn.scale : NoneType
I0930 03:45:04.816922 139840756766528 base_runner.py:59] input.vn.seed : NoneType
I0930 03:45:04.816997 139840756766528 base_runner.py:59] is_eval : NoneType
I0930 03:45:04.817071 139840756766528 base_runner.py:59] is_inference : NoneType
I0930 03:45:04.817142 139840756766528 base_runner.py:59] model : 'lm.one_billion_wds.OneBWdsGPipeTransformerWPM@/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/tasks/lm/params/one_billion_wds.py:187'
I0930 03:45:04.817219 139840756766528 base_runner.py:59] name : ''
I0930 03:45:04.817297 139840756766528 base_runner.py:59] params_init.method : 'xavier'
I0930 03:45:04.817368 139840756766528 base_runner.py:59] params_init.scale : 1.000001
I0930 03:45:04.817441 139840756766528 base_runner.py:59] params_init.seed : NoneType
I0930 03:45:04.817510 139840756766528 base_runner.py:59] random_seed : NoneType
I0930 03:45:04.817592 139840756766528 base_runner.py:59] skip_lp_regularization : NoneType
I0930 03:45:04.817665 139840756766528 base_runner.py:59] task.allow_implicit_capture : NoneType
I0930 03:45:04.817740 139840756766528 base_runner.py:59] task.cls : type/lingvo.tasks.lm.model/FixedShapeInputLanguageModel
I0930 03:45:04.817818 139840756766528 base_runner.py:59] task.decoder : NoneType
I0930 03:45:04.817887 139840756766528 base_runner.py:59] task.dtype : float32
I0930 03:45:04.817960 139840756766528 base_runner.py:59] task.encoder : NoneType
I0930 03:45:04.818032 139840756766528 base_runner.py:59] task.eval.decoder_samples_per_summary : 0
I0930 03:45:04.818119 139840756766528 base_runner.py:59] task.eval.load_checkpoint_from : NoneType
I0930 03:45:04.818199 139840756766528 base_runner.py:59] task.eval.samples_per_summary : 0
I0930 03:45:04.818274 139840756766528 base_runner.py:59] task.eval.start_decoder_after : 0
I0930 03:45:04.818348 139840756766528 base_runner.py:59] task.eval.start_eval_after : 0
I0930 03:45:04.818422 139840756766528 base_runner.py:59] task.fprop_dtype : NoneType
I0930 03:45:04.818488 139840756766528 base_runner.py:59] task.inference_driver_name : NoneType
I0930 03:45:04.818554 139840756766528 base_runner.py:59] task.input : NoneType
I0930 03:45:04.818631 139840756766528 base_runner.py:59] task.is_eval : NoneType
I0930 03:45:04.818701 139840756766528 base_runner.py:59] task.is_inference : NoneType
I0930 03:45:04.818773 139840756766528 base_runner.py:59] task.lm.allow_implicit_capture : NoneType
I0930 03:45:04.818850 139840756766528 base_runner.py:59] task.lm.cls : type/lingvo.tasks.lm.layers/GPipeTransformerLm
I0930 03:45:04.818928 139840756766528 base_runner.py:59] task.lm.dtype : float32
I0930 03:45:04.818995 139840756766528 base_runner.py:59] task.lm.fprop_dtype : NoneType
I0930 03:45:04.819060 139840756766528 base_runner.py:59] task.lm.inference_driver_name : NoneType
I0930 03:45:04.819135 139840756766528 base_runner.py:59] task.lm.is_eval : NoneType
I0930 03:45:04.819207 139840756766528 base_runner.py:59] task.lm.is_inference : NoneType
I0930 03:45:04.819279 139840756766528 base_runner.py:59] task.lm.name : 'transformerlm'
I0930 03:45:04.819352 139840756766528 base_runner.py:59] task.lm.params_init.method : 'xavier'
I0930 03:45:04.819426 139840756766528 base_runner.py:59] task.lm.params_init.scale : 1.000001
I0930 03:45:04.819488 139840756766528 base_runner.py:59] task.lm.params_init.seed : NoneType
I0930 03:45:04.819564 139840756766528 base_runner.py:59] task.lm.random_seed : NoneType
I0930 03:45:04.819642 139840756766528 base_runner.py:59] task.lm.skip_lp_regularization : NoneType
I0930 03:45:04.819705 139840756766528 base_runner.py:59] task.lm.stack.allow_implicit_capture : NoneType
I0930 03:45:04.819780 139840756766528 base_runner.py:59] task.lm.stack.batch_dim : 1
I0930 03:45:04.819852 139840756766528 base_runner.py:59] task.lm.stack.cls : type/lingvo.core.layers_with_gpipe/GPipeTransformerStack
I0930 03:45:04.819926 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.allow_implicit_capture : NoneType
I0930 03:45:04.819998 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.cls : type/lingvo.core.layers_with_gpipe/GPipeTransformerLayer
I0930 03:45:04.820071 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.dtype : float32
I0930 03:45:04.820147 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.final_enc_layer : False
I0930 03:45:04.820209 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.fprop_dtype : NoneType
I0930 03:45:04.820283 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.has_aux_atten : True
I0930 03:45:04.820358 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.inference_driver_name : NoneType
I0930 03:45:04.820431 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.is_decoder : False
I0930 03:45:04.820507 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.is_eval : NoneType
I0930 03:45:04.820582 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.is_inference : NoneType
I0930 03:45:04.820658 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.is_transparent : False
I0930 03:45:04.820724 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.allow_implicit_capture : NoneType
I0930 03:45:04.820798 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.cls : type/lingvo.core.layers/LayerNorm
I0930 03:45:04.820864 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.dtype : float32
I0930 03:45:04.820940 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.epsilon : 1e-06
I0930 03:45:04.821010 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.fprop_dtype : NoneType
I0930 03:45:04.821068 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.inference_driver_name : NoneType
I0930 03:45:04.821120 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.input_dim : 0
I0930 03:45:04.821170 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.is_eval : NoneType
I0930 03:45:04.821220 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.is_inference : NoneType
I0930 03:45:04.821270 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.name : ''
I0930 03:45:04.821321 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.params_init.method : 'xavier'
I0930 03:45:04.821371 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.params_init.scale : 1.000001
I0930 03:45:04.821422 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.params_init.seed : NoneType
I0930 03:45:04.821473 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.random_seed : NoneType
I0930 03:45:04.821538 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.skip_lp_regularization : NoneType
I0930 03:45:04.821593 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.vn.global_vn : False
I0930 03:45:04.821645 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.vn.per_step_vn : False
I0930 03:45:04.821695 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.vn.scale : NoneType
I0930 03:45:04.821746 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.vn.seed : NoneType
I0930 03:45:04.821798 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.mask_self_atten : True
I0930 03:45:04.821848 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.name : ''
I0930 03:45:04.821899 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.normalize_output : False
I0930 03:45:04.821949 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.output_dim : 0
I0930 03:45:04.821999 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.packed_input : False
I0930 03:45:04.822049 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.params_init.method : 'xavier'
I0930 03:45:04.822099 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.params_init.scale : 1.000001
I0930 03:45:04.822150 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.params_init.seed : NoneType
I0930 03:45:04.822200 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.random_seed : NoneType
I0930 03:45:04.822250 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.skip_lp_regularization : NoneType
I0930 03:45:04.822300 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.source_dim : 0
I0930 03:45:04.822351 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.add_unnormalized_input : False
I0930 03:45:04.822402 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.allow_implicit_capture : NoneType
I0930 03:45:04.822452 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_dropout_prob : 0.0
I0930 03:45:04.822502 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_hidden_dim : 0
I0930 03:45:04.822552 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.allow_implicit_capture : NoneType
I0930 03:45:04.822603 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.atten_dropout_deterministic : False
I0930 03:45:04.822653 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.atten_dropout_prob : 0.0
I0930 03:45:04.822702 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.cls : type/lingvo.core.attention/MultiHeadedAttention
I0930 03:45:04.822753 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.context_dim : 0
I0930 03:45:04.822803 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.ctx_post_proj_dim : 0
I0930 03:45:04.822854 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.dtype : float32
I0930 03:45:04.822908 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.enable_ctx_post_proj : True
I0930 03:45:04.822960 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.enable_ctx_pre_proj : False
I0930 03:45:04.823011 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.enable_query_proj : True
I0930 03:45:04.823061 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.enable_source_proj : True
I0930 03:45:04.823112 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.fprop_dtype : NoneType
I0930 03:45:04.823162 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.hidden_dim : 0
I0930 03:45:04.823212 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inference_driver_name : NoneType
I0930 03:45:04.823262 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.allow_implicit_capture : NoneType
I0930 03:45:04.823313 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.atten_dropout_deterministic : False
I0930 03:45:04.823364 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.atten_dropout_prob : 0.0
I0930 03:45:04.823414 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.cls : type/lingvo.core.attention/DotProductAttention
I0930 03:45:04.823465 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.dtype : float32
I0930 03:45:04.823515 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.fprop_dtype : NoneType
I0930 03:45:04.823565 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.hidden_dim : 0
I0930 03:45:04.823616 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.inference_driver_name : NoneType
I0930 03:45:04.823667 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.is_eval : NoneType
I0930 03:45:04.823718 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.is_inference : NoneType
I0930 03:45:04.823768 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.name : ''
I0930 03:45:04.823818 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.packed_input : False
I0930 03:45:04.823869 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.params_init.method : 'xavier'
I0930 03:45:04.823919 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.params_init.scale : 1.000001
I0930 03:45:04.823969 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.params_init.seed : NoneType
I0930 03:45:04.824020 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.qdomain.default : NoneType
I0930 03:45:04.824070 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.qdomain.fullyconnected : NoneType
I0930 03:45:04.824120 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.qdomain.softmax : NoneType
I0930 03:45:04.824170 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.query_dim : 0
I0930 03:45:04.824220 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.random_seed : NoneType
I0930 03:45:04.824269 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.skip_lp_regularization : NoneType
I0930 03:45:04.824324 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.source_dim : 0
I0930 03:45:04.824374 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.global_vn : False
I0930 03:45:04.824425 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.per_step_vn : False
I0930 03:45:04.824475 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.scale : NoneType
I0930 03:45:04.824525 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.seed : NoneType
I0930 03:45:04.824575 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.is_eval : NoneType
I0930 03:45:04.824626 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.is_inference : NoneType
I0930 03:45:04.824676 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.name : ''
I0930 03:45:04.824726 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.num_attention_heads : 2
I0930 03:45:04.824776 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.packed_input : False
I0930 03:45:04.824826 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.params_init.method : 'xavier'
I0930 03:45:04.824876 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.params_init.scale : 1.0
I0930 03:45:04.824925 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.params_init.seed : NoneType
I0930 03:45:04.824976 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.qdomain.atten_context : NoneType
I0930 03:45:04.825026 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.qdomain.default : NoneType
I0930 03:45:04.825076 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.qdomain.fullyconnected : NoneType
I0930 03:45:04.825126 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.qdomain.softmax : NoneType
I0930 03:45:04.825176 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.query_dim : 0
I0930 03:45:04.825227 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.random_seed : NoneType
I0930 03:45:04.825276 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.skip_lp_regularization : NoneType
I0930 03:45:04.825327 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.source_dim : 0
I0930 03:45:04.825377 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.use_source_vec_as_attention_value : False
I0930 03:45:04.825427 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.vn.global_vn : False
I0930 03:45:04.825477 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.vn.per_step_vn : False
I0930 03:45:04.825547 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.vn.scale : NoneType
I0930 03:45:04.825605 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.vn.seed : NoneType
I0930 03:45:04.825658 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.cls : type/lingvo.core.layers_with_attention/TransformerAttentionLayer
I0930 03:45:04.825708 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.context_dim : 0
I0930 03:45:04.825759 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.dtype : float32
I0930 03:45:04.825810 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.fprop_dtype : NoneType
I0930 03:45:04.825860 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.inference_driver_name : NoneType
I0930 03:45:04.825915 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.is_eval : NoneType
I0930 03:45:04.825966 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.is_inference : NoneType
I0930 03:45:04.826018 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.is_masked : False
I0930 03:45:04.826068 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.allow_implicit_capture : NoneType
I0930 03:45:04.826119 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.cls : type/lingvo.core.layers/LayerNorm
I0930 03:45:04.826169 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.dtype : float32
I0930 03:45:04.826220 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.epsilon : 1e-06
I0930 03:45:04.826270 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.fprop_dtype : NoneType
I0930 03:45:04.826320 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.inference_driver_name : NoneType
I0930 03:45:04.826370 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.input_dim : 0
I0930 03:45:04.826420 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.is_eval : NoneType
I0930 03:45:04.826470 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.is_inference : NoneType
I0930 03:45:04.826519 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.name : ''
I0930 03:45:04.826570 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.params_init.method : 'xavier'
I0930 03:45:04.826620 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.params_init.scale : 1.000001
I0930 03:45:04.826670 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.params_init.seed : NoneType
I0930 03:45:04.826721 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.random_seed : NoneType
I0930 03:45:04.826771 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.skip_lp_regularization : NoneType
I0930 03:45:04.826821 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.vn.global_vn : False
I0930 03:45:04.826871 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.vn.per_step_vn : False
I0930 03:45:04.826921 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.vn.scale : NoneType
I0930 03:45:04.826972 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.vn.seed : NoneType
I0930 03:45:04.827022 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.mask_type : 'future'
I0930 03:45:04.827072 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.name : ''
I0930 03:45:04.827122 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.num_attention_heads : 8
I0930 03:45:04.827172 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.packed_input : False
I0930 03:45:04.827222 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.params_init.method : 'xavier'
I0930 03:45:04.827276 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.params_init.scale : 1.000001
I0930 03:45:04.827328 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.params_init.seed : NoneType
I0930 03:45:04.827378 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.random_seed : NoneType
I0930 03:45:04.827428 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_prob : 0.0
I0930 03:45:04.827478 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.allow_implicit_capture : NoneType
I0930 03:45:04.827533 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.cls : type/lingvo.core.layers/DropoutLayer
I0930 03:45:04.827584 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.dropout_at_eval : False
I0930 03:45:04.827635 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.dtype : float32
I0930 03:45:04.827685 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.fprop_dtype : NoneType
I0930 03:45:04.827735 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.inference_driver_name : NoneType
I0930 03:45:04.827785 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.is_eval : NoneType
I0930 03:45:04.827834 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.is_inference : NoneType
I0930 03:45:04.827884 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.keep_prob : 1.0
I0930 03:45:04.827934 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.name : ''
I0930 03:45:04.827984 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.noise_shape : NoneType
I0930 03:45:04.828034 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.noise_shape_broadcast_dims : NoneType
I0930 03:45:04.828084 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.params_init.method : 'xavier'
I0930 03:45:04.828133 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.params_init.scale : 1.000001
I0930 03:45:04.828182 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.params_init.seed : NoneType
I0930 03:45:04.828232 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.random_seed : NoneType
I0930 03:45:04.828282 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.skip_lp_regularization : NoneType
I0930 03:45:04.828332 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.global_vn : False
I0930 03:45:04.828382 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.per_step_vn : False
I0930 03:45:04.828431 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.scale : NoneType
I0930 03:45:04.828481 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.seed : NoneType
I0930 03:45:04.828531 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.skip_lp_regularization : NoneType
I0930 03:45:04.828580 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.source_dim : 0
I0930 03:45:04.828629 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.vn.global_vn : False
I0930 03:45:04.828679 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.vn.per_step_vn : False
I0930 03:45:04.828729 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.vn.scale : NoneType
I0930 03:45:04.828778 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.vn.seed : NoneType
I0930 03:45:04.828828 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_aux_atten_tpl : NoneType
I0930 03:45:04.828877 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.activation : 'RELU'
I0930 03:45:04.828927 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.allow_implicit_capture : NoneType
I0930 03:45:04.828977 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.cls : type/lingvo.core.layers_with_attention/TransformerFeedForwardLayer
I0930 03:45:04.829032 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.dtype : float32
I0930 03:45:04.829083 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.activation : ['RELU', 'NONE']
I0930 03:45:04.829133 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.allow_implicit_capture : NoneType
I0930 03:45:04.829184 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.batch_norm : False
I0930 03:45:04.829233 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.bn_fold_weights : NoneType
I0930 03:45:04.829284 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.cls : type/lingvo.core.layers/FeedForwardNet
I0930 03:45:04.829334 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.allow_implicit_capture : NoneType
I0930 03:45:04.829384 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.cls : type/lingvo.core.layers/DropoutLayer
I0930 03:45:04.829433 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.dropout_at_eval : False
I0930 03:45:04.829483 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.dtype : float32
I0930 03:45:04.829546 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.fprop_dtype : NoneType
I0930 03:45:04.829601 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.inference_driver_name : NoneType
I0930 03:45:04.829651 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.is_eval : NoneType
I0930 03:45:04.829701 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.is_inference : NoneType
I0930 03:45:04.829750 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.keep_prob : 1.0
I0930 03:45:04.829800 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.name : ''
I0930 03:45:04.829849 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.noise_shape : NoneType
I0930 03:45:04.829899 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.noise_shape_broadcast_dims : NoneType
I0930 03:45:04.829949 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.params_init.method : 'xavier'
I0930 03:45:04.829999 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.params_init.scale : 1.000001
I0930 03:45:04.830049 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.params_init.seed : NoneType
I0930 03:45:04.830099 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.random_seed : NoneType
I0930 03:45:04.830148 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.skip_lp_regularization : NoneType
I0930 03:45:04.830198 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.global_vn : False
I0930 03:45:04.830248 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.per_step_vn : False
I0930 03:45:04.830298 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.scale : NoneType
I0930 03:45:04.830355 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.seed : NoneType
I0930 03:45:04.830405 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dtype : float32
I0930 03:45:04.830455 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.fprop_dtype : NoneType
I0930 03:45:04.830505 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.inference_driver_name : NoneType
I0930 03:45:04.830560 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.input_dim : 0
I0930 03:45:04.830611 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.is_eval : NoneType
I0930 03:45:04.830662 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.is_inference : NoneType
I0930 03:45:04.830712 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.name : ''
I0930 03:45:04.830762 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.params_init.method : 'xavier'
I0930 03:45:04.830813 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.params_init.scale : 1.000001
I0930 03:45:04.830863 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.params_init.seed : NoneType
I0930 03:45:04.830913 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.activation : 'RELU'
I0930 03:45:04.830964 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.affine_last : False
I0930 03:45:04.831014 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.allow_implicit_capture : NoneType
I0930 03:45:04.831064 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.batch_norm : True
I0930 03:45:04.831114 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.bias_init : 0.0
I0930 03:45:04.831164 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.bn_fold_weights : NoneType
I0930 03:45:04.831215 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.cls : type/lingvo.core.layers/ProjectionLayer
I0930 03:45:04.831265 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.dtype : float32
I0930 03:45:04.831315 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.fprop_dtype : NoneType
I0930 03:45:04.831365 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.has_bias : False
I0930 03:45:04.831414 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.inference_driver_name : NoneType
I0930 03:45:04.831465 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.input_dim : 0
I0930 03:45:04.831515 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.is_eval : NoneType
I0930 03:45:04.831566 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.is_inference : NoneType
I0930 03:45:04.831616 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.name : ''
I0930 03:45:04.831666 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.output_dim : 0
I0930 03:45:04.831717 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.params_init.method : 'xavier'
I0930 03:45:04.831767 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.params_init.scale : 1.000001
I0930 03:45:04.831818 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.params_init.seed : NoneType
I0930 03:45:04.831868 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.qdomain.default : NoneType
I0930 03:45:04.831919 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.random_seed : NoneType
I0930 03:45:04.831969 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.skip_lp_regularization : NoneType
I0930 03:45:04.832024 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.global_vn : False
I0930 03:45:04.832076 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.per_step_vn : False
I0930 03:45:04.832126 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.scale : NoneType
I0930 03:45:04.832176 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.seed : NoneType
I0930 03:45:04.832226 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.weight_norm : False
I0930 03:45:04.832277 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.qdomain.default : NoneType
I0930 03:45:04.832326 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.random_seed : NoneType
I0930 03:45:04.832376 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.skip_connections : NoneType
I0930 03:45:04.832426 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.skip_lp_regularization : NoneType
I0930 03:45:04.832475 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.global_vn : False
I0930 03:45:04.832525 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.per_step_vn : False
I0930 03:45:04.832575 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.scale : NoneType
I0930 03:45:04.832624 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.seed : NoneType
I0930 03:45:04.832674 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.weight_norm : False
I0930 03:45:04.832724 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fprop_dtype : NoneType
I0930 03:45:04.832774 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.hidden_dim : 2048
I0930 03:45:04.832824 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.inference_driver_name : NoneType
I0930 03:45:04.832875 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.input_dim : 0
I0930 03:45:04.832925 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.is_eval : NoneType
I0930 03:45:04.832975 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.is_inference : NoneType
I0930 03:45:04.833025 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.allow_implicit_capture : NoneType
I0930 03:45:04.833075 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.cls : type/lingvo.core.layers/LayerNorm
I0930 03:45:04.833125 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.dtype : float32
I0930 03:45:04.833175 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.epsilon : 1e-06
I0930 03:45:04.833225 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.fprop_dtype : NoneType
I0930 03:45:04.833274 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.inference_driver_name : NoneType
I0930 03:45:04.833324 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.input_dim : 0
I0930 03:45:04.833374 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.is_eval : NoneType
I0930 03:45:04.833424 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.is_inference : NoneType
I0930 03:45:04.833474 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.name : ''
I0930 03:45:04.833535 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.params_init.method : 'xavier'
I0930 03:45:04.833596 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.params_init.scale : 1.000001
I0930 03:45:04.833647 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.params_init.seed : NoneType
I0930 03:45:04.833698 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.random_seed : NoneType
I0930 03:45:04.833748 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.skip_lp_regularization : NoneType
I0930 03:45:04.833798 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.vn.global_vn : False
I0930 03:45:04.833847 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.vn.per_step_vn : False
I0930 03:45:04.833897 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.vn.scale : NoneType
I0930 03:45:04.833947 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.vn.seed : NoneType
I0930 03:45:04.833997 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.name : ''
I0930 03:45:04.834047 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.output_dim : 0
I0930 03:45:04.834096 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.params_init.method : 'xavier'
I0930 03:45:04.834146 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.params_init.scale : 1.000001
I0930 03:45:04.834196 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.params_init.seed : NoneType
I0930 03:45:04.834245 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.random_seed : NoneType
I0930 03:45:04.834295 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.relu_dropout_prob : 0.0
I0930 03:45:04.834344 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.activation : 'RELU'
I0930 03:45:04.834394 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.affine_last : False
I0930 03:45:04.834444 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.allow_implicit_capture : NoneType
I0930 03:45:04.834494 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.batch_norm : True
I0930 03:45:04.834545 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.bias_init : 0.0
I0930 03:45:04.834595 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.bn_fold_weights : NoneType
I0930 03:45:04.834646 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.cls : type/lingvo.core.layers/ProjectionLayer
I0930 03:45:04.834697 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.dtype : float32
I0930 03:45:04.834747 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.fprop_dtype : NoneType
I0930 03:45:04.834797 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.has_bias : False
I0930 03:45:04.834847 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.inference_driver_name : NoneType
I0930 03:45:04.834897 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.input_dim : 0
I0930 03:45:04.834947 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.is_eval : NoneType
I0930 03:45:04.834996 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.is_inference : NoneType
I0930 03:45:04.835047 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.name : ''
I0930 03:45:04.835097 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.output_dim : 0
I0930 03:45:04.835151 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.params_init.method : 'xavier'
I0930 03:45:04.835203 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.params_init.scale : 1.000001
I0930 03:45:04.835253 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.params_init.seed : NoneType
I0930 03:45:04.835303 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.qdomain.default : NoneType
I0930 03:45:04.835354 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.random_seed : NoneType
I0930 03:45:04.835404 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.skip_lp_regularization : NoneType
I0930 03:45:04.835455 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.global_vn : False
I0930 03:45:04.835505 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.per_step_vn : False
I0930 03:45:04.835556 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.scale : NoneType
I0930 03:45:04.835606 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.seed : NoneType
I0930 03:45:04.835656 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.weight_norm : False
I0930 03:45:04.835706 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_prob : 0.0
I0930 03:45:04.835757 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.allow_implicit_capture : NoneType
I0930 03:45:04.835807 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.cls : type/lingvo.core.layers/DropoutLayer
I0930 03:45:04.835857 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.dropout_at_eval : False
I0930 03:45:04.835908 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.dtype : float32
I0930 03:45:04.835958 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.fprop_dtype : NoneType
I0930 03:45:04.836008 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.inference_driver_name : NoneType
I0930 03:45:04.836058 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.is_eval : NoneType
I0930 03:45:04.836109 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.is_inference : NoneType
I0930 03:45:04.836158 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.keep_prob : 1.0
I0930 03:45:04.836209 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.name : ''
I0930 03:45:04.836259 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.noise_shape : NoneType
I0930 03:45:04.836309 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.noise_shape_broadcast_dims : NoneType
I0930 03:45:04.836360 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.params_init.method : 'xavier'
I0930 03:45:04.836410 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.params_init.scale : 1.000001
I0930 03:45:04.836460 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.params_init.seed : NoneType
I0930 03:45:04.836510 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.random_seed : NoneType
I0930 03:45:04.836560 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.skip_lp_regularization : NoneType
I0930 03:45:04.836616 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.global_vn : False
I0930 03:45:04.836667 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.per_step_vn : False
I0930 03:45:04.836717 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.scale : NoneType
I0930 03:45:04.836768 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.seed : NoneType
I0930 03:45:04.836819 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.skip_lp_regularization : NoneType
I0930 03:45:04.836870 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.vn.global_vn : False
I0930 03:45:04.836920 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.vn.per_step_vn : False
I0930 03:45:04.836971 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.vn.scale : NoneType
I0930 03:45:04.837021 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.vn.seed : NoneType
I0930 03:45:04.837071 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.transparent_merger_tpl : NoneType
I0930 03:45:04.837121 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.vn.global_vn : False
I0930 03:45:04.837170 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.vn.per_step_vn : False
I0930 03:45:04.837220 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.vn.scale : NoneType
I0930 03:45:04.837270 139840756766528 base_runner.py:59] task.lm.stack.decoder_tpl.vn.seed : NoneType
I0930 03:45:04.837323 139840756766528 base_runner.py:59] task.lm.stack.dtype : float32
I0930 03:45:04.837373 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.add_tgt_embedding_layer : False
I0930 03:45:04.837423 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.allow_implicit_capture : NoneType
I0930 03:45:04.837473 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.batch_dim : 1
I0930 03:45:04.837538 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.cls : type/lingvo.core.layers_with_gpipe/GPipeTransformerEmbeddingLayer
I0930 03:45:04.837595 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.dec_task_emb : NoneType
I0930 03:45:04.837646 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.allow_implicit_capture : NoneType
I0930 03:45:04.837697 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.cls : type/lingvo.core.layers/DropoutLayer
I0930 03:45:04.837747 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.dropout_at_eval : False
I0930 03:45:04.837797 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.dtype : float32
I0930 03:45:04.837848 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.fprop_dtype : NoneType
I0930 03:45:04.837898 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.inference_driver_name : NoneType
I0930 03:45:04.837949 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.is_eval : NoneType
I0930 03:45:04.838000 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.is_inference : NoneType
I0930 03:45:04.838050 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.keep_prob : 1.0
I0930 03:45:04.838100 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.name : ''
I0930 03:45:04.838150 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.noise_shape : NoneType
I0930 03:45:04.838204 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.noise_shape_broadcast_dims : NoneType
I0930 03:45:04.838256 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.params_init.method : 'xavier'
I0930 03:45:04.838307 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.params_init.scale : 1.000001
I0930 03:45:04.838357 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.params_init.seed : NoneType
I0930 03:45:04.838415 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.random_seed : NoneType
I0930 03:45:04.838466 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.skip_lp_regularization : NoneType
I0930 03:45:04.838517 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.vn.global_vn : False
I0930 03:45:04.838567 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.vn.per_step_vn : False
I0930 03:45:04.838618 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.vn.scale : NoneType
I0930 03:45:04.838668 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.vn.seed : NoneType
I0930 03:45:04.838719 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.dtype : float32
I0930 03:45:04.838769 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.enc_task_emb : NoneType
I0930 03:45:04.838819 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.fprop_dtype : NoneType
I0930 03:45:04.838870 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.inference_driver_name : NoneType
I0930 03:45:04.838928 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.input_dropout_prob : 0.0
I0930 03:45:04.838978 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.is_eval : NoneType
I0930 03:45:04.839028 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.is_inference : NoneType
I0930 03:45:04.839078 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.is_transparent : False
I0930 03:45:04.839129 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.max_seq_len : 300
I0930 03:45:04.839179 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.name : ''
I0930 03:45:04.839230 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.packed_input : False
I0930 03:45:04.839281 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.params_init.method : 'xavier'
I0930 03:45:04.839331 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.params_init.scale : 1.000001
I0930 03:45:04.839381 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.params_init.seed : NoneType
I0930 03:45:04.839432 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.allow_implicit_capture : NoneType
I0930 03:45:04.839482 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.cls : type/lingvo.core.layers/PositionalEmbeddingLayer
I0930 03:45:04.839533 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.dtype : float32
I0930 03:45:04.839582 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.embedding_dim : 2048
I0930 03:45:04.839633 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.fprop_dtype : NoneType
I0930 03:45:04.839683 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.inference_driver_name : NoneType
I0930 03:45:04.839734 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.is_eval : NoneType
I0930 03:45:04.839784 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.is_inference : NoneType
I0930 03:45:04.839834 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.max_timescale : 10000
I0930 03:45:04.839884 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.min_timescale : 1
I0930 03:45:04.839936 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.name : ''
I0930 03:45:04.839985 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.params_init.method : 'xavier'
I0930 03:45:04.840036 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.params_init.scale : 1.000001
I0930 03:45:04.840086 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.params_init.seed : NoneType
I0930 03:45:04.840136 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.random_seed : NoneType
I0930 03:45:04.840186 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.skip_lp_regularization : NoneType
I0930 03:45:04.840237 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.trainable_scaling : False
I0930 03:45:04.840293 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.trainable_scaling_init : 1.0
I0930 03:45:04.840343 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.vn.global_vn : False
I0930 03:45:04.840394 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.vn.per_step_vn : False
I0930 03:45:04.840444 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.vn.scale : NoneType
I0930 03:45:04.840495 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.vn.seed : NoneType
I0930 03:45:04.840545 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.random_seed : NoneType
I0930 03:45:04.840595 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.skip_lp_regularization : NoneType
I0930 03:45:04.840646 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.allow_implicit_capture : NoneType
I0930 03:45:04.840696 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.apply_pruning : False
I0930 03:45:04.840746 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.cls : type/lingvo.core.layers/SimpleEmbeddingLayer
I0930 03:45:04.840797 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.dtype : float32
I0930 03:45:04.840847 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.embedding_dim : 2048
I0930 03:45:04.840898 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.fprop_dtype : NoneType
I0930 03:45:04.840949 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.fprop_mode : NoneType
I0930 03:45:04.840999 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.inference_driver_name : NoneType
I0930 03:45:04.841050 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.is_eval : NoneType
I0930 03:45:04.841100 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.is_inference : NoneType
I0930 03:45:04.841151 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.name : ''
I0930 03:45:04.841202 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.params_init.method : 'gaussian'
I0930 03:45:04.841252 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.params_init.scale : 0.022097086912079608
I0930 03:45:04.841302 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.params_init.seed : NoneType
I0930 03:45:04.841353 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.qdomain.default : NoneType
I0930 03:45:04.841404 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.random_seed : NoneType
I0930 03:45:04.841454 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.skip_lp_regularization : NoneType
I0930 03:45:04.841505 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.use_3d_weight_tensor : False
I0930 03:45:04.841572 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.use_matmul : False
I0930 03:45:04.841624 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.vn.global_vn : False
I0930 03:45:04.841675 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.vn.per_step_vn : False
I0930 03:45:04.841726 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.vn.scale : NoneType
I0930 03:45:04.841776 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.vn.seed : NoneType
I0930 03:45:04.841827 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.vocab_size : 32000
I0930 03:45:04.841877 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.vn.global_vn : False
I0930 03:45:04.841927 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.vn.per_step_vn : False
I0930 03:45:04.841978 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.vn.scale : NoneType
I0930 03:45:04.842028 139840756766528 base_runner.py:59] task.lm.stack.emb_tpl.vn.seed : NoneType
I0930 03:45:04.842077 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.allow_implicit_capture : NoneType
I0930 03:45:04.842133 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.cls : type/lingvo.core.layers_with_gpipe/GPipeTransformerLayer
I0930 03:45:04.842185 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.dtype : float32
I0930 03:45:04.842234 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.final_enc_layer : False
I0930 03:45:04.842284 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.fprop_dtype : NoneType
I0930 03:45:04.842334 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.has_aux_atten : False
I0930 03:45:04.842385 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.inference_driver_name : NoneType
I0930 03:45:04.842435 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.is_decoder : False
I0930 03:45:04.842488 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.is_eval : NoneType
I0930 03:45:04.842538 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.is_inference : NoneType
I0930 03:45:04.842588 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.is_transparent : False
I0930 03:45:04.842638 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.allow_implicit_capture : NoneType
I0930 03:45:04.842688 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.cls : type/lingvo.core.layers/LayerNorm
I0930 03:45:04.842738 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.dtype : float32
I0930 03:45:04.842788 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.epsilon : 1e-06
I0930 03:45:04.842838 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.fprop_dtype : NoneType
I0930 03:45:04.842889 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.inference_driver_name : NoneType
I0930 03:45:04.842938 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.input_dim : 0
I0930 03:45:04.842988 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.is_eval : NoneType
I0930 03:45:04.843039 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.is_inference : NoneType
I0930 03:45:04.843089 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.name : ''
I0930 03:45:04.843139 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.params_init.method : 'xavier'
I0930 03:45:04.843189 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.params_init.scale : 1.000001
I0930 03:45:04.843239 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.params_init.seed : NoneType
I0930 03:45:04.843289 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.random_seed : NoneType
I0930 03:45:04.843339 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.skip_lp_regularization : NoneType
I0930 03:45:04.843389 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.vn.global_vn : False
I0930 03:45:04.843439 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.vn.per_step_vn : False
I0930 03:45:04.843489 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.vn.scale : NoneType
I0930 03:45:04.843539 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.vn.seed : NoneType
I0930 03:45:04.843589 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.mask_self_atten : True
I0930 03:45:04.843640 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.name : ''
I0930 03:45:04.843690 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.normalize_output : False
I0930 03:45:04.843740 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.output_dim : 0
I0930 03:45:04.843791 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.packed_input : False
I0930 03:45:04.843842 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.params_init.method : 'xavier'
I0930 03:45:04.843892 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.params_init.scale : 1.000001
I0930 03:45:04.843943 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.params_init.seed : NoneType
I0930 03:45:04.843998 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.random_seed : NoneType
I0930 03:45:04.844049 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.skip_lp_regularization : NoneType
I0930 03:45:04.844099 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.source_dim : 2048
I0930 03:45:04.844150 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.add_unnormalized_input : False
I0930 03:45:04.844200 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.allow_implicit_capture : NoneType
I0930 03:45:04.844250 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_dropout_prob : 0.0
I0930 03:45:04.844299 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_hidden_dim : 0
I0930 03:45:04.844349 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.allow_implicit_capture : NoneType
I0930 03:45:04.844399 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.atten_dropout_deterministic : False
I0930 03:45:04.844449 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.atten_dropout_prob : 0.0
I0930 03:45:04.844500 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.cls : type/lingvo.core.attention/MultiHeadedAttention
I0930 03:45:04.844550 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.context_dim : 0
I0930 03:45:04.844600 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.ctx_post_proj_dim : 0
I0930 03:45:04.844650 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.dtype : float32
I0930 03:45:04.844700 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.enable_ctx_post_proj : True
I0930 03:45:04.844750 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.enable_ctx_pre_proj : True
I0930 03:45:04.844799 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.enable_query_proj : True
I0930 03:45:04.844849 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.enable_source_proj : True
I0930 03:45:04.844899 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.fprop_dtype : NoneType
I0930 03:45:04.844949 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.hidden_dim : 0
I0930 03:45:04.844999 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inference_driver_name : NoneType
I0930 03:45:04.845049 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.allow_implicit_capture : NoneType
I0930 03:45:04.845099 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.atten_dropout_deterministic : False
I0930 03:45:04.845149 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.atten_dropout_prob : 0.0
I0930 03:45:04.845199 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.cls : type/lingvo.core.attention/DotProductAttention
I0930 03:45:04.845249 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.dtype : float32
I0930 03:45:04.845299 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.fprop_dtype : NoneType
I0930 03:45:04.845349 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.hidden_dim : 0
I0930 03:45:04.845399 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.inference_driver_name : NoneType
I0930 03:45:04.845449 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.is_eval : NoneType
I0930 03:45:04.845498 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.is_inference : NoneType
I0930 03:45:04.845571 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.name : ''
I0930 03:45:04.845624 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.packed_input : False
I0930 03:45:04.845674 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.params_init.method : 'xavier'
I0930 03:45:04.845724 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.params_init.scale : 1.000001
I0930 03:45:04.845775 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.params_init.seed : NoneType
I0930 03:45:04.845834 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.qdomain.default : NoneType
I0930 03:45:04.845885 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.qdomain.fullyconnected : NoneType
I0930 03:45:04.845935 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.qdomain.softmax : NoneType
I0930 03:45:04.845985 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.query_dim : 0
I0930 03:45:04.846035 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.random_seed : NoneType
I0930 03:45:04.846085 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.skip_lp_regularization : NoneType
I0930 03:45:04.846136 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.source_dim : 0
I0930 03:45:04.846186 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.global_vn : False
I0930 03:45:04.846236 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.per_step_vn : False
I0930 03:45:04.846286 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.scale : NoneType
I0930 03:45:04.846337 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.seed : NoneType
I0930 03:45:04.846387 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.is_eval : NoneType
I0930 03:45:04.846438 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.is_inference : NoneType
I0930 03:45:04.846488 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.name : ''
I0930 03:45:04.846539 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.num_attention_heads : 2
I0930 03:45:04.846590 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.packed_input : False
I0930 03:45:04.846640 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.params_init.method : 'xavier'
I0930 03:45:04.846690 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.params_init.scale : 1.0
I0930 03:45:04.846741 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.params_init.seed : NoneType
I0930 03:45:04.846791 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.qdomain.atten_context : NoneType
I0930 03:45:04.846842 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.qdomain.default : NoneType
I0930 03:45:04.846892 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.qdomain.fullyconnected : NoneType
I0930 03:45:04.846942 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.qdomain.softmax : NoneType
I0930 03:45:04.846993 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.query_dim : 0
I0930 03:45:04.847048 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.random_seed : NoneType
I0930 03:45:04.847099 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.skip_lp_regularization : NoneType
I0930 03:45:04.847150 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.source_dim : 0
I0930 03:45:04.847200 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.use_source_vec_as_attention_value : False
I0930 03:45:04.847251 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.vn.global_vn : False
I0930 03:45:04.847302 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.vn.per_step_vn : False
I0930 03:45:04.847355 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.vn.scale : NoneType
I0930 03:45:04.847406 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.vn.seed : NoneType
I0930 03:45:04.847457 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.cls : type/lingvo.core.layers_with_attention/TransformerAttentionLayer
I0930 03:45:04.847507 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.context_dim : 0
I0930 03:45:04.847557 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.dtype : float32
I0930 03:45:04.847607 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.fprop_dtype : NoneType
I0930 03:45:04.847657 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.inference_driver_name : NoneType
I0930 03:45:04.847707 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.is_eval : NoneType
I0930 03:45:04.847757 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.is_inference : NoneType
I0930 03:45:04.847807 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.is_masked : True
I0930 03:45:04.847857 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.allow_implicit_capture : NoneType
I0930 03:45:04.847907 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.cls : type/lingvo.core.layers/LayerNorm
I0930 03:45:04.847958 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.dtype : float32
I0930 03:45:04.848008 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.epsilon : 1e-06
I0930 03:45:04.848058 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.fprop_dtype : NoneType
I0930 03:45:04.848109 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.inference_driver_name : NoneType
I0930 03:45:04.848159 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.input_dim : 0
I0930 03:45:04.848209 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.is_eval : NoneType
I0930 03:45:04.848258 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.is_inference : NoneType
I0930 03:45:04.848309 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.name : ''
I0930 03:45:04.848359 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.params_init.method : 'xavier'
I0930 03:45:04.848409 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.params_init.scale : 1.000001
I0930 03:45:04.848459 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.params_init.seed : NoneType
I0930 03:45:04.848509 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.random_seed : NoneType
I0930 03:45:04.848560 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.skip_lp_regularization : NoneType
I0930 03:45:04.848610 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.vn.global_vn : False
I0930 03:45:04.848665 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.vn.per_step_vn : False
I0930 03:45:04.848716 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.vn.scale : NoneType
I0930 03:45:04.848767 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.vn.seed : NoneType
I0930 03:45:04.848817 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.mask_type : 'future'
I0930 03:45:04.848867 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.name : ''
I0930 03:45:04.848918 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.num_attention_heads : 16
I0930 03:45:04.848968 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.packed_input : False
I0930 03:45:04.849018 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.params_init.method : 'xavier'
I0930 03:45:04.849069 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.params_init.scale : 1.000001
I0930 03:45:04.849119 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.params_init.seed : NoneType
I0930 03:45:04.849169 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.random_seed : NoneType
I0930 03:45:04.849219 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_prob : 0.0
I0930 03:45:04.849270 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.allow_implicit_capture : NoneType
I0930 03:45:04.849319 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.cls : type/lingvo.core.layers/DropoutLayer
I0930 03:45:04.849370 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.dropout_at_eval : False
I0930 03:45:04.849421 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.dtype : float32
I0930 03:45:04.849471 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.fprop_dtype : NoneType
I0930 03:45:04.849535 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.inference_driver_name : NoneType
I0930 03:45:04.849593 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.is_eval : NoneType
I0930 03:45:04.849643 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.is_inference : NoneType
I0930 03:45:04.849694 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.keep_prob : 1.0
I0930 03:45:04.849744 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.name : ''
I0930 03:45:04.849795 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.noise_shape : NoneType
I0930 03:45:04.849847 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.noise_shape_broadcast_dims : NoneType
I0930 03:45:04.849897 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.params_init.method : 'xavier'
I0930 03:45:04.849948 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.params_init.scale : 1.000001
I0930 03:45:04.849998 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.params_init.seed : NoneType
I0930 03:45:04.850049 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.random_seed : NoneType
I0930 03:45:04.850099 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.skip_lp_regularization : NoneType
I0930 03:45:04.850150 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.global_vn : False
I0930 03:45:04.850206 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.per_step_vn : False
I0930 03:45:04.850257 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.scale : NoneType
I0930 03:45:04.850308 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.seed : NoneType
I0930 03:45:04.850358 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.skip_lp_regularization : NoneType
I0930 03:45:04.850409 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.source_dim : 0
I0930 03:45:04.850459 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.vn.global_vn : False
I0930 03:45:04.850510 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.vn.per_step_vn : False
I0930 03:45:04.850561 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.vn.scale : NoneType
I0930 03:45:04.850611 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.vn.seed : NoneType
I0930 03:45:04.850662 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_aux_atten_tpl : NoneType
I0930 03:45:04.850713 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.activation : 'RELU'
I0930 03:45:04.850764 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.allow_implicit_capture : NoneType
I0930 03:45:04.850814 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.cls : type/lingvo.core.layers_with_attention/TransformerFeedForwardLayer
I0930 03:45:04.850865 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.dtype : float32
I0930 03:45:04.850915 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.activation : ['RELU', 'NONE']
I0930 03:45:04.850966 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.allow_implicit_capture : NoneType
I0930 03:45:04.851016 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.batch_norm : False
I0930 03:45:04.851066 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.bn_fold_weights : NoneType
I0930 03:45:04.851117 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.cls : type/lingvo.core.layers/FeedForwardNet
I0930 03:45:04.851166 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.allow_implicit_capture : NoneType
I0930 03:45:04.851217 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.cls : type/lingvo.core.layers/DropoutLayer
I0930 03:45:04.851267 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.dropout_at_eval : False
I0930 03:45:04.851317 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.dtype : float32
I0930 03:45:04.851367 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.fprop_dtype : NoneType
I0930 03:45:04.851417 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.inference_driver_name : NoneType
I0930 03:45:04.851467 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.is_eval : NoneType
I0930 03:45:04.851518 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.is_inference : NoneType
I0930 03:45:04.851568 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.keep_prob : 1.0
I0930 03:45:04.851618 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.name : ''
I0930 03:45:04.851668 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.noise_shape : NoneType
I0930 03:45:04.851718 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.noise_shape_broadcast_dims : NoneType
I0930 03:45:04.851772 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.params_init.method : 'xavier'
I0930 03:45:04.851823 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.params_init.scale : 1.000001
I0930 03:45:04.851873 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.params_init.seed : NoneType
I0930 03:45:04.851923 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.random_seed : NoneType
I0930 03:45:04.851973 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.skip_lp_regularization : NoneType
I0930 03:45:04.852023 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.global_vn : False
I0930 03:45:04.852073 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.per_step_vn : False
I0930 03:45:04.852123 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.scale : NoneType
I0930 03:45:04.852173 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.seed : NoneType
I0930 03:45:04.852223 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dtype : float32
I0930 03:45:04.852273 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.fprop_dtype : NoneType
I0930 03:45:04.852323 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.inference_driver_name : NoneType
I0930 03:45:04.852374 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.input_dim : 0
I0930 03:45:04.852424 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.is_eval : NoneType
I0930 03:45:04.852474 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.is_inference : NoneType
I0930 03:45:04.852524 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.name : ''
I0930 03:45:04.852575 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.params_init.method : 'xavier'
I0930 03:45:04.852625 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.params_init.scale : 1.000001
I0930 03:45:04.852675 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.params_init.seed : NoneType
I0930 03:45:04.852725 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.activation : 'RELU'
I0930 03:45:04.852775 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.affine_last : False
I0930 03:45:04.852825 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.allow_implicit_capture : NoneType
I0930 03:45:04.852875 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.batch_norm : True
I0930 03:45:04.852925 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.bias_init : 0.0
I0930 03:45:04.852975 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.bn_fold_weights : NoneType
I0930 03:45:04.853026 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.cls : type/lingvo.core.layers/ProjectionLayer
I0930 03:45:04.853076 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.dtype : float32
I0930 03:45:04.853127 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.fprop_dtype : NoneType
I0930 03:45:04.853177 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.has_bias : False
I0930 03:45:04.853232 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.inference_driver_name : NoneType
I0930 03:45:04.853284 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.input_dim : 0
I0930 03:45:04.853335 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.is_eval : NoneType
I0930 03:45:04.853385 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.is_inference : NoneType
I0930 03:45:04.853436 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.name : ''
I0930 03:45:04.853486 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.output_dim : 0
I0930 03:45:04.853552 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.params_init.method : 'xavier'
I0930 03:45:04.853606 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.params_init.scale : 1.000001
I0930 03:45:04.853658 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.params_init.seed : NoneType
I0930 03:45:04.853708 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.qdomain.default : NoneType
I0930 03:45:04.853759 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.random_seed : NoneType
I0930 03:45:04.853810 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.skip_lp_regularization : NoneType
I0930 03:45:04.853861 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.global_vn : False
I0930 03:45:04.853912 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.per_step_vn : False
I0930 03:45:04.853963 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.scale : NoneType
I0930 03:45:04.854014 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.seed : NoneType
I0930 03:45:04.854065 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.weight_norm : False
I0930 03:45:04.854115 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.qdomain.default : NoneType
I0930 03:45:04.854166 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.random_seed : NoneType
I0930 03:45:04.854216 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.skip_connections : NoneType
I0930 03:45:04.854267 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.skip_lp_regularization : NoneType
I0930 03:45:04.854317 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.global_vn : False
I0930 03:45:04.854367 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.per_step_vn : False
I0930 03:45:04.854417 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.scale : NoneType
I0930 03:45:04.854468 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.seed : NoneType
I0930 03:45:04.854518 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.weight_norm : False
I0930 03:45:04.854568 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fprop_dtype : NoneType
I0930 03:45:04.854619 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.hidden_dim : 8192
I0930 03:45:04.854669 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.inference_driver_name : NoneType
I0930 03:45:04.854719 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.input_dim : 0
I0930 03:45:04.854778 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.is_eval : NoneType
I0930 03:45:04.854829 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.is_inference : NoneType
I0930 03:45:04.854880 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.allow_implicit_capture : NoneType
I0930 03:45:04.854930 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.cls : type/lingvo.core.layers/LayerNorm
I0930 03:45:04.854980 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.dtype : float32
I0930 03:45:04.855030 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.epsilon : 1e-06
I0930 03:45:04.855080 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.fprop_dtype : NoneType
I0930 03:45:04.855130 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.inference_driver_name : NoneType
I0930 03:45:04.855181 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.input_dim : 0
I0930 03:45:04.855231 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.is_eval : NoneType
I0930 03:45:04.855282 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.is_inference : NoneType
I0930 03:45:04.855332 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.name : ''
I0930 03:45:04.855383 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.params_init.method : 'xavier'
I0930 03:45:04.855434 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.params_init.scale : 1.000001
I0930 03:45:04.855485 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.params_init.seed : NoneType
I0930 03:45:04.855535 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.random_seed : NoneType
I0930 03:45:04.855586 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.skip_lp_regularization : NoneType
I0930 03:45:04.855637 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.vn.global_vn : False
I0930 03:45:04.855687 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.vn.per_step_vn : False
I0930 03:45:04.855738 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.vn.scale : NoneType
I0930 03:45:04.855788 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.vn.seed : NoneType
I0930 03:45:04.855839 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.name : ''
I0930 03:45:04.855888 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.output_dim : 0
I0930 03:45:04.855939 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.params_init.method : 'xavier'
I0930 03:45:04.855989 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.params_init.scale : 1.000001
I0930 03:45:04.856039 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.params_init.seed : NoneType
I0930 03:45:04.856089 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.random_seed : NoneType
I0930 03:45:04.856139 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.relu_dropout_prob : 0.0
I0930 03:45:04.856190 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.activation : 'RELU'
I0930 03:45:04.856241 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.affine_last : False
I0930 03:45:04.856291 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.allow_implicit_capture : NoneType
I0930 03:45:04.856342 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.batch_norm : True
I0930 03:45:04.856397 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.bias_init : 0.0
I0930 03:45:04.856448 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.bn_fold_weights : NoneType
I0930 03:45:04.856499 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.cls : type/lingvo.core.layers/ProjectionLayer
I0930 03:45:04.856550 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.dtype : float32
I0930 03:45:04.856600 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.fprop_dtype : NoneType
I0930 03:45:04.856651 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.has_bias : False
I0930 03:45:04.856701 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.inference_driver_name : NoneType
I0930 03:45:04.856752 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.input_dim : 0
I0930 03:45:04.856803 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.is_eval : NoneType
I0930 03:45:04.856853 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.is_inference : NoneType
I0930 03:45:04.856903 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.name : ''
I0930 03:45:04.856954 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.output_dim : 0
I0930 03:45:04.857003 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.params_init.method : 'xavier'
I0930 03:45:04.857053 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.params_init.scale : 1.000001
I0930 03:45:04.857103 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.params_init.seed : NoneType
I0930 03:45:04.857154 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.qdomain.default : NoneType
I0930 03:45:04.857204 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.random_seed : NoneType
I0930 03:45:04.857255 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.skip_lp_regularization : NoneType
I0930 03:45:04.857305 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.global_vn : False
I0930 03:45:04.857355 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.per_step_vn : False
I0930 03:45:04.857408 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.scale : NoneType
I0930 03:45:04.857459 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.seed : NoneType
I0930 03:45:04.857510 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.weight_norm : False
I0930 03:45:04.857577 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_prob : 0.0
I0930 03:45:04.857628 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.allow_implicit_capture : NoneType
I0930 03:45:04.857679 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.cls : type/lingvo.core.layers/DropoutLayer
I0930 03:45:04.857730 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.dropout_at_eval : False
I0930 03:45:04.857780 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.dtype : float32
I0930 03:45:04.857831 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.fprop_dtype : NoneType
I0930 03:45:04.857881 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.inference_driver_name : NoneType
I0930 03:45:04.857936 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.is_eval : NoneType
I0930 03:45:04.857988 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.is_inference : NoneType
I0930 03:45:04.858039 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.keep_prob : 1.0
I0930 03:45:04.858089 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.name : ''
I0930 03:45:04.858139 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.noise_shape : NoneType
I0930 03:45:04.858190 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.noise_shape_broadcast_dims : NoneType
I0930 03:45:04.858240 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.params_init.method : 'xavier'
I0930 03:45:04.858291 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.params_init.scale : 1.000001
I0930 03:45:04.858341 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.params_init.seed : NoneType
I0930 03:45:04.858391 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.random_seed : NoneType
I0930 03:45:04.858441 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.skip_lp_regularization : NoneType
I0930 03:45:04.858491 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.global_vn : False
I0930 03:45:04.858541 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.per_step_vn : False
I0930 03:45:04.858591 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.scale : NoneType
I0930 03:45:04.858641 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.seed : NoneType
I0930 03:45:04.858692 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.skip_lp_regularization : NoneType
I0930 03:45:04.858743 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.vn.global_vn : False
I0930 03:45:04.858794 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.vn.per_step_vn : False
I0930 03:45:04.858845 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.vn.scale : NoneType
I0930 03:45:04.858895 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.vn.seed : NoneType
I0930 03:45:04.858945 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.transparent_merger_tpl : NoneType
I0930 03:45:04.858995 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.vn.global_vn : False
I0930 03:45:04.859046 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.vn.per_step_vn : False
I0930 03:45:04.859096 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.vn.scale : NoneType
I0930 03:45:04.859147 139840756766528 base_runner.py:59] task.lm.stack.encoder_tpl.vn.seed : NoneType
I0930 03:45:04.859197 139840756766528 base_runner.py:59] task.lm.stack.fprop_dtype : NoneType
I0930 03:45:04.859248 139840756766528 base_runner.py:59] task.lm.stack.inference_driver_name : NoneType
I0930 03:45:04.859299 139840756766528 base_runner.py:59] task.lm.stack.is_eval : NoneType
I0930 03:45:04.859350 139840756766528 base_runner.py:59] task.lm.stack.is_inference : NoneType
I0930 03:45:04.859401 139840756766528 base_runner.py:59] task.lm.stack.is_transparent : False
I0930 03:45:04.859452 139840756766528 base_runner.py:59] task.lm.stack.label_smoothing : NoneType
I0930 03:45:04.859503 139840756766528 base_runner.py:59] task.lm.stack.model_dim : 2048
I0930 03:45:04.859553 139840756766528 base_runner.py:59] task.lm.stack.name : ''
I0930 03:45:04.859608 139840756766528 base_runner.py:59] task.lm.stack.normalize_encoder : False
I0930 03:45:04.859659 139840756766528 base_runner.py:59] task.lm.stack.num_decoder_layers : 0
I0930 03:45:04.859709 139840756766528 base_runner.py:59] task.lm.stack.num_encoder_layers : 32
I0930 03:45:04.859760 139840756766528 base_runner.py:59] task.lm.stack.num_micro_batches : 32
I0930 03:45:04.859810 139840756766528 base_runner.py:59] task.lm.stack.packed_input : False
I0930 03:45:04.859860 139840756766528 base_runner.py:59] task.lm.stack.params_init.method : 'xavier'
I0930 03:45:04.859911 139840756766528 base_runner.py:59] task.lm.stack.params_init.scale : 1.000001
I0930 03:45:04.859961 139840756766528 base_runner.py:59] task.lm.stack.params_init.seed : NoneType
I0930 03:45:04.860011 139840756766528 base_runner.py:59] task.lm.stack.random_seed : NoneType
I0930 03:45:04.860061 139840756766528 base_runner.py:59] task.lm.stack.skip_lp_regularization : NoneType
I0930 03:45:04.860111 139840756766528 base_runner.py:59] task.lm.stack.softmax_tpl.allow_implicit_capture : NoneType
I0930 03:45:04.860161 139840756766528 base_runner.py:59] task.lm.stack.softmax_tpl.apply_pruning : False
I0930 03:45:04.860212 139840756766528 base_runner.py:59] task.lm.stack.softmax_tpl.chunk_size : 4194
I0930 03:45:04.860262 139840756766528 base_runner.py:59] task.lm.stack.softmax_tpl.cls : type/lingvo.core.layers_with_gpipe/GPipeTransformerSoftmaxLayer
I0930 03:45:04.860312 139840756766528 base_runner.py:59] task.lm.stack.softmax_tpl.dtype : float32
I0930 03:45:04.860362 139840756766528 base_runner.py:59] task.lm.stack.softmax_tpl.fprop_dtype : NoneType
I0930 03:45:04.860412 139840756766528 base_runner.py:59] task.lm.stack.softmax_tpl.inference_driver_name : NoneType
I0930 03:45:04.860462 139840756766528 base_runner.py:59] task.lm.stack.softmax_tpl.input_dim : 2048
I0930 03:45:04.860512 139840756766528 base_runner.py:59] task.lm.stack.softmax_tpl.inputs_from_decoder : False
I0930 03:45:04.860563 139840756766528 base_runner.py:59] task.lm.stack.softmax_tpl.is_eval : NoneType
I0930 03:45:04.860613 139840756766528 base_runner.py:59] task.lm.stack.softmax_tpl.is_inference : NoneType
I0930 03:45:04.860662 139840756766528 base_runner.py:59] task.lm.stack.softmax_tpl.logits_abs_max : NoneType
I0930 03:45:04.860713 139840756766528 base_runner.py:59] task.lm.stack.softmax_tpl.name : ''
I0930 03:45:04.860763 139840756766528 base_runner.py:59] task.lm.stack.softmax_tpl.num_classes : 32000
I0930 03:45:04.860813 139840756766528 base_runner.py:59] task.lm.stack.softmax_tpl.num_sampled : 0
I0930 03:45:04.860863 139840756766528 base_runner.py:59] task.lm.stack.softmax_tpl.num_shards : 16
I0930 03:45:04.860912 139840756766528 base_runner.py:59] task.lm.stack.softmax_tpl.params_init.method : 'xavier'
I0930 03:45:04.860962 139840756766528 base_runner.py:59] task.lm.stack.softmax_tpl.params_init.scale : 1.000001
I0930 03:45:04.861012 139840756766528 base_runner.py:59] task.lm.stack.softmax_tpl.params_init.seed : NoneType
I0930 03:45:04.861062 139840756766528 base_runner.py:59] task.lm.stack.softmax_tpl.qdomain.default : NoneType
I0930 03:45:04.861111 139840756766528 base_runner.py:59] task.lm.stack.softmax_tpl.random_seed : NoneType
I0930 03:45:04.861161 139840756766528 base_runner.py:59] task.lm.stack.softmax_tpl.skip_lp_regularization : NoneType
I0930 03:45:04.861211 139840756766528 base_runner.py:59] task.lm.stack.softmax_tpl.vn.global_vn : False
I0930 03:45:04.861261 139840756766528 base_runner.py:59] task.lm.stack.softmax_tpl.vn.per_step_vn : False
I0930 03:45:04.861311 139840756766528 base_runner.py:59] task.lm.stack.softmax_tpl.vn.scale : NoneType
I0930 03:45:04.861361 139840756766528 base_runner.py:59] task.lm.stack.softmax_tpl.vn.seed : NoneType
I0930 03:45:04.861411 139840756766528 base_runner.py:59] task.lm.stack.splits : [8, 16, 24, 32]
I0930 03:45:04.861462 139840756766528 base_runner.py:59] task.lm.stack.state_dtype : float32
I0930 03:45:04.861512 139840756766528 base_runner.py:59] task.lm.stack.transparent_merger_dropout_prob : 0.1
I0930 03:45:04.861584 139840756766528 base_runner.py:59] task.lm.stack.transparent_merger_tpl.allow_implicit_capture : NoneType
I0930 03:45:04.861641 139840756766528 base_runner.py:59] task.lm.stack.transparent_merger_tpl.cls : type/lingvo.core.layers_with_gpipe/DeterministicWeightsLayer
I0930 03:45:04.861692 139840756766528 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.allow_implicit_capture : NoneType
I0930 03:45:04.861743 139840756766528 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.cls : type/lingvo.core.layers/DeterministicDropoutLayer
I0930 03:45:04.861793 139840756766528 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.dropout_at_eval : False
I0930 03:45:04.861844 139840756766528 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.dtype : float32
I0930 03:45:04.861894 139840756766528 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.fprop_dtype : NoneType
I0930 03:45:04.861945 139840756766528 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.inference_driver_name : NoneType
I0930 03:45:04.861996 139840756766528 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.is_eval : NoneType
I0930 03:45:04.862047 139840756766528 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.is_inference : NoneType
I0930 03:45:04.862097 139840756766528 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.keep_prob : 1.0
I0930 03:45:04.862147 139840756766528 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.name : ''
I0930 03:45:04.862197 139840756766528 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.noise_shape : NoneType
I0930 03:45:04.862247 139840756766528 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.noise_shape_broadcast_dims : NoneType
I0930 03:45:04.862298 139840756766528 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.params_init.method : 'xavier'
I0930 03:45:04.862348 139840756766528 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.params_init.scale : 1.000001
I0930 03:45:04.862402 139840756766528 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.params_init.seed : NoneType
I0930 03:45:04.862452 139840756766528 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.random_seed : NoneType
I0930 03:45:04.862503 139840756766528 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.skip_lp_regularization : NoneType
I0930 03:45:04.862553 139840756766528 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.vn.global_vn : False
I0930 03:45:04.862603 139840756766528 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.vn.per_step_vn : False
I0930 03:45:04.862653 139840756766528 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.vn.scale : NoneType
I0930 03:45:04.862703 139840756766528 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.vn.seed : NoneType
I0930 03:45:04.862753 139840756766528 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dtype : float32
I0930 03:45:04.862803 139840756766528 base_runner.py:59] task.lm.stack.transparent_merger_tpl.fprop_dtype : NoneType
I0930 03:45:04.862854 139840756766528 base_runner.py:59] task.lm.stack.transparent_merger_tpl.global_weight_scale : 1.0
I0930 03:45:04.862904 139840756766528 base_runner.py:59] task.lm.stack.transparent_merger_tpl.inference_driver_name : NoneType
I0930 03:45:04.862954 139840756766528 base_runner.py:59] task.lm.stack.transparent_merger_tpl.is_eval : NoneType
I0930 03:45:04.863004 139840756766528 base_runner.py:59] task.lm.stack.transparent_merger_tpl.is_inference : NoneType
I0930 03:45:04.863054 139840756766528 base_runner.py:59] task.lm.stack.transparent_merger_tpl.minimal_prob : 0.0
I0930 03:45:04.863104 139840756766528 base_runner.py:59] task.lm.stack.transparent_merger_tpl.name : ''
I0930 03:45:04.863155 139840756766528 base_runner.py:59] task.lm.stack.transparent_merger_tpl.num_sources : 0
I0930 03:45:04.863206 139840756766528 base_runner.py:59] task.lm.stack.transparent_merger_tpl.params_init.method : 'xavier'
I0930 03:45:04.863261 139840756766528 base_runner.py:59] task.lm.stack.transparent_merger_tpl.params_init.scale : 1.000001
I0930 03:45:04.863312 139840756766528 base_runner.py:59] task.lm.stack.transparent_merger_tpl.params_init.seed : NoneType
I0930 03:45:04.863363 139840756766528 base_runner.py:59] task.lm.stack.transparent_merger_tpl.random_seed : NoneType
I0930 03:45:04.863414 139840756766528 base_runner.py:59] task.lm.stack.transparent_merger_tpl.skip_lp_regularization : NoneType
I0930 03:45:04.863466 139840756766528 base_runner.py:59] task.lm.stack.transparent_merger_tpl.vn.global_vn : False
I0930 03:45:04.863517 139840756766528 base_runner.py:59] task.lm.stack.transparent_merger_tpl.vn.per_step_vn : False
I0930 03:45:04.863567 139840756766528 base_runner.py:59] task.lm.stack.transparent_merger_tpl.vn.scale : NoneType
I0930 03:45:04.863618 139840756766528 base_runner.py:59] task.lm.stack.transparent_merger_tpl.vn.seed : NoneType
I0930 03:45:04.863669 139840756766528 base_runner.py:59] task.lm.stack.transparent_merger_tpl.weighted_merger_dropout_prob : 0.0
I0930 03:45:04.863720 139840756766528 base_runner.py:59] task.lm.stack.transparent_merger_tpl.weighted_merger_softmax : True
I0930 03:45:04.863770 139840756766528 base_runner.py:59] task.lm.stack.use_pipelined_embeddings : True
I0930 03:45:04.863821 139840756766528 base_runner.py:59] task.lm.stack.vn.global_vn : False
I0930 03:45:04.863872 139840756766528 base_runner.py:59] task.lm.stack.vn.per_step_vn : False
I0930 03:45:04.863922 139840756766528 base_runner.py:59] task.lm.stack.vn.scale : NoneType
I0930 03:45:04.863973 139840756766528 base_runner.py:59] task.lm.stack.vn.seed : NoneType
I0930 03:45:04.864024 139840756766528 base_runner.py:59] task.lm.vn.global_vn : False
I0930 03:45:04.864075 139840756766528 base_runner.py:59] task.lm.vn.per_step_vn : False
I0930 03:45:04.864125 139840756766528 base_runner.py:59] task.lm.vn.scale : NoneType
I0930 03:45:04.864176 139840756766528 base_runner.py:59] task.lm.vn.seed : NoneType
I0930 03:45:04.864227 139840756766528 base_runner.py:59] task.lm.vocab_size : 32000
I0930 03:45:04.864277 139840756766528 base_runner.py:59] task.name : '1bwds_wpm_level_lm'
I0930 03:45:04.864328 139840756766528 base_runner.py:59] task.online_encoder : NoneType
I0930 03:45:04.864379 139840756766528 base_runner.py:59] task.params_init.method : 'xavier'
I0930 03:45:04.864430 139840756766528 base_runner.py:59] task.params_init.scale : 1.000001
I0930 03:45:04.864480 139840756766528 base_runner.py:59] task.params_init.seed : NoneType
I0930 03:45:04.864531 139840756766528 base_runner.py:59] task.random_seed : NoneType
I0930 03:45:04.864583 139840756766528 base_runner.py:59] task.skip_lp_regularization : NoneType
I0930 03:45:04.864633 139840756766528 base_runner.py:59] task.train.bprop_variable_exclusion : NoneType
I0930 03:45:04.864684 139840756766528 base_runner.py:59] task.train.bprop_variable_filter : NoneType
I0930 03:45:04.864735 139840756766528 base_runner.py:59] task.train.clip_gradient_norm_to_value : 0.0
I0930 03:45:04.864785 139840756766528 base_runner.py:59] task.train.clip_gradient_single_norm_to_value : 0.0
I0930 03:45:04.864835 139840756766528 base_runner.py:59] task.train.colocate_gradients_with_ops : True
I0930 03:45:04.864885 139840756766528 base_runner.py:59] task.train.early_stop.metric_history.jobname : 'eval_dev'
I0930 03:45:04.864935 139840756766528 base_runner.py:59] task.train.early_stop.metric_history.local_filesystem : False
I0930 03:45:04.864986 139840756766528 base_runner.py:59] task.train.early_stop.metric_history.logdir : ''
I0930 03:45:04.865036 139840756766528 base_runner.py:59] task.train.early_stop.metric_history.metric : 'log_pplx'
I0930 03:45:04.865087 139840756766528 base_runner.py:59] task.train.early_stop.metric_history.minimize : True
I0930 03:45:04.865137 139840756766528 base_runner.py:59] task.train.early_stop.metric_history.name : 'MetricHistory'
I0930 03:45:04.865188 139840756766528 base_runner.py:59] task.train.early_stop.metric_history.tfevent_file : False
I0930 03:45:04.865245 139840756766528 base_runner.py:59] task.train.early_stop.min_steps : 0
I0930 03:45:04.865297 139840756766528 base_runner.py:59] task.train.early_stop.name : 'EarlyStop'
I0930 03:45:04.865347 139840756766528 base_runner.py:59] task.train.early_stop.tolerance : 0.0
I0930 03:45:04.865397 139840756766528 base_runner.py:59] task.train.early_stop.verbose : True
I0930 03:45:04.865448 139840756766528 base_runner.py:59] task.train.early_stop.window : 0
I0930 03:45:04.865498 139840756766528 base_runner.py:59] task.train.ema_decay : 0.0
I0930 03:45:04.865565 139840756766528 base_runner.py:59] task.train.enqueue_max_steps : -1
I0930 03:45:04.865617 139840756766528 base_runner.py:59] task.train.gate_gradients : False
I0930 03:45:04.865669 139840756766528 base_runner.py:59] task.train.grad_aggregation_method : 1
I0930 03:45:04.865719 139840756766528 base_runner.py:59] task.train.grad_norm_to_clip_to_zero : 0.0
I0930 03:45:04.865769 139840756766528 base_runner.py:59] task.train.grad_norm_tracker : NoneType
I0930 03:45:04.865819 139840756766528 base_runner.py:59] task.train.init_from_checkpoint_rules : {}
I0930 03:45:04.865870 139840756766528 base_runner.py:59] task.train.l1_regularizer_weight : NoneType
I0930 03:45:04.865920 139840756766528 base_runner.py:59] task.train.l2_regularizer_weight : 1e-06
I0930 03:45:04.865970 139840756766528 base_runner.py:59] task.train.learner : NoneType
I0930 03:45:04.866020 139840756766528 base_runner.py:59] task.train.learning_rate : 0.5
I0930 03:45:04.866070 139840756766528 base_runner.py:59] task.train.lr_schedule.allow_implicit_capture : NoneType
I0930 03:45:04.866121 139840756766528 base_runner.py:59] task.train.lr_schedule.cls : type/lingvo.core.schedule/TransformerLearningRateSchedule
I0930 03:45:04.866170 139840756766528 base_runner.py:59] task.train.lr_schedule.decay_end : NoneType
I0930 03:45:04.866220 139840756766528 base_runner.py:59] task.train.lr_schedule.dtype : float32
I0930 03:45:04.866270 139840756766528 base_runner.py:59] task.train.lr_schedule.fprop_dtype : NoneType
I0930 03:45:04.866320 139840756766528 base_runner.py:59] task.train.lr_schedule.inference_driver_name : NoneType
I0930 03:45:04.866369 139840756766528 base_runner.py:59] task.train.lr_schedule.is_eval : NoneType
I0930 03:45:04.866419 139840756766528 base_runner.py:59] task.train.lr_schedule.is_inference : NoneType
I0930 03:45:04.866469 139840756766528 base_runner.py:59] task.train.lr_schedule.model_dim : 2048
I0930 03:45:04.866520 139840756766528 base_runner.py:59] task.train.lr_schedule.name : 'LRSched'
I0930 03:45:04.866570 139840756766528 base_runner.py:59] task.train.lr_schedule.params_init.method : 'xavier'
I0930 03:45:04.866620 139840756766528 base_runner.py:59] task.train.lr_schedule.params_init.scale : 1.000001
I0930 03:45:04.866671 139840756766528 base_runner.py:59] task.train.lr_schedule.params_init.seed : NoneType
I0930 03:45:04.866721 139840756766528 base_runner.py:59] task.train.lr_schedule.random_seed : NoneType
I0930 03:45:04.866771 139840756766528 base_runner.py:59] task.train.lr_schedule.skip_lp_regularization : NoneType
I0930 03:45:04.866822 139840756766528 base_runner.py:59] task.train.lr_schedule.vn.global_vn : False
I0930 03:45:04.866882 139840756766528 base_runner.py:59] task.train.lr_schedule.vn.per_step_vn : False
I0930 03:45:04.866933 139840756766528 base_runner.py:59] task.train.lr_schedule.vn.scale : NoneType
I0930 03:45:04.866983 139840756766528 base_runner.py:59] task.train.lr_schedule.vn.seed : NoneType
I0930 03:45:04.867033 139840756766528 base_runner.py:59] task.train.lr_schedule.warmup_steps : 40000
I0930 03:45:04.867084 139840756766528 base_runner.py:59] task.train.lr_schedule.worker_replicas : 1
I0930 03:45:04.867134 139840756766528 base_runner.py:59] task.train.max_lstm_gradient_norm : 0.0
I0930 03:45:04.867184 139840756766528 base_runner.py:59] task.train.max_steps : 4000000
I0930 03:45:04.867235 139840756766528 base_runner.py:59] task.train.optimizer.allow_implicit_capture : NoneType
I0930 03:45:04.867285 139840756766528 base_runner.py:59] task.train.optimizer.beta1 : 0.9
I0930 03:45:04.867341 139840756766528 base_runner.py:59] task.train.optimizer.beta2 : 0.997
I0930 03:45:04.867393 139840756766528 base_runner.py:59] task.train.optimizer.cls : type/lingvo.core.optimizer/Adam
I0930 03:45:04.867443 139840756766528 base_runner.py:59] task.train.optimizer.dtype : float32
I0930 03:45:04.867496 139840756766528 base_runner.py:59] task.train.optimizer.epsilon : 1e-09
I0930 03:45:04.867546 139840756766528 base_runner.py:59] task.train.optimizer.fprop_dtype : NoneType
I0930 03:45:04.867597 139840756766528 base_runner.py:59] task.train.optimizer.inference_driver_name : NoneType
I0930 03:45:04.867647 139840756766528 base_runner.py:59] task.train.optimizer.is_eval : NoneType
I0930 03:45:04.867697 139840756766528 base_runner.py:59] task.train.optimizer.is_inference : NoneType
I0930 03:45:04.867747 139840756766528 base_runner.py:59] task.train.optimizer.name : 'Adam'
I0930 03:45:04.867798 139840756766528 base_runner.py:59] task.train.optimizer.params_init.method : 'xavier'
I0930 03:45:04.867847 139840756766528 base_runner.py:59] task.train.optimizer.params_init.scale : 1.000001
I0930 03:45:04.867897 139840756766528 base_runner.py:59] task.train.optimizer.params_init.seed : NoneType
I0930 03:45:04.867948 139840756766528 base_runner.py:59] task.train.optimizer.random_seed : NoneType
I0930 03:45:04.867999 139840756766528 base_runner.py:59] task.train.optimizer.skip_lp_regularization : NoneType
I0930 03:45:04.868048 139840756766528 base_runner.py:59] task.train.optimizer.vn.global_vn : False
I0930 03:45:04.868098 139840756766528 base_runner.py:59] task.train.optimizer.vn.per_step_vn : False
I0930 03:45:04.868149 139840756766528 base_runner.py:59] task.train.optimizer.vn.scale : NoneType
I0930 03:45:04.868198 139840756766528 base_runner.py:59] task.train.optimizer.vn.seed : NoneType
I0930 03:45:04.868251 139840756766528 base_runner.py:59] task.train.pruning_hparams_dict : NoneType
I0930 03:45:04.868310 139840756766528 base_runner.py:59] task.train.save_interval_seconds : 600
I0930 03:45:04.868361 139840756766528 base_runner.py:59] task.train.save_keep_checkpoint_every_n_hours : 0.5
I0930 03:45:04.868412 139840756766528 base_runner.py:59] task.train.save_max_to_keep : 100
I0930 03:45:04.868462 139840756766528 base_runner.py:59] task.train.start_up_delay_steps : 200
I0930 03:45:04.868511 139840756766528 base_runner.py:59] task.train.sum_loss_across_tokens_in_batch : False
I0930 03:45:04.868561 139840756766528 base_runner.py:59] task.train.summary_interval_steps : 100
I0930 03:45:04.868612 139840756766528 base_runner.py:59] task.train.tpu_steps_per_loop : 100
I0930 03:45:04.868662 139840756766528 base_runner.py:59] task.train.vn_start_step : 20000
I0930 03:45:04.868711 139840756766528 base_runner.py:59] task.train.vn_std : 0.0
I0930 03:45:04.868761 139840756766528 base_runner.py:59] task.vn.global_vn : False
I0930 03:45:04.868810 139840756766528 base_runner.py:59] task.vn.per_step_vn : False
I0930 03:45:04.868861 139840756766528 base_runner.py:59] task.vn.scale : NoneType
I0930 03:45:04.868911 139840756766528 base_runner.py:59] task.vn.seed : NoneType
I0930 03:45:04.868961 139840756766528 base_runner.py:59] train.early_stop.metric_history.jobname : 'eval_dev'
I0930 03:45:04.869011 139840756766528 base_runner.py:59] train.early_stop.metric_history.local_filesystem : False
I0930 03:45:04.869061 139840756766528 base_runner.py:59] train.early_stop.metric_history.logdir : ''
I0930 03:45:04.869111 139840756766528 base_runner.py:59] train.early_stop.metric_history.metric : 'log_pplx'
I0930 03:45:04.869161 139840756766528 base_runner.py:59] train.early_stop.metric_history.minimize : True
I0930 03:45:04.869212 139840756766528 base_runner.py:59] train.early_stop.metric_history.name : 'MetricHistory'
I0930 03:45:04.869262 139840756766528 base_runner.py:59] train.early_stop.metric_history.tfevent_file : False
I0930 03:45:04.869312 139840756766528 base_runner.py:59] train.early_stop.min_steps : 0
I0930 03:45:04.869362 139840756766528 base_runner.py:59] train.early_stop.name : 'EarlyStop'
I0930 03:45:04.869417 139840756766528 base_runner.py:59] train.early_stop.tolerance : 0.0
I0930 03:45:04.869468 139840756766528 base_runner.py:59] train.early_stop.verbose : True
I0930 03:45:04.869542 139840756766528 base_runner.py:59] train.early_stop.window : 0
I0930 03:45:04.869598 139840756766528 base_runner.py:59] train.ema_decay : 0.0
I0930 03:45:04.869650 139840756766528 base_runner.py:59] train.enqueue_max_steps : -1
I0930 03:45:04.869700 139840756766528 base_runner.py:59] train.init_from_checkpoint_rules : {}
I0930 03:45:04.869751 139840756766528 base_runner.py:59] train.max_steps : 4000000
I0930 03:45:04.869801 139840756766528 base_runner.py:59] train.save_interval_seconds : 600
I0930 03:45:04.869852 139840756766528 base_runner.py:59] train.save_keep_checkpoint_every_n_hours : 0.5
I0930 03:45:04.869903 139840756766528 base_runner.py:59] train.save_max_to_keep : 100
I0930 03:45:04.869953 139840756766528 base_runner.py:59] train.start_up_delay_steps : 200
I0930 03:45:04.870003 139840756766528 base_runner.py:59] train.summary_interval_steps : 100
I0930 03:45:04.870054 139840756766528 base_runner.py:59] train.tpu_steps_per_loop : 100
I0930 03:45:04.870104 139840756766528 base_runner.py:59] vn.global_vn : False
I0930 03:45:04.870154 139840756766528 base_runner.py:59] vn.per_step_vn : False
I0930 03:45:04.870204 139840756766528 base_runner.py:59] vn.scale : NoneType
I0930 03:45:04.870254 139840756766528 base_runner.py:59] vn.seed : NoneType
I0930 03:45:04.870304 139840756766528 base_runner.py:59] 
I0930 03:45:04.870443 139840756766528 base_runner.py:60] ============================================================
I0930 03:45:04.873968 139840756766528 base_runner.py:106] Starting ...
I0930 03:45:04.875784 139840756766528 cluster.py:497] _LeastLoadedPlacer : ['/job:local/replica:0/task:0/device:CPU:0']
I0930 03:45:04.889725 139840756766528 cluster.py:515] Place variable global_step on /job:local/replica:0/task:0/device:CPU:0 8
I0930 03:45:04.903633 139840756766528 base_model.py:1093] Training parameters for <class 'lingvo.core.base_model.SingleTaskModel'>: {
  early_stop: {
    metric_history: {
"eval_dev"
      local_filesystem: False
"/tmp/mnist/log"
"log_pplx"
      minimize: True
"MetricHistory"
      tfevent_file: False
    }
    min_steps: 0
"EarlyStop"
    tolerance: 0.0
    verbose: True
    window: 0
  }
  ema_decay: 0.0
  enqueue_max_steps: -1
  init_from_checkpoint_rules: {}
  max_steps: 4000000
  save_interval_seconds: 600
  save_keep_checkpoint_every_n_hours: 0.5
  save_max_to_keep: 100
  start_up_delay_steps: 200
  summary_interval_steps: 100
  tpu_steps_per_loop: 100
}
I0930 03:45:04.919623 139840756766528 base_model.py:301] input_params: {
  allow_implicit_capture: None
  bucket_adjust_every_n: 0
  bucket_batch_limit: [32]
  bucket_upper_bound: [1024]
  cls: <class 'lingvo.tasks.lm.input_generator.LmInput'>
  dtype: <dtype: 'float32'>
  file_buffer_size: 10000000
  file_datasource: None
  file_parallelism: 10
"text:/tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en*"
  file_random_seed: 301
  fixed_input_shape: True
  flush_every_n: 0
  fprop_dtype: None
  inference_driver_name: None
  is_eval: None
  is_inference: None
"1bwds_train_set"
  num_batcher_threads: 16
  num_samples: 0
  pad_to_max_seq_length: False
  params_init: {
"xavier"
    scale: 1.000001
    seed: None
  }
  random_seed: None
  remote: {
    max_inflights_per_target: 32
    shardable_batch: False
  }
  require_sequential_order: False
  skip_lp_regularization: None
  source_max_length: None
  target_max_length: 1024
  tokenizer: {
    allow_implicit_capture: None
    append_eos: True
    cls: <class 'lingvo.core.tokenizers.AsciiTokenizer'>
    dtype: <dtype: 'float32'>
    fprop_dtype: None
    inference_driver_name: None
    is_eval: None
    is_inference: None
"tokenizer"
    pad_to_max_length: True
    params_init: {
"xavier"
      scale: 1.000001
      seed: None
    }
    random_seed: None
    skip_lp_regularization: None
    target_eos_id: 2
    target_sos_id: 1
    target_unk_id: 0
    vn: {
      global_vn: False
      per_step_vn: False
      scale: None
      seed: None
    }
    vocab_size: 32000
  }
  tokenizer_dict: {}
  tpu_infeed_parallelism: 1
  use_chaining: False
  use_per_host_infeed: False
  use_within_batch_mixing: False
  vn: {
    global_vn: False
    per_step_vn: False
    scale: None
    seed: None
  }
}
I0930 03:45:04.923642 139840756766528 base_input_generator.py:624] bucket_batch_limit [128]
I0930 03:45:04.987297 139840756766528 learner.py:351] Ignoring legacy param start_up_delay_steps=200 for optimization program
I0930 03:45:04.987402 139840756766528 learner.py:351] Ignoring legacy param max_steps=4000000 for optimization program
I0930 03:45:04.987469 139840756766528 learner.py:351] Ignoring legacy param tpu_steps_per_loop=100 for optimization program
I0930 03:45:04.987526 139840756766528 learner.py:351] Ignoring legacy param vn_start_step=20000 for optimization program
I0930 03:45:04.987579 139840756766528 learner.py:351] Ignoring legacy param vn_std=0.0 for optimization program
I0930 03:45:04.987632 139840756766528 learner.py:351] Ignoring legacy param early_stop={
  metric_history: {
"eval_dev"
    local_filesystem: False
"/tmp/mnist/log"
"log_pplx"
    minimize: True
"MetricHistory"
    tfevent_file: False
  }
  min_steps: 0
"EarlyStop"
  tolerance: 0.0
  verbose: True
  window: 0
} for optimization program
I0930 03:45:04.987739 139840756766528 learner.py:351] Ignoring legacy param ema_decay=0.0 for optimization program
I0930 03:45:04.987796 139840756766528 learner.py:351] Ignoring legacy param init_from_checkpoint_rules={} for optimization program
I0930 03:45:04.987849 139840756766528 learner.py:351] Ignoring legacy param pruning_hparams_dict=None for optimization program
I0930 03:45:04.987899 139840756766528 learner.py:351] Ignoring legacy param enqueue_max_steps=-1 for optimization program
I0930 03:45:04.987948 139840756766528 learner.py:351] Ignoring legacy param save_interval_seconds=600 for optimization program
I0930 03:45:04.987997 139840756766528 learner.py:351] Ignoring legacy param save_max_to_keep=100 for optimization program
I0930 03:45:04.988046 139840756766528 learner.py:351] Ignoring legacy param save_keep_checkpoint_every_n_hours=0.5 for optimization program
I0930 03:45:04.988097 139840756766528 learner.py:351] Ignoring legacy param summary_interval_steps=100 for optimization program
I0930 03:45:04.988147 139840756766528 learner.py:351] Ignoring legacy param learner=None for optimization program
I0930 03:45:04.988228 139840756766528 learner.py:351] Ignoring legacy param max_lstm_gradient_norm=0.0 for optimization program
I0930 03:45:04.988281 139840756766528 learner.py:351] Ignoring legacy param sum_loss_across_tokens_in_batch=False for optimization program
I0930 03:45:04.988726 139840756766528 learner.py:356] Learner params: allow_implicit_capture : NoneType
I0930 03:45:04.988808 139840756766528 learner.py:356] Learner params: bprop_variable_exclusion : NoneType
I0930 03:45:04.988871 139840756766528 learner.py:356] Learner params: bprop_variable_filter : NoneType
I0930 03:45:04.988927 139840756766528 learner.py:356] Learner params: clip_gradient_norm_to_value : 0.0
I0930 03:45:04.988980 139840756766528 learner.py:356] Learner params: clip_gradient_single_norm_to_value : 0.0
I0930 03:45:04.989032 139840756766528 learner.py:356] Learner params: cls : type/lingvo.core.learner/Learner
I0930 03:45:04.989084 139840756766528 learner.py:356] Learner params: colocate_gradients_with_ops : True
I0930 03:45:04.989135 139840756766528 learner.py:356] Learner params: dtype : float32
I0930 03:45:04.989186 139840756766528 learner.py:356] Learner params: fprop_dtype : NoneType
I0930 03:45:04.989238 139840756766528 learner.py:356] Learner params: gate_gradients : False
I0930 03:45:04.989289 139840756766528 learner.py:356] Learner params: grad_aggregation_method : 1
I0930 03:45:04.989339 139840756766528 learner.py:356] Learner params: grad_norm_to_clip_to_zero : 0.0
I0930 03:45:04.989389 139840756766528 learner.py:356] Learner params: grad_norm_tracker : NoneType
I0930 03:45:04.989447 139840756766528 learner.py:356] Learner params: inference_driver_name : NoneType
I0930 03:45:04.989500 139840756766528 learner.py:356] Learner params: is_eval : NoneType
I0930 03:45:04.989575 139840756766528 learner.py:356] Learner params: is_inference : NoneType
I0930 03:45:04.989629 139840756766528 learner.py:356] Learner params: l1_regularizer_weight : NoneType
I0930 03:45:04.989681 139840756766528 learner.py:356] Learner params: l2_regularizer_weight : 1e-06
I0930 03:45:04.989731 139840756766528 learner.py:356] Learner params: learning_rate : 0.5
I0930 03:45:04.989783 139840756766528 learner.py:356] Learner params: lr_schedule.allow_implicit_capture : NoneType
I0930 03:45:04.989834 139840756766528 learner.py:356] Learner params: lr_schedule.cls : type/lingvo.core.schedule/TransformerLearningRateSchedule
I0930 03:45:04.989885 139840756766528 learner.py:356] Learner params: lr_schedule.decay_end : NoneType
I0930 03:45:04.989936 139840756766528 learner.py:356] Learner params: lr_schedule.dtype : float32
I0930 03:45:04.989987 139840756766528 learner.py:356] Learner params: lr_schedule.fprop_dtype : NoneType
I0930 03:45:04.990038 139840756766528 learner.py:356] Learner params: lr_schedule.inference_driver_name : NoneType
I0930 03:45:04.990089 139840756766528 learner.py:356] Learner params: lr_schedule.is_eval : NoneType
I0930 03:45:04.990140 139840756766528 learner.py:356] Learner params: lr_schedule.is_inference : NoneType
I0930 03:45:04.990191 139840756766528 learner.py:356] Learner params: lr_schedule.model_dim : 2048
I0930 03:45:04.990242 139840756766528 learner.py:356] Learner params: lr_schedule.name : 'LRSched'
I0930 03:45:04.990293 139840756766528 learner.py:356] Learner params: lr_schedule.params_init.method : 'xavier'
I0930 03:45:04.990344 139840756766528 learner.py:356] Learner params: lr_schedule.params_init.scale : 1.000001
I0930 03:45:04.990395 139840756766528 learner.py:356] Learner params: lr_schedule.params_init.seed : NoneType
I0930 03:45:04.990446 139840756766528 learner.py:356] Learner params: lr_schedule.random_seed : NoneType
I0930 03:45:04.990497 139840756766528 learner.py:356] Learner params: lr_schedule.skip_lp_regularization : NoneType
I0930 03:45:04.990547 139840756766528 learner.py:356] Learner params: lr_schedule.vn.global_vn : False
I0930 03:45:04.990597 139840756766528 learner.py:356] Learner params: lr_schedule.vn.per_step_vn : False
I0930 03:45:04.990648 139840756766528 learner.py:356] Learner params: lr_schedule.vn.scale : NoneType
I0930 03:45:04.990698 139840756766528 learner.py:356] Learner params: lr_schedule.vn.seed : NoneType
I0930 03:45:04.990748 139840756766528 learner.py:356] Learner params: lr_schedule.warmup_steps : 40000
I0930 03:45:04.990799 139840756766528 learner.py:356] Learner params: lr_schedule.worker_replicas : 1
I0930 03:45:04.990849 139840756766528 learner.py:356] Learner params: name : 'loss'
I0930 03:45:04.990899 139840756766528 learner.py:356] Learner params: optimizer.allow_implicit_capture : NoneType
I0930 03:45:04.990950 139840756766528 learner.py:356] Learner params: optimizer.beta1 : 0.9
I0930 03:45:04.991000 139840756766528 learner.py:356] Learner params: optimizer.beta2 : 0.997
I0930 03:45:04.991051 139840756766528 learner.py:356] Learner params: optimizer.cls : type/lingvo.core.optimizer/Adam
I0930 03:45:04.991101 139840756766528 learner.py:356] Learner params: optimizer.dtype : float32
I0930 03:45:04.991153 139840756766528 learner.py:356] Learner params: optimizer.epsilon : 1e-09
I0930 03:45:04.991203 139840756766528 learner.py:356] Learner params: optimizer.fprop_dtype : NoneType
I0930 03:45:04.991253 139840756766528 learner.py:356] Learner params: optimizer.inference_driver_name : NoneType
I0930 03:45:04.991304 139840756766528 learner.py:356] Learner params: optimizer.is_eval : NoneType
I0930 03:45:04.991354 139840756766528 learner.py:356] Learner params: optimizer.is_inference : NoneType
I0930 03:45:04.991404 139840756766528 learner.py:356] Learner params: optimizer.name : 'Adam'
I0930 03:45:04.991455 139840756766528 learner.py:356] Learner params: optimizer.params_init.method : 'xavier'
I0930 03:45:04.991511 139840756766528 learner.py:356] Learner params: optimizer.params_init.scale : 1.000001
I0930 03:45:04.991563 139840756766528 learner.py:356] Learner params: optimizer.params_init.seed : NoneType
I0930 03:45:04.991614 139840756766528 learner.py:356] Learner params: optimizer.random_seed : NoneType
I0930 03:45:04.991665 139840756766528 learner.py:356] Learner params: optimizer.skip_lp_regularization : NoneType
I0930 03:45:04.991715 139840756766528 learner.py:356] Learner params: optimizer.vn.global_vn : False
I0930 03:45:04.991766 139840756766528 learner.py:356] Learner params: optimizer.vn.per_step_vn : False
I0930 03:45:04.991816 139840756766528 learner.py:356] Learner params: optimizer.vn.scale : NoneType
I0930 03:45:04.991867 139840756766528 learner.py:356] Learner params: optimizer.vn.seed : NoneType
I0930 03:45:04.991918 139840756766528 learner.py:356] Learner params: params_init.method : 'xavier'
I0930 03:45:04.991968 139840756766528 learner.py:356] Learner params: params_init.scale : 1.000001
I0930 03:45:04.992018 139840756766528 learner.py:356] Learner params: params_init.seed : NoneType
I0930 03:45:04.992069 139840756766528 learner.py:356] Learner params: random_seed : NoneType
I0930 03:45:04.992120 139840756766528 learner.py:356] Learner params: skip_lp_regularization : NoneType
I0930 03:45:04.992171 139840756766528 learner.py:356] Learner params: vn.global_vn : False
I0930 03:45:04.992222 139840756766528 learner.py:356] Learner params: vn.per_step_vn : False
I0930 03:45:04.992273 139840756766528 learner.py:356] Learner params: vn.scale : NoneType
I0930 03:45:04.992323 139840756766528 learner.py:356] Learner params: vn.seed : NoneType
I0930 03:45:04.992374 139840756766528 learner.py:356] Learner params: 
I0930 03:45:05.376226 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var on /job:local/replica:0/task:0/device:CPU:0 262144008
I0930 03:45:05.378237 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var:0 shape=(32000, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:05.397199 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 278921224
I0930 03:45:05.399134 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:05.401846 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 278929416
I0930 03:45:05.403463 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:05.410415 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 295706632
I0930 03:45:05.412302 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:05.414969 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 295714824
I0930 03:45:05.416584 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:05.423537 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 312492040
I0930 03:45:05.425457 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:05.428071 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 312500232
I0930 03:45:05.429684 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:05.436630 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 329277448
I0930 03:45:05.438562 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:05.441237 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 329285640
I0930 03:45:05.442878 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:05.446988 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 329286152
I0930 03:45:05.448601 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:05.452419 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 329294344
I0930 03:45:05.454050 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:05.456688 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 329302536
I0930 03:45:05.458331 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:45:05.467695 139840756766528 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:45:05.473959 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 396411400
I0930 03:45:05.475930 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:05.478513 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 396444168
I0930 03:45:05.480136 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:45:05.482035 139840756766528 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:45:05.488263 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 463553032
I0930 03:45:05.490188 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:05.492821 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 463561224
I0930 03:45:05.494454 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:05.498987 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 463569416
I0930 03:45:05.500590 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:05.503280 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 463577608
I0930 03:45:05.504885 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:05.525054 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 480354824
I0930 03:45:05.526999 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:05.529584 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 480363016
I0930 03:45:05.531318 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:05.538241 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 497140232
I0930 03:45:05.540156 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:05.542817 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 497148424
I0930 03:45:05.544439 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:05.551355 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 513925640
I0930 03:45:05.553808 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:05.556371 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 513933832
I0930 03:45:05.558030 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:05.564990 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 530711048
I0930 03:45:05.566935 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:05.569553 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 530719240
I0930 03:45:05.571300 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:05.574928 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 530719752
I0930 03:45:05.576555 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:05.580418 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 530727944
I0930 03:45:05.582085 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:05.584733 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 530736136
I0930 03:45:05.586415 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:45:05.595795 139840756766528 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:45:05.602035 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 597845000
I0930 03:45:05.604021 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:05.606608 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 597877768
I0930 03:45:05.608228 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:45:05.610630 139840756766528 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:45:05.616839 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 664986632
I0930 03:45:05.618787 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:05.621439 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 664994824
I0930 03:45:05.623090 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:05.627618 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 665003016
I0930 03:45:05.629229 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:05.631955 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 665011208
I0930 03:45:05.633598 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:05.653409 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 681788424
I0930 03:45:05.655355 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:05.657975 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 681796616
I0930 03:45:05.660197 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:05.667145 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 698573832
I0930 03:45:05.669063 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:05.671760 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 698582024
I0930 03:45:05.673398 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:05.680445 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 715359240
I0930 03:45:05.682462 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:05.685017 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 715367432
I0930 03:45:05.686663 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:05.693714 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 732144648
I0930 03:45:05.695659 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:05.698286 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 732152840
I0930 03:45:05.700022 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:05.703690 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 732153352
I0930 03:45:05.705330 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:05.709225 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 732161544
I0930 03:45:05.710878 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:05.713574 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 732169736
I0930 03:45:05.715219 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:45:05.725171 139840756766528 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:45:05.731463 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 799278600
I0930 03:45:05.733450 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:05.736043 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 799311368
I0930 03:45:05.737695 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:45:05.739614 139840756766528 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:45:05.745838 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 866420232
I0930 03:45:05.747753 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:05.750441 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 866428424
I0930 03:45:05.752088 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:05.756678 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 866436616
I0930 03:45:05.758327 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:05.761023 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 866444808
I0930 03:45:05.762684 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:05.946751 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 883222024
I0930 03:45:05.948880 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:05.951617 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 883230216
I0930 03:45:05.953270 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:05.960319 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 900007432
I0930 03:45:05.962625 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:05.965252 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 900015624
I0930 03:45:05.966927 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:05.974015 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 916792840
I0930 03:45:05.975955 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:05.978571 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 916801032
I0930 03:45:05.980348 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:05.987381 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 933578248
I0930 03:45:05.989333 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:05.992094 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 933586440
I0930 03:45:05.993777 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:05.997414 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 933586952
I0930 03:45:05.999095 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.003057 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 933595144
I0930 03:45:06.005357 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.007947 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 933603336
I0930 03:45:06.009613 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:45:06.019217 139840756766528 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:45:06.025582 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 1000712200
I0930 03:45:06.027522 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.030134 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 1000744968
I0930 03:45:06.031782 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:45:06.033756 139840756766528 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:45:06.039989 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 1067853832
I0930 03:45:06.042012 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.044596 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 1067862024
I0930 03:45:06.046274 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.050921 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1067870216
I0930 03:45:06.052736 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.055329 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1067878408
I0930 03:45:06.056977 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.077329 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 1084655624
I0930 03:45:06.079282 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.081886 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1084663816
I0930 03:45:06.083654 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.090648 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 1101441032
I0930 03:45:06.092569 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.095267 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1101449224
I0930 03:45:06.096920 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.103932 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 1118226440
I0930 03:45:06.105910 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.108510 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1118234632
I0930 03:45:06.110193 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.117619 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 1135011848
I0930 03:45:06.119564 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.122312 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1135020040
I0930 03:45:06.123983 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.127650 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 1135020552
I0930 03:45:06.129310 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.133220 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1135028744
I0930 03:45:06.134901 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.137632 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1135036936
I0930 03:45:06.139288 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:45:06.148746 139840756766528 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:45:06.155043 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 1202145800
I0930 03:45:06.157081 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.159696 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 1202178568
I0930 03:45:06.161351 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:45:06.163319 139840756766528 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:45:06.170121 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 1269287432
I0930 03:45:06.172081 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.174855 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 1269295624
I0930 03:45:06.176498 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.181108 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1269303816
I0930 03:45:06.182793 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.185504 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1269312008
I0930 03:45:06.187169 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.207037 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 1286089224
I0930 03:45:06.209138 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.211725 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1286097416
I0930 03:45:06.213477 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.221023 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 1302874632
I0930 03:45:06.223012 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.225732 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1302882824
I0930 03:45:06.227391 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.234397 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 1319660040
I0930 03:45:06.236395 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.238999 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1319668232
I0930 03:45:06.240653 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.247751 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 1336445448
I0930 03:45:06.249719 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.252346 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1336453640
I0930 03:45:06.254158 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.257824 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 1336454152
I0930 03:45:06.259486 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.263406 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1336462344
I0930 03:45:06.265074 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.267778 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1336470536
I0930 03:45:06.269428 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:45:06.279433 139840756766528 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:45:06.285729 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 1403579400
I0930 03:45:06.287744 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.290364 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 1403612168
I0930 03:45:06.292038 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:45:06.294030 139840756766528 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:45:06.300278 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 1470721032
I0930 03:45:06.302255 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.304944 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 1470729224
I0930 03:45:06.306611 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.311252 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1470737416
I0930 03:45:06.312901 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.315652 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1470745608
I0930 03:45:06.317309 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.337673 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 1487522824
I0930 03:45:06.339627 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.342241 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1487531016
I0930 03:45:06.344025 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.351010 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 1504308232
I0930 03:45:06.352961 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.355703 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1504316424
I0930 03:45:06.357358 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.364372 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 1521093640
I0930 03:45:06.366418 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.369025 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1521101832
I0930 03:45:06.370750 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.377922 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 1537879048
I0930 03:45:06.379876 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.382532 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1537887240
I0930 03:45:06.384296 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.388024 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 1537887752
I0930 03:45:06.389707 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.393626 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1537895944
I0930 03:45:06.395293 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.398450 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1537904136
I0930 03:45:06.400127 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:45:06.410111 139840756766528 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:45:06.416366 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 1605013000
I0930 03:45:06.418404 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.421032 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 1605045768
I0930 03:45:06.422750 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:45:06.424706 139840756766528 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:45:06.430993 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 1672154632
I0930 03:45:06.432948 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.435661 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 1672162824
I0930 03:45:06.437498 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.442185 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1672171016
I0930 03:45:06.443847 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.446595 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1672179208
I0930 03:45:06.448256 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.468606 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 1688956424
I0930 03:45:06.470598 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.473222 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1688964616
I0930 03:45:06.475021 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.482037 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 1705741832
I0930 03:45:06.483989 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.486726 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1705750024
I0930 03:45:06.488395 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.495427 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 1722527240
I0930 03:45:06.497435 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.500073 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1722535432
I0930 03:45:06.501762 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.509232 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 1739312648
I0930 03:45:06.511226 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.514002 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1739320840
I0930 03:45:06.515789 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.519514 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 1739321352
I0930 03:45:06.521192 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.525161 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1739329544
I0930 03:45:06.526852 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.529601 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1739337736
I0930 03:45:06.531265 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:45:06.540846 139840756766528 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:45:06.547178 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 1806446600
I0930 03:45:06.549200 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.551871 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 1806479368
I0930 03:45:06.553569 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:45:06.555577 139840756766528 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:45:06.562421 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 1873588232
I0930 03:45:06.564386 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.567119 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 1873596424
I0930 03:45:06.568793 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.573493 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1873604616
I0930 03:45:06.575192 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.577972 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1873612808
I0930 03:45:06.579658 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.652545 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 1890390024
I0930 03:45:06.654530 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.657237 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1890398216
I0930 03:45:06.658925 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.665943 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 1907175432
I0930 03:45:06.668379 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.671005 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1907183624
I0930 03:45:06.672689 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.679732 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 1923960840
I0930 03:45:06.681708 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.684343 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1923969032
I0930 03:45:06.686156 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.693155 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 1940746248
I0930 03:45:06.695147 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.697942 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1940754440
I0930 03:45:06.699666 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.703365 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 1940754952
I0930 03:45:06.705061 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.709076 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1940763144
I0930 03:45:06.710797 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.713551 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1940771336
I0930 03:45:06.715230 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:45:06.724793 139840756766528 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:45:06.731657 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 2007880200
I0930 03:45:06.921612 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.924843 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 2007912968
I0930 03:45:06.926633 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:45:06.928740 139840756766528 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:45:06.935198 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 2075021832
I0930 03:45:06.937186 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.939959 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 2075030024
I0930 03:45:06.941674 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.946472 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2075038216
I0930 03:45:06.948149 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.950949 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2075046408
I0930 03:45:06.952656 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.973213 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 2091823624
I0930 03:45:06.975212 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.977953 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2091831816
I0930 03:45:06.979637 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.986728 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 2108609032
I0930 03:45:06.988774 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:06.991433 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2108617224
I0930 03:45:06.993138 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.000238 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 2125394440
I0930 03:45:07.002248 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.004878 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2125402632
I0930 03:45:07.006695 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.013741 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 2142179848
I0930 03:45:07.015715 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.018509 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2142188040
I0930 03:45:07.020199 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.023961 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 2142188552
I0930 03:45:07.025673 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.029778 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2142196744
I0930 03:45:07.031465 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.034207 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2142204936
I0930 03:45:07.035895 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:45:07.046239 139840756766528 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:45:07.052579 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 2209313800
I0930 03:45:07.054645 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.057274 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 2209346568
I0930 03:45:07.059007 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:45:07.061013 139840756766528 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:45:07.067343 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 2276455432
I0930 03:45:07.069328 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.072068 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 2276463624
I0930 03:45:07.073783 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.078553 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2276471816
I0930 03:45:07.080240 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.083019 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2276480008
I0930 03:45:07.084695 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.105402 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 2293257224
I0930 03:45:07.107405 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.110054 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2293265416
I0930 03:45:07.111836 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.118858 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 2310042632
I0930 03:45:07.120831 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.123618 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2310050824
I0930 03:45:07.125295 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.132358 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 2326828040
I0930 03:45:07.134450 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.137082 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2326836232
I0930 03:45:07.138803 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.145919 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 2343613448
I0930 03:45:07.147908 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.150600 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2343621640
I0930 03:45:07.152851 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.156617 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 2343622152
I0930 03:45:07.158334 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.162427 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2343630344
I0930 03:45:07.164106 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.166863 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2343638536
I0930 03:45:07.168556 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:45:07.178153 139840756766528 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:45:07.184500 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 2410747400
I0930 03:45:07.186568 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.189204 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 2410780168
I0930 03:45:07.190917 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:45:07.192931 139840756766528 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:45:07.199256 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 2477889032
I0930 03:45:07.201244 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.204017 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 2477897224
I0930 03:45:07.205730 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.210516 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2477905416
I0930 03:45:07.212186 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.215456 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2477913608
I0930 03:45:07.217573 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.237793 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 2494690824
I0930 03:45:07.239789 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.242448 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2494699016
I0930 03:45:07.244243 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.251288 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 2511476232
I0930 03:45:07.253276 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.256050 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2511484424
I0930 03:45:07.257760 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.265252 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 2528261640
I0930 03:45:07.267334 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.270014 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2528269832
I0930 03:45:07.271694 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.278799 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 2545047048
I0930 03:45:07.280787 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.283500 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2545055240
I0930 03:45:07.285291 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.289088 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 2545055752
I0930 03:45:07.290826 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.294889 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2545063944
I0930 03:45:07.296580 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.299336 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2545072136
I0930 03:45:07.301027 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:45:07.310708 139840756766528 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:45:07.317030 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 2612181000
I0930 03:45:07.319675 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.322364 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 2612213768
I0930 03:45:07.324081 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:45:07.326150 139840756766528 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:45:07.332443 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 2679322632
I0930 03:45:07.334477 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.337188 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 2679330824
I0930 03:45:07.338920 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.343717 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2679339016
I0930 03:45:07.345387 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.348192 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2679347208
I0930 03:45:07.349915 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.370526 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 2696124424
I0930 03:45:07.372509 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.375179 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2696132616
I0930 03:45:07.377029 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.384086 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 2712909832
I0930 03:45:07.386087 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.388843 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2712918024
I0930 03:45:07.390565 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.397606 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 2729695240
I0930 03:45:07.399671 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.402341 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2729703432
I0930 03:45:07.404056 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.411169 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 2746480648
I0930 03:45:07.413176 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.415870 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2746488840
I0930 03:45:07.417693 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.421444 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 2746489352
I0930 03:45:07.423176 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.427216 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2746497544
I0930 03:45:07.428905 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.431684 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2746505736
I0930 03:45:07.433397 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:45:07.443792 139840756766528 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:45:07.450151 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 2813614600
I0930 03:45:07.452219 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.454890 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 2813647368
I0930 03:45:07.456591 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:45:07.458664 139840756766528 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:45:07.464982 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 2880756232
I0930 03:45:07.467004 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.469782 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 2880764424
I0930 03:45:07.471487 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.476261 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2880772616
I0930 03:45:07.477970 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.480758 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2880780808
I0930 03:45:07.482488 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.503293 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 2897558024
I0930 03:45:07.505285 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.507951 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2897566216
I0930 03:45:07.509787 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.516923 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 2914343432
I0930 03:45:07.518941 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.521734 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2914351624
I0930 03:45:07.523433 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.530480 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 2931128840
I0930 03:45:07.532538 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.535218 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2931137032
I0930 03:45:07.536915 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.544043 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 2947914248
I0930 03:45:07.546064 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.548752 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2947922440
I0930 03:45:07.551059 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.554953 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 2947922952
I0930 03:45:07.556691 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.560764 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2947931144
I0930 03:45:07.562489 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.565237 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2947939336
I0930 03:45:07.566958 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:45:07.576666 139840756766528 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:45:07.583082 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 3015048200
I0930 03:45:07.585163 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.587850 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 3015080968
I0930 03:45:07.589600 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:45:07.591671 139840756766528 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:45:07.598022 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 3082189832
I0930 03:45:07.600039 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.602799 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 3082198024
I0930 03:45:07.604508 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.609310 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3082206216
I0930 03:45:07.611066 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.614344 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3082214408
I0930 03:45:07.616061 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.636256 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 3098991624
I0930 03:45:07.638271 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.640950 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3098999816
I0930 03:45:07.642789 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.649885 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 3115777032
I0930 03:45:07.651895 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.654696 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3115785224
I0930 03:45:07.656398 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.663961 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 3132562440
I0930 03:45:07.666064 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.668747 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3132570632
I0930 03:45:07.670491 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.677612 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 3149347848
I0930 03:45:07.679621 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.682346 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3149356040
I0930 03:45:07.684160 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.687977 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 3149356552
I0930 03:45:07.689719 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.693840 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3149364744
I0930 03:45:07.695551 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.698331 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3149372936
I0930 03:45:07.700070 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:45:07.709835 139840756766528 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:45:07.716169 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 3216481800
I0930 03:45:07.718800 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.721477 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 3216514568
I0930 03:45:07.723229 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:45:07.725288 139840756766528 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:45:07.731653 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 3283623432
I0930 03:45:07.733699 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.736479 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 3283631624
I0930 03:45:07.738225 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.743064 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3283639816
I0930 03:45:07.744780 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.747582 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3283648008
I0930 03:45:07.749319 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.770148 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 3300425224
I0930 03:45:07.772165 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.774864 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3300433416
I0930 03:45:07.776682 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.783761 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 3317210632
I0930 03:45:07.785788 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.788561 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3317218824
I0930 03:45:07.790309 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.797412 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 3333996040
I0930 03:45:07.799509 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.802205 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3334004232
I0930 03:45:07.803923 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.811074 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 3350781448
I0930 03:45:07.813139 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.815887 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3350789640
I0930 03:45:07.817737 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.821574 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 3350790152
I0930 03:45:07.823323 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.827463 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3350798344
I0930 03:45:07.829176 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.831982 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3350806536
I0930 03:45:07.833735 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:45:07.844075 139840756766528 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:45:07.850479 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 3417915400
I0930 03:45:07.852566 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.855280 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 3417948168
I0930 03:45:07.857026 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:45:07.859131 139840756766528 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:45:07.865542 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 3485057032
I0930 03:45:07.867576 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.870364 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 3485065224
I0930 03:45:07.872085 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.876959 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3485073416
I0930 03:45:07.878702 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:07.881486 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3485081608
I0930 03:45:07.883249 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.168010 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 3501858824
I0930 03:45:08.170261 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.172948 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3501867016
I0930 03:45:08.174814 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.181864 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 3518644232
I0930 03:45:08.183887 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.186684 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3518652424
I0930 03:45:08.188406 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.195505 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 3535429640
I0930 03:45:08.197609 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.200301 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3535437832
I0930 03:45:08.202044 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.209203 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 3552215048
I0930 03:45:08.211270 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.214016 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3552223240
I0930 03:45:08.215845 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.220159 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 3552223752
I0930 03:45:08.221921 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.226135 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3552231944
I0930 03:45:08.227856 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.230650 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3552240136
I0930 03:45:08.232382 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:45:08.242162 139840756766528 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:45:08.248573 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 3619349000
I0930 03:45:08.250689 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.253393 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 3619381768
I0930 03:45:08.255149 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:45:08.257240 139840756766528 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:45:08.263610 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 3686490632
I0930 03:45:08.265662 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.268441 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 3686498824
I0930 03:45:08.270194 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.275102 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3686507016
I0930 03:45:08.276823 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.279659 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3686515208
I0930 03:45:08.281372 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.302005 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 3703292424
I0930 03:45:08.304025 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.306748 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3703300616
I0930 03:45:08.308576 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.315691 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 3720077832
I0930 03:45:08.317744 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.320542 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3720086024
I0930 03:45:08.322302 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.329437 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 3736863240
I0930 03:45:08.332018 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.334750 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3736871432
I0930 03:45:08.336482 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.343624 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 3753648648
I0930 03:45:08.345665 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.348395 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3753656840
I0930 03:45:08.350247 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.354121 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 3753657352
I0930 03:45:08.355866 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.360039 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3753665544
I0930 03:45:08.361786 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.364574 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3753673736
I0930 03:45:08.366357 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:45:08.376189 139840756766528 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:45:08.382610 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 3820782600
I0930 03:45:08.384713 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.387448 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 3820815368
I0930 03:45:08.389189 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:45:08.391815 139840756766528 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:45:08.398212 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 3887924232
I0930 03:45:08.400230 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.403037 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 3887932424
I0930 03:45:08.404773 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.409703 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3887940616
I0930 03:45:08.411423 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.414276 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3887948808
I0930 03:45:08.416011 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.436326 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 3904726024
I0930 03:45:08.438400 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.441110 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3904734216
I0930 03:45:08.443458 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.450570 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 3921511432
I0930 03:45:08.452596 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.455476 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3921519624
I0930 03:45:08.457223 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.464326 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 3938296840
I0930 03:45:08.466451 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.469145 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3938305032
I0930 03:45:08.470909 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.478110 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 3955082248
I0930 03:45:08.480136 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.482879 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3955090440
I0930 03:45:08.484735 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.488619 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 3955090952
I0930 03:45:08.490398 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.494605 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3955099144
I0930 03:45:08.496338 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.499163 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3955107336
I0930 03:45:08.500913 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:45:08.511329 139840756766528 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:45:08.517902 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 4022216200
I0930 03:45:08.520002 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.522721 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 4022248968
I0930 03:45:08.524466 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:45:08.526647 139840756766528 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:45:08.533000 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 4089357832
I0930 03:45:08.535058 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.537894 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 4089366024
I0930 03:45:08.539626 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.544546 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4089374216
I0930 03:45:08.546310 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.549132 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4089382408
I0930 03:45:08.550912 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.571760 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 4106159624
I0930 03:45:08.573830 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.576537 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4106167816
I0930 03:45:08.578413 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.585517 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 4122945032
I0930 03:45:08.587579 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.590407 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4122953224
I0930 03:45:08.592145 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.599273 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 4139730440
I0930 03:45:08.601395 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.604135 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4139738632
I0930 03:45:08.605908 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.613058 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 4156515848
I0930 03:45:08.615120 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.617910 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4156524040
I0930 03:45:08.619751 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.624073 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 4156524552
I0930 03:45:08.625844 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.630045 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4156532744
I0930 03:45:08.631777 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.634597 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4156540936
I0930 03:45:08.636359 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:45:08.646266 139840756766528 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:45:08.652706 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 4223649800
I0930 03:45:08.654848 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.657613 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 4223682568
I0930 03:45:08.659361 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:45:08.661491 139840756766528 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:45:08.667927 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 4290791432
I0930 03:45:08.670003 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.672786 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 4290799624
I0930 03:45:08.674570 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.679528 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4290807816
I0930 03:45:08.681272 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.684141 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4290816008
I0930 03:45:08.685911 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.706896 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 4307593224
I0930 03:45:08.708937 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.711690 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4307601416
I0930 03:45:08.713574 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.720689 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 4324378632
I0930 03:45:08.722745 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.725572 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4324386824
I0930 03:45:08.727321 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.734731 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 4341164040
I0930 03:45:08.737291 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.740018 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4341172232
I0930 03:45:08.741790 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.748924 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 4357949448
I0930 03:45:08.751029 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.753800 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4357957640
I0930 03:45:08.755654 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.759538 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 4357958152
I0930 03:45:08.761301 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.765571 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4357966344
I0930 03:45:08.767326 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.770177 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4357974536
I0930 03:45:08.771939 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:45:08.781835 139840756766528 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:45:08.788310 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 4425083400
I0930 03:45:08.790430 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.793152 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 4425116168
I0930 03:45:08.794972 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:45:08.797650 139840756766528 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:45:08.804037 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 4492225032
I0930 03:45:08.806129 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.808944 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 4492233224
I0930 03:45:08.810734 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.815709 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4492241416
I0930 03:45:08.817456 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.820317 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4492249608
I0930 03:45:08.822113 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.842750 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 4509026824
I0930 03:45:08.844819 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.847580 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4509035016
I0930 03:45:08.849957 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.857039 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 4525812232
I0930 03:45:08.859127 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.861965 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4525820424
I0930 03:45:08.863724 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.870853 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 4542597640
I0930 03:45:08.872967 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.875730 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4542605832
I0930 03:45:08.877504 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.884690 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 4559383048
I0930 03:45:08.886777 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.889562 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4559391240
I0930 03:45:08.891413 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.895360 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 4559391752
I0930 03:45:08.897124 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.901404 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4559399944
I0930 03:45:08.903192 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.906047 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4559408136
I0930 03:45:08.907801 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:45:08.918329 139840756766528 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:45:08.924763 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 4626517000
I0930 03:45:08.926899 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.929683 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 4626549768
I0930 03:45:08.931493 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:45:08.933677 139840756766528 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:45:08.940083 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 4693658632
I0930 03:45:08.942167 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.944996 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 4693666824
I0930 03:45:08.946776 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.951762 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4693675016
I0930 03:45:08.953506 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.956373 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4693683208
I0930 03:45:08.958157 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.979192 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 4710460424
I0930 03:45:08.981237 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.983977 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4710468616
I0930 03:45:08.985855 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.992984 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 4727245832
I0930 03:45:08.995079 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:08.997937 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4727254024
I0930 03:45:08.999703 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:09.006838 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 4744031240
I0930 03:45:09.008977 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:09.011735 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4744039432
I0930 03:45:09.013487 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:09.020688 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 4760816648
I0930 03:45:09.022770 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:09.025585 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4760824840
I0930 03:45:09.027461 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:09.031832 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 4760825352
I0930 03:45:09.033629 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:09.037937 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4760833544
I0930 03:45:09.039691 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:09.042532 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4760841736
I0930 03:45:09.044310 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:45:09.054255 139840756766528 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:45:09.060763 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 4827950600
I0930 03:45:09.062921 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:09.065695 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 4827983368
I0930 03:45:09.067458 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:45:09.069649 139840756766528 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:45:09.076042 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 4895092232
I0930 03:45:09.078139 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:09.080973 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 4895100424
I0930 03:45:09.082752 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:09.087767 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4895108616
I0930 03:45:09.089551 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:09.092425 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4895116808
I0930 03:45:09.094202 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:09.115169 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 4911894024
I0930 03:45:09.117251 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:09.120058 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4911902216
I0930 03:45:09.121963 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:09.129083 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 4928679432
I0930 03:45:09.131182 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:09.134043 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4928687624
I0930 03:45:09.135800 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:09.142928 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 4945464840
I0930 03:45:09.145539 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:09.148309 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4945473032
I0930 03:45:09.150110 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:09.157304 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 4962250248
I0930 03:45:09.159400 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:09.162213 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4962258440
I0930 03:45:09.164079 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:09.168027 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 4962258952
I0930 03:45:09.169829 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:09.174125 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4962267144
I0930 03:45:09.175956 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:09.178807 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4962275336
I0930 03:45:09.180589 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:45:09.190532 139840756766528 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:45:09.196987 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 5029384200
I0930 03:45:09.199148 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:09.201937 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 5029416968
I0930 03:45:09.203715 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:45:09.206416 139840756766528 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:45:09.212823 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 5096525832
I0930 03:45:09.214939 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:09.218758 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 5096534024
I0930 03:45:09.220545 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:09.225570 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5096542216
I0930 03:45:09.227350 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:09.230278 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5096550408
I0930 03:45:09.232057 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:09.304907 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 5113327624
I0930 03:45:09.307067 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:09.309845 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5113335816
I0930 03:45:09.311616 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:09.562102 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 5130113032
I0930 03:45:09.564602 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:09.567712 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5130121224
I0930 03:45:09.569635 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:09.576945 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 5146898440
I0930 03:45:09.579085 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:09.581992 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5146906632
I0930 03:45:09.583769 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:09.590978 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 5163683848
I0930 03:45:09.593130 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:09.595969 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5163692040
I0930 03:45:09.597769 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:09.601802 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 5163692552
I0930 03:45:09.603719 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:09.608174 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5163700744
I0930 03:45:09.609978 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:09.612820 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5163708936
I0930 03:45:09.614640 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:45:09.625343 139840756766528 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:45:09.631904 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 5230817800
I0930 03:45:09.634011 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:09.636781 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 5230850568
I0930 03:45:09.638595 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:45:09.640789 139840756766528 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:45:09.647240 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 5297959432
I0930 03:45:09.649389 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:09.652186 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 5297967624
I0930 03:45:09.653992 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:09.659044 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5297975816
I0930 03:45:09.660999 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:09.663750 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5297984008
I0930 03:45:09.665559 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:09.686681 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 5314761224
I0930 03:45:09.688769 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:09.691657 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5314769416
I0930 03:45:09.693575 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:09.700708 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 5331546632
I0930 03:45:09.702805 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:09.705684 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5331554824
I0930 03:45:09.707508 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:09.714748 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 5348332040
I0930 03:45:09.716820 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:09.719623 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5348340232
I0930 03:45:09.721386 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:09.728591 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 5365117448
I0930 03:45:09.730708 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:09.733632 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5365125640
I0930 03:45:09.735414 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:09.739365 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 5365126152
I0930 03:45:09.741159 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:09.745543 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5365134344
I0930 03:45:09.747324 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:09.750620 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5365142536
I0930 03:45:09.752419 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:45:09.762439 139840756766528 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:45:09.768891 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 5432251400
I0930 03:45:09.771057 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:09.773882 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 5432284168
I0930 03:45:09.775675 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:45:09.777917 139840756766528 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:45:09.784355 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 5499393032
I0930 03:45:09.786455 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:09.789317 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 5499401224
I0930 03:45:09.791129 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:09.796215 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5499409416
I0930 03:45:09.798020 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:09.800891 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5499417608
I0930 03:45:09.802707 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:09.823771 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 5516194824
I0930 03:45:09.825894 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:09.828651 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5516203016
I0930 03:45:09.830562 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:09.837722 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 5532980232
I0930 03:45:09.839810 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:09.842706 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5532988424
I0930 03:45:09.844475 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:09.851612 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 5549765640
I0930 03:45:09.853782 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:09.856570 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5549773832
I0930 03:45:09.858376 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:09.866024 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 5566551048
I0930 03:45:09.868106 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:09.870930 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5566559240
I0930 03:45:09.872832 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:09.876791 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 5566559752
I0930 03:45:09.878617 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:09.883004 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5566567944
I0930 03:45:09.884774 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:09.887647 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5566576136
I0930 03:45:09.889452 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:45:09.899578 139840756766528 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:45:09.906082 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 5633685000
I0930 03:45:09.908239 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:09.911079 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 5633717768
I0930 03:45:09.912872 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:45:09.915112 139840756766528 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:45:09.922129 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 5700826632
I0930 03:45:09.924230 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:09.927124 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 5700834824
I0930 03:45:09.928950 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:09.934046 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5700843016
I0930 03:45:09.935842 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:09.938771 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5700851208
I0930 03:45:09.940573 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:09.961399 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 5717628424
I0930 03:45:09.963529 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:09.966342 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5717636616
I0930 03:45:09.968246 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:09.975956 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 5734413832
I0930 03:45:09.978072 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:09.981193 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5734422024
I0930 03:45:09.982993 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:09.990136 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 5751199240
I0930 03:45:09.992265 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:09.995060 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5751207432
I0930 03:45:09.996842 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.004024 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 5767984648
I0930 03:45:10.006121 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.008936 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5767992840
I0930 03:45:10.010869 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.014886 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 5767993352
I0930 03:45:10.016688 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.021071 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5768001544
I0930 03:45:10.022902 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.025796 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5768009736
I0930 03:45:10.027588 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:45:10.042606 139840756766528 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:45:10.049071 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 5835118600
I0930 03:45:10.051248 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.054069 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 5835151368
I0930 03:45:10.055867 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:45:10.058125 139840756766528 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:45:10.064550 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 5902260232
I0930 03:45:10.066660 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.069541 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 5902268424
I0930 03:45:10.071361 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.076449 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5902276616
I0930 03:45:10.078284 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.081167 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5902284808
I0930 03:45:10.082990 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.104370 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 5919062024
I0930 03:45:10.106507 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.109295 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5919070216
I0930 03:45:10.111211 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.118419 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 5935847432
I0930 03:45:10.120516 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.123440 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5935855624
I0930 03:45:10.125257 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.132483 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 5952632840
I0930 03:45:10.134668 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.137456 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5952641032
I0930 03:45:10.139274 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.146526 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 5969418248
I0930 03:45:10.148624 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.151466 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5969426440
I0930 03:45:10.153368 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.157369 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 5969426952
I0930 03:45:10.159204 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.163635 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5969435144
I0930 03:45:10.165425 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.168745 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5969443336
I0930 03:45:10.170590 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:45:10.180658 139840756766528 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:45:10.187241 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 6036552200
I0930 03:45:10.189402 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.192213 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 6036584968
I0930 03:45:10.194052 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:45:10.196295 139840756766528 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:45:10.202868 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 6103693832
I0930 03:45:10.204981 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.207896 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 6103702024
I0930 03:45:10.209729 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.214847 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 6103710216
I0930 03:45:10.216661 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.219571 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 6103718408
I0930 03:45:10.221389 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.242621 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 6120495624
I0930 03:45:10.244725 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.247529 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6120503816
I0930 03:45:10.249422 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.256634 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 6137281032
I0930 03:45:10.258757 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.261681 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6137289224
I0930 03:45:10.263474 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.270637 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 6154066440
I0930 03:45:10.272807 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.275715 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6154074632
I0930 03:45:10.277537 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.285155 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 6170851848
I0930 03:45:10.287303 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.290163 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6170860040
I0930 03:45:10.292068 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.296105 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 6170860552
I0930 03:45:10.297964 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.302374 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 6170868744
I0930 03:45:10.304165 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.307054 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 6170876936
I0930 03:45:10.308866 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:45:10.318993 139840756766528 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:45:10.325469 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 6237985800
I0930 03:45:10.327658 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.330492 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 6238018568
I0930 03:45:10.332312 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:45:10.334613 139840756766528 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:45:10.341628 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 6305127432
I0930 03:45:10.343752 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.346754 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 6305135624
I0930 03:45:10.348560 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.353715 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 6305143816
I0930 03:45:10.355530 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.358456 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 6305152008
I0930 03:45:10.360269 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.381085 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 6321929224
I0930 03:45:10.383237 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.386055 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6321937416
I0930 03:45:10.387965 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.395673 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 6338714632
I0930 03:45:10.397815 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.400692 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6338722824
I0930 03:45:10.402519 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.409712 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 6355500040
I0930 03:45:10.411895 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.414731 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6355508232
I0930 03:45:10.416541 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.423800 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 6372285448
I0930 03:45:10.425940 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.428758 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6372293640
I0930 03:45:10.430706 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.434756 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 6372294152
I0930 03:45:10.436570 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.440995 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 6372302344
I0930 03:45:10.442833 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.445725 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 6372310536
I0930 03:45:10.447558 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:45:10.458200 139840756766528 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:45:10.464693 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 6439419400
I0930 03:45:10.466914 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.469749 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 6439452168
I0930 03:45:10.471555 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:45:10.473843 139840756766528 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:45:10.480292 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 6506561032
I0930 03:45:10.482424 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.485309 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 6506569224
I0930 03:45:10.487152 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.492309 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 6506577416
I0930 03:45:10.494159 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.497074 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 6506585608
I0930 03:45:10.498917 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.520409 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 6523362824
I0930 03:45:10.522561 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.525349 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6523371016
I0930 03:45:10.527294 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.534512 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 6540148232
I0930 03:45:10.536636 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.539571 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6540156424
I0930 03:45:10.541379 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.548601 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 6556933640
I0930 03:45:10.550831 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.553656 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6556941832
I0930 03:45:10.555459 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.562855 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 6573719048
I0930 03:45:10.564983 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.567859 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6573727240
I0930 03:45:10.569794 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.573860 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 6573727752
I0930 03:45:10.575700 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.580150 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 6573735944
I0930 03:45:10.581991 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.585320 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 6573744136
I0930 03:45:10.587184 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:45:10.597380 139840756766528 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:45:10.603890 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 6640853000
I0930 03:45:10.606110 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.608936 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 6640885768
I0930 03:45:10.610794 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:45:10.613147 139840756766528 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:45:10.619642 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 6707994632
I0930 03:45:10.621798 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.624695 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 6708002824
I0930 03:45:10.626545 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.631731 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 6708011016
I0930 03:45:10.633640 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.636576 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 6708019208
I0930 03:45:10.638419 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:45:10.643776 139840756766528 py_utils.py:1229] WARNING!!! var weight_0 is using the default xavier initializer. Make sure this is intended.
I0930 03:45:10.650207 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var on /job:local/replica:0/task:0/device:CPU:0 6724403208
I0930 03:45:10.652306 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:45:10.653146 139840756766528 py_utils.py:1229] WARNING!!! var weight_1 is using the default xavier initializer. Make sure this is intended.
I0930 03:45:10.660039 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var on /job:local/replica:0/task:0/device:CPU:0 6740787208
I0930 03:45:10.662175 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:45:10.663041 139840756766528 py_utils.py:1229] WARNING!!! var weight_2 is using the default xavier initializer. Make sure this is intended.
I0930 03:45:10.669380 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var on /job:local/replica:0/task:0/device:CPU:0 6757171208
I0930 03:45:10.671510 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:45:10.672360 139840756766528 py_utils.py:1229] WARNING!!! var weight_3 is using the default xavier initializer. Make sure this is intended.
I0930 03:45:10.678750 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var on /job:local/replica:0/task:0/device:CPU:0 6773555208
I0930 03:45:10.680867 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:45:10.681742 139840756766528 py_utils.py:1229] WARNING!!! var weight_4 is using the default xavier initializer. Make sure this is intended.
I0930 03:45:10.688081 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var on /job:local/replica:0/task:0/device:CPU:0 6789939208
I0930 03:45:10.690207 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:45:10.691061 139840756766528 py_utils.py:1229] WARNING!!! var weight_5 is using the default xavier initializer. Make sure this is intended.
I0930 03:45:10.697435 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var on /job:local/replica:0/task:0/device:CPU:0 6806323208
I0930 03:45:10.699577 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:45:10.700433 139840756766528 py_utils.py:1229] WARNING!!! var weight_6 is using the default xavier initializer. Make sure this is intended.
I0930 03:45:10.706846 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var on /job:local/replica:0/task:0/device:CPU:0 6822707208
I0930 03:45:10.708947 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:45:10.709819 139840756766528 py_utils.py:1229] WARNING!!! var weight_7 is using the default xavier initializer. Make sure this is intended.
I0930 03:45:10.716214 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var on /job:local/replica:0/task:0/device:CPU:0 6839091208
I0930 03:45:10.718352 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:45:10.719203 139840756766528 py_utils.py:1229] WARNING!!! var weight_8 is using the default xavier initializer. Make sure this is intended.
I0930 03:45:10.725564 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var on /job:local/replica:0/task:0/device:CPU:0 6855475208
I0930 03:45:10.727679 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:45:10.728529 139840756766528 py_utils.py:1229] WARNING!!! var weight_9 is using the default xavier initializer. Make sure this is intended.
I0930 03:45:10.735446 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var on /job:local/replica:0/task:0/device:CPU:0 6871859208
I0930 03:45:10.737592 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:45:10.738448 139840756766528 py_utils.py:1229] WARNING!!! var weight_10 is using the default xavier initializer. Make sure this is intended.
I0930 03:45:10.744854 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var on /job:local/replica:0/task:0/device:CPU:0 6888243208
I0930 03:45:10.747006 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:45:10.747859 139840756766528 py_utils.py:1229] WARNING!!! var weight_11 is using the default xavier initializer. Make sure this is intended.
I0930 03:45:10.754284 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var on /job:local/replica:0/task:0/device:CPU:0 6904627208
I0930 03:45:10.756393 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:45:10.757244 139840756766528 py_utils.py:1229] WARNING!!! var weight_12 is using the default xavier initializer. Make sure this is intended.
I0930 03:45:10.763650 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var on /job:local/replica:0/task:0/device:CPU:0 6921011208
I0930 03:45:10.765779 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:45:10.766627 139840756766528 py_utils.py:1229] WARNING!!! var weight_13 is using the default xavier initializer. Make sure this is intended.
I0930 03:45:10.773032 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var on /job:local/replica:0/task:0/device:CPU:0 6937395208
I0930 03:45:10.775243 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:45:10.776100 139840756766528 py_utils.py:1229] WARNING!!! var weight_14 is using the default xavier initializer. Make sure this is intended.
I0930 03:45:10.782458 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var on /job:local/replica:0/task:0/device:CPU:0 6953779208
I0930 03:45:10.784607 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:45:10.785450 139840756766528 py_utils.py:1229] WARNING!!! var weight_15 is using the default xavier initializer. Make sure this is intended.
I0930 03:45:10.791887 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var on /job:local/replica:0/task:0/device:CPU:0 6970163208
I0930 03:45:10.794053 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.796891 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var on /job:local/replica:0/task:0/device:CPU:0 6970171208
I0930 03:45:10.798834 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.801625 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var on /job:local/replica:0/task:0/device:CPU:0 6970179208
I0930 03:45:10.803450 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.806779 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var on /job:local/replica:0/task:0/device:CPU:0 6970187208
I0930 03:45:10.808605 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.811416 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var on /job:local/replica:0/task:0/device:CPU:0 6970195208
I0930 03:45:10.813220 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.816148 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var on /job:local/replica:0/task:0/device:CPU:0 6970203208
I0930 03:45:10.817971 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.820772 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var on /job:local/replica:0/task:0/device:CPU:0 6970211208
I0930 03:45:10.822609 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.825492 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var on /job:local/replica:0/task:0/device:CPU:0 6970219208
I0930 03:45:10.827340 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.830149 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var on /job:local/replica:0/task:0/device:CPU:0 6970227208
I0930 03:45:10.832063 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.834865 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var on /job:local/replica:0/task:0/device:CPU:0 6970235208
I0930 03:45:10.836691 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.839594 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var on /job:local/replica:0/task:0/device:CPU:0 6970243208
I0930 03:45:10.841418 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.844228 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var on /job:local/replica:0/task:0/device:CPU:0 6970251208
I0930 03:45:10.846053 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.848948 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var on /job:local/replica:0/task:0/device:CPU:0 6970259208
I0930 03:45:10.850784 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.853571 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var on /job:local/replica:0/task:0/device:CPU:0 6970267208
I0930 03:45:10.855404 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.858320 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var on /job:local/replica:0/task:0/device:CPU:0 6970275208
I0930 03:45:10.860141 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.862941 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var on /job:local/replica:0/task:0/device:CPU:0 6970283208
I0930 03:45:10.864865 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:10.867665 139840756766528 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var on /job:local/replica:0/task:0/device:CPU:0 6970291208
I0930 03:45:10.869501 139840756766528 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:45:11.860137 139840756766528 py_utils.py:1478] === worker 0 ===
I0930 03:45:11.875353 139840756766528 py_utils.py:1468] worker 0: global_step                                                           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.875466 139840756766528 py_utils.py:1468] worker 0: input._tokenizer_default.global_step                                  /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.875544 139840756766528 py_utils.py:1468] worker 0: input.global_step                                                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.875613 139840756766528 py_utils.py:1468] worker 0: learners[0].global_step                                               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.875678 139840756766528 py_utils.py:1468] worker 0: learners[0].lr_schedule.global_step                                   /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.875741 139840756766528 py_utils.py:1468] worker 0: learners[0].optimizer.global_step                                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.875813 139840756766528 py_utils.py:1468] worker 0: lm.global_step                                                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.875876 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.emb.global_step                                       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.875937 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.emb.src_dropout.global_step                           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.875999 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.emb.src_pos_emb.global_step                           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.876060 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.emb.src_token_emb.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.876121 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.emb.src_token_emb.wm                                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.876182 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.876251 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.876312 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.876372 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.876433 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.876492 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.876551 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.876611 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.876671 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.876730 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.fflayer.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.876789 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.876848 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.876911 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.876972 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.877032 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.global_step                                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.877092 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.877151 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.877211 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.877270 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.877331 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.877391 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.877450 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.877511 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.877592 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.877654 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.877713 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.877773 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.self_atten.global_step                      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.877832 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.877893 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.877953 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.878013 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.878078 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.878139 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.878200 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.878259 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.878320 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.878381 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.878442 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.878502 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.878562 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.878623 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.fflayer.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.878683 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.878743 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.878803 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.878864 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.878924 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.global_step                                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.878984 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.879045 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.879104 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.879164 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.879228 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.879289 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.879350 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.879410 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.879470 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.879530 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.879590 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.879651 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.self_atten.global_step                      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.879711 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.879771 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.879832 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.879892 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.879953 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.880013 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.880073 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.880133 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.880193 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.880254 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.880317 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.880378 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.880439 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.880499 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.fflayer.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.880559 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.880619 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.880679 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.880739 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.880799 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.global_step                                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.880859 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.880919 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.880980 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.881040 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.881106 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.881167 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.881228 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.881289 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.881349 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.881410 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.881473 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.881553 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.self_atten.global_step                      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.881620 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.881681 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.881742 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.881802 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.881862 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.881923 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.881983 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.882043 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.882103 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.882164 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.882224 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.882284 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.882345 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.882405 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.fflayer.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.882468 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.882531 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.882599 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.882661 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.882722 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.global_step                                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.882782 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.882844 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.882904 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.882965 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.883025 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.883085 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.883146 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.883206 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.883267 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.883327 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.883386 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.883446 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.self_atten.global_step                      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.883506 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.883567 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.883627 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.883688 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.883752 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.883812 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.883873 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.883933 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.883993 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.884054 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.884114 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.884173 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.884233 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.884293 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.fflayer.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.884353 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.884413 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.884472 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.884532 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.884592 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.global_step                                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.884652 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.884716 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.884776 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.884840 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.884900 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.884960 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.885020 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.885080 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.885139 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.885200 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.885260 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.885320 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.self_atten.global_step                      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.885380 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.885440 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.885500 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.885579 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.885641 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.885702 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.885762 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.885823 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.885883 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.885942 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.886007 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.886067 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.886128 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.886188 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.fflayer.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.886249 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.886309 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.886369 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.886429 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.886489 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.global_step                                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.886549 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.886610 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.886670 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.886730 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.886790 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.886851 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.886911 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.886971 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.887031 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.887094 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.887156 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.887216 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.self_atten.global_step                      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.887276 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.887336 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.887396 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.887456 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.887516 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.887577 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.887636 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.887697 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.887757 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.887817 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.887876 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.887936 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.887996 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.888056 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.fflayer.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.888116 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.888176 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.888240 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.888301 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.888362 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.global_step                                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.888422 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.888483 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.888542 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.888603 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.888663 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.888722 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.888782 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.888843 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.888903 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.888964 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.889024 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.889085 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.self_atten.global_step                      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.889145 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.889206 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.889267 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.889331 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.889392 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.889454 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.889514 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.889594 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.889656 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.889717 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.889778 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.889838 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.889899 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.889960 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.fflayer.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.890021 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.890081 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.890141 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.890201 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.890262 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.global_step                                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.890323 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.890383 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.890443 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.890507 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.890568 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.890629 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.890690 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.890751 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.890812 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.890873 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.890933 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.890994 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.self_atten.global_step                      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.891054 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.891115 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.891175 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.891236 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.891296 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_0.global_step                                           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.891357 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.891418 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.891478 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.891538 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.891602 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.891664 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.891724 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.891785 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.891848 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.891908 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.891968 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.892029 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.892089 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.892149 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.892209 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.892269 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.892330 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.892390 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.892450 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.892510 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.892574 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.892635 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.892695 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.892759 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.892820 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.892880 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.892940 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.893001 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.893061 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.893121 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.893181 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.893241 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.893301 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.893360 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.893420 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.893481 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.893560 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.893625 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.893686 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.893746 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.893807 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.893871 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.893932 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.893992 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.894052 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.894113 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.894173 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.894232 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.894293 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.894353 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.894413 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.894473 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.894533 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.894593 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.894654 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.894715 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.894776 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.894836 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.894897 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.894957 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.895020 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.895081 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.895141 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.895201 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.895262 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.895322 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.895381 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.895442 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.895502 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.895562 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.895622 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.895682 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.895742 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.895802 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.895862 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.895922 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.895982 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.896041 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.896101 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.896164 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.896225 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.896286 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.896346 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.896406 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.896466 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.896527 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.896588 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.896649 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.896709 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.896770 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.896830 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.896890 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.896951 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.897011 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.897071 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.897132 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.897192 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.897257 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.897318 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.897379 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.897439 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.897500 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.897577 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.897639 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.897701 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.897761 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.897822 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.897883 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.897943 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.898004 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.898064 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.898125 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.898185 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.898246 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.898306 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.898367 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.898431 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.898492 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.898553 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.898613 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.898674 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.898738 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.898799 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.898859 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.898919 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.898980 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.899040 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.899100 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.899161 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.899221 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.899281 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.899341 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.899401 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.899460 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.899524 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.899586 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.899646 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.899707 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.899767 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.899827 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.899888 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.899948 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.900008 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.900069 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.900129 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.900190 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.900250 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.900310 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.900371 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.900431 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.900491 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.900552 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.900612 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.900676 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.900738 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.900797 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.900858 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.900918 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.900979 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.901039 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.901099 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.901160 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.901220 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.901280 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.901340 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.901400 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.901460 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.901533 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.901600 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.901661 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.901722 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.901788 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.901849 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.901910 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.901971 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.902032 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.902092 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.902153 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.902214 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.902274 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.902334 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.902395 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.902455 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.902515 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.902577 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.902639 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.902699 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.902760 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.902821 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.902881 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.902946 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.903007 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.903068 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.903129 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.903190 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.903250 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.fflayer.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.903311 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.903372 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.903433 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.903494 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.903555 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.global_step                                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.903615 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.903676 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.903737 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.903798 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.903858 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.903918 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.903979 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.904044 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.904105 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.904166 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.904227 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.904287 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.self_atten.global_step                      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.904347 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.904407 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.904468 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.904528 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.904588 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.904648 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.904709 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.904769 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.904829 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.904890 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.904949 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.905009 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.905069 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.905129 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.fflayer.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.905199 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.905260 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.905320 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.905380 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.905439 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.global_step                                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.905499 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.905582 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.905644 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.905704 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.905764 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.905824 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.905885 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.905945 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.906005 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.906065 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.906125 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.906185 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.self_atten.global_step                      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.906245 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.906309 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.906369 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.906429 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.906489 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_1.global_step                                           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.906548 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.906608 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.906668 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.906728 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.906787 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.906847 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.906906 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.906966 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.907027 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.907087 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.907146 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.907206 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.907265 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.907325 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.907385 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.907449 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.907509 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.907569 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.907629 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.907688 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.907748 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.907809 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.907868 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.907928 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.907988 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.908047 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.908107 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.908166 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.908225 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.908284 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.908344 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.908403 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.908462 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.908524 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.908585 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.908644 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.908703 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.908763 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.908823 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.908882 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.908942 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.909002 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.909062 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.909122 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.909181 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.909241 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.909301 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.909361 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.909420 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.909480 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.909554 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.909620 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.909684 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.909745 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.909806 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.909866 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.909926 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.909986 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.910047 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.910107 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.910166 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.910226 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.910286 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.910345 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.910404 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.910464 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.910523 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.910583 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.910643 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.910702 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.910765 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.910825 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.910885 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.910945 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.911005 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.911066 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.911130 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.911191 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.911250 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.911310 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.911370 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.911431 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.911490 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.911550 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.911609 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.911669 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.911729 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.911788 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.911847 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.911911 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.911971 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.912030 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.912090 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.912149 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.912208 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.912268 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.912328 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.912387 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.912446 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.912506 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.912565 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.912627 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.912688 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.912748 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.912808 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.912867 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.912928 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.912988 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.913050 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.913111 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.913171 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.913231 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.913290 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.913350 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.913410 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.913470 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.913546 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.913612 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.913673 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.913733 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.913793 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.913853 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.913912 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.913972 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.914032 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.914092 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.914156 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.914216 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.914276 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.914336 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.914395 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.914455 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.914515 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.914575 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.914635 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.914695 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.914755 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.914814 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.914875 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.914935 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.914994 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.915055 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.915116 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.915176 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.915236 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.915306 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.915367 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.915428 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.915488 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.915548 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.915609 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.915669 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.915729 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.915789 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.915849 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.915909 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.915968 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.916034 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.916093 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.916153 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.916212 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.916272 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.916332 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.916395 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.916455 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.916515 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.916574 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.916634 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.916693 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.916752 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.916812 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.916871 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.916930 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.916990 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.917049 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.917109 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.917168 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.917228 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.917289 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.917348 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.917407 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.917467 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.917548 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.917615 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.917676 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.917736 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.917796 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.917856 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.917916 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.917976 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.918035 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.918095 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.918154 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.918214 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.918274 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.918334 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.918394 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.918453 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.918514 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.918574 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.918638 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.918699 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.918759 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.918818 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.918879 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.918939 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.918999 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.919059 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.919119 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.919180 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.919240 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.919301 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.919361 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.919422 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.919482 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.919543 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.919604 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.919664 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.919724 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.919789 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.919851 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.919911 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.919971 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.920031 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.920091 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.920151 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.920212 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.920272 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.920333 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.920392 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.920453 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.920513 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.920573 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.920634 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.920693 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.920754 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.920813 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.920885 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.920945 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.921006 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.921066 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.921126 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.921186 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.921246 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.921306 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.921366 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.921427 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.921487 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.921563 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_2.global_step                                           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.921627 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.921688 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.921748 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.921808 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.921868 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.921928 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.921988 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.922052 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.922113 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.922173 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.922233 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.922292 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.922352 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.922412 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.922472 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.922532 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.922593 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.922653 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.922716 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.922777 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.922837 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.922897 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.922957 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.923017 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.923078 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.923142 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.923204 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.923264 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.923324 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.923384 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.923444 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.923504 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.923565 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.923624 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.923684 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.923744 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.923804 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.923863 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.923923 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.923982 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.924041 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.924100 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.924160 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.924219 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.924284 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.924344 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.924403 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.924463 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.924523 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.924583 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.924643 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.924702 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.924763 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.924823 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.924882 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.924942 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.925002 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.925062 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.925122 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.925182 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.925242 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.925302 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.925366 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.925427 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.925487 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.925564 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.925627 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.925688 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.925748 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.925808 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.925868 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.925928 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.925988 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.926048 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.926107 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.926167 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.926227 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.926286 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.926346 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.926406 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.926465 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.926531 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.926591 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.926651 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.926711 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.926770 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.926830 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.926891 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.926951 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.927011 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.927071 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.927131 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.927191 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.927252 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.927312 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.927372 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.927432 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.927492 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.927551 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.927615 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.927676 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.927735 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.927795 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.927854 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.927914 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.927974 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.928034 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.928093 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.928153 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.928213 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.928272 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.928332 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.928392 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.928451 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.928511 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.928570 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.928629 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.928689 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.928752 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.928813 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.928873 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.928932 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.928992 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.929050 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.929110 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.929169 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.929229 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.929288 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.929348 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.929407 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.929467 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.929543 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.929610 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.929671 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.929730 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.929790 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.929850 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.929915 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.929977 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.930037 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.930097 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.930157 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.930217 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.930277 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.930338 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.930397 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.930458 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.930517 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.930577 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.930637 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.930697 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.930757 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.930817 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.930877 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.930937 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.931002 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.931062 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.931122 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.931182 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.931241 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.931300 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.931360 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.931419 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.931479 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.931538 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.931598 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.931658 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.931718 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.931778 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.931838 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.931898 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.931957 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.932017 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.932077 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.932141 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.932201 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.932261 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.932321 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.932381 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.932440 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.932501 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.932560 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.932620 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.932680 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.932742 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.932803 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.932863 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.932923 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.932984 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.933044 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.933104 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.933164 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.933229 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.933290 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.933351 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.933411 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.933471 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.933545 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.933611 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.933672 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.933732 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.933792 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.933852 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.933912 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.933972 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.934032 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.934092 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.934152 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.934212 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.934272 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.934333 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.934397 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.934458 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.934519 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.934578 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.934639 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.934698 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.934758 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.934818 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.934878 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.934938 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.934998 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.935058 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.935118 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.935178 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.935237 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.935297 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.935358 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.935417 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.935481 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.935542 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.935602 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.935662 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.935721 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.935781 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.935841 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.935904 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.935963 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.936023 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.936082 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.936142 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.936201 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.936260 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.936320 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.936379 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.936439 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.936498 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.936557 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.936622 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.global_step                                           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.936682 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.bias_0                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.936742 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.bias_1                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.936801 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.bias_10                                       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.936861 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.bias_11                                       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.936920 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.bias_12                                       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.936980 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.bias_13                                       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.937039 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.bias_14                                       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.937098 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.bias_15                                       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.937157 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.bias_2                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.937217 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.bias_3                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.937277 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.bias_4                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.937336 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.bias_5                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.937396 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.bias_6                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.937455 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.bias_7                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.937514 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.bias_8                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.937592 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.bias_9                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.937653 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.global_step                                   /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.937717 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.weight_0                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.937778 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.weight_1                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.937838 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.weight_10                                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.937897 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.weight_11                                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.937958 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.weight_12                                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.938018 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.weight_13                                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.938077 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.weight_14                                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.938137 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.weight_15                                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.938197 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.weight_2                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.938257 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.weight_3                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.938317 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.weight_4                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.938377 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.weight_5                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.938437 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.weight_6                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.938496 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.weight_7                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:2
I0930 03:45:11.938556 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.weight_8                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:3
I0930 03:45:11.938616 139840756766528 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.weight_9                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:45:11.938676 139840756766528 py_utils.py:1468] worker 0: lm.stack.global_step                                                  /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:1
I0930 03:45:11.938753 139840756766528 py_utils.py:1484] ==========
I0930 03:45:14.483500 139840756766528 gpipe.py:457] cell 0 input [<tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/GatherV2_1:0' shape=(1024, 1) dtype=int32>, <tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/GatherV2_2:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None, None, None]
I0930 03:45:16.722296 139840756766528 gpipe.py:457] cell 1 input [<tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/encoder_7/add:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/GatherV2_2:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I0930 03:45:19.222780 139840756766528 gpipe.py:457] cell 2 input [<tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/encoder_15/add:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/GatherV2_2:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I0930 03:45:21.361414 139840756766528 gpipe.py:457] cell 3 input [<tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/encoder_23/add:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/GatherV2_2:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I0930 03:45:24.043101 139840756766528 gpipe.py:457] cell 0 input [<tf.Tensor 'arg259:0' shape=(1024, 1) dtype=int32>, <tf.Tensor 'arg260:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None, None, None]
W0930 03:45:25.943703 139840756766528 recurrent.py:886] cell_fn contains stateful ops: [('emb/Assert/Assert', 'Assert'), ('emb/Assert_1/Assert', 'Assert'), ('encoder_0/fflayer_0/encoder_0/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_0/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_0/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_1/fflayer_0/encoder_1/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_1/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_1/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_2/fflayer_0/encoder_2/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_2/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_2/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_3/fflayer_0/encoder_3/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_3/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_3/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_4/fflayer_0/encoder_4/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_4/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_4/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_5/fflayer_0/encoder_5/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_5/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_5/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_6/fflayer_0/encoder_6/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_6/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_6/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_7/fflayer_0/encoder_7/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_7/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_7/fflayer_1/Assert/AssertGuard/Assert', 'Assert')]
I0930 03:45:26.066048 139840756766528 gpipe.py:457] cell 1 input [<tf.Tensor 'arg254:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'arg255:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
W0930 03:45:28.411516 139840756766528 recurrent.py:886] cell_fn contains stateful ops: [('encoder_8/fflayer_0/encoder_8/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_8/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_8/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_9/fflayer_0/encoder_9/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_9/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_9/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_10/fflayer_0/encoder_10/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_10/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_10/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_11/fflayer_0/encoder_11/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_11/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_11/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_12/fflayer_0/encoder_12/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_12/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_12/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_13/fflayer_0/encoder_13/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_13/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_13/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_14/fflayer_0/encoder_14/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_14/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_14/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_15/fflayer_0/encoder_15/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_15/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_15/fflayer_1/Assert/AssertGuard/Assert', 'Assert')]
I0930 03:45:28.533587 139840756766528 gpipe.py:457] cell 2 input [<tf.Tensor 'arg254:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'arg255:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
W0930 03:45:30.386487 139840756766528 recurrent.py:886] cell_fn contains stateful ops: [('encoder_16/fflayer_0/encoder_16/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_16/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_16/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_17/fflayer_0/encoder_17/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_17/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_17/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_18/fflayer_0/encoder_18/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_18/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_18/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_19/fflayer_0/encoder_19/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_19/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_19/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_20/fflayer_0/encoder_20/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_20/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_20/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_21/fflayer_0/encoder_21/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_21/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_21/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_22/fflayer_0/encoder_22/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_22/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_22/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_23/fflayer_0/encoder_23/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_23/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_23/fflayer_1/Assert/AssertGuard/Assert', 'Assert')]
I0930 03:45:30.524434 139840756766528 gpipe.py:457] cell 3 input [<tf.Tensor 'arg286:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'arg287:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
W0930 03:45:32.986070 139840756766528 recurrent.py:886] cell_fn contains stateful ops: [('encoder_24/fflayer_0/encoder_24/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_24/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_24/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_25/fflayer_0/encoder_25/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_25/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_25/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_26/fflayer_0/encoder_26/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_26/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_26/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_27/fflayer_0/encoder_27/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_27/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_27/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_28/fflayer_0/encoder_28/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_28/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_28/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_29/fflayer_0/encoder_29/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_29/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_29/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_30/fflayer_0/encoder_30/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_30/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_30/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_31/fflayer_0/encoder_31/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_31/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_31/fflayer_1/Assert/AssertGuard/Assert', 'Assert')]
I0930 03:45:33.475217 139840756766528 gpipe.py:457] cell 0 input [<tf.Tensor 'arg259:0' shape=(1024, 1) dtype=int32>, <tf.Tensor 'arg260:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None, None, None]
I0930 03:45:36.022311 139840756766528 gpipe.py:457] cell 1 input [<tf.Tensor 'Recv_1:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'Recv_2:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I0930 03:45:39.197518 139840756766528 gpipe.py:457] cell 2 input [<tf.Tensor 'Recv_1:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'Recv_2:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I0930 03:45:42.360502 139840756766528 gpipe.py:457] cell 3 input [<tf.Tensor 'Recv_1:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'Recv_2:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I0930 03:45:44.396104 139840756766528 gpipe.py:548] pipeline output = [<tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/Reshape_2:0' shape=(1024, 32, 32000) dtype=float32>]
I0930 03:45:44.400479 139840756766528 layers.py:2786] Using sparse_softmax_cross_entropy_with_logits() in SimpleFullSoftmax::_FProp2D logits_shape=[32768, 32000]
I0930 03:45:47.376153 139840756766528 gpipe.py:457] cell 0 input [<tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_1/GatherV2_1:0' shape=(1024, 1) dtype=int32>, <tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_1/GatherV2_2:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None, None, None]
I0930 03:45:49.228683 139840756766528 gpipe.py:457] cell 1 input [<tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_1/encoder_7/add:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_1/GatherV2_2:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I0930 03:45:51.036043 139840756766528 gpipe.py:457] cell 2 input [<tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_1/encoder_15/add:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_1/GatherV2_2:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I0930 03:45:53.387630 139840756766528 gpipe.py:457] cell 3 input [<tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_1/encoder_23/add:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_1/GatherV2_2:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I0930 03:45:55.341133 139840756766528 gpipe.py:457] cell 0 input [<tf.Tensor 'arg259:0' shape=(1024, 1) dtype=int32>, <tf.Tensor 'arg260:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None, None, None]
W0930 03:45:57.246290 139840756766528 recurrent.py:886] cell_fn contains stateful ops: [('emb/Assert/Assert', 'Assert'), ('emb/Assert_1/Assert', 'Assert'), ('encoder_0/fflayer_0/encoder_0/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_0/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_0/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_1/fflayer_0/encoder_1/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_1/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_1/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_2/fflayer_0/encoder_2/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_2/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_2/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_3/fflayer_0/encoder_3/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_3/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_3/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_4/fflayer_0/encoder_4/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_4/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_4/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_5/fflayer_0/encoder_5/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_5/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_5/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_6/fflayer_0/encoder_6/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_6/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_6/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_7/fflayer_0/encoder_7/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_7/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_7/fflayer_1/Assert/AssertGuard/Assert', 'Assert')]
I0930 03:45:57.368896 139840756766528 gpipe.py:457] cell 1 input [<tf.Tensor 'arg254:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'arg255:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
W0930 03:45:59.887631 139840756766528 recurrent.py:886] cell_fn contains stateful ops: [('encoder_8/fflayer_0/encoder_8/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_8/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_8/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_9/fflayer_0/encoder_9/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_9/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_9/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_10/fflayer_0/encoder_10/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_10/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_10/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_11/fflayer_0/encoder_11/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_11/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_11/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_12/fflayer_0/encoder_12/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_12/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_12/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_13/fflayer_0/encoder_13/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_13/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_13/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_14/fflayer_0/encoder_14/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_14/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_14/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_15/fflayer_0/encoder_15/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_15/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_15/fflayer_1/Assert/AssertGuard/Assert', 'Assert')]
I0930 03:46:00.012684 139840756766528 gpipe.py:457] cell 2 input [<tf.Tensor 'arg254:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'arg255:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
W0930 03:46:01.877136 139840756766528 recurrent.py:886] cell_fn contains stateful ops: [('encoder_16/fflayer_0/encoder_16/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_16/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_16/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_17/fflayer_0/encoder_17/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_17/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_17/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_18/fflayer_0/encoder_18/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_18/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_18/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_19/fflayer_0/encoder_19/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_19/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_19/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_20/fflayer_0/encoder_20/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_20/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_20/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_21/fflayer_0/encoder_21/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_21/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_21/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_22/fflayer_0/encoder_22/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_22/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_22/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_23/fflayer_0/encoder_23/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_23/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_23/fflayer_1/Assert/AssertGuard/Assert', 'Assert')]
I0930 03:46:02.016138 139840756766528 gpipe.py:457] cell 3 input [<tf.Tensor 'arg286:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'arg287:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
W0930 03:46:03.892152 139840756766528 recurrent.py:886] cell_fn contains stateful ops: [('encoder_24/fflayer_0/encoder_24/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_24/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_24/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_25/fflayer_0/encoder_25/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_25/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_25/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_26/fflayer_0/encoder_26/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_26/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_26/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_27/fflayer_0/encoder_27/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_27/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_27/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_28/fflayer_0/encoder_28/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_28/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_28/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_29/fflayer_0/encoder_29/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_29/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_29/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_30/fflayer_0/encoder_30/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_30/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_30/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_31/fflayer_0/encoder_31/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_31/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_31/fflayer_1/Assert/AssertGuard/Assert', 'Assert')]
I0930 03:46:04.385292 139840756766528 gpipe.py:457] cell 0 input [<tf.Tensor 'arg259:0' shape=(1024, 1) dtype=int32>, <tf.Tensor 'arg260:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None, None, None]
I0930 03:46:07.780525 139840756766528 gpipe.py:457] cell 1 input [<tf.Tensor 'Recv_1:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'Recv_2:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I0930 03:46:10.279795 139840756766528 gpipe.py:457] cell 2 input [<tf.Tensor 'Recv_1:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'Recv_2:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
Traceback (most recent call last):
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 1821, in <module>
    tf.app.run(main)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File "/usr/local/lib/python3.6/dist-packages/absl/app.py", line 299, in run
    _run_main(main, args)
  File "/usr/local/lib/python3.6/dist-packages/absl/app.py", line 250, in _run_main
    sys.exit(main(argv))
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 1817, in main
    RunnerManager(FLAGS.model).Start()
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 1809, in Start
    self.StartRunners(self.CreateRunners(FLAGS.job.split(','), FLAGS.logdir))
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 1557, in CreateRunners
    trial)
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 1506, in _CreateRunner
    return self.Controller(cfg, *common_args)
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 261, in __init__
    self._model.ConstructFPropBPropGraph()
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/base_model.py", line 1220, in ConstructFPropBPropGraph
    self._task.FPropDefaultTheta()
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/base_model.py", line 550, in FPropDefaultTheta
    return self.FProp(self.theta, input_batch)
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/base_model.py", line 464, in FProp
    metrics, per_example = self._FPropSplitInputBatch(theta, input_batch)
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/base_model.py", line 510, in _FPropSplitInputBatch
    metrics, per_example = self.FPropTower(theta_local, batch)
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/tasks/lm/model.py", line 107, in FPropTower
    xent_output, _ = self.lm.FProp(theta.lm, ids, paddings, state0, labels)
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/tasks/lm/layers.py", line 1337, in FProp
    labels.class_weights)
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/layers_with_gpipe.py", line 846, in FProp
    source_task_id, target_task_id)
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/gpipe.py", line 530, in FProp
    unused_acc_state=True)
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/recurrent.py", line 1795, in StackedRecurrent
    acc_states, final = layer.Compute()  # Don't care of final state yet.
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/recurrent.py", line 1474, in Compute
    unused_acc_state=self._unused_acc_state).Compute()
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/recurrent.py", line 825, in Compute
    *Flatten([self._theta, self._state, self._inputs, self._extras]))
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/function.py", line 563, in __call__
    self.add_to_graph(ops.get_default_graph())
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/function.py", line 544, in add_to_graph
    self._create_definition_if_needed()
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/function.py", line 376, in _create_definition_if_needed
    self._create_definition_if_needed_impl()
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/function.py", line 407, in _create_definition_if_needed_impl
    capture_resource_var_by_value=self._capture_resource_var_by_value)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/function.py", line 943, in func_graph_from_py_func
    outputs = func(*func_graph.inputs)
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/recurrent.py", line 586, in Forward
    body=ForwardLoopBody)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/functional_ops.py", line 646, in While
    if body.captured_inputs:
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/function.py", line 360, in captured_inputs
    self._create_definition_if_needed()
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/function.py", line 376, in _create_definition_if_needed
    self._create_definition_if_needed_impl()
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/function.py", line 407, in _create_definition_if_needed_impl
    capture_resource_var_by_value=self._capture_resource_var_by_value)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/function.py", line 943, in func_graph_from_py_func
    outputs = func(*func_graph.inputs)
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/recurrent.py", line 432, in ForwardLoopBody
    Fwd(*Flatten([theta, state0, inputs_t])),
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/function.py", line 563, in __call__
    self.add_to_graph(ops.get_default_graph())
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/function.py", line 544, in add_to_graph
    self._create_definition_if_needed()
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/function.py", line 376, in _create_definition_if_needed
    self._create_definition_if_needed_impl()
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/function.py", line 407, in _create_definition_if_needed_impl
    capture_resource_var_by_value=self._capture_resource_var_by_value)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/function.py", line 943, in func_graph_from_py_func
    outputs = func(*func_graph.inputs)
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/recurrent.py", line 396, in Fwd
    state1, extras = self._cell_fn(theta, state0, inputs)
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/recurrent.py", line 1041, in WrappedCellFn
    state1, extras = cell_fn(theta, state0, *args, **kwargs)
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/recurrent.py", line 1000, in WrappedCellFn
    state1, extras = cell_fn(theta, state0, inputs)
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/recurrent.py", line 1426, in MiddleFn
    state1, extras = self._cell_fn(theta, state0, inputs)
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/gpipe.py", line 461, in CellFn
    outputs = cell.FProp(theta, *fprop_inputs)
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/gpipe.py", line 164, in FProp
    out_args = ch.FProp(th, *out_args)
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/layers_with_gpipe.py", line 223, in FProp
    target_task_id)
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/layers_with_gpipe.py", line 88, in _common_gpipe_transformer_encoder_fprop
    source_segment_id=source_segment_id)
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/layers_with_attention.py", line 566, in FProp
    query_segment_id=source_segment_id)
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/layers_with_attention.py", line 221, in FProp
    query_segment_id=query_segment_id)
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/py_utils.py", line 2970, in Wrapped
    return f(*args, **kwargs)
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/attention.py", line 1533, in ComputeContextVectorWithSource
    per_step_source_padding, query_segment_id)
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/attention.py", line 1087, in ComputeContextVectorWithSource
    per_step_source_padding, [query_batch_size, source_sequence_length])
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/py_utils.py", line 358, in HasShape
    tf.shape(tensor)[:ndims], expected_shape, msg=msg)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/array_ops.py", line 778, in _slice_helper
    stack(strides))
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/dispatch.py", line 180, in wrapper
    return target(*args, **kwargs)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/array_ops.py", line 1143, in stack
    return ops.convert_to_tensor(values, name=name)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py", line 1184, in convert_to_tensor
    return convert_to_tensor_v2(value, dtype, preferred_dtype, name)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py", line 1242, in convert_to_tensor_v2
    as_ref=False)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py", line 1297, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/constant_op.py", line 286, in _constant_tensor_conversion_function
    return constant(v, dtype=dtype, name=name)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/constant_op.py", line 227, in constant
    allow_broadcast=True)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/constant_op.py", line 271, in _constant_impl
    name=name).outputs[0]
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/function.py", line 807, in create_op
    dtypes=dtypes, **kwargs)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py", line 507, in new_func
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py", line 3357, in create_op
    attrs, op_def, compute_device)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py", line 3427, in _create_op_internal
    self._create_op_helper(ret, compute_device=compute_device)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/lock_util.py", line 128, in __exit__
    self._lock.release(self._group_id)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/lock_util.py", line 103, in release
    self._ready.notifyAll()
  File "/usr/lib/python3.6/threading.py", line 364, in notify_all
    self.notify(len(self._waiters))
  File "/usr/lib/python3.6/threading.py", line 347, in notify
    waiters_to_notify = _deque(_islice(all_waiters, n))
KeyboardInterrupt
