WARNING:tensorflow:

  TensorFlow's `tf-nightly` package will soon be updated to TensorFlow 2.0.

  Please upgrade your code to TensorFlow 2.0:
    * https://www.tensorflow.org/beta/guide/migration_guide

  Or install the latest stable TensorFlow 1.X release:
    * `pip install -U "tensorflow==1.*"`

  Otherwise your code may be broken by the change.

  
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/compat/v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
I1002 01:27:07.761457 139650252134208 model_imports.py:47] Importing lingvo.tasks.asr.params
I1002 01:27:07.827199 139650252134208 model_registry.py:124] Registering models from module: lingvo.tasks.asr.params.librispeech
I1002 01:27:07.833331 139650252134208 model_imports.py:47] Importing lingvo.tasks.car.params
I1002 01:27:07.922646 139650252134208 model_registry.py:124] Registering models from module: lingvo.tasks.car.params.kitti
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/waymo_open_dataset/metrics/ops/py_metrics_ops.py:24: The name tf.resource_loader.get_path_to_datafile is deprecated. Please use tf.compat.v1.resource_loader.get_path_to_datafile instead.

W1002 01:27:07.964220 139650252134208 module_wrapper.py:137] From /usr/local/lib/python3.6/dist-packages/waymo_open_dataset/metrics/ops/py_metrics_ops.py:24: The name tf.resource_loader.get_path_to_datafile is deprecated. Please use tf.compat.v1.resource_loader.get_path_to_datafile instead.

I1002 01:27:08.031981 139650252134208 model_registry.py:124] Registering models from module: lingvo.tasks.car.params.waymo
I1002 01:27:08.037773 139650252134208 model_imports.py:47] Importing lingvo.tasks.image.params
I1002 01:27:08.048988 139650252134208 model_registry.py:124] Registering models from module: lingvo.tasks.image.params.mnist
I1002 01:27:08.049156 139650252134208 model_imports.py:47] Importing lingvo.tasks.lm.params
I1002 01:27:08.056140 139650252134208 model_registry.py:124] Registering models from module: lingvo.tasks.lm.params.one_billion_wds
I1002 01:27:08.059428 139650252134208 model_imports.py:47] Importing lingvo.tasks.mt.params
I1002 01:27:08.076488 139650252134208 model_registry.py:124] Registering models from module: lingvo.tasks.mt.params.wmt14_en_de
I1002 01:27:08.084814 139650252134208 model_registry.py:124] Registering models from module: lingvo.tasks.mt.params.wmtm16_en_de
I1002 01:27:08.085921 139650252134208 model_imports.py:47] Importing lingvo.tasks.punctuator.params
I1002 01:27:08.095538 139650252134208 model_registry.py:124] Registering models from module: lingvo.tasks.punctuator.params.codelab
2019-10-02 01:27:08.096186: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-10-02 01:27:08.125351: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300050000 Hz
2019-10-02 01:27:08.126841: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x73e6fb0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2019-10-02 01:27:08.126858: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2019-10-02 01:27:08.132721: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2019-10-02 01:27:08.284584: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-10-02 01:27:08.285635: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x74be330 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2019-10-02 01:27:08.285663: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-SXM2-16GB, Compute Capability 7.0
2019-10-02 01:27:08.286767: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-10-02 01:27:08.287699: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:00:1e.0
2019-10-02 01:27:08.291759: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2019-10-02 01:27:08.378217: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2019-10-02 01:27:08.481342: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2019-10-02 01:27:08.495308: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2019-10-02 01:27:08.581066: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2019-10-02 01:27:08.633668: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2019-10-02 01:27:08.852151: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2019-10-02 01:27:08.852448: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-10-02 01:27:08.853528: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-10-02 01:27:08.854399: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2019-10-02 01:27:08.855646: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2019-10-02 01:27:08.858304: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-10-02 01:27:08.858324: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2019-10-02 01:27:08.858332: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2019-10-02 01:27:08.859543: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-10-02 01:27:08.860505: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-10-02 01:27:08.861411: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:local/replica:0/task:0/device:GPU:0 with 15024 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1e.0, compute capability: 7.0)
2019-10-02 01:27:08.879521: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:258] Initialize GrpcChannelCache for job local -> {0 -> localhost:38095}
2019-10-02 01:27:08.883904: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:365] Started server with target: grpc://localhost:38095
I1002 01:27:08.884238 139650252134208 trainer.py:1515] Job controller start
I1002 01:27:08.992092 139650252134208 base_runner.py:57] ============================================================
I1002 01:27:09.010533 139650252134208 base_runner.py:59] allow_implicit_capture : NoneType
I1002 01:27:09.010736 139650252134208 base_runner.py:59] cls : type/lingvo.core.base_model/SingleTaskModel
I1002 01:27:09.010818 139650252134208 base_runner.py:59] cluster.add_summary : NoneType
I1002 01:27:09.010897 139650252134208 base_runner.py:59] cluster.cls : type/lingvo.core.cluster/_Cluster
I1002 01:27:09.010988 139650252134208 base_runner.py:59] cluster.controller.cpus_per_replica : 1
I1002 01:27:09.011062 139650252134208 base_runner.py:59] cluster.controller.devices_per_split : 1
I1002 01:27:09.011133 139650252134208 base_runner.py:59] cluster.controller.gpus_per_replica : 0
I1002 01:27:09.011209 139650252134208 base_runner.py:59] cluster.controller.name : '/job:local'
I1002 01:27:09.011266 139650252134208 base_runner.py:59] cluster.controller.num_tpu_hosts : 0
I1002 01:27:09.011365 139650252134208 base_runner.py:59] cluster.controller.replicas : 1
I1002 01:27:09.011432 139650252134208 base_runner.py:59] cluster.controller.targets : ''
I1002 01:27:09.011498 139650252134208 base_runner.py:59] cluster.controller.tpus_per_replica : 0
I1002 01:27:09.011562 139650252134208 base_runner.py:59] cluster.decoder.cpus_per_replica : 1
I1002 01:27:09.011630 139650252134208 base_runner.py:59] cluster.decoder.devices_per_split : 1
I1002 01:27:09.011705 139650252134208 base_runner.py:59] cluster.decoder.gpus_per_replica : 1
I1002 01:27:09.011776 139650252134208 base_runner.py:59] cluster.decoder.name : '/job:local'
I1002 01:27:09.011840 139650252134208 base_runner.py:59] cluster.decoder.num_tpu_hosts : 0
I1002 01:27:09.011909 139650252134208 base_runner.py:59] cluster.decoder.replicas : 1
I1002 01:27:09.011980 139650252134208 base_runner.py:59] cluster.decoder.targets : ''
I1002 01:27:09.012054 139650252134208 base_runner.py:59] cluster.decoder.tpus_per_replica : 0
I1002 01:27:09.012111 139650252134208 base_runner.py:59] cluster.evaler.cpus_per_replica : 1
I1002 01:27:09.012186 139650252134208 base_runner.py:59] cluster.evaler.devices_per_split : 1
I1002 01:27:09.012248 139650252134208 base_runner.py:59] cluster.evaler.gpus_per_replica : 1
I1002 01:27:09.012319 139650252134208 base_runner.py:59] cluster.evaler.name : '/job:local'
I1002 01:27:09.012387 139650252134208 base_runner.py:59] cluster.evaler.num_tpu_hosts : 0
I1002 01:27:09.012459 139650252134208 base_runner.py:59] cluster.evaler.replicas : 1
I1002 01:27:09.012515 139650252134208 base_runner.py:59] cluster.evaler.targets : ''
I1002 01:27:09.012584 139650252134208 base_runner.py:59] cluster.evaler.tpus_per_replica : 0
I1002 01:27:09.012652 139650252134208 base_runner.py:59] cluster.input.cpus_per_replica : 1
I1002 01:27:09.012724 139650252134208 base_runner.py:59] cluster.input.devices_per_split : 1
I1002 01:27:09.012780 139650252134208 base_runner.py:59] cluster.input.gpus_per_replica : 0
I1002 01:27:09.012852 139650252134208 base_runner.py:59] cluster.input.name : '/job:local'
I1002 01:27:09.012914 139650252134208 base_runner.py:59] cluster.input.num_tpu_hosts : 0
I1002 01:27:09.012986 139650252134208 base_runner.py:59] cluster.input.replicas : 0
I1002 01:27:09.013048 139650252134208 base_runner.py:59] cluster.input.targets : ''
I1002 01:27:09.013118 139650252134208 base_runner.py:59] cluster.input.tpus_per_replica : 0
I1002 01:27:09.013187 139650252134208 base_runner.py:59] cluster.job : 'controller'
I1002 01:27:09.013260 139650252134208 base_runner.py:59] cluster.logdir : ''
I1002 01:27:09.013323 139650252134208 base_runner.py:59] cluster.mode : 'sync'
I1002 01:27:09.013398 139650252134208 base_runner.py:59] cluster.ps.cpus_per_replica : 1
I1002 01:27:09.013469 139650252134208 base_runner.py:59] cluster.ps.devices_per_split : 1
I1002 01:27:09.013528 139650252134208 base_runner.py:59] cluster.ps.gpus_per_replica : 0
I1002 01:27:09.013598 139650252134208 base_runner.py:59] cluster.ps.name : '/job:local'
I1002 01:27:09.013663 139650252134208 base_runner.py:59] cluster.ps.num_tpu_hosts : 0
I1002 01:27:09.013735 139650252134208 base_runner.py:59] cluster.ps.replicas : 1
I1002 01:27:09.013806 139650252134208 base_runner.py:59] cluster.ps.targets : ''
I1002 01:27:09.013871 139650252134208 base_runner.py:59] cluster.ps.tpus_per_replica : 0
I1002 01:27:09.013937 139650252134208 base_runner.py:59] cluster.task : 0
I1002 01:27:09.013999 139650252134208 base_runner.py:59] cluster.worker.cpus_per_replica : 1
I1002 01:27:09.014068 139650252134208 base_runner.py:59] cluster.worker.devices_per_split : 1
I1002 01:27:09.014144 139650252134208 base_runner.py:59] cluster.worker.gpus_per_replica : 1
I1002 01:27:09.014206 139650252134208 base_runner.py:59] cluster.worker.name : '/job:local'
I1002 01:27:09.014279 139650252134208 base_runner.py:59] cluster.worker.num_tpu_hosts : 0
I1002 01:27:09.014352 139650252134208 base_runner.py:59] cluster.worker.replicas : 1
I1002 01:27:09.014410 139650252134208 base_runner.py:59] cluster.worker.targets : ''
I1002 01:27:09.014486 139650252134208 base_runner.py:59] cluster.worker.tpus_per_replica : 0
I1002 01:27:09.014546 139650252134208 base_runner.py:59] dtype : float32
I1002 01:27:09.014616 139650252134208 base_runner.py:59] fprop_dtype : NoneType
I1002 01:27:09.014685 139650252134208 base_runner.py:59] inference_driver_name : NoneType
I1002 01:27:09.014757 139650252134208 base_runner.py:59] input.allow_implicit_capture : NoneType
I1002 01:27:09.014814 139650252134208 base_runner.py:59] input.bucket_adjust_every_n : 0
I1002 01:27:09.014882 139650252134208 base_runner.py:59] input.bucket_batch_limit : [32]
I1002 01:27:09.014952 139650252134208 base_runner.py:59] input.bucket_upper_bound : [1024]
I1002 01:27:09.015025 139650252134208 base_runner.py:59] input.cls : type/lingvo.tasks.lm.input_generator/LmInput
I1002 01:27:09.015087 139650252134208 base_runner.py:59] input.dtype : float32
I1002 01:27:09.015149 139650252134208 base_runner.py:59] input.file_buffer_size : 10000000
I1002 01:27:09.015216 139650252134208 base_runner.py:59] input.file_datasource : NoneType
I1002 01:27:09.015289 139650252134208 base_runner.py:59] input.file_parallelism : 10
I1002 01:27:09.015366 139650252134208 base_runner.py:59] input.file_pattern : 'text:/tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en*'
I1002 01:27:09.015435 139650252134208 base_runner.py:59] input.file_random_seed : 301
I1002 01:27:09.015503 139650252134208 base_runner.py:59] input.fixed_input_shape : True
I1002 01:27:09.015576 139650252134208 base_runner.py:59] input.flush_every_n : 0
I1002 01:27:09.015635 139650252134208 base_runner.py:59] input.fprop_dtype : NoneType
I1002 01:27:09.015700 139650252134208 base_runner.py:59] input.inference_driver_name : NoneType
I1002 01:27:09.015769 139650252134208 base_runner.py:59] input.is_eval : NoneType
I1002 01:27:09.015840 139650252134208 base_runner.py:59] input.is_inference : NoneType
I1002 01:27:09.015893 139650252134208 base_runner.py:59] input.name : '1bwds_train_set'
I1002 01:27:09.015967 139650252134208 base_runner.py:59] input.num_batcher_threads : 16
I1002 01:27:09.016031 139650252134208 base_runner.py:59] input.num_samples : 0
I1002 01:27:09.016099 139650252134208 base_runner.py:59] input.pad_to_max_seq_length : False
I1002 01:27:09.016164 139650252134208 base_runner.py:59] input.params_init.method : 'xavier'
I1002 01:27:09.016237 139650252134208 base_runner.py:59] input.params_init.scale : 1.000001
I1002 01:27:09.016296 139650252134208 base_runner.py:59] input.params_init.seed : NoneType
I1002 01:27:09.016365 139650252134208 base_runner.py:59] input.random_seed : NoneType
I1002 01:27:09.016434 139650252134208 base_runner.py:59] input.remote.max_inflights_per_target : 32
I1002 01:27:09.016498 139650252134208 base_runner.py:59] input.remote.shardable_batch : False
I1002 01:27:09.016558 139650252134208 base_runner.py:59] input.require_sequential_order : False
I1002 01:27:09.016626 139650252134208 base_runner.py:59] input.skip_lp_regularization : NoneType
I1002 01:27:09.016700 139650252134208 base_runner.py:59] input.source_max_length : NoneType
I1002 01:27:09.016757 139650252134208 base_runner.py:59] input.target_max_length : 1024
I1002 01:27:09.016825 139650252134208 base_runner.py:59] input.tokenizer.allow_implicit_capture : NoneType
I1002 01:27:09.016893 139650252134208 base_runner.py:59] input.tokenizer.append_eos : True
I1002 01:27:09.016965 139650252134208 base_runner.py:59] input.tokenizer.cls : type/lingvo.core.tokenizers/AsciiTokenizer
I1002 01:27:09.017028 139650252134208 base_runner.py:59] input.tokenizer.dtype : float32
I1002 01:27:09.017108 139650252134208 base_runner.py:59] input.tokenizer.fprop_dtype : NoneType
I1002 01:27:09.017174 139650252134208 base_runner.py:59] input.tokenizer.inference_driver_name : NoneType
I1002 01:27:09.017232 139650252134208 base_runner.py:59] input.tokenizer.is_eval : NoneType
I1002 01:27:09.017301 139650252134208 base_runner.py:59] input.tokenizer.is_inference : NoneType
I1002 01:27:09.017376 139650252134208 base_runner.py:59] input.tokenizer.name : 'tokenizer'
I1002 01:27:09.017433 139650252134208 base_runner.py:59] input.tokenizer.pad_to_max_length : True
I1002 01:27:09.017507 139650252134208 base_runner.py:59] input.tokenizer.params_init.method : 'xavier'
I1002 01:27:09.017568 139650252134208 base_runner.py:59] input.tokenizer.params_init.scale : 1.000001
I1002 01:27:09.017638 139650252134208 base_runner.py:59] input.tokenizer.params_init.seed : NoneType
I1002 01:27:09.017700 139650252134208 base_runner.py:59] input.tokenizer.random_seed : NoneType
I1002 01:27:09.017771 139650252134208 base_runner.py:59] input.tokenizer.skip_lp_regularization : NoneType
I1002 01:27:09.017834 139650252134208 base_runner.py:59] input.tokenizer.target_eos_id : 2
I1002 01:27:09.017902 139650252134208 base_runner.py:59] input.tokenizer.target_sos_id : 1
I1002 01:27:09.017971 139650252134208 base_runner.py:59] input.tokenizer.target_unk_id : 0
I1002 01:27:09.018040 139650252134208 base_runner.py:59] input.tokenizer.vn.global_vn : False
I1002 01:27:09.018100 139650252134208 base_runner.py:59] input.tokenizer.vn.per_step_vn : False
I1002 01:27:09.018168 139650252134208 base_runner.py:59] input.tokenizer.vn.scale : NoneType
I1002 01:27:09.018230 139650252134208 base_runner.py:59] input.tokenizer.vn.seed : NoneType
I1002 01:27:09.018296 139650252134208 base_runner.py:59] input.tokenizer.vocab_size : 32000
I1002 01:27:09.018363 139650252134208 base_runner.py:59] input.tokenizer_dict : {}
I1002 01:27:09.018429 139650252134208 base_runner.py:59] input.tpu_infeed_parallelism : 1
I1002 01:27:09.018486 139650252134208 base_runner.py:59] input.use_chaining : False
I1002 01:27:09.018555 139650252134208 base_runner.py:59] input.use_per_host_infeed : False
I1002 01:27:09.018630 139650252134208 base_runner.py:59] input.use_within_batch_mixing : False
I1002 01:27:09.018694 139650252134208 base_runner.py:59] input.vn.global_vn : False
I1002 01:27:09.018754 139650252134208 base_runner.py:59] input.vn.per_step_vn : False
I1002 01:27:09.018827 139650252134208 base_runner.py:59] input.vn.scale : NoneType
I1002 01:27:09.018885 139650252134208 base_runner.py:59] input.vn.seed : NoneType
I1002 01:27:09.018964 139650252134208 base_runner.py:59] is_eval : NoneType
I1002 01:27:09.019033 139650252134208 base_runner.py:59] is_inference : NoneType
I1002 01:27:09.019092 139650252134208 base_runner.py:59] model : 'lm.one_billion_wds.OneBWdsGPipeTransformerWPM@/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/tasks/lm/params/one_billion_wds.py:188'
I1002 01:27:09.019158 139650252134208 base_runner.py:59] name : ''
I1002 01:27:09.019227 139650252134208 base_runner.py:59] params_init.method : 'xavier'
I1002 01:27:09.019313 139650252134208 base_runner.py:59] params_init.scale : 1.000001
I1002 01:27:09.019385 139650252134208 base_runner.py:59] params_init.seed : NoneType
I1002 01:27:09.019455 139650252134208 base_runner.py:59] random_seed : NoneType
I1002 01:27:09.019516 139650252134208 base_runner.py:59] skip_lp_regularization : NoneType
I1002 01:27:09.019578 139650252134208 base_runner.py:59] task.allow_implicit_capture : NoneType
I1002 01:27:09.019646 139650252134208 base_runner.py:59] task.cls : type/lingvo.tasks.lm.model/FixedShapeInputLanguageModel
I1002 01:27:09.019720 139650252134208 base_runner.py:59] task.decoder : NoneType
I1002 01:27:09.019784 139650252134208 base_runner.py:59] task.dtype : float32
I1002 01:27:09.019843 139650252134208 base_runner.py:59] task.encoder : NoneType
I1002 01:27:09.019912 139650252134208 base_runner.py:59] task.eval.decoder_samples_per_summary : 0
I1002 01:27:09.019986 139650252134208 base_runner.py:59] task.eval.load_checkpoint_from : NoneType
I1002 01:27:09.020057 139650252134208 base_runner.py:59] task.eval.samples_per_summary : 0
I1002 01:27:09.020118 139650252134208 base_runner.py:59] task.eval.start_decoder_after : 0
I1002 01:27:09.020195 139650252134208 base_runner.py:59] task.eval.start_eval_after : 0
I1002 01:27:09.020261 139650252134208 base_runner.py:59] task.fprop_dtype : NoneType
I1002 01:27:09.020328 139650252134208 base_runner.py:59] task.inference_driver_name : NoneType
I1002 01:27:09.020396 139650252134208 base_runner.py:59] task.input : NoneType
I1002 01:27:09.020458 139650252134208 base_runner.py:59] task.is_eval : NoneType
I1002 01:27:09.020524 139650252134208 base_runner.py:59] task.is_inference : NoneType
I1002 01:27:09.020597 139650252134208 base_runner.py:59] task.lm.allow_implicit_capture : NoneType
I1002 01:27:09.020658 139650252134208 base_runner.py:59] task.lm.cls : type/lingvo.tasks.lm.layers/GPipeTransformerLm
I1002 01:27:09.020731 139650252134208 base_runner.py:59] task.lm.dtype : float32
I1002 01:27:09.020806 139650252134208 base_runner.py:59] task.lm.fprop_dtype : NoneType
I1002 01:27:09.020867 139650252134208 base_runner.py:59] task.lm.inference_driver_name : NoneType
I1002 01:27:09.020929 139650252134208 base_runner.py:59] task.lm.is_eval : NoneType
I1002 01:27:09.020998 139650252134208 base_runner.py:59] task.lm.is_inference : NoneType
I1002 01:27:09.021071 139650252134208 base_runner.py:59] task.lm.name : 'transformerlm'
I1002 01:27:09.021131 139650252134208 base_runner.py:59] task.lm.params_init.method : 'xavier'
I1002 01:27:09.021206 139650252134208 base_runner.py:59] task.lm.params_init.scale : 1.000001
I1002 01:27:09.021264 139650252134208 base_runner.py:59] task.lm.params_init.seed : NoneType
I1002 01:27:09.021333 139650252134208 base_runner.py:59] task.lm.random_seed : NoneType
I1002 01:27:09.021400 139650252134208 base_runner.py:59] task.lm.skip_lp_regularization : NoneType
I1002 01:27:09.021465 139650252134208 base_runner.py:59] task.lm.stack.allow_implicit_capture : NoneType
I1002 01:27:09.021538 139650252134208 base_runner.py:59] task.lm.stack.batch_dim : 1
I1002 01:27:09.021610 139650252134208 base_runner.py:59] task.lm.stack.cls : type/lingvo.core.layers_with_gpipe/GPipeTransformerStack
I1002 01:27:09.021670 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.allow_implicit_capture : NoneType
I1002 01:27:09.021738 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.cls : type/lingvo.core.layers_with_gpipe/GPipeTransformerLayer
I1002 01:27:09.021804 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.dtype : float32
I1002 01:27:09.021874 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.final_enc_layer : False
I1002 01:27:09.021937 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.fprop_dtype : NoneType
I1002 01:27:09.022009 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.has_aux_atten : True
I1002 01:27:09.022071 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.inference_driver_name : NoneType
I1002 01:27:09.022140 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.is_decoder : False
I1002 01:27:09.022207 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.is_eval : NoneType
I1002 01:27:09.022269 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.is_inference : NoneType
I1002 01:27:09.022336 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.is_transparent : False
I1002 01:27:09.022406 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.allow_implicit_capture : NoneType
I1002 01:27:09.022469 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.cls : type/lingvo.core.layers/LayerNorm
I1002 01:27:09.022537 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.dtype : float32
I1002 01:27:09.022602 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.epsilon : 1e-06
I1002 01:27:09.022671 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.fprop_dtype : NoneType
I1002 01:27:09.022744 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.inference_driver_name : NoneType
I1002 01:27:09.022811 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.input_dim : 0
I1002 01:27:09.022883 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.is_eval : NoneType
I1002 01:27:09.022957 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.is_inference : NoneType
I1002 01:27:09.023011 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.name : ''
I1002 01:27:09.023081 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.params_init.method : 'xavier'
I1002 01:27:09.023149 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.params_init.scale : 1.000001
I1002 01:27:09.023219 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.params_init.seed : NoneType
I1002 01:27:09.023274 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.random_seed : NoneType
I1002 01:27:09.023371 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.skip_lp_regularization : NoneType
I1002 01:27:09.023430 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.vn.global_vn : False
I1002 01:27:09.023503 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.vn.per_step_vn : False
I1002 01:27:09.023569 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.vn.scale : NoneType
I1002 01:27:09.023635 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.vn.seed : NoneType
I1002 01:27:09.023709 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.mask_self_atten : True
I1002 01:27:09.023779 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.name : ''
I1002 01:27:09.023842 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.normalize_output : False
I1002 01:27:09.023910 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.output_dim : 0
I1002 01:27:09.023981 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.packed_input : False
I1002 01:27:09.024054 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.params_init.method : 'xavier'
I1002 01:27:09.024109 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.params_init.scale : 1.000001
I1002 01:27:09.024181 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.params_init.seed : NoneType
I1002 01:27:09.024250 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.random_seed : NoneType
I1002 01:27:09.024319 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.skip_lp_regularization : NoneType
I1002 01:27:09.024374 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.source_dim : 0
I1002 01:27:09.024448 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.add_unnormalized_input : False
I1002 01:27:09.024516 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.allow_implicit_capture : NoneType
I1002 01:27:09.024579 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_dropout_prob : 0.0
I1002 01:27:09.024650 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_hidden_dim : 0
I1002 01:27:09.024712 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.allow_implicit_capture : NoneType
I1002 01:27:09.024779 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.atten_dropout_deterministic : False
I1002 01:27:09.024840 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.atten_dropout_prob : 0.0
I1002 01:27:09.024909 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.cls : type/lingvo.core.attention/MultiHeadedAttention
I1002 01:27:09.024980 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.context_dim : 0
I1002 01:27:09.025052 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.ctx_post_proj_dim : 0
I1002 01:27:09.025107 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.dtype : float32
I1002 01:27:09.025182 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.enable_ctx_post_proj : True
I1002 01:27:09.025252 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.enable_ctx_pre_proj : False
I1002 01:27:09.025321 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.enable_query_proj : True
I1002 01:27:09.025383 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.enable_source_proj : True
I1002 01:27:09.025453 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.fprop_dtype : NoneType
I1002 01:27:09.025519 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.hidden_dim : 0
I1002 01:27:09.025592 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inference_driver_name : NoneType
I1002 01:27:09.025653 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.allow_implicit_capture : NoneType
I1002 01:27:09.025723 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.atten_dropout_deterministic : False
I1002 01:27:09.025780 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.atten_dropout_prob : 0.0
I1002 01:27:09.025852 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.cls : type/lingvo.core.attention/DotProductAttention
I1002 01:27:09.025921 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.dtype : float32
I1002 01:27:09.025985 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.fprop_dtype : NoneType
I1002 01:27:09.026051 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.hidden_dim : 0
I1002 01:27:09.026113 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.inference_driver_name : NoneType
I1002 01:27:09.026183 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.is_eval : NoneType
I1002 01:27:09.026239 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.is_inference : NoneType
I1002 01:27:09.026313 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.name : ''
I1002 01:27:09.026376 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.packed_input : False
I1002 01:27:09.026454 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.params_init.method : 'xavier'
I1002 01:27:09.026513 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.params_init.scale : 1.000001
I1002 01:27:09.026583 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.params_init.seed : NoneType
I1002 01:27:09.026644 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.qdomain.default : NoneType
I1002 01:27:09.026712 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.qdomain.fullyconnected : NoneType
I1002 01:27:09.026779 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.qdomain.softmax : NoneType
I1002 01:27:09.026848 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.query_dim : 0
I1002 01:27:09.026917 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.random_seed : NoneType
I1002 01:27:09.026980 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.skip_lp_regularization : NoneType
I1002 01:27:09.027047 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.source_dim : 0
I1002 01:27:09.027117 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.global_vn : False
I1002 01:27:09.027190 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.per_step_vn : False
I1002 01:27:09.027248 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.scale : NoneType
I1002 01:27:09.027338 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.seed : NoneType
I1002 01:27:09.027408 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.is_eval : NoneType
I1002 01:27:09.027464 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.is_inference : NoneType
I1002 01:27:09.027536 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.name : ''
I1002 01:27:09.027607 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.num_attention_heads : 2
I1002 01:27:09.027669 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.packed_input : False
I1002 01:27:09.027738 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.params_init.method : 'xavier'
I1002 01:27:09.027801 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.params_init.scale : 1.0
I1002 01:27:09.027873 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.params_init.seed : NoneType
I1002 01:27:09.027936 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.qdomain.atten_context : NoneType
I1002 01:27:09.027995 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.qdomain.default : NoneType
I1002 01:27:09.028064 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.qdomain.fullyconnected : NoneType
I1002 01:27:09.028137 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.qdomain.softmax : NoneType
I1002 01:27:09.028198 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.query_dim : 0
I1002 01:27:09.028273 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.random_seed : NoneType
I1002 01:27:09.028329 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.skip_lp_regularization : NoneType
I1002 01:27:09.028398 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.source_dim : 0
I1002 01:27:09.028467 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.use_source_vec_as_attention_value : False
I1002 01:27:09.028541 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.vn.global_vn : False
I1002 01:27:09.028595 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.vn.per_step_vn : False
I1002 01:27:09.028677 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.vn.scale : NoneType
I1002 01:27:09.028733 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.vn.seed : NoneType
I1002 01:27:09.028803 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.cls : type/lingvo.core.layers_with_attention/TransformerAttentionLayer
I1002 01:27:09.028871 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.context_dim : 0
I1002 01:27:09.028941 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.dtype : float32
I1002 01:27:09.029001 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.fprop_dtype : NoneType
I1002 01:27:09.029072 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.inference_driver_name : NoneType
I1002 01:27:09.029147 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.is_eval : NoneType
I1002 01:27:09.029220 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.is_inference : NoneType
I1002 01:27:09.029277 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.is_masked : False
I1002 01:27:09.029344 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.allow_implicit_capture : NoneType
I1002 01:27:09.029414 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.cls : type/lingvo.core.layers/LayerNorm
I1002 01:27:09.029483 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.dtype : float32
I1002 01:27:09.029543 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.epsilon : 1e-06
I1002 01:27:09.029617 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.fprop_dtype : NoneType
I1002 01:27:09.029682 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.inference_driver_name : NoneType
I1002 01:27:09.029756 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.input_dim : 0
I1002 01:27:09.029815 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.is_eval : NoneType
I1002 01:27:09.029883 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.is_inference : NoneType
I1002 01:27:09.029953 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.name : ''
I1002 01:27:09.030017 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.params_init.method : 'xavier'
I1002 01:27:09.030076 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.params_init.scale : 1.000001
I1002 01:27:09.030144 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.params_init.seed : NoneType
I1002 01:27:09.030219 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.random_seed : NoneType
I1002 01:27:09.030277 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.skip_lp_regularization : NoneType
I1002 01:27:09.030352 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.vn.global_vn : False
I1002 01:27:09.030413 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.vn.per_step_vn : False
I1002 01:27:09.030483 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.vn.scale : NoneType
I1002 01:27:09.030552 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.vn.seed : NoneType
I1002 01:27:09.030621 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.mask_type : 'future'
I1002 01:27:09.030678 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.name : ''
I1002 01:27:09.030752 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.num_attention_heads : 8
I1002 01:27:09.030816 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.packed_input : False
I1002 01:27:09.030886 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.params_init.method : 'xavier'
I1002 01:27:09.030939 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.params_init.scale : 1.000001
I1002 01:27:09.031017 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.params_init.seed : NoneType
I1002 01:27:09.031086 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.random_seed : NoneType
I1002 01:27:09.031154 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_prob : 0.0
I1002 01:27:09.031215 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.allow_implicit_capture : NoneType
I1002 01:27:09.031311 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.cls : type/lingvo.core.layers/DropoutLayer
I1002 01:27:09.031385 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.dropout_at_eval : False
I1002 01:27:09.031460 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.dtype : float32
I1002 01:27:09.031524 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.fprop_dtype : NoneType
I1002 01:27:09.031585 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.inference_driver_name : NoneType
I1002 01:27:09.031635 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.is_eval : NoneType
I1002 01:27:09.031690 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.is_inference : NoneType
I1002 01:27:09.031756 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.keep_prob : 1.0
I1002 01:27:09.031808 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.name : ''
I1002 01:27:09.031857 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.noise_shape : NoneType
I1002 01:27:09.031905 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.noise_shape_broadcast_dims : NoneType
I1002 01:27:09.031954 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.params_init.method : 'xavier'
I1002 01:27:09.032003 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.params_init.scale : 1.000001
I1002 01:27:09.032052 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.params_init.seed : NoneType
I1002 01:27:09.032100 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.random_seed : NoneType
I1002 01:27:09.032149 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.skip_lp_regularization : NoneType
I1002 01:27:09.032198 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.global_vn : False
I1002 01:27:09.032247 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.per_step_vn : False
I1002 01:27:09.032296 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.scale : NoneType
I1002 01:27:09.032345 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.seed : NoneType
I1002 01:27:09.032393 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.skip_lp_regularization : NoneType
I1002 01:27:09.032441 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.source_dim : 0
I1002 01:27:09.032489 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.vn.global_vn : False
I1002 01:27:09.032537 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.vn.per_step_vn : False
I1002 01:27:09.032585 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.vn.scale : NoneType
I1002 01:27:09.032634 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.vn.seed : NoneType
I1002 01:27:09.032682 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_aux_atten_tpl : NoneType
I1002 01:27:09.032730 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.activation : 'RELU'
I1002 01:27:09.032778 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.allow_implicit_capture : NoneType
I1002 01:27:09.032827 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.cls : type/lingvo.core.layers_with_attention/TransformerFeedForwardLayer
I1002 01:27:09.032883 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.dtype : float32
I1002 01:27:09.032933 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.activation : ['RELU', 'NONE']
I1002 01:27:09.032982 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.allow_implicit_capture : NoneType
I1002 01:27:09.033030 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.batch_norm : False
I1002 01:27:09.033079 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.bn_fold_weights : NoneType
I1002 01:27:09.033128 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.cls : type/lingvo.core.layers/FeedForwardNet
I1002 01:27:09.033176 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.allow_implicit_capture : NoneType
I1002 01:27:09.033224 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.cls : type/lingvo.core.layers/DropoutLayer
I1002 01:27:09.033272 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.dropout_at_eval : False
I1002 01:27:09.033320 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.dtype : float32
I1002 01:27:09.033369 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.fprop_dtype : NoneType
I1002 01:27:09.033417 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.inference_driver_name : NoneType
I1002 01:27:09.033466 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.is_eval : NoneType
I1002 01:27:09.033518 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.is_inference : NoneType
I1002 01:27:09.033567 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.keep_prob : 1.0
I1002 01:27:09.033617 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.name : ''
I1002 01:27:09.033665 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.noise_shape : NoneType
I1002 01:27:09.033714 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.noise_shape_broadcast_dims : NoneType
I1002 01:27:09.033763 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.params_init.method : 'xavier'
I1002 01:27:09.033811 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.params_init.scale : 1.000001
I1002 01:27:09.033859 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.params_init.seed : NoneType
I1002 01:27:09.033907 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.random_seed : NoneType
I1002 01:27:09.033955 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.skip_lp_regularization : NoneType
I1002 01:27:09.034003 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.global_vn : False
I1002 01:27:09.034051 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.per_step_vn : False
I1002 01:27:09.034100 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.scale : NoneType
I1002 01:27:09.034147 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.seed : NoneType
I1002 01:27:09.034195 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dtype : float32
I1002 01:27:09.034243 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.fprop_dtype : NoneType
I1002 01:27:09.034291 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.inference_driver_name : NoneType
I1002 01:27:09.034344 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.input_dim : 0
I1002 01:27:09.034393 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.is_eval : NoneType
I1002 01:27:09.034441 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.is_inference : NoneType
I1002 01:27:09.034489 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.name : ''
I1002 01:27:09.034537 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.params_init.method : 'xavier'
I1002 01:27:09.034586 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.params_init.scale : 1.000001
I1002 01:27:09.034633 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.params_init.seed : NoneType
I1002 01:27:09.034682 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.activation : 'RELU'
I1002 01:27:09.034730 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.affine_last : False
I1002 01:27:09.034779 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.allow_implicit_capture : NoneType
I1002 01:27:09.034827 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.batch_norm : True
I1002 01:27:09.034875 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.bias_init : 0.0
I1002 01:27:09.034923 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.bn_fold_weights : NoneType
I1002 01:27:09.034970 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.cls : type/lingvo.core.layers/ProjectionLayer
I1002 01:27:09.035018 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.dtype : float32
I1002 01:27:09.035066 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.fprop_dtype : NoneType
I1002 01:27:09.035114 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.has_bias : False
I1002 01:27:09.035163 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.inference_driver_name : NoneType
I1002 01:27:09.035211 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.input_dim : 0
I1002 01:27:09.035259 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.is_eval : NoneType
I1002 01:27:09.035327 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.is_inference : NoneType
I1002 01:27:09.035380 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.name : ''
I1002 01:27:09.035429 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.output_dim : 0
I1002 01:27:09.035478 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.params_init.method : 'xavier'
I1002 01:27:09.035526 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.params_init.scale : 1.000001
I1002 01:27:09.035574 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.params_init.seed : NoneType
I1002 01:27:09.035623 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.qdomain.default : NoneType
I1002 01:27:09.035671 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.random_seed : NoneType
I1002 01:27:09.035719 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.skip_lp_regularization : NoneType
I1002 01:27:09.035773 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.global_vn : False
I1002 01:27:09.035823 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.per_step_vn : False
I1002 01:27:09.035872 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.scale : NoneType
I1002 01:27:09.035920 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.seed : NoneType
I1002 01:27:09.035969 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.weight_norm : False
I1002 01:27:09.036017 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.qdomain.default : NoneType
I1002 01:27:09.036066 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.random_seed : NoneType
I1002 01:27:09.036115 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.skip_connections : NoneType
I1002 01:27:09.036163 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.skip_lp_regularization : NoneType
I1002 01:27:09.036212 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.global_vn : False
I1002 01:27:09.036260 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.per_step_vn : False
I1002 01:27:09.036309 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.scale : NoneType
I1002 01:27:09.036358 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.seed : NoneType
I1002 01:27:09.036406 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.weight_norm : False
I1002 01:27:09.036454 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fprop_dtype : NoneType
I1002 01:27:09.036502 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.hidden_dim : 2048
I1002 01:27:09.036550 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.inference_driver_name : NoneType
I1002 01:27:09.036597 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.input_dim : 0
I1002 01:27:09.036645 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.is_eval : NoneType
I1002 01:27:09.036693 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.is_inference : NoneType
I1002 01:27:09.036741 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.allow_implicit_capture : NoneType
I1002 01:27:09.036789 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.cls : type/lingvo.core.layers/LayerNorm
I1002 01:27:09.036837 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.dtype : float32
I1002 01:27:09.036885 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.epsilon : 1e-06
I1002 01:27:09.036932 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.fprop_dtype : NoneType
I1002 01:27:09.036980 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.inference_driver_name : NoneType
I1002 01:27:09.037028 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.input_dim : 0
I1002 01:27:09.037076 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.is_eval : NoneType
I1002 01:27:09.037125 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.is_inference : NoneType
I1002 01:27:09.037173 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.name : ''
I1002 01:27:09.037222 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.params_init.method : 'xavier'
I1002 01:27:09.037278 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.params_init.scale : 1.000001
I1002 01:27:09.037329 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.params_init.seed : NoneType
I1002 01:27:09.037377 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.random_seed : NoneType
I1002 01:27:09.037426 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.skip_lp_regularization : NoneType
I1002 01:27:09.037474 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.vn.global_vn : False
I1002 01:27:09.037522 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.vn.per_step_vn : False
I1002 01:27:09.037570 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.vn.scale : NoneType
I1002 01:27:09.037619 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.vn.seed : NoneType
I1002 01:27:09.037667 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.name : ''
I1002 01:27:09.037715 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.output_dim : 0
I1002 01:27:09.037763 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.params_init.method : 'xavier'
I1002 01:27:09.037811 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.params_init.scale : 1.000001
I1002 01:27:09.037858 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.params_init.seed : NoneType
I1002 01:27:09.037906 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.random_seed : NoneType
I1002 01:27:09.037954 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.relu_dropout_prob : 0.0
I1002 01:27:09.038002 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.activation : 'RELU'
I1002 01:27:09.038050 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.affine_last : False
I1002 01:27:09.038099 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.allow_implicit_capture : NoneType
I1002 01:27:09.038146 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.batch_norm : True
I1002 01:27:09.038194 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.bias_init : 0.0
I1002 01:27:09.038242 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.bn_fold_weights : NoneType
I1002 01:27:09.038290 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.cls : type/lingvo.core.layers/ProjectionLayer
I1002 01:27:09.038339 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.dtype : float32
I1002 01:27:09.038387 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.fprop_dtype : NoneType
I1002 01:27:09.038434 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.has_bias : False
I1002 01:27:09.038483 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.inference_driver_name : NoneType
I1002 01:27:09.038531 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.input_dim : 0
I1002 01:27:09.038579 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.is_eval : NoneType
I1002 01:27:09.038628 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.is_inference : NoneType
I1002 01:27:09.038676 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.name : ''
I1002 01:27:09.038724 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.output_dim : 0
I1002 01:27:09.038777 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.params_init.method : 'xavier'
I1002 01:27:09.038826 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.params_init.scale : 1.000001
I1002 01:27:09.038875 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.params_init.seed : NoneType
I1002 01:27:09.038923 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.qdomain.default : NoneType
I1002 01:27:09.038971 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.random_seed : NoneType
I1002 01:27:09.039020 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.skip_lp_regularization : NoneType
I1002 01:27:09.039068 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.global_vn : False
I1002 01:27:09.039117 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.per_step_vn : False
I1002 01:27:09.039165 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.scale : NoneType
I1002 01:27:09.039212 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.seed : NoneType
I1002 01:27:09.039260 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.weight_norm : False
I1002 01:27:09.039325 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_prob : 0.0
I1002 01:27:09.039377 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.allow_implicit_capture : NoneType
I1002 01:27:09.039427 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.cls : type/lingvo.core.layers/DropoutLayer
I1002 01:27:09.039475 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.dropout_at_eval : False
I1002 01:27:09.039524 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.dtype : float32
I1002 01:27:09.039572 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.fprop_dtype : NoneType
I1002 01:27:09.039621 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.inference_driver_name : NoneType
I1002 01:27:09.039669 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.is_eval : NoneType
I1002 01:27:09.039717 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.is_inference : NoneType
I1002 01:27:09.039765 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.keep_prob : 1.0
I1002 01:27:09.039814 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.name : ''
I1002 01:27:09.039862 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.noise_shape : NoneType
I1002 01:27:09.039911 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.noise_shape_broadcast_dims : NoneType
I1002 01:27:09.039959 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.params_init.method : 'xavier'
I1002 01:27:09.040007 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.params_init.scale : 1.000001
I1002 01:27:09.040055 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.params_init.seed : NoneType
I1002 01:27:09.040104 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.random_seed : NoneType
I1002 01:27:09.040152 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.skip_lp_regularization : NoneType
I1002 01:27:09.040207 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.global_vn : False
I1002 01:27:09.040257 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.per_step_vn : False
I1002 01:27:09.040305 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.scale : NoneType
I1002 01:27:09.040354 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.seed : NoneType
I1002 01:27:09.040403 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.skip_lp_regularization : NoneType
I1002 01:27:09.040452 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.vn.global_vn : False
I1002 01:27:09.040500 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.vn.per_step_vn : False
I1002 01:27:09.040549 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.vn.scale : NoneType
I1002 01:27:09.040597 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.vn.seed : NoneType
I1002 01:27:09.040646 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.transparent_merger_tpl : NoneType
I1002 01:27:09.040694 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.vn.global_vn : False
I1002 01:27:09.040748 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.vn.per_step_vn : False
I1002 01:27:09.040799 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.vn.scale : NoneType
I1002 01:27:09.040847 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.vn.seed : NoneType
I1002 01:27:09.040896 139650252134208 base_runner.py:59] task.lm.stack.dtype : float32
I1002 01:27:09.040944 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.add_tgt_embedding_layer : False
I1002 01:27:09.040993 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.allow_implicit_capture : NoneType
I1002 01:27:09.041042 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.batch_dim : 1
I1002 01:27:09.041091 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.cls : type/lingvo.core.layers_with_gpipe/GPipeTransformerEmbeddingLayer
I1002 01:27:09.041140 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.dec_task_emb : NoneType
I1002 01:27:09.041189 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.allow_implicit_capture : NoneType
I1002 01:27:09.041238 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.cls : type/lingvo.core.layers/DropoutLayer
I1002 01:27:09.041286 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.dropout_at_eval : False
I1002 01:27:09.041335 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.dtype : float32
I1002 01:27:09.041384 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.fprop_dtype : NoneType
I1002 01:27:09.041433 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.inference_driver_name : NoneType
I1002 01:27:09.041481 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.is_eval : NoneType
I1002 01:27:09.041530 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.is_inference : NoneType
I1002 01:27:09.041579 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.keep_prob : 1.0
I1002 01:27:09.041628 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.name : ''
I1002 01:27:09.041676 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.noise_shape : NoneType
I1002 01:27:09.041725 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.noise_shape_broadcast_dims : NoneType
I1002 01:27:09.041773 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.params_init.method : 'xavier'
I1002 01:27:09.041821 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.params_init.scale : 1.000001
I1002 01:27:09.041870 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.params_init.seed : NoneType
I1002 01:27:09.041923 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.random_seed : NoneType
I1002 01:27:09.041973 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.skip_lp_regularization : NoneType
I1002 01:27:09.042021 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.vn.global_vn : False
I1002 01:27:09.042070 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.vn.per_step_vn : False
I1002 01:27:09.042117 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.vn.scale : NoneType
I1002 01:27:09.042165 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.vn.seed : NoneType
I1002 01:27:09.042213 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.dtype : float32
I1002 01:27:09.042262 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.enc_task_emb : NoneType
I1002 01:27:09.042310 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.fprop_dtype : NoneType
I1002 01:27:09.042357 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.inference_driver_name : NoneType
I1002 01:27:09.042405 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.input_dropout_prob : 0.0
I1002 01:27:09.042453 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.is_eval : NoneType
I1002 01:27:09.042501 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.is_inference : NoneType
I1002 01:27:09.042549 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.is_transparent : False
I1002 01:27:09.042597 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.max_seq_len : 300
I1002 01:27:09.042646 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.name : ''
I1002 01:27:09.042694 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.packed_input : False
I1002 01:27:09.042742 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.params_init.method : 'xavier'
I1002 01:27:09.042791 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.params_init.scale : 1.000001
I1002 01:27:09.042839 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.params_init.seed : NoneType
I1002 01:27:09.042888 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.allow_implicit_capture : NoneType
I1002 01:27:09.042936 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.cls : type/lingvo.core.layers/PositionalEmbeddingLayer
I1002 01:27:09.042985 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.dtype : float32
I1002 01:27:09.043033 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.embedding_dim : 2048
I1002 01:27:09.043081 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.fprop_dtype : NoneType
I1002 01:27:09.043130 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.inference_driver_name : NoneType
I1002 01:27:09.043178 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.is_eval : NoneType
I1002 01:27:09.043227 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.is_inference : NoneType
I1002 01:27:09.043276 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.max_timescale : 10000
I1002 01:27:09.043339 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.min_timescale : 1
I1002 01:27:09.043390 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.name : ''
I1002 01:27:09.043439 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.params_init.method : 'xavier'
I1002 01:27:09.043488 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.params_init.scale : 1.000001
I1002 01:27:09.043536 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.params_init.seed : NoneType
I1002 01:27:09.043587 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.random_seed : NoneType
I1002 01:27:09.043637 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.skip_lp_regularization : NoneType
I1002 01:27:09.043686 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.trainable_scaling : False
I1002 01:27:09.043740 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.trainable_scaling_init : 1.0
I1002 01:27:09.043790 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.vn.global_vn : False
I1002 01:27:09.043838 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.vn.per_step_vn : False
I1002 01:27:09.043887 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.vn.scale : NoneType
I1002 01:27:09.043936 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.vn.seed : NoneType
I1002 01:27:09.043984 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.random_seed : NoneType
I1002 01:27:09.044033 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.skip_lp_regularization : NoneType
I1002 01:27:09.044081 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.allow_implicit_capture : NoneType
I1002 01:27:09.044130 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.apply_pruning : False
I1002 01:27:09.044178 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.cls : type/lingvo.core.layers/SimpleEmbeddingLayer
I1002 01:27:09.044227 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.dtype : float32
I1002 01:27:09.044275 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.embedding_dim : 2048
I1002 01:27:09.044324 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.fprop_dtype : NoneType
I1002 01:27:09.044372 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.fprop_mode : NoneType
I1002 01:27:09.044420 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.inference_driver_name : NoneType
I1002 01:27:09.044467 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.is_eval : NoneType
I1002 01:27:09.044515 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.is_inference : NoneType
I1002 01:27:09.044564 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.name : ''
I1002 01:27:09.044612 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.params_init.method : 'gaussian'
I1002 01:27:09.044660 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.params_init.scale : 0.022097086912079608
I1002 01:27:09.044709 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.params_init.seed : NoneType
I1002 01:27:09.044757 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.qdomain.default : NoneType
I1002 01:27:09.044805 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.random_seed : NoneType
I1002 01:27:09.044853 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.skip_lp_regularization : NoneType
I1002 01:27:09.044902 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.use_3d_weight_tensor : False
I1002 01:27:09.044950 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.use_matmul : False
I1002 01:27:09.044999 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.vn.global_vn : False
I1002 01:27:09.045047 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.vn.per_step_vn : False
I1002 01:27:09.045095 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.vn.scale : NoneType
I1002 01:27:09.045144 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.vn.seed : NoneType
I1002 01:27:09.045192 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.vocab_size : 32000
I1002 01:27:09.045240 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.vn.global_vn : False
I1002 01:27:09.045288 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.vn.per_step_vn : False
I1002 01:27:09.045336 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.vn.scale : NoneType
I1002 01:27:09.045384 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.vn.seed : NoneType
I1002 01:27:09.045433 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.allow_implicit_capture : NoneType
I1002 01:27:09.045485 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.cls : type/lingvo.core.layers_with_gpipe/GPipeTransformerLayer
I1002 01:27:09.045535 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.dtype : float32
I1002 01:27:09.045584 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.final_enc_layer : False
I1002 01:27:09.045633 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.fprop_dtype : NoneType
I1002 01:27:09.045680 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.has_aux_atten : False
I1002 01:27:09.045728 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.inference_driver_name : NoneType
I1002 01:27:09.045776 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.is_decoder : False
I1002 01:27:09.045824 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.is_eval : NoneType
I1002 01:27:09.045872 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.is_inference : NoneType
I1002 01:27:09.045921 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.is_transparent : False
I1002 01:27:09.045969 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.allow_implicit_capture : NoneType
I1002 01:27:09.046016 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.cls : type/lingvo.core.layers/LayerNorm
I1002 01:27:09.046065 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.dtype : float32
I1002 01:27:09.046113 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.epsilon : 1e-06
I1002 01:27:09.046161 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.fprop_dtype : NoneType
I1002 01:27:09.046209 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.inference_driver_name : NoneType
I1002 01:27:09.046256 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.input_dim : 0
I1002 01:27:09.046304 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.is_eval : NoneType
I1002 01:27:09.046353 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.is_inference : NoneType
I1002 01:27:09.046401 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.name : ''
I1002 01:27:09.046449 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.params_init.method : 'xavier'
I1002 01:27:09.046498 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.params_init.scale : 1.000001
I1002 01:27:09.046546 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.params_init.seed : NoneType
I1002 01:27:09.046595 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.random_seed : NoneType
I1002 01:27:09.046644 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.skip_lp_regularization : NoneType
I1002 01:27:09.046692 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.vn.global_vn : False
I1002 01:27:09.046741 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.vn.per_step_vn : False
I1002 01:27:09.046789 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.vn.scale : NoneType
I1002 01:27:09.046837 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.vn.seed : NoneType
I1002 01:27:09.046886 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.mask_self_atten : True
I1002 01:27:09.046934 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.name : ''
I1002 01:27:09.046983 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.normalize_output : False
I1002 01:27:09.047031 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.output_dim : 0
I1002 01:27:09.047080 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.packed_input : False
I1002 01:27:09.047127 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.params_init.method : 'xavier'
I1002 01:27:09.047175 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.params_init.scale : 1.000001
I1002 01:27:09.047223 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.params_init.seed : NoneType
I1002 01:27:09.047277 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.random_seed : NoneType
I1002 01:27:09.047348 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.skip_lp_regularization : NoneType
I1002 01:27:09.047399 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.source_dim : 2048
I1002 01:27:09.047448 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.add_unnormalized_input : False
I1002 01:27:09.047497 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.allow_implicit_capture : NoneType
I1002 01:27:09.047546 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_dropout_prob : 0.0
I1002 01:27:09.047595 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_hidden_dim : 0
I1002 01:27:09.047644 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.allow_implicit_capture : NoneType
I1002 01:27:09.047693 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.atten_dropout_deterministic : False
I1002 01:27:09.047741 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.atten_dropout_prob : 0.0
I1002 01:27:09.047789 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.cls : type/lingvo.core.attention/MultiHeadedAttention
I1002 01:27:09.047838 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.context_dim : 0
I1002 01:27:09.047886 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.ctx_post_proj_dim : 0
I1002 01:27:09.047935 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.dtype : float32
I1002 01:27:09.047984 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.enable_ctx_post_proj : True
I1002 01:27:09.048032 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.enable_ctx_pre_proj : True
I1002 01:27:09.048080 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.enable_query_proj : True
I1002 01:27:09.048129 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.enable_source_proj : True
I1002 01:27:09.048177 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.fprop_dtype : NoneType
I1002 01:27:09.048226 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.hidden_dim : 0
I1002 01:27:09.048275 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inference_driver_name : NoneType
I1002 01:27:09.048323 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.allow_implicit_capture : NoneType
I1002 01:27:09.048372 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.atten_dropout_deterministic : False
I1002 01:27:09.048420 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.atten_dropout_prob : 0.0
I1002 01:27:09.048468 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.cls : type/lingvo.core.attention/DotProductAttention
I1002 01:27:09.048516 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.dtype : float32
I1002 01:27:09.048564 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.fprop_dtype : NoneType
I1002 01:27:09.048612 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.hidden_dim : 0
I1002 01:27:09.048660 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.inference_driver_name : NoneType
I1002 01:27:09.048709 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.is_eval : NoneType
I1002 01:27:09.048757 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.is_inference : NoneType
I1002 01:27:09.048811 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.name : ''
I1002 01:27:09.048861 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.packed_input : False
I1002 01:27:09.048911 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.params_init.method : 'xavier'
I1002 01:27:09.048959 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.params_init.scale : 1.000001
I1002 01:27:09.049008 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.params_init.seed : NoneType
I1002 01:27:09.049057 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.qdomain.default : NoneType
I1002 01:27:09.049105 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.qdomain.fullyconnected : NoneType
I1002 01:27:09.049153 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.qdomain.softmax : NoneType
I1002 01:27:09.049202 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.query_dim : 0
I1002 01:27:09.049250 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.random_seed : NoneType
I1002 01:27:09.049299 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.skip_lp_regularization : NoneType
I1002 01:27:09.049347 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.source_dim : 0
I1002 01:27:09.049396 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.global_vn : False
I1002 01:27:09.049443 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.per_step_vn : False
I1002 01:27:09.049491 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.scale : NoneType
I1002 01:27:09.049539 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.seed : NoneType
I1002 01:27:09.049587 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.is_eval : NoneType
I1002 01:27:09.049635 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.is_inference : NoneType
I1002 01:27:09.049684 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.name : ''
I1002 01:27:09.049732 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.num_attention_heads : 2
I1002 01:27:09.049780 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.packed_input : False
I1002 01:27:09.049828 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.params_init.method : 'xavier'
I1002 01:27:09.049876 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.params_init.scale : 1.0
I1002 01:27:09.049925 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.params_init.seed : NoneType
I1002 01:27:09.049973 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.qdomain.atten_context : NoneType
I1002 01:27:09.050021 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.qdomain.default : NoneType
I1002 01:27:09.050069 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.qdomain.fullyconnected : NoneType
I1002 01:27:09.050118 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.qdomain.softmax : NoneType
I1002 01:27:09.050166 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.query_dim : 0
I1002 01:27:09.050219 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.random_seed : NoneType
I1002 01:27:09.050268 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.skip_lp_regularization : NoneType
I1002 01:27:09.050316 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.source_dim : 0
I1002 01:27:09.050364 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.use_source_vec_as_attention_value : False
I1002 01:27:09.050413 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.vn.global_vn : False
I1002 01:27:09.050461 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.vn.per_step_vn : False
I1002 01:27:09.050509 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.vn.scale : NoneType
I1002 01:27:09.050557 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.vn.seed : NoneType
I1002 01:27:09.050605 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.cls : type/lingvo.core.layers_with_attention/TransformerAttentionLayer
I1002 01:27:09.050653 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.context_dim : 0
I1002 01:27:09.050701 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.dtype : float32
I1002 01:27:09.050749 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.fprop_dtype : NoneType
I1002 01:27:09.050797 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.inference_driver_name : NoneType
I1002 01:27:09.050845 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.is_eval : NoneType
I1002 01:27:09.050893 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.is_inference : NoneType
I1002 01:27:09.050940 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.is_masked : True
I1002 01:27:09.050989 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.allow_implicit_capture : NoneType
I1002 01:27:09.051036 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.cls : type/lingvo.core.layers/LayerNorm
I1002 01:27:09.051084 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.dtype : float32
I1002 01:27:09.051133 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.epsilon : 1e-06
I1002 01:27:09.051181 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.fprop_dtype : NoneType
I1002 01:27:09.051229 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.inference_driver_name : NoneType
I1002 01:27:09.051277 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.input_dim : 0
I1002 01:27:09.051342 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.is_eval : NoneType
I1002 01:27:09.051392 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.is_inference : NoneType
I1002 01:27:09.051440 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.name : ''
I1002 01:27:09.051488 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.params_init.method : 'xavier'
I1002 01:27:09.051536 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.params_init.scale : 1.000001
I1002 01:27:09.051584 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.params_init.seed : NoneType
I1002 01:27:09.051633 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.random_seed : NoneType
I1002 01:27:09.051681 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.skip_lp_regularization : NoneType
I1002 01:27:09.051729 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.vn.global_vn : False
I1002 01:27:09.051788 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.vn.per_step_vn : False
I1002 01:27:09.051838 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.vn.scale : NoneType
I1002 01:27:09.051887 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.vn.seed : NoneType
I1002 01:27:09.051935 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.mask_type : 'future'
I1002 01:27:09.051983 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.name : ''
I1002 01:27:09.052031 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.num_attention_heads : 16
I1002 01:27:09.052080 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.packed_input : False
I1002 01:27:09.052128 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.params_init.method : 'xavier'
I1002 01:27:09.052176 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.params_init.scale : 1.000001
I1002 01:27:09.052225 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.params_init.seed : NoneType
I1002 01:27:09.052273 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.random_seed : NoneType
I1002 01:27:09.052321 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_prob : 0.0
I1002 01:27:09.052369 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.allow_implicit_capture : NoneType
I1002 01:27:09.052417 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.cls : type/lingvo.core.layers/DropoutLayer
I1002 01:27:09.052465 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.dropout_at_eval : False
I1002 01:27:09.052514 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.dtype : float32
I1002 01:27:09.052561 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.fprop_dtype : NoneType
I1002 01:27:09.052610 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.inference_driver_name : NoneType
I1002 01:27:09.052658 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.is_eval : NoneType
I1002 01:27:09.052706 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.is_inference : NoneType
I1002 01:27:09.052754 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.keep_prob : 1.0
I1002 01:27:09.052802 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.name : ''
I1002 01:27:09.052850 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.noise_shape : NoneType
I1002 01:27:09.052898 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.noise_shape_broadcast_dims : NoneType
I1002 01:27:09.052946 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.params_init.method : 'xavier'
I1002 01:27:09.052994 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.params_init.scale : 1.000001
I1002 01:27:09.053043 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.params_init.seed : NoneType
I1002 01:27:09.053091 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.random_seed : NoneType
I1002 01:27:09.053139 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.skip_lp_regularization : NoneType
I1002 01:27:09.053188 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.global_vn : False
I1002 01:27:09.053240 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.per_step_vn : False
I1002 01:27:09.053289 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.scale : NoneType
I1002 01:27:09.053338 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.seed : NoneType
I1002 01:27:09.053386 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.skip_lp_regularization : NoneType
I1002 01:27:09.053433 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.source_dim : 0
I1002 01:27:09.053482 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.vn.global_vn : False
I1002 01:27:09.053530 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.vn.per_step_vn : False
I1002 01:27:09.053577 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.vn.scale : NoneType
I1002 01:27:09.053628 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.vn.seed : NoneType
I1002 01:27:09.053677 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_aux_atten_tpl : NoneType
I1002 01:27:09.053726 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.activation : 'RELU'
I1002 01:27:09.053774 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.allow_implicit_capture : NoneType
I1002 01:27:09.053822 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.cls : type/lingvo.core.layers_with_attention/TransformerFeedForwardLayer
I1002 01:27:09.053871 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.dtype : float32
I1002 01:27:09.053919 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.activation : ['RELU', 'NONE']
I1002 01:27:09.053966 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.allow_implicit_capture : NoneType
I1002 01:27:09.054015 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.batch_norm : False
I1002 01:27:09.054063 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.bn_fold_weights : NoneType
I1002 01:27:09.054111 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.cls : type/lingvo.core.layers/FeedForwardNet
I1002 01:27:09.054158 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.allow_implicit_capture : NoneType
I1002 01:27:09.054206 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.cls : type/lingvo.core.layers/DropoutLayer
I1002 01:27:09.054254 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.dropout_at_eval : False
I1002 01:27:09.054302 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.dtype : float32
I1002 01:27:09.054350 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.fprop_dtype : NoneType
I1002 01:27:09.054398 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.inference_driver_name : NoneType
I1002 01:27:09.054447 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.is_eval : NoneType
I1002 01:27:09.054495 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.is_inference : NoneType
I1002 01:27:09.054543 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.keep_prob : 1.0
I1002 01:27:09.054591 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.name : ''
I1002 01:27:09.054640 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.noise_shape : NoneType
I1002 01:27:09.054688 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.noise_shape_broadcast_dims : NoneType
I1002 01:27:09.054741 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.params_init.method : 'xavier'
I1002 01:27:09.054790 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.params_init.scale : 1.000001
I1002 01:27:09.054838 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.params_init.seed : NoneType
I1002 01:27:09.054886 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.random_seed : NoneType
I1002 01:27:09.054934 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.skip_lp_regularization : NoneType
I1002 01:27:09.054981 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.global_vn : False
I1002 01:27:09.055029 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.per_step_vn : False
I1002 01:27:09.055077 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.scale : NoneType
I1002 01:27:09.055125 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.seed : NoneType
I1002 01:27:09.055174 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dtype : float32
I1002 01:27:09.055222 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.fprop_dtype : NoneType
I1002 01:27:09.055270 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.inference_driver_name : NoneType
I1002 01:27:09.055334 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.input_dim : 0
I1002 01:27:09.055385 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.is_eval : NoneType
I1002 01:27:09.055434 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.is_inference : NoneType
I1002 01:27:09.055483 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.name : ''
I1002 01:27:09.055531 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.params_init.method : 'xavier'
I1002 01:27:09.055579 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.params_init.scale : 1.000001
I1002 01:27:09.055628 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.params_init.seed : NoneType
I1002 01:27:09.055676 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.activation : 'RELU'
I1002 01:27:09.055725 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.affine_last : False
I1002 01:27:09.055773 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.allow_implicit_capture : NoneType
I1002 01:27:09.055822 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.batch_norm : True
I1002 01:27:09.055871 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.bias_init : 0.0
I1002 01:27:09.055919 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.bn_fold_weights : NoneType
I1002 01:27:09.055967 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.cls : type/lingvo.core.layers/ProjectionLayer
I1002 01:27:09.056016 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.dtype : float32
I1002 01:27:09.056064 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.fprop_dtype : NoneType
I1002 01:27:09.056113 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.has_bias : False
I1002 01:27:09.056166 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.inference_driver_name : NoneType
I1002 01:27:09.056216 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.input_dim : 0
I1002 01:27:09.056264 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.is_eval : NoneType
I1002 01:27:09.056313 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.is_inference : NoneType
I1002 01:27:09.056361 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.name : ''
I1002 01:27:09.056410 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.output_dim : 0
I1002 01:27:09.056459 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.params_init.method : 'xavier'
I1002 01:27:09.056508 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.params_init.scale : 1.000001
I1002 01:27:09.056556 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.params_init.seed : NoneType
I1002 01:27:09.056605 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.qdomain.default : NoneType
I1002 01:27:09.056654 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.random_seed : NoneType
I1002 01:27:09.056702 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.skip_lp_regularization : NoneType
I1002 01:27:09.056751 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.global_vn : False
I1002 01:27:09.056799 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.per_step_vn : False
I1002 01:27:09.056847 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.scale : NoneType
I1002 01:27:09.056896 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.seed : NoneType
I1002 01:27:09.056944 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.weight_norm : False
I1002 01:27:09.056993 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.qdomain.default : NoneType
I1002 01:27:09.057041 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.random_seed : NoneType
I1002 01:27:09.057090 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.skip_connections : NoneType
I1002 01:27:09.057139 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.skip_lp_regularization : NoneType
I1002 01:27:09.057187 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.global_vn : False
I1002 01:27:09.057235 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.per_step_vn : False
I1002 01:27:09.057283 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.scale : NoneType
I1002 01:27:09.057331 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.seed : NoneType
I1002 01:27:09.057379 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.weight_norm : False
I1002 01:27:09.057428 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fprop_dtype : NoneType
I1002 01:27:09.057476 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.hidden_dim : 8192
I1002 01:27:09.057526 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.inference_driver_name : NoneType
I1002 01:27:09.057574 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.input_dim : 0
I1002 01:27:09.057627 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.is_eval : NoneType
I1002 01:27:09.057677 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.is_inference : NoneType
I1002 01:27:09.057725 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.allow_implicit_capture : NoneType
I1002 01:27:09.057773 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.cls : type/lingvo.core.layers/LayerNorm
I1002 01:27:09.057821 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.dtype : float32
I1002 01:27:09.057869 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.epsilon : 1e-06
I1002 01:27:09.057917 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.fprop_dtype : NoneType
I1002 01:27:09.057965 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.inference_driver_name : NoneType
I1002 01:27:09.058013 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.input_dim : 0
I1002 01:27:09.058062 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.is_eval : NoneType
I1002 01:27:09.058110 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.is_inference : NoneType
I1002 01:27:09.058158 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.name : ''
I1002 01:27:09.058206 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.params_init.method : 'xavier'
I1002 01:27:09.058255 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.params_init.scale : 1.000001
I1002 01:27:09.058303 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.params_init.seed : NoneType
I1002 01:27:09.058351 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.random_seed : NoneType
I1002 01:27:09.058400 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.skip_lp_regularization : NoneType
I1002 01:27:09.058449 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.vn.global_vn : False
I1002 01:27:09.058497 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.vn.per_step_vn : False
I1002 01:27:09.058546 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.vn.scale : NoneType
I1002 01:27:09.058594 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.vn.seed : NoneType
I1002 01:27:09.058642 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.name : ''
I1002 01:27:09.058691 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.output_dim : 0
I1002 01:27:09.058739 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.params_init.method : 'xavier'
I1002 01:27:09.058787 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.params_init.scale : 1.000001
I1002 01:27:09.058835 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.params_init.seed : NoneType
I1002 01:27:09.058883 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.random_seed : NoneType
I1002 01:27:09.058931 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.relu_dropout_prob : 0.0
I1002 01:27:09.058979 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.activation : 'RELU'
I1002 01:27:09.059028 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.affine_last : False
I1002 01:27:09.059076 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.allow_implicit_capture : NoneType
I1002 01:27:09.059125 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.batch_norm : True
I1002 01:27:09.059177 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.bias_init : 0.0
I1002 01:27:09.059228 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.bn_fold_weights : NoneType
I1002 01:27:09.059276 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.cls : type/lingvo.core.layers/ProjectionLayer
I1002 01:27:09.059345 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.dtype : float32
I1002 01:27:09.059396 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.fprop_dtype : NoneType
I1002 01:27:09.059444 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.has_bias : False
I1002 01:27:09.059493 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.inference_driver_name : NoneType
I1002 01:27:09.059541 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.input_dim : 0
I1002 01:27:09.059590 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.is_eval : NoneType
I1002 01:27:09.059639 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.is_inference : NoneType
I1002 01:27:09.059687 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.name : ''
I1002 01:27:09.059736 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.output_dim : 0
I1002 01:27:09.059786 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.params_init.method : 'xavier'
I1002 01:27:09.059834 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.params_init.scale : 1.000001
I1002 01:27:09.059884 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.params_init.seed : NoneType
I1002 01:27:09.059933 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.qdomain.default : NoneType
I1002 01:27:09.059982 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.random_seed : NoneType
I1002 01:27:09.060030 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.skip_lp_regularization : NoneType
I1002 01:27:09.060079 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.global_vn : False
I1002 01:27:09.060127 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.per_step_vn : False
I1002 01:27:09.060175 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.scale : NoneType
I1002 01:27:09.060224 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.seed : NoneType
I1002 01:27:09.060272 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.weight_norm : False
I1002 01:27:09.060321 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_prob : 0.0
I1002 01:27:09.060369 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.allow_implicit_capture : NoneType
I1002 01:27:09.060418 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.cls : type/lingvo.core.layers/DropoutLayer
I1002 01:27:09.060466 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.dropout_at_eval : False
I1002 01:27:09.060514 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.dtype : float32
I1002 01:27:09.060562 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.fprop_dtype : NoneType
I1002 01:27:09.060610 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.inference_driver_name : NoneType
I1002 01:27:09.060663 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.is_eval : NoneType
I1002 01:27:09.060713 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.is_inference : NoneType
I1002 01:27:09.060761 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.keep_prob : 1.0
I1002 01:27:09.060810 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.name : ''
I1002 01:27:09.060858 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.noise_shape : NoneType
I1002 01:27:09.060906 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.noise_shape_broadcast_dims : NoneType
I1002 01:27:09.060955 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.params_init.method : 'xavier'
I1002 01:27:09.061003 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.params_init.scale : 1.000001
I1002 01:27:09.061051 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.params_init.seed : NoneType
I1002 01:27:09.061099 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.random_seed : NoneType
I1002 01:27:09.061147 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.skip_lp_regularization : NoneType
I1002 01:27:09.061195 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.global_vn : False
I1002 01:27:09.061243 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.per_step_vn : False
I1002 01:27:09.061291 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.scale : NoneType
I1002 01:27:09.061339 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.seed : NoneType
I1002 01:27:09.061387 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.skip_lp_regularization : NoneType
I1002 01:27:09.061435 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.vn.global_vn : False
I1002 01:27:09.061483 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.vn.per_step_vn : False
I1002 01:27:09.061531 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.vn.scale : NoneType
I1002 01:27:09.061579 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.vn.seed : NoneType
I1002 01:27:09.061628 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.transparent_merger_tpl : NoneType
I1002 01:27:09.061676 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.vn.global_vn : False
I1002 01:27:09.061723 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.vn.per_step_vn : False
I1002 01:27:09.061771 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.vn.scale : NoneType
I1002 01:27:09.061819 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.vn.seed : NoneType
I1002 01:27:09.061866 139650252134208 base_runner.py:59] task.lm.stack.fprop_dtype : NoneType
I1002 01:27:09.061914 139650252134208 base_runner.py:59] task.lm.stack.inference_driver_name : NoneType
I1002 01:27:09.061961 139650252134208 base_runner.py:59] task.lm.stack.is_eval : NoneType
I1002 01:27:09.062009 139650252134208 base_runner.py:59] task.lm.stack.is_inference : NoneType
I1002 01:27:09.062057 139650252134208 base_runner.py:59] task.lm.stack.is_transparent : False
I1002 01:27:09.062105 139650252134208 base_runner.py:59] task.lm.stack.label_smoothing : NoneType
I1002 01:27:09.062153 139650252134208 base_runner.py:59] task.lm.stack.model_dim : 2048
I1002 01:27:09.062201 139650252134208 base_runner.py:59] task.lm.stack.name : ''
I1002 01:27:09.062254 139650252134208 base_runner.py:59] task.lm.stack.normalize_encoder : False
I1002 01:27:09.062303 139650252134208 base_runner.py:59] task.lm.stack.num_decoder_layers : 0
I1002 01:27:09.062351 139650252134208 base_runner.py:59] task.lm.stack.num_encoder_layers : 32
I1002 01:27:09.062399 139650252134208 base_runner.py:59] task.lm.stack.num_micro_batches : 32
I1002 01:27:09.062447 139650252134208 base_runner.py:59] task.lm.stack.packed_input : False
I1002 01:27:09.062495 139650252134208 base_runner.py:59] task.lm.stack.params_init.method : 'xavier'
I1002 01:27:09.062543 139650252134208 base_runner.py:59] task.lm.stack.params_init.scale : 1.000001
I1002 01:27:09.062591 139650252134208 base_runner.py:59] task.lm.stack.params_init.seed : NoneType
I1002 01:27:09.062638 139650252134208 base_runner.py:59] task.lm.stack.random_seed : NoneType
I1002 01:27:09.062686 139650252134208 base_runner.py:59] task.lm.stack.skip_lp_regularization : NoneType
I1002 01:27:09.062734 139650252134208 base_runner.py:59] task.lm.stack.softmax_tpl.allow_implicit_capture : NoneType
I1002 01:27:09.062782 139650252134208 base_runner.py:59] task.lm.stack.softmax_tpl.apply_pruning : False
I1002 01:27:09.062830 139650252134208 base_runner.py:59] task.lm.stack.softmax_tpl.chunk_size : 4194
I1002 01:27:09.062877 139650252134208 base_runner.py:59] task.lm.stack.softmax_tpl.cls : type/lingvo.core.layers_with_gpipe/GPipeTransformerSoftmaxLayer
I1002 01:27:09.062926 139650252134208 base_runner.py:59] task.lm.stack.softmax_tpl.dtype : float32
I1002 01:27:09.062974 139650252134208 base_runner.py:59] task.lm.stack.softmax_tpl.fprop_dtype : NoneType
I1002 01:27:09.063022 139650252134208 base_runner.py:59] task.lm.stack.softmax_tpl.inference_driver_name : NoneType
I1002 01:27:09.063070 139650252134208 base_runner.py:59] task.lm.stack.softmax_tpl.input_dim : 2048
I1002 01:27:09.063117 139650252134208 base_runner.py:59] task.lm.stack.softmax_tpl.inputs_from_decoder : False
I1002 01:27:09.063165 139650252134208 base_runner.py:59] task.lm.stack.softmax_tpl.is_eval : NoneType
I1002 01:27:09.063213 139650252134208 base_runner.py:59] task.lm.stack.softmax_tpl.is_inference : NoneType
I1002 01:27:09.063261 139650252134208 base_runner.py:59] task.lm.stack.softmax_tpl.logits_abs_max : NoneType
I1002 01:27:09.063323 139650252134208 base_runner.py:59] task.lm.stack.softmax_tpl.name : ''
I1002 01:27:09.063376 139650252134208 base_runner.py:59] task.lm.stack.softmax_tpl.num_classes : 32000
I1002 01:27:09.063424 139650252134208 base_runner.py:59] task.lm.stack.softmax_tpl.num_sampled : 0
I1002 01:27:09.063473 139650252134208 base_runner.py:59] task.lm.stack.softmax_tpl.num_shards : 16
I1002 01:27:09.063521 139650252134208 base_runner.py:59] task.lm.stack.softmax_tpl.params_init.method : 'xavier'
I1002 01:27:09.063570 139650252134208 base_runner.py:59] task.lm.stack.softmax_tpl.params_init.scale : 1.000001
I1002 01:27:09.063617 139650252134208 base_runner.py:59] task.lm.stack.softmax_tpl.params_init.seed : NoneType
I1002 01:27:09.063666 139650252134208 base_runner.py:59] task.lm.stack.softmax_tpl.qdomain.default : NoneType
I1002 01:27:09.063718 139650252134208 base_runner.py:59] task.lm.stack.softmax_tpl.random_seed : NoneType
I1002 01:27:09.063766 139650252134208 base_runner.py:59] task.lm.stack.softmax_tpl.skip_lp_regularization : NoneType
I1002 01:27:09.063814 139650252134208 base_runner.py:59] task.lm.stack.softmax_tpl.vn.global_vn : False
I1002 01:27:09.063863 139650252134208 base_runner.py:59] task.lm.stack.softmax_tpl.vn.per_step_vn : False
I1002 01:27:09.063910 139650252134208 base_runner.py:59] task.lm.stack.softmax_tpl.vn.scale : NoneType
I1002 01:27:09.063959 139650252134208 base_runner.py:59] task.lm.stack.softmax_tpl.vn.seed : NoneType
I1002 01:27:09.064007 139650252134208 base_runner.py:59] task.lm.stack.splits : [8, 16, 24, 32]
I1002 01:27:09.064055 139650252134208 base_runner.py:59] task.lm.stack.state_dtype : float32
I1002 01:27:09.064103 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_dropout_prob : 0.1
I1002 01:27:09.064152 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.allow_implicit_capture : NoneType
I1002 01:27:09.064205 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.cls : type/lingvo.core.layers_with_gpipe/DeterministicWeightsLayer
I1002 01:27:09.064254 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.allow_implicit_capture : NoneType
I1002 01:27:09.064302 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.cls : type/lingvo.core.layers/DeterministicDropoutLayer
I1002 01:27:09.064351 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.dropout_at_eval : False
I1002 01:27:09.064398 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.dtype : float32
I1002 01:27:09.064446 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.fprop_dtype : NoneType
I1002 01:27:09.064494 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.inference_driver_name : NoneType
I1002 01:27:09.064542 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.is_eval : NoneType
I1002 01:27:09.064590 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.is_inference : NoneType
I1002 01:27:09.064639 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.keep_prob : 1.0
I1002 01:27:09.064686 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.name : ''
I1002 01:27:09.064734 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.noise_shape : NoneType
I1002 01:27:09.064782 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.noise_shape_broadcast_dims : NoneType
I1002 01:27:09.064831 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.params_init.method : 'xavier'
I1002 01:27:09.064878 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.params_init.scale : 1.000001
I1002 01:27:09.064925 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.params_init.seed : NoneType
I1002 01:27:09.064973 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.random_seed : NoneType
I1002 01:27:09.065021 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.skip_lp_regularization : NoneType
I1002 01:27:09.065068 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.vn.global_vn : False
I1002 01:27:09.065116 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.vn.per_step_vn : False
I1002 01:27:09.065164 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.vn.scale : NoneType
I1002 01:27:09.065212 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.vn.seed : NoneType
I1002 01:27:09.065259 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dtype : float32
I1002 01:27:09.065307 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.fprop_dtype : NoneType
I1002 01:27:09.065355 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.global_weight_scale : 1.0
I1002 01:27:09.065402 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.inference_driver_name : NoneType
I1002 01:27:09.065450 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.is_eval : NoneType
I1002 01:27:09.065498 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.is_inference : NoneType
I1002 01:27:09.065545 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.minimal_prob : 0.0
I1002 01:27:09.065593 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.name : ''
I1002 01:27:09.065641 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.num_sources : 0
I1002 01:27:09.065689 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.params_init.method : 'xavier'
I1002 01:27:09.065741 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.params_init.scale : 1.000001
I1002 01:27:09.065790 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.params_init.seed : NoneType
I1002 01:27:09.065838 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.random_seed : NoneType
I1002 01:27:09.065886 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.skip_lp_regularization : NoneType
I1002 01:27:09.065933 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.vn.global_vn : False
I1002 01:27:09.065981 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.vn.per_step_vn : False
I1002 01:27:09.066029 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.vn.scale : NoneType
I1002 01:27:09.066076 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.vn.seed : NoneType
I1002 01:27:09.066124 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.weighted_merger_dropout_prob : 0.0
I1002 01:27:09.066172 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.weighted_merger_softmax : True
I1002 01:27:09.066219 139650252134208 base_runner.py:59] task.lm.stack.use_pipelined_embeddings : True
I1002 01:27:09.066267 139650252134208 base_runner.py:59] task.lm.stack.vn.global_vn : False
I1002 01:27:09.066314 139650252134208 base_runner.py:59] task.lm.stack.vn.per_step_vn : False
I1002 01:27:09.066362 139650252134208 base_runner.py:59] task.lm.stack.vn.scale : NoneType
I1002 01:27:09.066410 139650252134208 base_runner.py:59] task.lm.stack.vn.seed : NoneType
I1002 01:27:09.066458 139650252134208 base_runner.py:59] task.lm.vn.global_vn : False
I1002 01:27:09.066505 139650252134208 base_runner.py:59] task.lm.vn.per_step_vn : False
I1002 01:27:09.066553 139650252134208 base_runner.py:59] task.lm.vn.scale : NoneType
I1002 01:27:09.066600 139650252134208 base_runner.py:59] task.lm.vn.seed : NoneType
I1002 01:27:09.066648 139650252134208 base_runner.py:59] task.lm.vocab_size : 32000
I1002 01:27:09.066695 139650252134208 base_runner.py:59] task.name : '1bwds_wpm_level_lm'
I1002 01:27:09.066742 139650252134208 base_runner.py:59] task.online_encoder : NoneType
I1002 01:27:09.066790 139650252134208 base_runner.py:59] task.params_init.method : 'xavier'
I1002 01:27:09.066838 139650252134208 base_runner.py:59] task.params_init.scale : 1.000001
I1002 01:27:09.066885 139650252134208 base_runner.py:59] task.params_init.seed : NoneType
I1002 01:27:09.066933 139650252134208 base_runner.py:59] task.random_seed : NoneType
I1002 01:27:09.066980 139650252134208 base_runner.py:59] task.skip_lp_regularization : NoneType
I1002 01:27:09.067028 139650252134208 base_runner.py:59] task.train.bprop_variable_exclusion : NoneType
I1002 01:27:09.067076 139650252134208 base_runner.py:59] task.train.bprop_variable_filter : NoneType
I1002 01:27:09.067123 139650252134208 base_runner.py:59] task.train.clip_gradient_norm_to_value : 0.0
I1002 01:27:09.067172 139650252134208 base_runner.py:59] task.train.clip_gradient_single_norm_to_value : 0.0
I1002 01:27:09.067220 139650252134208 base_runner.py:59] task.train.colocate_gradients_with_ops : True
I1002 01:27:09.067268 139650252134208 base_runner.py:59] task.train.early_stop.metric_history.jobname : 'eval_dev'
I1002 01:27:09.067330 139650252134208 base_runner.py:59] task.train.early_stop.metric_history.local_filesystem : False
I1002 01:27:09.067381 139650252134208 base_runner.py:59] task.train.early_stop.metric_history.logdir : ''
I1002 01:27:09.067429 139650252134208 base_runner.py:59] task.train.early_stop.metric_history.metric : 'log_pplx'
I1002 01:27:09.067477 139650252134208 base_runner.py:59] task.train.early_stop.metric_history.minimize : True
I1002 01:27:09.067525 139650252134208 base_runner.py:59] task.train.early_stop.metric_history.name : 'MetricHistory'
I1002 01:27:09.067573 139650252134208 base_runner.py:59] task.train.early_stop.metric_history.tfevent_file : False
I1002 01:27:09.067626 139650252134208 base_runner.py:59] task.train.early_stop.min_steps : 0
I1002 01:27:09.067676 139650252134208 base_runner.py:59] task.train.early_stop.name : 'EarlyStop'
I1002 01:27:09.067723 139650252134208 base_runner.py:59] task.train.early_stop.tolerance : 0.0
I1002 01:27:09.067771 139650252134208 base_runner.py:59] task.train.early_stop.verbose : True
I1002 01:27:09.067819 139650252134208 base_runner.py:59] task.train.early_stop.window : 0
I1002 01:27:09.067867 139650252134208 base_runner.py:59] task.train.ema_decay : 0.0
I1002 01:27:09.067915 139650252134208 base_runner.py:59] task.train.enqueue_max_steps : -1
I1002 01:27:09.067963 139650252134208 base_runner.py:59] task.train.gate_gradients : False
I1002 01:27:09.068011 139650252134208 base_runner.py:59] task.train.grad_aggregation_method : 1
I1002 01:27:09.068059 139650252134208 base_runner.py:59] task.train.grad_norm_to_clip_to_zero : 0.0
I1002 01:27:09.068106 139650252134208 base_runner.py:59] task.train.grad_norm_tracker : NoneType
I1002 01:27:09.068154 139650252134208 base_runner.py:59] task.train.init_from_checkpoint_rules : {}
I1002 01:27:09.068201 139650252134208 base_runner.py:59] task.train.l1_regularizer_weight : NoneType
I1002 01:27:09.068248 139650252134208 base_runner.py:59] task.train.l2_regularizer_weight : 1e-06
I1002 01:27:09.068296 139650252134208 base_runner.py:59] task.train.learner : NoneType
I1002 01:27:09.068344 139650252134208 base_runner.py:59] task.train.learning_rate : 0.5
I1002 01:27:09.068391 139650252134208 base_runner.py:59] task.train.lr_schedule.allow_implicit_capture : NoneType
I1002 01:27:09.068439 139650252134208 base_runner.py:59] task.train.lr_schedule.cls : type/lingvo.core.schedule/TransformerLearningRateSchedule
I1002 01:27:09.068486 139650252134208 base_runner.py:59] task.train.lr_schedule.decay_end : NoneType
I1002 01:27:09.068533 139650252134208 base_runner.py:59] task.train.lr_schedule.dtype : float32
I1002 01:27:09.068581 139650252134208 base_runner.py:59] task.train.lr_schedule.fprop_dtype : NoneType
I1002 01:27:09.068628 139650252134208 base_runner.py:59] task.train.lr_schedule.inference_driver_name : NoneType
I1002 01:27:09.068675 139650252134208 base_runner.py:59] task.train.lr_schedule.is_eval : NoneType
I1002 01:27:09.068722 139650252134208 base_runner.py:59] task.train.lr_schedule.is_inference : NoneType
I1002 01:27:09.068769 139650252134208 base_runner.py:59] task.train.lr_schedule.model_dim : 2048
I1002 01:27:09.068817 139650252134208 base_runner.py:59] task.train.lr_schedule.name : 'LRSched'
I1002 01:27:09.068864 139650252134208 base_runner.py:59] task.train.lr_schedule.params_init.method : 'xavier'
I1002 01:27:09.068911 139650252134208 base_runner.py:59] task.train.lr_schedule.params_init.scale : 1.000001
I1002 01:27:09.068959 139650252134208 base_runner.py:59] task.train.lr_schedule.params_init.seed : NoneType
I1002 01:27:09.069006 139650252134208 base_runner.py:59] task.train.lr_schedule.random_seed : NoneType
I1002 01:27:09.069053 139650252134208 base_runner.py:59] task.train.lr_schedule.skip_lp_regularization : NoneType
I1002 01:27:09.069100 139650252134208 base_runner.py:59] task.train.lr_schedule.vn.global_vn : False
I1002 01:27:09.069147 139650252134208 base_runner.py:59] task.train.lr_schedule.vn.per_step_vn : False
I1002 01:27:09.069195 139650252134208 base_runner.py:59] task.train.lr_schedule.vn.scale : NoneType
I1002 01:27:09.069242 139650252134208 base_runner.py:59] task.train.lr_schedule.vn.seed : NoneType
I1002 01:27:09.069290 139650252134208 base_runner.py:59] task.train.lr_schedule.warmup_steps : 40000
I1002 01:27:09.069337 139650252134208 base_runner.py:59] task.train.lr_schedule.worker_replicas : 1
I1002 01:27:09.069385 139650252134208 base_runner.py:59] task.train.max_lstm_gradient_norm : 0.0
I1002 01:27:09.069432 139650252134208 base_runner.py:59] task.train.max_steps : 4000000
I1002 01:27:09.069480 139650252134208 base_runner.py:59] task.train.optimizer.allow_implicit_capture : NoneType
I1002 01:27:09.069527 139650252134208 base_runner.py:59] task.train.optimizer.beta1 : 0.9
I1002 01:27:09.069581 139650252134208 base_runner.py:59] task.train.optimizer.beta2 : 0.997
I1002 01:27:09.069630 139650252134208 base_runner.py:59] task.train.optimizer.cls : type/lingvo.core.optimizer/Adam
I1002 01:27:09.069678 139650252134208 base_runner.py:59] task.train.optimizer.dtype : float32
I1002 01:27:09.069726 139650252134208 base_runner.py:59] task.train.optimizer.epsilon : 1e-09
I1002 01:27:09.069774 139650252134208 base_runner.py:59] task.train.optimizer.fprop_dtype : NoneType
I1002 01:27:09.069821 139650252134208 base_runner.py:59] task.train.optimizer.inference_driver_name : NoneType
I1002 01:27:09.069869 139650252134208 base_runner.py:59] task.train.optimizer.is_eval : NoneType
I1002 01:27:09.069916 139650252134208 base_runner.py:59] task.train.optimizer.is_inference : NoneType
I1002 01:27:09.069963 139650252134208 base_runner.py:59] task.train.optimizer.name : 'Adam'
I1002 01:27:09.070011 139650252134208 base_runner.py:59] task.train.optimizer.params_init.method : 'xavier'
I1002 01:27:09.070059 139650252134208 base_runner.py:59] task.train.optimizer.params_init.scale : 1.000001
I1002 01:27:09.070107 139650252134208 base_runner.py:59] task.train.optimizer.params_init.seed : NoneType
I1002 01:27:09.070154 139650252134208 base_runner.py:59] task.train.optimizer.random_seed : NoneType
I1002 01:27:09.070201 139650252134208 base_runner.py:59] task.train.optimizer.skip_lp_regularization : NoneType
I1002 01:27:09.070249 139650252134208 base_runner.py:59] task.train.optimizer.vn.global_vn : False
I1002 01:27:09.070296 139650252134208 base_runner.py:59] task.train.optimizer.vn.per_step_vn : False
I1002 01:27:09.070343 139650252134208 base_runner.py:59] task.train.optimizer.vn.scale : NoneType
I1002 01:27:09.070391 139650252134208 base_runner.py:59] task.train.optimizer.vn.seed : NoneType
I1002 01:27:09.070438 139650252134208 base_runner.py:59] task.train.pruning_hparams_dict : NoneType
I1002 01:27:09.070486 139650252134208 base_runner.py:59] task.train.save_interval_seconds : 600
I1002 01:27:09.070534 139650252134208 base_runner.py:59] task.train.save_keep_checkpoint_every_n_hours : 0.5
I1002 01:27:09.070582 139650252134208 base_runner.py:59] task.train.save_max_to_keep : 100
I1002 01:27:09.070629 139650252134208 base_runner.py:59] task.train.start_up_delay_steps : 200
I1002 01:27:09.070677 139650252134208 base_runner.py:59] task.train.sum_loss_across_tokens_in_batch : False
I1002 01:27:09.070725 139650252134208 base_runner.py:59] task.train.summary_interval_steps : 100
I1002 01:27:09.070773 139650252134208 base_runner.py:59] task.train.tpu_steps_per_loop : 100
I1002 01:27:09.070826 139650252134208 base_runner.py:59] task.train.vn_start_step : 20000
I1002 01:27:09.070874 139650252134208 base_runner.py:59] task.train.vn_std : 0.0
I1002 01:27:09.070921 139650252134208 base_runner.py:59] task.vn.global_vn : False
I1002 01:27:09.070969 139650252134208 base_runner.py:59] task.vn.per_step_vn : False
I1002 01:27:09.071017 139650252134208 base_runner.py:59] task.vn.scale : NoneType
I1002 01:27:09.071065 139650252134208 base_runner.py:59] task.vn.seed : NoneType
I1002 01:27:09.071112 139650252134208 base_runner.py:59] train.early_stop.metric_history.jobname : 'eval_dev'
I1002 01:27:09.071160 139650252134208 base_runner.py:59] train.early_stop.metric_history.local_filesystem : False
I1002 01:27:09.071208 139650252134208 base_runner.py:59] train.early_stop.metric_history.logdir : ''
I1002 01:27:09.071257 139650252134208 base_runner.py:59] train.early_stop.metric_history.metric : 'log_pplx'
I1002 01:27:09.071321 139650252134208 base_runner.py:59] train.early_stop.metric_history.minimize : True
I1002 01:27:09.071375 139650252134208 base_runner.py:59] train.early_stop.metric_history.name : 'MetricHistory'
I1002 01:27:09.071424 139650252134208 base_runner.py:59] train.early_stop.metric_history.tfevent_file : False
I1002 01:27:09.071473 139650252134208 base_runner.py:59] train.early_stop.min_steps : 0
I1002 01:27:09.071521 139650252134208 base_runner.py:59] train.early_stop.name : 'EarlyStop'
I1002 01:27:09.071574 139650252134208 base_runner.py:59] train.early_stop.tolerance : 0.0
I1002 01:27:09.071624 139650252134208 base_runner.py:59] train.early_stop.verbose : True
I1002 01:27:09.071672 139650252134208 base_runner.py:59] train.early_stop.window : 0
I1002 01:27:09.071721 139650252134208 base_runner.py:59] train.ema_decay : 0.0
I1002 01:27:09.071769 139650252134208 base_runner.py:59] train.enqueue_max_steps : -1
I1002 01:27:09.071818 139650252134208 base_runner.py:59] train.init_from_checkpoint_rules : {}
I1002 01:27:09.071866 139650252134208 base_runner.py:59] train.max_steps : 4000000
I1002 01:27:09.071915 139650252134208 base_runner.py:59] train.save_interval_seconds : 600
I1002 01:27:09.071964 139650252134208 base_runner.py:59] train.save_keep_checkpoint_every_n_hours : 0.5
I1002 01:27:09.072012 139650252134208 base_runner.py:59] train.save_max_to_keep : 100
I1002 01:27:09.072060 139650252134208 base_runner.py:59] train.start_up_delay_steps : 200
I1002 01:27:09.072108 139650252134208 base_runner.py:59] train.summary_interval_steps : 100
I1002 01:27:09.072157 139650252134208 base_runner.py:59] train.tpu_steps_per_loop : 100
I1002 01:27:09.072206 139650252134208 base_runner.py:59] vn.global_vn : False
I1002 01:27:09.072254 139650252134208 base_runner.py:59] vn.per_step_vn : False
I1002 01:27:09.072303 139650252134208 base_runner.py:59] vn.scale : NoneType
I1002 01:27:09.072351 139650252134208 base_runner.py:59] vn.seed : NoneType
I1002 01:27:09.072399 139650252134208 base_runner.py:59] 
I1002 01:27:09.072516 139650252134208 base_runner.py:60] ============================================================
I1002 01:27:09.076078 139650252134208 base_runner.py:106] Starting ...
I1002 01:27:09.077049 139650252134208 cluster.py:497] _LeastLoadedPlacer : ['/job:local/replica:0/task:0/device:CPU:0']
I1002 01:27:09.091204 139650252134208 cluster.py:515] Place variable global_step on /job:local/replica:0/task:0/device:CPU:0 8
I1002 01:27:09.105244 139650252134208 base_model.py:1093] Training parameters for <class 'lingvo.core.base_model.SingleTaskModel'>: {
  early_stop: {
    metric_history: {
"eval_dev"
      local_filesystem: False
"/tmp/mnist/log"
"log_pplx"
      minimize: True
"MetricHistory"
      tfevent_file: False
    }
    min_steps: 0
"EarlyStop"
    tolerance: 0.0
    verbose: True
    window: 0
  }
  ema_decay: 0.0
  enqueue_max_steps: -1
  init_from_checkpoint_rules: {}
  max_steps: 4000000
  save_interval_seconds: 600
  save_keep_checkpoint_every_n_hours: 0.5
  save_max_to_keep: 100
  start_up_delay_steps: 200
  summary_interval_steps: 100
  tpu_steps_per_loop: 100
}
I1002 01:27:09.121560 139650252134208 base_model.py:301] input_params: {
  allow_implicit_capture: None
  bucket_adjust_every_n: 0
  bucket_batch_limit: [32]
  bucket_upper_bound: [1024]
  cls: <class 'lingvo.tasks.lm.input_generator.LmInput'>
  dtype: <dtype: 'float32'>
  file_buffer_size: 10000000
  file_datasource: None
  file_parallelism: 10
"text:/tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en*"
  file_random_seed: 301
  fixed_input_shape: True
  flush_every_n: 0
  fprop_dtype: None
  inference_driver_name: None
  is_eval: None
  is_inference: None
"1bwds_train_set"
  num_batcher_threads: 16
  num_samples: 0
  pad_to_max_seq_length: False
  params_init: {
"xavier"
    scale: 1.000001
    seed: None
  }
  random_seed: None
  remote: {
    max_inflights_per_target: 32
    shardable_batch: False
  }
  require_sequential_order: False
  skip_lp_regularization: None
  source_max_length: None
  target_max_length: 1024
  tokenizer: {
    allow_implicit_capture: None
    append_eos: True
    cls: <class 'lingvo.core.tokenizers.AsciiTokenizer'>
    dtype: <dtype: 'float32'>
    fprop_dtype: None
    inference_driver_name: None
    is_eval: None
    is_inference: None
"tokenizer"
    pad_to_max_length: True
    params_init: {
"xavier"
      scale: 1.000001
      seed: None
    }
    random_seed: None
    skip_lp_regularization: None
    target_eos_id: 2
    target_sos_id: 1
    target_unk_id: 0
    vn: {
      global_vn: False
      per_step_vn: False
      scale: None
      seed: None
    }
    vocab_size: 32000
  }
  tokenizer_dict: {}
  tpu_infeed_parallelism: 1
  use_chaining: False
  use_per_host_infeed: False
  use_within_batch_mixing: False
  vn: {
    global_vn: False
    per_step_vn: False
    scale: None
    seed: None
  }
}
I1002 01:27:09.125567 139650252134208 base_input_generator.py:624] bucket_batch_limit [32]
I1002 01:27:09.191762 139650252134208 learner.py:351] Ignoring legacy param start_up_delay_steps=200 for optimization program
I1002 01:27:09.191932 139650252134208 learner.py:351] Ignoring legacy param max_steps=4000000 for optimization program
I1002 01:27:09.192000 139650252134208 learner.py:351] Ignoring legacy param tpu_steps_per_loop=100 for optimization program
I1002 01:27:09.192057 139650252134208 learner.py:351] Ignoring legacy param vn_start_step=20000 for optimization program
I1002 01:27:09.192111 139650252134208 learner.py:351] Ignoring legacy param vn_std=0.0 for optimization program
I1002 01:27:09.192166 139650252134208 learner.py:351] Ignoring legacy param early_stop={
  metric_history: {
"eval_dev"
    local_filesystem: False
"/tmp/mnist/log"
"log_pplx"
    minimize: True
"MetricHistory"
    tfevent_file: False
  }
  min_steps: 0
"EarlyStop"
  tolerance: 0.0
  verbose: True
  window: 0
} for optimization program
I1002 01:27:09.192288 139650252134208 learner.py:351] Ignoring legacy param ema_decay=0.0 for optimization program
I1002 01:27:09.192344 139650252134208 learner.py:351] Ignoring legacy param init_from_checkpoint_rules={} for optimization program
I1002 01:27:09.192396 139650252134208 learner.py:351] Ignoring legacy param pruning_hparams_dict=None for optimization program
I1002 01:27:09.192446 139650252134208 learner.py:351] Ignoring legacy param enqueue_max_steps=-1 for optimization program
I1002 01:27:09.192495 139650252134208 learner.py:351] Ignoring legacy param save_interval_seconds=600 for optimization program
I1002 01:27:09.192544 139650252134208 learner.py:351] Ignoring legacy param save_max_to_keep=100 for optimization program
I1002 01:27:09.192592 139650252134208 learner.py:351] Ignoring legacy param save_keep_checkpoint_every_n_hours=0.5 for optimization program
I1002 01:27:09.192645 139650252134208 learner.py:351] Ignoring legacy param summary_interval_steps=100 for optimization program
I1002 01:27:09.192694 139650252134208 learner.py:351] Ignoring legacy param learner=None for optimization program
I1002 01:27:09.192779 139650252134208 learner.py:351] Ignoring legacy param max_lstm_gradient_norm=0.0 for optimization program
I1002 01:27:09.192832 139650252134208 learner.py:351] Ignoring legacy param sum_loss_across_tokens_in_batch=False for optimization program
I1002 01:27:09.193295 139650252134208 learner.py:356] Learner params: allow_implicit_capture : NoneType
I1002 01:27:09.193377 139650252134208 learner.py:356] Learner params: bprop_variable_exclusion : NoneType
I1002 01:27:09.193440 139650252134208 learner.py:356] Learner params: bprop_variable_filter : NoneType
I1002 01:27:09.193504 139650252134208 learner.py:356] Learner params: clip_gradient_norm_to_value : 0.0
I1002 01:27:09.193562 139650252134208 learner.py:356] Learner params: clip_gradient_single_norm_to_value : 0.0
I1002 01:27:09.193614 139650252134208 learner.py:356] Learner params: cls : type/lingvo.core.learner/Learner
I1002 01:27:09.193665 139650252134208 learner.py:356] Learner params: colocate_gradients_with_ops : True
I1002 01:27:09.193715 139650252134208 learner.py:356] Learner params: dtype : float32
I1002 01:27:09.193766 139650252134208 learner.py:356] Learner params: fprop_dtype : NoneType
I1002 01:27:09.193817 139650252134208 learner.py:356] Learner params: gate_gradients : False
I1002 01:27:09.193868 139650252134208 learner.py:356] Learner params: grad_aggregation_method : 1
I1002 01:27:09.193919 139650252134208 learner.py:356] Learner params: grad_norm_to_clip_to_zero : 0.0
I1002 01:27:09.193969 139650252134208 learner.py:356] Learner params: grad_norm_tracker : NoneType
I1002 01:27:09.194028 139650252134208 learner.py:356] Learner params: inference_driver_name : NoneType
I1002 01:27:09.194079 139650252134208 learner.py:356] Learner params: is_eval : NoneType
I1002 01:27:09.194130 139650252134208 learner.py:356] Learner params: is_inference : NoneType
I1002 01:27:09.194180 139650252134208 learner.py:356] Learner params: l1_regularizer_weight : NoneType
I1002 01:27:09.194230 139650252134208 learner.py:356] Learner params: l2_regularizer_weight : 1e-06
I1002 01:27:09.194279 139650252134208 learner.py:356] Learner params: learning_rate : 0.5
I1002 01:27:09.194329 139650252134208 learner.py:356] Learner params: lr_schedule.allow_implicit_capture : NoneType
I1002 01:27:09.194380 139650252134208 learner.py:356] Learner params: lr_schedule.cls : type/lingvo.core.schedule/TransformerLearningRateSchedule
I1002 01:27:09.194430 139650252134208 learner.py:356] Learner params: lr_schedule.decay_end : NoneType
I1002 01:27:09.194479 139650252134208 learner.py:356] Learner params: lr_schedule.dtype : float32
I1002 01:27:09.194529 139650252134208 learner.py:356] Learner params: lr_schedule.fprop_dtype : NoneType
I1002 01:27:09.194579 139650252134208 learner.py:356] Learner params: lr_schedule.inference_driver_name : NoneType
I1002 01:27:09.194629 139650252134208 learner.py:356] Learner params: lr_schedule.is_eval : NoneType
I1002 01:27:09.194678 139650252134208 learner.py:356] Learner params: lr_schedule.is_inference : NoneType
I1002 01:27:09.194728 139650252134208 learner.py:356] Learner params: lr_schedule.model_dim : 2048
I1002 01:27:09.194778 139650252134208 learner.py:356] Learner params: lr_schedule.name : 'LRSched'
I1002 01:27:09.194827 139650252134208 learner.py:356] Learner params: lr_schedule.params_init.method : 'xavier'
I1002 01:27:09.194876 139650252134208 learner.py:356] Learner params: lr_schedule.params_init.scale : 1.000001
I1002 01:27:09.194925 139650252134208 learner.py:356] Learner params: lr_schedule.params_init.seed : NoneType
I1002 01:27:09.194973 139650252134208 learner.py:356] Learner params: lr_schedule.random_seed : NoneType
I1002 01:27:09.195022 139650252134208 learner.py:356] Learner params: lr_schedule.skip_lp_regularization : NoneType
I1002 01:27:09.195071 139650252134208 learner.py:356] Learner params: lr_schedule.vn.global_vn : False
I1002 01:27:09.195120 139650252134208 learner.py:356] Learner params: lr_schedule.vn.per_step_vn : False
I1002 01:27:09.195170 139650252134208 learner.py:356] Learner params: lr_schedule.vn.scale : NoneType
I1002 01:27:09.195220 139650252134208 learner.py:356] Learner params: lr_schedule.vn.seed : NoneType
I1002 01:27:09.195270 139650252134208 learner.py:356] Learner params: lr_schedule.warmup_steps : 40000
I1002 01:27:09.195342 139650252134208 learner.py:356] Learner params: lr_schedule.worker_replicas : 1
I1002 01:27:09.195395 139650252134208 learner.py:356] Learner params: name : 'loss'
I1002 01:27:09.195445 139650252134208 learner.py:356] Learner params: optimizer.allow_implicit_capture : NoneType
I1002 01:27:09.195495 139650252134208 learner.py:356] Learner params: optimizer.beta1 : 0.9
I1002 01:27:09.195545 139650252134208 learner.py:356] Learner params: optimizer.beta2 : 0.997
I1002 01:27:09.195595 139650252134208 learner.py:356] Learner params: optimizer.cls : type/lingvo.core.optimizer/Adam
I1002 01:27:09.195646 139650252134208 learner.py:356] Learner params: optimizer.dtype : float32
I1002 01:27:09.195696 139650252134208 learner.py:356] Learner params: optimizer.epsilon : 1e-09
I1002 01:27:09.195745 139650252134208 learner.py:356] Learner params: optimizer.fprop_dtype : NoneType
I1002 01:27:09.195795 139650252134208 learner.py:356] Learner params: optimizer.inference_driver_name : NoneType
I1002 01:27:09.195845 139650252134208 learner.py:356] Learner params: optimizer.is_eval : NoneType
I1002 01:27:09.195894 139650252134208 learner.py:356] Learner params: optimizer.is_inference : NoneType
I1002 01:27:09.195944 139650252134208 learner.py:356] Learner params: optimizer.name : 'Adam'
I1002 01:27:09.195994 139650252134208 learner.py:356] Learner params: optimizer.params_init.method : 'xavier'
I1002 01:27:09.196049 139650252134208 learner.py:356] Learner params: optimizer.params_init.scale : 1.000001
I1002 01:27:09.196101 139650252134208 learner.py:356] Learner params: optimizer.params_init.seed : NoneType
I1002 01:27:09.196151 139650252134208 learner.py:356] Learner params: optimizer.random_seed : NoneType
I1002 01:27:09.196199 139650252134208 learner.py:356] Learner params: optimizer.skip_lp_regularization : NoneType
I1002 01:27:09.196249 139650252134208 learner.py:356] Learner params: optimizer.vn.global_vn : False
I1002 01:27:09.196299 139650252134208 learner.py:356] Learner params: optimizer.vn.per_step_vn : False
I1002 01:27:09.196349 139650252134208 learner.py:356] Learner params: optimizer.vn.scale : NoneType
I1002 01:27:09.196399 139650252134208 learner.py:356] Learner params: optimizer.vn.seed : NoneType
I1002 01:27:09.196448 139650252134208 learner.py:356] Learner params: params_init.method : 'xavier'
I1002 01:27:09.196497 139650252134208 learner.py:356] Learner params: params_init.scale : 1.000001
I1002 01:27:09.196546 139650252134208 learner.py:356] Learner params: params_init.seed : NoneType
I1002 01:27:09.196595 139650252134208 learner.py:356] Learner params: random_seed : NoneType
I1002 01:27:09.196645 139650252134208 learner.py:356] Learner params: skip_lp_regularization : NoneType
I1002 01:27:09.196695 139650252134208 learner.py:356] Learner params: vn.global_vn : False
I1002 01:27:09.196744 139650252134208 learner.py:356] Learner params: vn.per_step_vn : False
I1002 01:27:09.196794 139650252134208 learner.py:356] Learner params: vn.scale : NoneType
I1002 01:27:09.196845 139650252134208 learner.py:356] Learner params: vn.seed : NoneType
I1002 01:27:09.196894 139650252134208 learner.py:356] Learner params: 
I1002 01:27:09.596833 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var on /job:local/replica:0/task:0/device:CPU:0 262144008
I1002 01:27:09.598814 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var:0 shape=(32000, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:09.618042 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 278921224
I1002 01:27:09.619984 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:09.622631 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 278929416
I1002 01:27:09.624256 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:09.631096 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 295706632
I1002 01:27:09.633016 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:09.635650 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 295714824
I1002 01:27:09.637258 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:09.644185 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 312492040
I1002 01:27:09.646086 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:09.648643 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 312500232
I1002 01:27:09.650221 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:09.657181 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 329277448
I1002 01:27:09.659097 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:09.661768 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 329285640
I1002 01:27:09.663394 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:09.667520 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 329286152
I1002 01:27:09.669128 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:09.672959 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 329294344
I1002 01:27:09.674561 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:09.677219 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 329302536
I1002 01:27:09.678829 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:27:09.688487 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:27:09.694727 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 396411400
I1002 01:27:09.696714 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:09.699252 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 396444168
I1002 01:27:09.700879 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:27:09.702802 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:27:09.709049 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 463553032
I1002 01:27:09.710939 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:09.713582 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 463561224
I1002 01:27:09.715212 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:09.719884 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 463569416
I1002 01:27:09.721487 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:09.724174 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 463577608
I1002 01:27:09.725779 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:09.746201 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 480354824
I1002 01:27:09.748127 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:09.750667 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 480363016
I1002 01:27:09.752417 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:09.759284 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 497140232
I1002 01:27:09.761199 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:09.763870 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 497148424
I1002 01:27:09.765481 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:09.772388 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 513925640
I1002 01:27:09.774828 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:09.777405 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 513933832
I1002 01:27:09.779025 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:09.786021 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 530711048
I1002 01:27:09.787971 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:09.790536 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 530719240
I1002 01:27:09.792305 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:09.795974 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 530719752
I1002 01:27:09.797597 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:09.801550 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 530727944
I1002 01:27:09.803158 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:09.805817 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 530736136
I1002 01:27:09.807469 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:27:09.817096 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:27:09.823287 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 597845000
I1002 01:27:09.825276 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:09.827866 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 597877768
I1002 01:27:09.829490 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:27:09.831941 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:27:09.838143 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 664986632
I1002 01:27:09.840075 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:09.842703 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 664994824
I1002 01:27:09.844355 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:09.849016 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 665003016
I1002 01:27:09.850627 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:09.853327 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 665011208
I1002 01:27:09.854928 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:09.875222 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 681788424
I1002 01:27:09.877161 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:09.879733 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 681796616
I1002 01:27:09.881961 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:09.888871 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 698573832
I1002 01:27:09.890767 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:09.893432 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 698582024
I1002 01:27:09.895119 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:09.902149 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 715359240
I1002 01:27:09.904138 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:09.906684 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 715367432
I1002 01:27:09.908338 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:09.915294 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 732144648
I1002 01:27:09.917237 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:09.919856 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 732152840
I1002 01:27:09.921600 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:09.925300 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 732153352
I1002 01:27:09.926923 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:09.931070 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 732161544
I1002 01:27:09.932722 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:09.935403 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 732169736
I1002 01:27:09.937035 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:27:09.947354 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:27:09.953578 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 799278600
I1002 01:27:09.955589 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:09.958153 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 799311368
I1002 01:27:09.959794 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:27:09.961763 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:27:09.968018 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 866420232
I1002 01:27:09.969929 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:09.972623 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 866428424
I1002 01:27:09.974242 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:09.978968 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 866436616
I1002 01:27:09.980628 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:09.983337 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 866444808
I1002 01:27:09.984964 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.182663 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 883222024
I1002 01:27:10.184803 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.187525 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 883230216
I1002 01:27:10.189159 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.196166 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 900007432
I1002 01:27:10.198178 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.200827 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 900015624
I1002 01:27:10.202463 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.209515 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 916792840
I1002 01:27:10.211457 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.214029 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 916801032
I1002 01:27:10.215817 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.222787 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 933578248
I1002 01:27:10.224735 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.227485 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 933586440
I1002 01:27:10.229135 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.232852 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 933586952
I1002 01:27:10.234508 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.238474 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 933595144
I1002 01:27:10.240808 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.243401 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 933603336
I1002 01:27:10.245048 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:27:10.254871 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:27:10.261179 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 1000712200
I1002 01:27:10.263108 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.265710 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 1000744968
I1002 01:27:10.267365 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:27:10.269332 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:27:10.275558 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 1067853832
I1002 01:27:10.277545 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.280148 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 1067862024
I1002 01:27:10.281804 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.286516 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1067870216
I1002 01:27:10.288350 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.290917 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1067878408
I1002 01:27:10.292590 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.313291 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 1084655624
I1002 01:27:10.315207 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.317806 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1084663816
I1002 01:27:10.319582 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.326514 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 1101441032
I1002 01:27:10.328450 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.331098 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1101449224
I1002 01:27:10.332771 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.339758 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 1118226440
I1002 01:27:10.341739 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.344325 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1118234632
I1002 01:27:10.345980 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.353385 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 1135011848
I1002 01:27:10.355327 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.358041 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1135020040
I1002 01:27:10.359721 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.363419 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 1135020552
I1002 01:27:10.365094 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.369035 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1135028744
I1002 01:27:10.370680 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.373396 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1135036936
I1002 01:27:10.375037 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:27:10.384838 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:27:10.391179 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 1202145800
I1002 01:27:10.393198 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.395797 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 1202178568
I1002 01:27:10.397452 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:27:10.399439 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:27:10.406206 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 1269287432
I1002 01:27:10.408212 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.410946 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 1269295624
I1002 01:27:10.412615 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.417329 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1269303816
I1002 01:27:10.418960 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.421703 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1269312008
I1002 01:27:10.423367 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.443579 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 1286089224
I1002 01:27:10.445727 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.448307 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1286097416
I1002 01:27:10.450065 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.457581 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 1302874632
I1002 01:27:10.459543 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.462234 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1302882824
I1002 01:27:10.463922 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.470906 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 1319660040
I1002 01:27:10.472982 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.475583 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1319668232
I1002 01:27:10.477244 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.484273 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 1336445448
I1002 01:27:10.486211 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.488852 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1336453640
I1002 01:27:10.490633 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.494381 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 1336454152
I1002 01:27:10.496083 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.500056 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1336462344
I1002 01:27:10.501718 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.504432 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1336470536
I1002 01:27:10.506088 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:27:10.516421 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:27:10.522690 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 1403579400
I1002 01:27:10.524713 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.527327 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 1403612168
I1002 01:27:10.528999 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:27:10.531001 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:27:10.537253 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 1470721032
I1002 01:27:10.539183 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.541874 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 1470729224
I1002 01:27:10.543552 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.548296 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1470737416
I1002 01:27:10.549942 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.552706 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1470745608
I1002 01:27:10.554394 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.575215 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 1487522824
I1002 01:27:10.577182 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.579792 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1487531016
I1002 01:27:10.581571 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.588541 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 1504308232
I1002 01:27:10.590497 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.593213 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1504316424
I1002 01:27:10.594869 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.601835 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 1521093640
I1002 01:27:10.603889 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.606475 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1521101832
I1002 01:27:10.608160 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.615162 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 1537879048
I1002 01:27:10.617135 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.619778 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1537887240
I1002 01:27:10.621551 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.625329 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 1537887752
I1002 01:27:10.626997 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.631024 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1537895944
I1002 01:27:10.632704 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.635889 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1537904136
I1002 01:27:10.637551 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:27:10.647488 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:27:10.653724 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 1605013000
I1002 01:27:10.655759 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.658401 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 1605045768
I1002 01:27:10.660117 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:27:10.662127 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:27:10.668399 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 1672154632
I1002 01:27:10.670346 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.673043 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 1672162824
I1002 01:27:10.674725 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.679513 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1672171016
I1002 01:27:10.681168 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.683923 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1672179208
I1002 01:27:10.685586 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.706362 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 1688956424
I1002 01:27:10.708334 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.710908 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1688964616
I1002 01:27:10.712705 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.719690 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 1705741832
I1002 01:27:10.721652 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.724380 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1705750024
I1002 01:27:10.726042 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.733074 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 1722527240
I1002 01:27:10.735089 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.737726 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1722535432
I1002 01:27:10.739415 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.746866 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 1739312648
I1002 01:27:10.748849 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.751520 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1739320840
I1002 01:27:10.753297 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.757077 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 1739321352
I1002 01:27:10.758742 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.762754 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1739329544
I1002 01:27:10.764433 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.767144 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1739337736
I1002 01:27:10.768843 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:27:10.778686 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:27:10.785018 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 1806446600
I1002 01:27:10.787025 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.789664 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 1806479368
I1002 01:27:10.791357 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:27:10.793392 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:27:10.800213 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 1873588232
I1002 01:27:10.802161 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.804871 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 1873596424
I1002 01:27:10.806532 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.811344 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1873604616
I1002 01:27:10.813021 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.815773 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1873612808
I1002 01:27:10.817426 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.892385 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 1890390024
I1002 01:27:10.894475 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.897232 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1890398216
I1002 01:27:10.898891 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.905899 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 1907175432
I1002 01:27:10.908409 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.911011 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1907183624
I1002 01:27:10.912723 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.919798 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 1923960840
I1002 01:27:10.921785 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.924444 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1923969032
I1002 01:27:10.926223 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.933247 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 1940746248
I1002 01:27:10.935199 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.937969 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1940754440
I1002 01:27:10.939690 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.943509 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 1940754952
I1002 01:27:10.945199 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.949274 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1940763144
I1002 01:27:10.950952 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:10.953699 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1940771336
I1002 01:27:10.955399 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:27:10.965398 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:27:10.972400 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 2007880200
I1002 01:27:11.173525 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.176878 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 2007912968
I1002 01:27:11.178628 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:27:11.180814 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:27:11.187268 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 2075021832
I1002 01:27:11.189312 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.192070 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 2075030024
I1002 01:27:11.193760 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.198734 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2075038216
I1002 01:27:11.200432 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.203202 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2075046408
I1002 01:27:11.204928 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.226366 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 2091823624
I1002 01:27:11.228425 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.231154 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2091831816
I1002 01:27:11.232855 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.239952 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 2108609032
I1002 01:27:11.242003 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.244679 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2108617224
I1002 01:27:11.246366 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.253526 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 2125394440
I1002 01:27:11.255543 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.258176 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2125402632
I1002 01:27:11.259987 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.267019 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 2142179848
I1002 01:27:11.269049 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.271901 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2142188040
I1002 01:27:11.273590 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.277463 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 2142188552
I1002 01:27:11.279137 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.283315 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2142196744
I1002 01:27:11.285017 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.287770 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2142204936
I1002 01:27:11.289467 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:27:11.300284 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:27:11.306703 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 2209313800
I1002 01:27:11.308799 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.311471 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 2209346568
I1002 01:27:11.313171 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:27:11.315249 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:27:11.321638 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 2276455432
I1002 01:27:11.323642 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.326352 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 2276463624
I1002 01:27:11.328069 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.332957 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2276471816
I1002 01:27:11.334643 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.337424 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2276480008
I1002 01:27:11.339099 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.360433 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 2293257224
I1002 01:27:11.362409 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.365049 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2293265416
I1002 01:27:11.366840 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.373814 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 2310042632
I1002 01:27:11.375826 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.378545 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2310050824
I1002 01:27:11.380249 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.387227 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 2326828040
I1002 01:27:11.389318 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.391965 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2326836232
I1002 01:27:11.393662 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.400758 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 2343613448
I1002 01:27:11.402724 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.405394 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2343621640
I1002 01:27:11.407695 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.411520 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 2343622152
I1002 01:27:11.413210 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.417323 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2343630344
I1002 01:27:11.418990 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.421735 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2343638536
I1002 01:27:11.423449 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:27:11.433350 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:27:11.439695 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 2410747400
I1002 01:27:11.441731 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.444377 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 2410780168
I1002 01:27:11.446062 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:27:11.448143 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:27:11.454440 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 2477889032
I1002 01:27:11.456447 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.459234 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 2477897224
I1002 01:27:11.460958 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.465831 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2477905416
I1002 01:27:11.467540 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.470795 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2477913608
I1002 01:27:11.472974 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.493452 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 2494690824
I1002 01:27:11.495455 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.498073 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2494699016
I1002 01:27:11.499891 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.506855 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 2511476232
I1002 01:27:11.508866 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.511715 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2511484424
I1002 01:27:11.513396 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.520898 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 2528261640
I1002 01:27:11.522935 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.525611 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2528269832
I1002 01:27:11.527284 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.534364 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 2545047048
I1002 01:27:11.536364 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.539013 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2545055240
I1002 01:27:11.540833 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.544786 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 2545055752
I1002 01:27:11.546489 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.550582 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2545063944
I1002 01:27:11.552287 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.555024 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2545072136
I1002 01:27:11.556749 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:27:11.566702 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:27:11.573022 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 2612181000
I1002 01:27:11.575631 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.578301 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 2612213768
I1002 01:27:11.580029 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:27:11.582103 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:27:11.588396 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 2679322632
I1002 01:27:11.590373 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.593090 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 2679330824
I1002 01:27:11.594784 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.599713 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2679339016
I1002 01:27:11.601387 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.604162 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2679347208
I1002 01:27:11.605856 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.626845 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 2696124424
I1002 01:27:11.628864 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.631518 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2696132616
I1002 01:27:11.633322 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.640365 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 2712909832
I1002 01:27:11.642352 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.645108 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2712918024
I1002 01:27:11.646799 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.653839 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 2729695240
I1002 01:27:11.655898 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.658533 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2729703432
I1002 01:27:11.660247 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.667349 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 2746480648
I1002 01:27:11.669342 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.672035 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2746488840
I1002 01:27:11.673836 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.677681 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 2746489352
I1002 01:27:11.679399 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.683520 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2746497544
I1002 01:27:11.685198 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.687974 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2746505736
I1002 01:27:11.689678 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:27:11.700262 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:27:11.706613 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 2813614600
I1002 01:27:11.708694 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.711346 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 2813647368
I1002 01:27:11.713042 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:27:11.715131 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:27:11.721480 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 2880756232
I1002 01:27:11.723493 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.726237 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 2880764424
I1002 01:27:11.727988 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.732863 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2880772616
I1002 01:27:11.734595 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.737417 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2880780808
I1002 01:27:11.739103 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.760098 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 2897558024
I1002 01:27:11.762077 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.764743 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2897566216
I1002 01:27:11.766563 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.773602 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 2914343432
I1002 01:27:11.775615 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.778377 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2914351624
I1002 01:27:11.780096 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.787094 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 2931128840
I1002 01:27:11.789177 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.791856 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2931137032
I1002 01:27:11.793546 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.800644 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 2947914248
I1002 01:27:11.802635 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.805336 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2947922440
I1002 01:27:11.807631 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.811600 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 2947922952
I1002 01:27:11.813293 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.817440 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2947931144
I1002 01:27:11.819140 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.821893 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2947939336
I1002 01:27:11.823610 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:27:11.833608 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:27:11.839978 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 3015048200
I1002 01:27:11.842040 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.844717 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 3015080968
I1002 01:27:11.846431 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:27:11.848562 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:27:11.854860 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 3082189832
I1002 01:27:11.856877 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.859618 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 3082198024
I1002 01:27:11.861324 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.866242 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3082206216
I1002 01:27:11.867982 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.871271 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3082214408
I1002 01:27:11.872990 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.893569 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 3098991624
I1002 01:27:11.895597 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.898247 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3098999816
I1002 01:27:11.900102 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.907193 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 3115777032
I1002 01:27:11.909240 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.912019 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3115785224
I1002 01:27:11.913721 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.921332 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 3132562440
I1002 01:27:11.923441 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.926108 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3132570632
I1002 01:27:11.927829 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.934890 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 3149347848
I1002 01:27:11.936938 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.939654 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3149356040
I1002 01:27:11.941470 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.945343 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 3149356552
I1002 01:27:11.947071 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.951254 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3149364744
I1002 01:27:11.952988 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.955770 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3149372936
I1002 01:27:11.957487 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:27:11.967529 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:27:11.973879 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 3216481800
I1002 01:27:11.976531 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.979252 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 3216514568
I1002 01:27:11.980979 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:27:11.983095 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:27:11.989454 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 3283623432
I1002 01:27:11.991477 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:11.994227 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 3283631624
I1002 01:27:11.996050 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:12.000977 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3283639816
I1002 01:27:12.002691 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:12.005526 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3283648008
I1002 01:27:12.007222 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:12.028626 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 3300425224
I1002 01:27:12.030634 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:12.033325 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3300433416
I1002 01:27:12.035151 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:12.042218 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 3317210632
I1002 01:27:12.044243 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:12.047014 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3317218824
I1002 01:27:12.048751 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:12.055800 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 3333996040
I1002 01:27:12.057864 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:12.060561 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3334004232
I1002 01:27:12.062284 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:12.069406 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 3350781448
I1002 01:27:12.071428 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:12.074151 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3350789640
I1002 01:27:12.075992 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:12.079866 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 3350790152
I1002 01:27:12.081596 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:12.085762 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3350798344
I1002 01:27:12.087490 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:12.090273 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3350806536
I1002 01:27:12.092044 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:27:12.102632 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:27:12.109158 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 3417915400
I1002 01:27:12.111233 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:12.113941 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 3417948168
I1002 01:27:12.115697 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:27:12.117825 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:27:12.124192 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 3485057032
I1002 01:27:12.126227 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:12.129010 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 3485065224
I1002 01:27:12.130730 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:12.135807 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3485073416
I1002 01:27:12.137518 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:12.140330 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3485081608
I1002 01:27:12.142048 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:12.434547 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 3501858824
I1002 01:27:12.436749 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:12.439445 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3501867016
I1002 01:27:12.441278 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:12.448292 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 3518644232
I1002 01:27:12.450317 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:12.453100 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3518652424
I1002 01:27:12.454827 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:12.461880 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 3535429640
I1002 01:27:12.463982 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:12.466655 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3535437832
I1002 01:27:12.468410 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:12.475547 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 3552215048
I1002 01:27:12.477535 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:12.480290 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3552223240
I1002 01:27:12.482131 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:12.486495 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 3552223752
I1002 01:27:12.488257 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:12.492489 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3552231944
I1002 01:27:12.494204 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:12.496988 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3552240136
I1002 01:27:12.498717 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:27:12.508769 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:27:12.515151 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 3619349000
I1002 01:27:12.517295 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:12.519990 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 3619381768
I1002 01:27:12.521707 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:27:12.523869 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:27:12.530168 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 3686490632
I1002 01:27:12.532212 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:12.535030 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 3686498824
I1002 01:27:12.536769 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:12.541773 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3686507016
I1002 01:27:12.543514 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:12.546317 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3686515208
I1002 01:27:12.548063 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:12.568989 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 3703292424
I1002 01:27:12.571009 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:12.573705 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3703300616
I1002 01:27:12.575566 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:12.582650 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 3720077832
I1002 01:27:12.584712 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:12.587512 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3720086024
I1002 01:27:12.589239 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:12.596310 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 3736863240
I1002 01:27:12.598860 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:12.601571 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3736871432
I1002 01:27:12.603280 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:12.610416 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 3753648648
I1002 01:27:12.612469 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:12.615163 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3753656840
I1002 01:27:12.617006 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:12.620924 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 3753657352
I1002 01:27:12.622656 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:12.626884 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3753665544
I1002 01:27:12.628628 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:12.631436 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3753673736
I1002 01:27:12.633172 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:27:12.643207 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:27:12.649605 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 3820782600
I1002 01:27:12.651724 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:12.654398 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 3820815368
I1002 01:27:12.656159 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:27:12.658808 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:27:12.665172 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 3887924232
I1002 01:27:12.667181 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:12.669976 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 3887932424
I1002 01:27:12.671744 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:12.676746 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3887940616
I1002 01:27:12.678462 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:12.681295 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3887948808
I1002 01:27:12.683025 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:12.703719 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 3904726024
I1002 01:27:12.705739 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:12.708433 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3904734216
I1002 01:27:12.710753 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:12.717815 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 3921511432
I1002 01:27:12.719857 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:12.722682 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3921519624
I1002 01:27:12.724467 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:12.731520 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 3938296840
I1002 01:27:12.733646 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:12.736346 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3938305032
I1002 01:27:12.738078 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:12.745242 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 3955082248
I1002 01:27:12.747255 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:12.749996 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3955090440
I1002 01:27:12.751877 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:12.755799 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 3955090952
I1002 01:27:12.757552 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:12.761855 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3955099144
I1002 01:27:12.763607 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:12.766411 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3955107336
I1002 01:27:12.768177 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:27:12.778837 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:27:12.785272 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 4022216200
I1002 01:27:12.787378 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:12.790067 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 4022248968
I1002 01:27:12.791904 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:27:12.794082 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:27:12.800450 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 4089357832
I1002 01:27:12.802489 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:12.805313 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 4089366024
I1002 01:27:12.807049 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:12.812098 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4089374216
I1002 01:27:12.813823 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:12.816666 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4089382408
I1002 01:27:12.818408 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:12.839590 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 4106159624
I1002 01:27:12.841619 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:12.844332 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4106167816
I1002 01:27:12.846176 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:12.853321 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 4122945032
I1002 01:27:12.855355 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:12.858140 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4122953224
I1002 01:27:12.859901 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:12.866944 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 4139730440
I1002 01:27:12.869077 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:12.871812 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4139738632
I1002 01:27:12.873543 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:12.880680 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 4156515848
I1002 01:27:12.882713 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:12.885470 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4156524040
I1002 01:27:12.887351 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:12.891765 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 4156524552
I1002 01:27:12.893544 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:12.897809 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4156532744
I1002 01:27:12.899575 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:12.902362 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4156540936
I1002 01:27:12.904152 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:27:12.914371 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:27:12.920803 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 4223649800
I1002 01:27:12.922903 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:12.925639 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 4223682568
I1002 01:27:12.927417 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:27:12.929585 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:27:12.935994 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 4290791432
I1002 01:27:12.938032 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:12.940830 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 4290799624
I1002 01:27:12.942584 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:12.947637 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4290807816
I1002 01:27:12.949378 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:12.952232 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4290816008
I1002 01:27:12.953986 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:12.975225 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 4307593224
I1002 01:27:12.977295 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:12.980053 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4307601416
I1002 01:27:12.981965 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:12.989104 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 4324378632
I1002 01:27:12.991132 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:12.993970 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4324386824
I1002 01:27:12.995733 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:13.003042 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 4341164040
I1002 01:27:13.005648 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:13.008358 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4341172232
I1002 01:27:13.010103 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:13.017239 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 4357949448
I1002 01:27:13.019261 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:13.022015 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4357957640
I1002 01:27:13.023888 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:13.027827 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 4357958152
I1002 01:27:13.029597 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:13.033884 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4357966344
I1002 01:27:13.035652 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:13.038457 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4357974536
I1002 01:27:13.040236 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:27:13.050409 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:27:13.056845 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 4425083400
I1002 01:27:13.058941 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:13.061690 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 4425116168
I1002 01:27:13.063467 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:27:13.066157 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:27:13.072552 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 4492225032
I1002 01:27:13.074670 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:13.077496 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 4492233224
I1002 01:27:13.079251 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:13.084349 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4492241416
I1002 01:27:13.086088 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:13.088938 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4492249608
I1002 01:27:13.090698 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:13.111626 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 4509026824
I1002 01:27:13.113683 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:13.116429 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4509035016
I1002 01:27:13.118790 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:13.125885 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 4525812232
I1002 01:27:13.127941 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:13.130745 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4525820424
I1002 01:27:13.132615 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:13.139716 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 4542597640
I1002 01:27:13.141826 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:13.144581 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4542605832
I1002 01:27:13.146342 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:13.153504 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 4559383048
I1002 01:27:13.155591 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:13.158346 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4559391240
I1002 01:27:13.160219 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:13.164159 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 4559391752
I1002 01:27:13.165931 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:13.170251 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4559399944
I1002 01:27:13.172039 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:13.174840 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4559408136
I1002 01:27:13.176619 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:27:13.187361 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:27:13.193782 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 4626517000
I1002 01:27:13.195942 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:13.198656 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 4626549768
I1002 01:27:13.200463 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:27:13.202663 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:27:13.209062 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 4693658632
I1002 01:27:13.211105 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:13.213937 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 4693666824
I1002 01:27:13.215720 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:13.220804 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4693675016
I1002 01:27:13.222549 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:13.225436 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4693683208
I1002 01:27:13.227196 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:13.248625 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 4710460424
I1002 01:27:13.250680 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:13.253413 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4710468616
I1002 01:27:13.255284 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:13.262376 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 4727245832
I1002 01:27:13.264459 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:13.267279 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4727254024
I1002 01:27:13.269075 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:13.276179 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 4744031240
I1002 01:27:13.278300 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:13.281085 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4744039432
I1002 01:27:13.282844 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:13.290028 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 4760816648
I1002 01:27:13.292117 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:13.294906 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4760824840
I1002 01:27:13.296784 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:13.301187 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 4760825352
I1002 01:27:13.302963 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:13.307293 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4760833544
I1002 01:27:13.309054 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:13.311893 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4760841736
I1002 01:27:13.313665 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:27:13.323899 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:27:13.330345 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 4827950600
I1002 01:27:13.332473 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:13.335202 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 4827983368
I1002 01:27:13.336985 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:27:13.339214 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:27:13.345625 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 4895092232
I1002 01:27:13.347721 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:13.350554 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 4895100424
I1002 01:27:13.352339 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:13.357415 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4895108616
I1002 01:27:13.359179 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:13.362063 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4895116808
I1002 01:27:13.363831 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:13.385056 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 4911894024
I1002 01:27:13.387111 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:13.389893 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4911902216
I1002 01:27:13.391798 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:13.398857 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 4928679432
I1002 01:27:13.400960 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:13.403818 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4928687624
I1002 01:27:13.405585 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:13.412724 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 4945464840
I1002 01:27:13.415356 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:13.418102 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4945473032
I1002 01:27:13.419876 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:13.427036 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 4962250248
I1002 01:27:13.429115 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:13.431913 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4962258440
I1002 01:27:13.433782 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:13.437781 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 4962258952
I1002 01:27:13.439561 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:13.443926 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4962267144
I1002 01:27:13.445705 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:13.448557 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4962275336
I1002 01:27:13.450318 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:27:13.460551 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:27:13.466986 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 5029384200
I1002 01:27:13.469142 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:13.471926 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 5029416968
I1002 01:27:13.473685 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:27:13.476428 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:27:13.482805 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 5096525832
I1002 01:27:13.484901 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:13.488772 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 5096534024
I1002 01:27:13.490545 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:13.495667 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5096542216
I1002 01:27:13.497435 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:13.500376 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5096550408
I1002 01:27:13.502153 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:13.576749 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 5113327624
I1002 01:27:13.578963 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:13.581751 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5113335816
I1002 01:27:13.583547 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:13.839037 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 5130113032
I1002 01:27:13.841564 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:13.844644 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5130121224
I1002 01:27:13.846536 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:13.853878 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 5146898440
I1002 01:27:13.856089 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:13.859040 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5146906632
I1002 01:27:13.860851 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:13.868159 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 5163683848
I1002 01:27:13.870363 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:13.873256 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5163692040
I1002 01:27:13.875044 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:13.879136 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 5163692552
I1002 01:27:13.881070 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:13.885645 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5163700744
I1002 01:27:13.887449 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:13.890316 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5163708936
I1002 01:27:13.892154 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:27:13.903713 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:27:13.910828 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 5230817800
I1002 01:27:13.913066 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:13.915934 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 5230850568
I1002 01:27:13.917700 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:27:13.920099 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:27:13.926624 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 5297959432
I1002 01:27:13.928898 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:13.931716 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 5297967624
I1002 01:27:13.933494 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:13.938890 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5297975816
I1002 01:27:13.940900 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:13.943642 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5297984008
I1002 01:27:13.945426 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:13.968187 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 5314761224
I1002 01:27:13.970458 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:13.973422 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5314769416
I1002 01:27:13.975404 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:13.982640 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 5331546632
I1002 01:27:13.984812 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:13.987776 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5331554824
I1002 01:27:13.989547 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:13.996851 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 5348332040
I1002 01:27:13.998960 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.001816 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5348340232
I1002 01:27:14.003615 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.010873 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 5365117448
I1002 01:27:14.013028 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.016009 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5365125640
I1002 01:27:14.017791 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.021935 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 5365126152
I1002 01:27:14.023760 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.028276 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5365134344
I1002 01:27:14.030051 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.033524 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5365142536
I1002 01:27:14.035339 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:27:14.045929 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:27:14.053038 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 5432251400
I1002 01:27:14.055353 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.058277 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 5432284168
I1002 01:27:14.060104 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:27:14.062520 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:27:14.069137 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 5499393032
I1002 01:27:14.071229 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.074162 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 5499401224
I1002 01:27:14.075965 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.081283 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5499409416
I1002 01:27:14.083070 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.085985 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5499417608
I1002 01:27:14.087782 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.110162 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 5516194824
I1002 01:27:14.112481 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.115425 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5516203016
I1002 01:27:14.117327 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.124651 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 5532980232
I1002 01:27:14.126793 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.129705 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5532988424
I1002 01:27:14.131604 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.138747 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 5549765640
I1002 01:27:14.140918 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.143775 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5549773832
I1002 01:27:14.145554 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.153360 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 5566551048
I1002 01:27:14.155586 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.158496 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5566559240
I1002 01:27:14.160416 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.164548 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 5566559752
I1002 01:27:14.166355 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.170875 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5566567944
I1002 01:27:14.172668 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.175540 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5566576136
I1002 01:27:14.177351 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:27:14.187965 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:27:14.194597 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 5633685000
I1002 01:27:14.196786 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.199588 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 5633717768
I1002 01:27:14.201383 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:27:14.203678 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:27:14.210740 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 5700826632
I1002 01:27:14.212845 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.215729 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 5700834824
I1002 01:27:14.217544 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.222713 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5700843016
I1002 01:27:14.224532 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.227454 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5700851208
I1002 01:27:14.229249 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.250353 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 5717628424
I1002 01:27:14.252476 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.255241 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5717636616
I1002 01:27:14.257149 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.264840 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 5734413832
I1002 01:27:14.266931 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.270081 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5734422024
I1002 01:27:14.271896 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.278986 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 5751199240
I1002 01:27:14.281149 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.283919 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5751207432
I1002 01:27:14.285694 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.292830 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 5767984648
I1002 01:27:14.294901 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.302034 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5767992840
I1002 01:27:14.303962 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.308056 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 5767993352
I1002 01:27:14.309852 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.314289 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5768001544
I1002 01:27:14.316098 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.318964 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5768009736
I1002 01:27:14.320785 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:27:14.331749 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:27:14.338194 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 5835118600
I1002 01:27:14.340377 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.343170 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 5835151368
I1002 01:27:14.344970 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:27:14.347247 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:27:14.353698 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 5902260232
I1002 01:27:14.355808 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.358652 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 5902268424
I1002 01:27:14.360476 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.365656 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5902276616
I1002 01:27:14.367468 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.370361 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5902284808
I1002 01:27:14.372191 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.393895 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 5919062024
I1002 01:27:14.396023 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.398783 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5919070216
I1002 01:27:14.400704 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.407891 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 5935847432
I1002 01:27:14.409970 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.412884 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5935855624
I1002 01:27:14.414698 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.421868 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 5952632840
I1002 01:27:14.424044 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.426802 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5952641032
I1002 01:27:14.428609 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.435828 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 5969418248
I1002 01:27:14.437903 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.440738 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5969426440
I1002 01:27:14.442633 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.446665 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 5969426952
I1002 01:27:14.448504 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.452975 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5969435144
I1002 01:27:14.454751 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.458082 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5969443336
I1002 01:27:14.459917 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:27:14.470308 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:27:14.476928 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 6036552200
I1002 01:27:14.479124 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.481935 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 6036584968
I1002 01:27:14.483767 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:27:14.486055 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:27:14.492528 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 6103693832
I1002 01:27:14.494626 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.497523 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 6103702024
I1002 01:27:14.499354 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.504600 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 6103710216
I1002 01:27:14.506461 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.509376 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 6103718408
I1002 01:27:14.511176 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.532937 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 6120495624
I1002 01:27:14.535057 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.537841 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6120503816
I1002 01:27:14.539763 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.546915 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 6137281032
I1002 01:27:14.549028 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.551952 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6137289224
I1002 01:27:14.553755 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.560866 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 6154066440
I1002 01:27:14.563036 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.565871 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6154074632
I1002 01:27:14.567688 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.575436 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 6170851848
I1002 01:27:14.577549 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.580393 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6170860040
I1002 01:27:14.582321 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.586524 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 6170860552
I1002 01:27:14.588446 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.593284 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 6170868744
I1002 01:27:14.595126 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.598034 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 6170876936
I1002 01:27:14.599884 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:27:14.610486 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:27:14.617088 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 6237985800
I1002 01:27:14.619238 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.622076 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 6238018568
I1002 01:27:14.623908 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:27:14.626207 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:27:14.633314 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 6305127432
I1002 01:27:14.635449 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.638433 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 6305135624
I1002 01:27:14.640268 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.645510 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 6305143816
I1002 01:27:14.647345 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.650250 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 6305152008
I1002 01:27:14.652092 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.673451 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 6321929224
I1002 01:27:14.675603 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.678394 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6321937416
I1002 01:27:14.680320 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.688147 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 6338714632
I1002 01:27:14.690273 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.693174 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6338722824
I1002 01:27:14.694977 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.702177 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 6355500040
I1002 01:27:14.704390 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.707172 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6355508232
I1002 01:27:14.709024 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.716260 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 6372285448
I1002 01:27:14.718375 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.721244 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6372293640
I1002 01:27:14.723159 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.727353 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 6372294152
I1002 01:27:14.729169 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.733753 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 6372302344
I1002 01:27:14.735579 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.738453 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 6372310536
I1002 01:27:14.740305 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:27:14.751614 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:27:14.758383 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 6439419400
I1002 01:27:14.760787 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.763734 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 6439452168
I1002 01:27:14.765545 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:27:14.767988 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:27:14.774508 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 6506561032
I1002 01:27:14.776643 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.779563 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 6506569224
I1002 01:27:14.781387 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.786723 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 6506577416
I1002 01:27:14.788574 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.791519 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 6506585608
I1002 01:27:14.793344 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.815454 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 6523362824
I1002 01:27:14.817589 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.820394 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6523371016
I1002 01:27:14.822315 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.829511 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 6540148232
I1002 01:27:14.831637 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.834540 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6540156424
I1002 01:27:14.836423 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.843595 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 6556933640
I1002 01:27:14.845782 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.848595 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6556941832
I1002 01:27:14.850407 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.857756 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 6573719048
I1002 01:27:14.859911 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.862759 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6573727240
I1002 01:27:14.864689 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.868840 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 6573727752
I1002 01:27:14.870668 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.875245 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 6573735944
I1002 01:27:14.877099 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.880549 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 6573744136
I1002 01:27:14.882365 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:27:14.892986 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:27:14.899493 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 6640853000
I1002 01:27:14.901678 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.904521 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 6640885768
I1002 01:27:14.906354 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:27:14.908759 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:27:14.915215 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 6707994632
I1002 01:27:14.917358 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.920276 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 6708002824
I1002 01:27:14.922093 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.927440 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 6708011016
I1002 01:27:14.929321 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:14.932249 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 6708019208
I1002 01:27:14.934067 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:27:14.939745 139650252134208 py_utils.py:1229] WARNING!!! var weight_0 is using the default xavier initializer. Make sure this is intended.
I1002 01:27:14.946162 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var on /job:local/replica:0/task:0/device:CPU:0 6724403208
I1002 01:27:14.948300 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:27:14.949147 139650252134208 py_utils.py:1229] WARNING!!! var weight_1 is using the default xavier initializer. Make sure this is intended.
I1002 01:27:14.956078 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var on /job:local/replica:0/task:0/device:CPU:0 6740787208
I1002 01:27:14.958174 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:27:14.959020 139650252134208 py_utils.py:1229] WARNING!!! var weight_2 is using the default xavier initializer. Make sure this is intended.
I1002 01:27:14.965389 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var on /job:local/replica:0/task:0/device:CPU:0 6757171208
I1002 01:27:14.967520 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:27:14.968370 139650252134208 py_utils.py:1229] WARNING!!! var weight_3 is using the default xavier initializer. Make sure this is intended.
I1002 01:27:14.974741 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var on /job:local/replica:0/task:0/device:CPU:0 6773555208
I1002 01:27:14.976877 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:27:14.977724 139650252134208 py_utils.py:1229] WARNING!!! var weight_4 is using the default xavier initializer. Make sure this is intended.
I1002 01:27:14.984076 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var on /job:local/replica:0/task:0/device:CPU:0 6789939208
I1002 01:27:14.986165 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:27:14.987015 139650252134208 py_utils.py:1229] WARNING!!! var weight_5 is using the default xavier initializer. Make sure this is intended.
I1002 01:27:14.993428 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var on /job:local/replica:0/task:0/device:CPU:0 6806323208
I1002 01:27:14.995574 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:27:14.996424 139650252134208 py_utils.py:1229] WARNING!!! var weight_6 is using the default xavier initializer. Make sure this is intended.
I1002 01:27:15.002787 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var on /job:local/replica:0/task:0/device:CPU:0 6822707208
I1002 01:27:15.004935 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:27:15.005779 139650252134208 py_utils.py:1229] WARNING!!! var weight_7 is using the default xavier initializer. Make sure this is intended.
I1002 01:27:15.012207 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var on /job:local/replica:0/task:0/device:CPU:0 6839091208
I1002 01:27:15.014300 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:27:15.015141 139650252134208 py_utils.py:1229] WARNING!!! var weight_8 is using the default xavier initializer. Make sure this is intended.
I1002 01:27:15.021504 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var on /job:local/replica:0/task:0/device:CPU:0 6855475208
I1002 01:27:15.023627 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:27:15.024470 139650252134208 py_utils.py:1229] WARNING!!! var weight_9 is using the default xavier initializer. Make sure this is intended.
I1002 01:27:15.031504 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var on /job:local/replica:0/task:0/device:CPU:0 6871859208
I1002 01:27:15.033615 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:27:15.034506 139650252134208 py_utils.py:1229] WARNING!!! var weight_10 is using the default xavier initializer. Make sure this is intended.
I1002 01:27:15.040849 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var on /job:local/replica:0/task:0/device:CPU:0 6888243208
I1002 01:27:15.042960 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:27:15.043835 139650252134208 py_utils.py:1229] WARNING!!! var weight_11 is using the default xavier initializer. Make sure this is intended.
I1002 01:27:15.050210 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var on /job:local/replica:0/task:0/device:CPU:0 6904627208
I1002 01:27:15.052371 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:27:15.053221 139650252134208 py_utils.py:1229] WARNING!!! var weight_12 is using the default xavier initializer. Make sure this is intended.
I1002 01:27:15.059585 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var on /job:local/replica:0/task:0/device:CPU:0 6921011208
I1002 01:27:15.061650 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:27:15.062496 139650252134208 py_utils.py:1229] WARNING!!! var weight_13 is using the default xavier initializer. Make sure this is intended.
I1002 01:27:15.068925 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var on /job:local/replica:0/task:0/device:CPU:0 6937395208
I1002 01:27:15.071014 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:27:15.071890 139650252134208 py_utils.py:1229] WARNING!!! var weight_14 is using the default xavier initializer. Make sure this is intended.
I1002 01:27:15.078197 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var on /job:local/replica:0/task:0/device:CPU:0 6953779208
I1002 01:27:15.080337 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:27:15.081186 139650252134208 py_utils.py:1229] WARNING!!! var weight_15 is using the default xavier initializer. Make sure this is intended.
I1002 01:27:15.087603 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var on /job:local/replica:0/task:0/device:CPU:0 6970163208
I1002 01:27:15.089691 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:15.092572 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var on /job:local/replica:0/task:0/device:CPU:0 6970171208
I1002 01:27:15.094490 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:15.097274 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var on /job:local/replica:0/task:0/device:CPU:0 6970179208
I1002 01:27:15.099081 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:15.102411 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var on /job:local/replica:0/task:0/device:CPU:0 6970187208
I1002 01:27:15.104269 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:15.107041 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var on /job:local/replica:0/task:0/device:CPU:0 6970195208
I1002 01:27:15.108869 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:15.111815 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var on /job:local/replica:0/task:0/device:CPU:0 6970203208
I1002 01:27:15.113624 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:15.116431 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var on /job:local/replica:0/task:0/device:CPU:0 6970211208
I1002 01:27:15.118243 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:15.121152 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var on /job:local/replica:0/task:0/device:CPU:0 6970219208
I1002 01:27:15.122949 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:15.125761 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var on /job:local/replica:0/task:0/device:CPU:0 6970227208
I1002 01:27:15.127706 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:15.130468 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var on /job:local/replica:0/task:0/device:CPU:0 6970235208
I1002 01:27:15.132408 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:15.135288 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var on /job:local/replica:0/task:0/device:CPU:0 6970243208
I1002 01:27:15.137120 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:15.139925 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var on /job:local/replica:0/task:0/device:CPU:0 6970251208
I1002 01:27:15.141718 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:15.144635 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var on /job:local/replica:0/task:0/device:CPU:0 6970259208
I1002 01:27:15.146439 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:15.149227 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var on /job:local/replica:0/task:0/device:CPU:0 6970267208
I1002 01:27:15.151036 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:15.153963 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var on /job:local/replica:0/task:0/device:CPU:0 6970275208
I1002 01:27:15.155796 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:15.158558 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var on /job:local/replica:0/task:0/device:CPU:0 6970283208
I1002 01:27:15.160485 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:15.163254 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var on /job:local/replica:0/task:0/device:CPU:0 6970291208
I1002 01:27:15.165116 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.102086 139650252134208 py_utils.py:1484] === worker 0 ===
I1002 01:27:16.118349 139650252134208 py_utils.py:1474] worker 0: global_step                                                           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.118439 139650252134208 py_utils.py:1474] worker 0: input._tokenizer_default.global_step                                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.118499 139650252134208 py_utils.py:1474] worker 0: input.global_step                                                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.118552 139650252134208 py_utils.py:1474] worker 0: learners[0].global_step                                               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.118602 139650252134208 py_utils.py:1474] worker 0: learners[0].lr_schedule.global_step                                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.118649 139650252134208 py_utils.py:1474] worker 0: learners[0].optimizer.global_step                                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.118706 139650252134208 py_utils.py:1474] worker 0: lm.global_step                                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.118754 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.emb.global_step                                       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.118800 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.emb.src_dropout.global_step                           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.118846 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.emb.src_pos_emb.global_step                           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.118892 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.emb.src_token_emb.global_step                         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.118938 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.emb.src_token_emb.wm                                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.118984 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.119029 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.119075 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.119120 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.119171 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.119216 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.119261 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.119322 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.119373 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.119420 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.global_step                         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.119466 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.119511 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.119560 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.119607 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.119653 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.global_step                                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.119698 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.119744 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.119790 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.119835 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.119880 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.119925 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.119970 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.120015 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.120060 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.120105 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.120150 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.120195 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.global_step                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.120239 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.120284 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.120329 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.120374 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.120423 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.120469 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.120514 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.120559 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.120604 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.120649 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.120694 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.120740 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.120785 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.120830 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.global_step                         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.120875 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.120920 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.120964 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.121009 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.121054 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.global_step                                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.121098 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.121143 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.121188 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.121234 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.121282 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.121328 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.121374 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.121418 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.121463 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.121508 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.121553 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.121598 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.global_step                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.121643 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.121688 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.121733 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.121778 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.121823 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.121867 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.121913 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.121957 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.122003 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.122047 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.122096 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.122142 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.122188 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.122233 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.global_step                         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.122278 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.122323 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.122368 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.122413 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.122458 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.global_step                                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.122503 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.122548 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.122593 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.122638 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.122683 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.122728 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.122772 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.122818 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.122863 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.122908 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.122958 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.123004 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.global_step                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.123049 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.123094 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.123139 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.123184 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.123229 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.123274 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.123340 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.123389 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.123434 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.123480 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.123535 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.123580 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.123624 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.123669 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.global_step                         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.123714 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.123759 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.123810 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.123856 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.123902 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.global_step                                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.123947 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.123991 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.124036 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.124082 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.124127 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.124173 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.124218 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.124263 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.124307 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.124352 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.124397 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.124442 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.global_step                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.124487 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.124532 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.124577 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.124623 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.124672 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.124718 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.124764 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.124810 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.124855 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.124900 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.124945 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.124991 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.125036 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.125082 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.global_step                         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.125127 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.125173 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.125218 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.125264 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.125309 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.global_step                                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.125354 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.125399 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.125444 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.125494 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.125541 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.125586 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.125632 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.125677 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.125722 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.125767 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.125812 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.125857 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.global_step                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.125902 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.125947 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.125993 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.126038 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.126083 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.126128 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.126173 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.126218 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.126263 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.126309 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.126358 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.126404 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.126449 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.126494 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.global_step                         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.126539 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.126584 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.126629 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.126674 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.126720 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.global_step                                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.126765 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.126810 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.126855 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.126900 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.126945 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.126990 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.127035 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.127080 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.127125 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.127174 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.127220 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.127265 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.global_step                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.127326 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.127377 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.127423 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.127468 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.127514 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.127559 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.127604 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.127649 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.127694 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.127738 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.127784 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.127829 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.127874 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.127919 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.global_step                         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.127964 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.128009 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.128060 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.128106 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.128151 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.global_step                                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.128196 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.128242 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.128287 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.128332 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.128377 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.128422 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.128467 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.128513 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.128558 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.128603 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.128648 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.128694 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.global_step                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.128739 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.128784 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.128829 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.128879 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.128925 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.128971 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.129015 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.129060 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.129105 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.129150 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.129199 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.129244 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.129289 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.129334 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.global_step                         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.129379 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.129424 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.129469 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.129514 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.129559 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.global_step                                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.129605 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.129650 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.129695 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.129746 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.129792 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.129837 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.129883 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.129928 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.129973 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.130018 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.130063 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.130108 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.global_step                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.130153 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.130199 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.130244 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.130290 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.130335 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.global_step                                           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.130381 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.130426 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.130471 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.130515 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.130566 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.130612 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.130657 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.130703 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.130748 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.130793 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.global_step                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.130839 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.130883 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.130928 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.130974 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.131019 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.global_step                                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.131064 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.131109 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.131154 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.131199 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.131244 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.131289 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.131446 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.global_step               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.131495 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.131547 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.131595 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.131641 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.131686 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.global_step                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.131732 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.131778 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.131823 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.131870 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.131915 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.131961 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.132006 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.132051 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.132097 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.132142 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.132187 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.132232 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.132277 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.132323 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.global_step                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.132373 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.132420 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.132465 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.132510 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.132555 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.global_step                                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.132600 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.132646 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.132691 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.132736 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.132781 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.132827 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.132873 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.global_step               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.132918 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.132963 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.133008 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.133053 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.133097 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.global_step                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.133142 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.133187 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.133236 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.133283 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.133327 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.133372 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.133418 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.133463 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.133508 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.133552 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.133597 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.133642 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.133687 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.133732 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.global_step                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.133777 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.133823 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.133868 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.133913 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.133959 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.global_step                                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.134003 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.134048 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.134100 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.134148 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.134193 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.134239 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.134284 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.global_step               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.134329 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.134375 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.134420 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.134465 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.134510 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.global_step                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.134556 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.134601 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.134647 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.134692 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.134737 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.134783 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.134829 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.134874 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.134924 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.134971 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.135016 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.135062 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.135107 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.135153 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.global_step                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.135198 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.135244 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.135289 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.135359 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.135406 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.global_step                                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.135451 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.135497 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.135543 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.135588 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.135634 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.135679 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.135725 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.global_step               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.135770 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.135821 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.135868 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.135914 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.135959 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.global_step                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.136004 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.136050 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.136095 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.136141 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.136187 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.136232 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.136277 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.136322 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.136367 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.136412 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.136457 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.136502 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.136548 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.136593 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.global_step                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.136643 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.136689 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.136735 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.136780 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.136825 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.global_step                                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.136871 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.136916 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.136962 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.137008 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.137053 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.137098 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.137143 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.global_step               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.137188 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.137233 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.137277 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.137322 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.137367 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.global_step                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.137412 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.137457 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.137507 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.137553 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.137599 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.137644 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.137690 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.137735 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.137780 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.137825 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.137871 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.137916 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.137961 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.138006 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.global_step                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.138051 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.138097 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.138143 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.138188 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.138233 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.global_step                                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.138278 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.138327 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.138374 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.138420 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.138466 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.138511 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.138556 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.global_step               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.138601 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.138647 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.138692 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.138737 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.138782 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.global_step                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.138828 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.138873 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.138918 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.138964 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.139009 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.139054 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.139099 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.139144 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.139193 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.139243 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.139288 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.139351 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.139398 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.139443 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.global_step                         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.139488 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.139533 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.139579 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.139624 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.139670 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.global_step                                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.139714 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.139759 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.139805 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.139849 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.139894 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.139940 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.139985 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.140035 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.140081 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.140126 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.140171 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.140216 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.global_step                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.140261 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.140306 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.140350 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.140395 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.140440 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.140485 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.140530 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.140574 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.140619 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.140664 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.140709 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.140754 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.140798 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.140843 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.global_step                         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.140892 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.140938 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.140983 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.141027 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.141072 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.global_step                                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.141116 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.141161 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.141206 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.141250 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.141294 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.141339 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.141383 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.141428 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.141472 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.141516 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.141561 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.141606 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.global_step                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.141650 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.141700 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.141746 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.141792 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.141836 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.global_step                                           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.141881 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.141926 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.141970 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.142015 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.142060 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.142105 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.142149 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.142194 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.142239 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.142283 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.global_step                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.142327 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.142372 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.142417 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.142462 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.142507 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.global_step                                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.142557 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.142603 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.142648 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.142693 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.142739 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.142783 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.142828 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.global_step               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.142873 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.142919 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.142965 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.143009 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.143054 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.global_step                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.143099 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.143144 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.143189 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.143234 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.143279 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.143341 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.143393 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.143439 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.143485 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.143531 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.143576 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.143621 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.143666 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.143711 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.global_step                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.143757 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.143803 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.143848 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.143893 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.143938 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.global_step                                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.143983 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.144029 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.144074 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.144119 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.144164 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.144210 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.144259 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.global_step               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.144306 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.144351 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.144397 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.144441 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.144486 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.global_step                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.144531 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.144576 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.144621 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.144666 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.144711 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.144757 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.144801 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.144846 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.144891 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.144936 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.144980 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.145025 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.145074 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.145120 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.global_step                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.145165 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.145216 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.145263 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.145309 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.145354 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.global_step                                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.145399 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.145444 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.145489 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.145534 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.145579 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.145624 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.145669 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.global_step               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.145714 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.145759 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.145803 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.145849 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.145894 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.global_step                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.145944 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.145990 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.146035 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.146080 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.146125 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.146170 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.146216 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.146260 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.146305 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.146350 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.146395 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.146440 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.146486 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.146531 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.global_step                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.146576 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.146621 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.146666 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.146711 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.146756 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.global_step                                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.146805 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.146852 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.146898 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.146943 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.146989 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.147034 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.147079 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.global_step               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.147125 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.147170 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.147215 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.147261 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.147322 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.global_step                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.147376 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.147428 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.147475 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.147521 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.147567 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.147612 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.147662 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.147708 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.147754 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.147799 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.147845 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.147890 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.147934 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.147979 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.global_step                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.148024 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.148069 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.148113 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.148158 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.148202 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.global_step                                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.148247 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.148292 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.148337 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.148382 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.148427 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.148471 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.148520 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.global_step               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.148566 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.148611 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.148655 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.148700 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.148746 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.global_step                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.148791 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.148836 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.148881 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.148926 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.148970 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.149015 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.149060 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.149105 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.149151 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.149196 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.149241 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.149288 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.149339 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.149385 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.global_step                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.149430 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.149475 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.149519 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.149564 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.149610 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.global_step                                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.149655 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.149700 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.149745 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.149790 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.149836 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.149881 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.149926 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.global_step               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.149971 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.150016 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.150061 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.150106 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.150151 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.global_step                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.150200 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.150246 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.150291 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.150336 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.150381 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.150425 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.150470 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.150515 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.150559 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.150604 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.150648 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.150693 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.150738 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.150782 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.global_step                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.150827 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.150871 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.150917 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.150961 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.151010 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.global_step                                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.151056 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.151101 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.151146 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.151191 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.151235 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.151280 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.151341 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.global_step               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.151388 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.151433 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.151478 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.151524 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.151568 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.global_step                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.151612 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.151657 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.151702 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.151746 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.151791 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.151836 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.151885 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.151931 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.151977 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.152021 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.152066 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.152111 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.152156 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.152200 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.global_step                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.152245 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.152290 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.152335 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.152379 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.152424 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.global_step                                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.152469 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.152516 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.152561 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.152606 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.152652 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.152701 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.152747 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.global_step               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.152793 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.152838 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.152883 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.152929 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.152974 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.global_step                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.153019 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.153064 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.153109 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.153153 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.153197 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.global_step                                           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.153242 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.153287 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.153332 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.153377 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.153422 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.153467 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.153512 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.153561 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.153607 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.153653 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.global_step                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.153698 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.153743 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.153788 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.153833 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.153878 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.global_step                                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.153923 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.153967 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.154012 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.154057 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.154102 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.154147 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.154192 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.global_step               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.154238 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.154282 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.154327 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.154377 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.154423 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.global_step                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.154469 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.154513 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.154559 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.154604 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.154650 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.154695 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.154741 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.154786 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.154832 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.154877 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.154922 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.154967 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.155013 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.155058 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.global_step                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.155103 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.155148 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.155194 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.155243 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.155289 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.global_step                                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.155350 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.155397 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.155443 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.155488 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.155534 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.155579 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.155624 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.global_step               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.155670 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.155715 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.155760 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.155805 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.155850 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.global_step                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.155896 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.155941 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.155986 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.156031 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.156085 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.156131 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.156176 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.156221 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.156267 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.156312 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.156357 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.156402 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.156447 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.156492 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.global_step                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.156537 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.156582 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.156627 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.156673 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.156718 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.global_step                                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.156763 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.156809 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.156854 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.156899 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.156948 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.156994 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.157040 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.global_step               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.157085 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.157131 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.157176 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.157221 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.157267 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.global_step                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.157312 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.157358 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.157403 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.157448 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.157493 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.157538 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.157584 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.157629 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.157675 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.157720 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.157770 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.157817 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.157862 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.157907 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.global_step                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.157952 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.157997 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.158042 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.158087 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.158133 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.global_step                                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.158178 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.158223 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.158268 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.158313 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.158359 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.158404 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.158449 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.global_step               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.158495 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.158540 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.158585 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.158635 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.158682 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.global_step                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.158727 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.158772 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.158818 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.158863 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.158908 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.158954 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.159000 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.159045 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.159090 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.159136 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.159181 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.159227 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.159272 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.159337 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.global_step                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.159385 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.159430 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.159475 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.159525 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.159571 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.global_step                                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.159616 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.159662 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.159707 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.159751 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.159796 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.159841 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.159887 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.global_step               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.159932 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.159977 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.160022 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.160068 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.160113 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.global_step                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.160158 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.160203 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.160248 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.160293 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.160343 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.160390 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.160435 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.160480 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.160525 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.160570 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.160615 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.160660 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.160706 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.160751 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.global_step                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.160795 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.160841 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.160887 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.160932 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.160977 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.global_step                                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.161023 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.161068 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.161114 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.161159 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.161210 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.161257 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.161302 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.global_step               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.161348 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.161394 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.161439 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.161485 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.161530 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.global_step                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.161576 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.161622 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.161667 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.161713 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.161758 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.161803 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.161849 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.161896 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.161942 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.161987 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.162036 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.162082 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.162127 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.162172 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.global_step                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.162217 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.162262 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.162307 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.162352 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.162397 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.global_step                                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.162442 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.162487 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.162532 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.162577 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.162622 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.162667 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.162712 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.global_step               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.162758 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.162803 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.162848 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.162899 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.162945 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.global_step                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.162990 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.163035 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.163081 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.163126 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.163172 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.163216 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.163261 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.163318 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.163369 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.163415 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.163461 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.163506 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.163552 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.163597 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.global_step                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.163642 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.163687 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.163738 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.163784 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.163829 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.global_step                                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.163874 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.163920 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.163964 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.164009 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.164054 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.164099 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.164144 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.global_step               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.164190 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.164235 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.164280 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.164325 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.164370 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.global_step                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.164415 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.164460 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.164505 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.164550 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.164599 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.global_step                                           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.164646 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_0                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.164691 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_1                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.164736 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_10                                       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.164781 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_11                                       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.164825 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_12                                       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.164870 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_13                                       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.164915 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_14                                       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.164960 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_15                                       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.165005 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_2                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.165050 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_3                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.165095 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_4                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.165140 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_5                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.165185 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_6                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.165231 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_7                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.165277 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_8                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.165321 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_9                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.165367 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.global_step                                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.165416 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_0                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.165462 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_1                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.165508 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_10                                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.165553 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_11                                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.165598 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_12                                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.165643 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_13                                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.165689 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_14                                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.165734 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_15                                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.165778 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_2                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.165823 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_3                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.165868 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_4                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.165913 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_5                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.165958 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_6                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.166003 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_7                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.166049 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_8                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.166093 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_9                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.166139 139650252134208 py_utils.py:1474] worker 0: lm.stack.global_step                                                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:16.166201 139650252134208 py_utils.py:1490] ==========
I1002 01:27:18.702354 139650252134208 gpipe.py:457] cell 0 input [<tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/GatherV2_1:0' shape=(1024, 1) dtype=int32>, <tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/GatherV2_2:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None, None, None]
I1002 01:27:20.958102 139650252134208 gpipe.py:457] cell 1 input [<tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/encoder_7/add:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/GatherV2_2:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I1002 01:27:23.438557 139650252134208 gpipe.py:457] cell 2 input [<tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/encoder_15/add:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/GatherV2_2:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I1002 01:27:25.574270 139650252134208 gpipe.py:457] cell 3 input [<tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/encoder_23/add:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/GatherV2_2:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I1002 01:27:28.254937 139650252134208 gpipe.py:457] cell 0 input [<tf.Tensor 'arg259:0' shape=(1024, 1) dtype=int32>, <tf.Tensor 'arg260:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None, None, None]
W1002 01:27:30.152808 139650252134208 recurrent.py:886] cell_fn contains stateful ops: [('emb/Assert/Assert', 'Assert'), ('emb/Assert_1/Assert', 'Assert'), ('encoder_0/fflayer_0/encoder_0/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_0/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_0/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_1/fflayer_0/encoder_1/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_1/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_1/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_2/fflayer_0/encoder_2/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_2/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_2/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_3/fflayer_0/encoder_3/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_3/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_3/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_4/fflayer_0/encoder_4/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_4/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_4/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_5/fflayer_0/encoder_5/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_5/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_5/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_6/fflayer_0/encoder_6/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_6/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_6/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_7/fflayer_0/encoder_7/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_7/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_7/fflayer_1/Assert/AssertGuard/Assert', 'Assert')]
I1002 01:27:30.276692 139650252134208 gpipe.py:457] cell 1 input [<tf.Tensor 'arg254:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'arg255:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
W1002 01:27:32.636886 139650252134208 recurrent.py:886] cell_fn contains stateful ops: [('encoder_8/fflayer_0/encoder_8/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_8/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_8/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_9/fflayer_0/encoder_9/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_9/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_9/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_10/fflayer_0/encoder_10/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_10/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_10/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_11/fflayer_0/encoder_11/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_11/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_11/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_12/fflayer_0/encoder_12/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_12/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_12/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_13/fflayer_0/encoder_13/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_13/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_13/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_14/fflayer_0/encoder_14/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_14/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_14/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_15/fflayer_0/encoder_15/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_15/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_15/fflayer_1/Assert/AssertGuard/Assert', 'Assert')]
I1002 01:27:32.759285 139650252134208 gpipe.py:457] cell 2 input [<tf.Tensor 'arg254:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'arg255:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
W1002 01:27:34.621101 139650252134208 recurrent.py:886] cell_fn contains stateful ops: [('encoder_16/fflayer_0/encoder_16/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_16/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_16/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_17/fflayer_0/encoder_17/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_17/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_17/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_18/fflayer_0/encoder_18/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_18/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_18/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_19/fflayer_0/encoder_19/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_19/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_19/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_20/fflayer_0/encoder_20/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_20/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_20/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_21/fflayer_0/encoder_21/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_21/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_21/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_22/fflayer_0/encoder_22/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_22/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_22/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_23/fflayer_0/encoder_23/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_23/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_23/fflayer_1/Assert/AssertGuard/Assert', 'Assert')]
I1002 01:27:34.761178 139650252134208 gpipe.py:457] cell 3 input [<tf.Tensor 'arg286:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'arg287:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
W1002 01:27:37.232554 139650252134208 recurrent.py:886] cell_fn contains stateful ops: [('encoder_24/fflayer_0/encoder_24/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_24/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_24/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_25/fflayer_0/encoder_25/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_25/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_25/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_26/fflayer_0/encoder_26/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_26/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_26/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_27/fflayer_0/encoder_27/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_27/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_27/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_28/fflayer_0/encoder_28/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_28/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_28/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_29/fflayer_0/encoder_29/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_29/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_29/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_30/fflayer_0/encoder_30/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_30/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_30/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_31/fflayer_0/encoder_31/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_31/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_31/fflayer_1/Assert/AssertGuard/Assert', 'Assert')]
I1002 01:27:37.717351 139650252134208 gpipe.py:457] cell 0 input [<tf.Tensor 'arg259:0' shape=(1024, 1) dtype=int32>, <tf.Tensor 'arg260:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None, None, None]
I1002 01:27:40.250487 139650252134208 gpipe.py:457] cell 1 input [<tf.Tensor 'Recv_1:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'Recv_2:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I1002 01:27:43.426643 139650252134208 gpipe.py:457] cell 2 input [<tf.Tensor 'Recv_1:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'Recv_2:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I1002 01:27:46.650807 139650252134208 gpipe.py:457] cell 3 input [<tf.Tensor 'Recv_1:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'Recv_2:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I1002 01:27:48.680203 139650252134208 gpipe.py:548] pipeline output = [<tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/Reshape_2:0' shape=(1024, 32, 32000) dtype=float32>]
I1002 01:27:48.684531 139650252134208 layers.py:2786] Using sparse_softmax_cross_entropy_with_logits() in SimpleFullSoftmax::_FProp2D logits_shape=[32768, 32000]
I1002 01:27:48.777618 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/total_samples/var on /job:local/replica:0/task:0/device:CPU:0 6970291216
I1002 01:27:48.779696 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/total_samples/var:0 shape=() on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:27:48.788609 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var:0
I1002 01:27:48.788719 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var:0
I1002 01:27:48.788800 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var:0
I1002 01:27:48.788872 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var:0
I1002 01:27:48.788939 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var:0
I1002 01:27:48.789004 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var:0
I1002 01:27:48.789068 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var:0
I1002 01:27:48.789128 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1002 01:27:48.789183 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1002 01:27:48.789239 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1002 01:27:48.789296 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1002 01:27:48.789365 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1002 01:27:48.789427 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var:0
I1002 01:27:48.789499 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1002 01:27:48.789554 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var:0
I1002 01:27:48.789615 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1002 01:27:48.789671 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var:0
I1002 01:27:48.789727 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var:0
I1002 01:27:48.789787 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var:0
I1002 01:27:48.789844 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var:0
I1002 01:27:48.789905 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var:0
I1002 01:27:48.789967 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var:0
I1002 01:27:48.790028 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var:0
I1002 01:27:48.790091 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var:0
I1002 01:27:48.790153 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1002 01:27:48.790214 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1002 01:27:48.790276 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1002 01:27:48.790339 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1002 01:27:48.790400 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1002 01:27:48.790457 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var:0
I1002 01:27:48.790518 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1002 01:27:48.790580 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var:0
I1002 01:27:48.790636 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1002 01:27:48.790687 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var:0
I1002 01:27:48.790741 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var:0
I1002 01:27:48.790800 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var:0
I1002 01:27:48.790859 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var:0
I1002 01:27:48.790927 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var:0
I1002 01:27:48.790988 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var:0
I1002 01:27:48.791041 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var:0
I1002 01:27:48.791100 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var:0
I1002 01:27:48.791157 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1002 01:27:48.791216 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1002 01:27:48.791273 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1002 01:27:48.791344 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1002 01:27:48.791405 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1002 01:27:48.791466 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var:0
I1002 01:27:48.791527 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1002 01:27:48.791587 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var:0
I1002 01:27:48.791649 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1002 01:27:48.791705 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var:0
I1002 01:27:48.791765 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var:0
I1002 01:27:48.791828 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var:0
I1002 01:27:48.791889 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var:0
I1002 01:27:48.791955 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var:0
I1002 01:27:48.792010 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var:0
I1002 01:27:48.792071 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var:0
I1002 01:27:48.792133 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var:0
I1002 01:27:48.792194 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1002 01:27:48.792254 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1002 01:27:48.792315 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1002 01:27:48.792376 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1002 01:27:48.792441 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1002 01:27:48.792502 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var:0
I1002 01:27:48.792562 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1002 01:27:48.792624 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var:0
I1002 01:27:48.792686 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1002 01:27:48.792747 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var:0
I1002 01:27:48.792808 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var:0
I1002 01:27:48.792867 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var:0
I1002 01:27:48.792928 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var:0
I1002 01:27:48.792990 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var:0
I1002 01:27:48.793049 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var:0
I1002 01:27:48.793109 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var:0
I1002 01:27:48.793168 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var:0
I1002 01:27:48.793228 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1002 01:27:48.793287 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1002 01:27:48.793348 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1002 01:27:48.793407 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1002 01:27:48.793466 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1002 01:27:48.793525 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var:0
I1002 01:27:48.793586 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1002 01:27:48.793643 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var:0
I1002 01:27:48.793694 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1002 01:27:48.793753 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var:0
I1002 01:27:48.793808 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var:0
I1002 01:27:48.793873 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var:0
I1002 01:27:48.793929 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var:0
I1002 01:27:48.793983 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var:0
I1002 01:27:48.794042 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var:0
I1002 01:27:48.794102 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var:0
I1002 01:27:48.794157 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var:0
I1002 01:27:48.794218 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1002 01:27:48.794274 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1002 01:27:48.794330 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1002 01:27:48.794391 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1002 01:27:48.794445 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1002 01:27:48.794505 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var:0
I1002 01:27:48.794558 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1002 01:27:48.794617 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var:0
I1002 01:27:48.794677 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1002 01:27:48.794733 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var:0
I1002 01:27:48.794787 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var:0
I1002 01:27:48.794847 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var:0
I1002 01:27:48.794907 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var:0
I1002 01:27:48.794961 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var:0
I1002 01:27:48.795020 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var:0
I1002 01:27:48.795072 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var:0
I1002 01:27:48.795126 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var:0
I1002 01:27:48.795181 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1002 01:27:48.795241 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1002 01:27:48.795317 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1002 01:27:48.795382 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1002 01:27:48.795438 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1002 01:27:48.795497 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var:0
I1002 01:27:48.795553 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1002 01:27:48.795612 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var:0
I1002 01:27:48.795672 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1002 01:27:48.795734 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var:0
I1002 01:27:48.795794 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var:0
I1002 01:27:48.795860 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var:0
I1002 01:27:48.795917 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var:0
I1002 01:27:48.795976 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var:0
I1002 01:27:48.796031 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var:0
I1002 01:27:48.796085 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var:0
I1002 01:27:48.796144 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var:0
I1002 01:27:48.796199 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1002 01:27:48.796254 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1002 01:27:48.796313 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1002 01:27:48.796370 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1002 01:27:48.796429 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1002 01:27:48.796482 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var:0
I1002 01:27:48.796536 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1002 01:27:48.796595 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var:0
I1002 01:27:48.796651 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1002 01:27:48.796720 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var:0
I1002 01:27:48.796781 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var:0
I1002 01:27:48.796840 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var:0
I1002 01:27:48.796900 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var:0
I1002 01:27:48.796960 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var:0
I1002 01:27:48.797020 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var:0
I1002 01:27:48.797075 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var:0
I1002 01:27:48.797134 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var:0
I1002 01:27:48.797193 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1002 01:27:48.797251 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1002 01:27:48.797311 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1002 01:27:48.797370 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1002 01:27:48.797431 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1002 01:27:48.797487 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var:0
I1002 01:27:48.797542 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1002 01:27:48.797600 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var:0
I1002 01:27:48.797660 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1002 01:27:48.797715 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var:0
I1002 01:27:48.797770 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var:0
I1002 01:27:48.797824 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var:0
I1002 01:27:48.797878 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var:0
I1002 01:27:48.797932 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var:0
I1002 01:27:48.797986 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var:0
I1002 01:27:48.798045 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var:0
I1002 01:27:48.798105 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var:0
I1002 01:27:48.798171 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1002 01:27:48.798231 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1002 01:27:48.798294 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1002 01:27:48.798353 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1002 01:27:48.798413 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1002 01:27:48.798474 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var:0
I1002 01:27:48.798530 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1002 01:27:48.798582 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var:0
I1002 01:27:48.798637 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1002 01:27:48.798692 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var:0
I1002 01:27:48.798749 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var:0
I1002 01:27:48.798803 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var:0
I1002 01:27:48.798863 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var:0
I1002 01:27:48.798918 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var:0
I1002 01:27:48.798972 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var:0
I1002 01:27:48.799033 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var:0
I1002 01:27:48.799093 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var:0
I1002 01:27:48.799154 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1002 01:27:48.799215 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1002 01:27:48.799276 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1002 01:27:48.799355 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1002 01:27:48.799411 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1002 01:27:48.799471 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var:0
I1002 01:27:48.799530 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1002 01:27:48.799596 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var:0
I1002 01:27:48.799653 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1002 01:27:48.799707 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var:0
I1002 01:27:48.799762 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var:0
I1002 01:27:48.799816 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var:0
I1002 01:27:48.799870 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var:0
I1002 01:27:48.799930 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var:0
I1002 01:27:48.799990 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var:0
I1002 01:27:48.800050 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var:0
I1002 01:27:48.800105 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var:0
I1002 01:27:48.800163 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1002 01:27:48.800224 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1002 01:27:48.800283 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1002 01:27:48.800344 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1002 01:27:48.800404 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1002 01:27:48.800463 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var:0
I1002 01:27:48.800523 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1002 01:27:48.800581 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var:0
I1002 01:27:48.800641 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1002 01:27:48.800702 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var:0
I1002 01:27:48.800761 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var:0
I1002 01:27:48.800828 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var:0
I1002 01:27:48.800886 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var:0
I1002 01:27:48.800946 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var:0
I1002 01:27:48.801005 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var:0
I1002 01:27:48.801071 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var:0
I1002 01:27:48.801130 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var:0
I1002 01:27:48.801187 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1002 01:27:48.801239 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1002 01:27:48.801299 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1002 01:27:48.801359 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1002 01:27:48.801426 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1002 01:27:48.801482 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var:0
I1002 01:27:48.801543 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1002 01:27:48.801599 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var:0
I1002 01:27:48.801658 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1002 01:27:48.801711 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var:0
I1002 01:27:48.801762 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var:0
I1002 01:27:48.801817 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var:0
I1002 01:27:48.801878 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var:0
I1002 01:27:48.801948 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var:0
I1002 01:27:48.802009 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var:0
I1002 01:27:48.802068 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var:0
I1002 01:27:48.802128 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var:0
I1002 01:27:48.802187 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1002 01:27:48.802247 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1002 01:27:48.802303 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1002 01:27:48.802355 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1002 01:27:48.802412 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1002 01:27:48.802478 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var:0
I1002 01:27:48.802540 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1002 01:27:48.802598 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var:0
I1002 01:27:48.802657 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1002 01:27:48.802717 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var:0
I1002 01:27:48.802774 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var:0
I1002 01:27:48.802841 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var:0
I1002 01:27:48.802902 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var:0
I1002 01:27:48.802956 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var:0
I1002 01:27:48.803010 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var:0
I1002 01:27:48.803062 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var:0
I1002 01:27:48.803121 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var:0
I1002 01:27:48.803176 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1002 01:27:48.803231 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1002 01:27:48.803285 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1002 01:27:48.803361 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1002 01:27:48.803418 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1002 01:27:48.803478 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var:0
I1002 01:27:48.803539 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1002 01:27:48.803599 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var:0
I1002 01:27:48.803654 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1002 01:27:48.803713 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var:0
I1002 01:27:48.803773 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var:0
I1002 01:27:48.803835 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var:0
I1002 01:27:48.803895 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var:0
I1002 01:27:48.803954 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var:0
I1002 01:27:48.804014 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var:0
I1002 01:27:48.804074 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var:0
I1002 01:27:48.804130 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var:0
I1002 01:27:48.804188 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1002 01:27:48.804249 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1002 01:27:48.804309 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1002 01:27:48.804373 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1002 01:27:48.804435 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1002 01:27:48.804491 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var:0
I1002 01:27:48.804552 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1002 01:27:48.804613 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var:0
I1002 01:27:48.804670 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1002 01:27:48.804729 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var:0
I1002 01:27:48.804787 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var:0
I1002 01:27:48.804847 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var:0
I1002 01:27:48.804909 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var:0
I1002 01:27:48.804971 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var:0
I1002 01:27:48.805025 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var:0
I1002 01:27:48.805080 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var:0
I1002 01:27:48.805139 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var:0
I1002 01:27:48.805196 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1002 01:27:48.805252 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1002 01:27:48.805313 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1002 01:27:48.805379 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1002 01:27:48.805441 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1002 01:27:48.805501 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var:0
I1002 01:27:48.805562 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1002 01:27:48.805624 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var:0
I1002 01:27:48.805679 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1002 01:27:48.805738 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var:0
I1002 01:27:48.805791 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var:0
I1002 01:27:48.805841 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var:0
I1002 01:27:48.805895 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var:0
I1002 01:27:48.805946 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var:0
I1002 01:27:48.806001 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var:0
I1002 01:27:48.806053 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var:0
I1002 01:27:48.806112 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var:0
I1002 01:27:48.806172 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1002 01:27:48.806225 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1002 01:27:48.806285 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1002 01:27:48.806337 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1002 01:27:48.806397 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1002 01:27:48.806459 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var:0
I1002 01:27:48.806519 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1002 01:27:48.806574 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var:0
I1002 01:27:48.806634 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1002 01:27:48.806694 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var:0
I1002 01:27:48.806753 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var:0
I1002 01:27:48.806818 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var:0
I1002 01:27:48.806874 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var:0
I1002 01:27:48.806927 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var:0
I1002 01:27:48.806977 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var:0
I1002 01:27:48.807032 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var:0
I1002 01:27:48.807083 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var:0
I1002 01:27:48.807142 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1002 01:27:48.807202 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1002 01:27:48.807263 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1002 01:27:48.807331 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1002 01:27:48.807395 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1002 01:27:48.807453 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var:0
I1002 01:27:48.807507 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1002 01:27:48.807560 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var:0
I1002 01:27:48.807613 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1002 01:27:48.807673 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var:0
I1002 01:27:48.807734 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var:0
I1002 01:27:48.807795 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var:0
I1002 01:27:48.807850 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var:0
I1002 01:27:48.807911 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var:0
I1002 01:27:48.807972 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var:0
I1002 01:27:48.808032 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var:0
I1002 01:27:48.808092 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var:0
I1002 01:27:48.808146 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1002 01:27:48.808211 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1002 01:27:48.808273 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1002 01:27:48.808327 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1002 01:27:48.808379 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1002 01:27:48.808433 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var:0
I1002 01:27:48.808493 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1002 01:27:48.808555 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var:0
I1002 01:27:48.808613 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1002 01:27:48.808673 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var:0
I1002 01:27:48.808734 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var:0
I1002 01:27:48.808786 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var:0
I1002 01:27:48.808845 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var:0
I1002 01:27:48.808897 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var:0
I1002 01:27:48.808950 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var:0
I1002 01:27:48.809007 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var:0
I1002 01:27:48.809066 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var:0
I1002 01:27:48.809126 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1002 01:27:48.809182 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1002 01:27:48.809236 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1002 01:27:48.809298 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1002 01:27:48.809354 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1002 01:27:48.809413 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var:0
I1002 01:27:48.809470 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1002 01:27:48.809530 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var:0
I1002 01:27:48.809587 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1002 01:27:48.809648 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var:0
I1002 01:27:48.809700 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var:0
I1002 01:27:48.809759 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var:0
I1002 01:27:48.809817 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var:0
I1002 01:27:48.809872 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var:0
I1002 01:27:48.809931 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var:0
I1002 01:27:48.809984 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var:0
I1002 01:27:48.810042 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var:0
I1002 01:27:48.810103 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1002 01:27:48.810163 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1002 01:27:48.810224 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1002 01:27:48.810284 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1002 01:27:48.810347 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1002 01:27:48.810402 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var:0
I1002 01:27:48.810457 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1002 01:27:48.810516 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var:0
I1002 01:27:48.810576 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1002 01:27:48.810632 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var:0
I1002 01:27:48.810690 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var:0
I1002 01:27:48.810751 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var:0
I1002 01:27:48.810811 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var:0
I1002 01:27:48.810872 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var:0
I1002 01:27:48.810932 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var:0
I1002 01:27:48.810996 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var:0
I1002 01:27:48.811060 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var:0
I1002 01:27:48.811116 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1002 01:27:48.811175 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1002 01:27:48.811235 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1002 01:27:48.811312 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1002 01:27:48.811377 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1002 01:27:48.811437 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var:0
I1002 01:27:48.811493 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1002 01:27:48.811553 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var:0
I1002 01:27:48.811615 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1002 01:27:48.811676 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var:0
I1002 01:27:48.811731 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var:0
I1002 01:27:48.811789 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var:0
I1002 01:27:48.811845 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var:0
I1002 01:27:48.811904 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var:0
I1002 01:27:48.811960 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var:0
I1002 01:27:48.812015 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var:0
I1002 01:27:48.812074 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var:0
I1002 01:27:48.812130 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1002 01:27:48.812189 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1002 01:27:48.812240 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1002 01:27:48.812298 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1002 01:27:48.812350 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1002 01:27:48.812409 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var:0
I1002 01:27:48.812474 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1002 01:27:48.812528 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var:0
I1002 01:27:48.812587 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1002 01:27:48.812647 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var:0
I1002 01:27:48.812702 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var:0
I1002 01:27:48.812754 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var:0
I1002 01:27:48.812808 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var:0
I1002 01:27:48.812868 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var:0
I1002 01:27:48.812923 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var:0
I1002 01:27:48.812978 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var:0
I1002 01:27:48.813037 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var:0
I1002 01:27:48.813099 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1002 01:27:48.813156 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1002 01:27:48.813215 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1002 01:27:48.813274 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1002 01:27:48.813326 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1002 01:27:48.813384 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var:0
I1002 01:27:48.813444 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1002 01:27:48.813499 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var:0
I1002 01:27:48.813554 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1002 01:27:48.813614 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var:0
I1002 01:27:48.813674 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var:0
I1002 01:27:48.813727 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var:0
I1002 01:27:48.813785 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var:0
I1002 01:27:48.813837 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var:0
I1002 01:27:48.813895 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var:0
I1002 01:27:48.813956 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var:0
I1002 01:27:48.814016 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var:0
I1002 01:27:48.814072 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1002 01:27:48.814127 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1002 01:27:48.814188 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1002 01:27:48.814254 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1002 01:27:48.814310 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1002 01:27:48.814362 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var:0
I1002 01:27:48.814423 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1002 01:27:48.814481 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var:0
I1002 01:27:48.814540 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1002 01:27:48.814599 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var:0
I1002 01:27:48.814654 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var:0
I1002 01:27:48.814708 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var:0
I1002 01:27:48.814767 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var:0
I1002 01:27:48.814827 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var:0
I1002 01:27:48.814887 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var:0
I1002 01:27:48.814943 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var:0
I1002 01:27:48.814995 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var:0
I1002 01:27:48.815049 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1002 01:27:48.815108 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1002 01:27:48.815162 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1002 01:27:48.815214 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1002 01:27:48.815273 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1002 01:27:48.815349 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var:0
I1002 01:27:48.815412 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1002 01:27:48.815472 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var:0
I1002 01:27:48.815526 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1002 01:27:48.815586 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var:0
I1002 01:27:48.815642 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var:0
I1002 01:27:48.815701 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var:0
I1002 01:27:48.815761 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var:0
I1002 01:27:48.815817 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var:0
I1002 01:27:48.815871 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var:0
I1002 01:27:48.815927 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var:0
I1002 01:27:48.815982 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var:0
I1002 01:27:48.816042 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1002 01:27:48.816094 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1002 01:27:48.816153 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1002 01:27:48.816206 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1002 01:27:48.816264 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1002 01:27:48.816321 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var:0
I1002 01:27:48.816380 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1002 01:27:48.816439 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var:0
I1002 01:27:48.816500 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1002 01:27:48.816555 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var:0
I1002 01:27:48.816613 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var:0
I1002 01:27:48.816665 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var:0
I1002 01:27:48.816729 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var:0
I1002 01:27:48.816790 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var:0
I1002 01:27:48.816850 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var:0
I1002 01:27:48.816906 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var:0
I1002 01:27:48.816958 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var:0
I1002 01:27:48.817013 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1002 01:27:48.817072 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1002 01:27:48.817129 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1002 01:27:48.817184 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1002 01:27:48.817243 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1002 01:27:48.817313 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var:0
I1002 01:27:48.817373 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1002 01:27:48.817434 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var:0
I1002 01:27:48.817495 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1002 01:27:48.817551 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var:0
I1002 01:27:48.817611 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var:0
I1002 01:27:48.817672 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var:0
I1002 01:27:48.817724 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var:0
I1002 01:27:48.817794 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var:0
I1002 01:27:48.817854 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var:0
I1002 01:27:48.817914 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var:0
I1002 01:27:48.817974 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var:0
I1002 01:27:48.818035 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1002 01:27:48.818096 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1002 01:27:48.818161 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1002 01:27:48.818218 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1002 01:27:48.818270 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1002 01:27:48.818326 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var:0
I1002 01:27:48.818387 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1002 01:27:48.818445 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var:0
I1002 01:27:48.818505 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1002 01:27:48.818564 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var:0
I1002 01:27:48.818623 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var:0
I1002 01:27:48.818679 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var:0
I1002 01:27:48.818737 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var:0
I1002 01:27:48.818793 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var:0
I1002 01:27:48.818846 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var:0
I1002 01:27:48.818901 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var:0
I1002 01:27:48.818958 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var:0
I1002 01:27:48.819014 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1002 01:27:48.819073 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1002 01:27:48.819128 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1002 01:27:48.819188 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1002 01:27:48.819241 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1002 01:27:48.819319 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var:0
I1002 01:27:48.819382 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1002 01:27:48.819437 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var:0
I1002 01:27:48.819498 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1002 01:27:48.819554 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var:0
I1002 01:27:48.819619 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var:0
I1002 01:27:48.819675 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var:0
I1002 01:27:48.819734 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var:0
I1002 01:27:48.819794 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var:0
I1002 01:27:48.819849 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var:0
I1002 01:27:48.819910 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var:0
I1002 01:27:48.819963 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var:0
I1002 01:27:48.820023 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1002 01:27:48.820085 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1002 01:27:48.820146 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1002 01:27:48.820203 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1002 01:27:48.820263 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1002 01:27:48.820319 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var:0
I1002 01:27:48.820375 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1002 01:27:48.820435 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var:0
I1002 01:27:48.820495 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1002 01:27:48.820555 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var:0
I1002 01:27:48.820614 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var:0
I1002 01:27:48.820675 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var:0
I1002 01:27:48.820736 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var:0
I1002 01:27:48.820791 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var:0
I1002 01:27:48.820851 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var:0
I1002 01:27:48.820912 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var:0
I1002 01:27:48.820968 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var:0
I1002 01:27:48.821023 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var:0
I1002 01:27:48.821082 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var:0
I1002 01:27:48.821143 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var:0
I1002 01:27:48.821196 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var:0
I1002 01:27:48.821249 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var:0
I1002 01:27:48.821308 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var:0
I1002 01:27:48.821360 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var:0
I1002 01:27:48.821418 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var:0
I1002 01:27:48.821471 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var:0
I1002 01:27:48.821522 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var:0
I1002 01:27:48.821580 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var:0
I1002 01:27:48.821640 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var:0
I1002 01:27:48.821698 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var:0
I1002 01:27:48.821758 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var:0
I1002 01:27:48.821817 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var:0
I1002 01:27:48.821877 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var:0
I1002 01:27:48.821942 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var:0
I1002 01:27:48.822013 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var:0
I1002 01:27:48.822072 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var:0
I1002 01:27:48.822133 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var:0
I1002 01:27:48.822193 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var:0
I1002 01:27:48.822258 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var:0
I1002 01:27:48.822320 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var:0
I1002 01:27:48.822381 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var:0
I1002 01:27:48.822443 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var:0
I1002 01:27:48.822495 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var:0
I1002 01:27:54.669445 139650252134208 gpipe.py:457] cell 3 input [<tf.Tensor 'arg287:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'arg288:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I1002 01:28:00.689887 139650252134208 gpipe.py:457] cell 2 input [<tf.Tensor 'arg255:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'arg256:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I1002 01:28:06.482120 139650252134208 gpipe.py:457] cell 1 input [<tf.Tensor 'arg255:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'arg256:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I1002 01:28:12.380090 139650252134208 gpipe.py:457] cell 0 input [<tf.Tensor 'arg259:0' shape=(1024, 1) dtype=int32>, <tf.Tensor 'arg260:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None, None, None]
I1002 01:28:23.897172 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.emb.src_token_emb.wm: <tf.Variable '1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var:0' shape=(32000, 2048) dtype=float32_ref>
I1002 01:28:23.897456 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1002 01:28:23.897553 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1002 01:28:23.897640 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.897717 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1002 01:28:23.897794 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.897867 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.897940 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1002 01:28:23.898013 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.898089 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.898157 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.898228 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.898292 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.898359 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.898421 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.898497 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.898560 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.898631 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.898700 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1002 01:28:23.898770 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1002 01:28:23.898842 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.898908 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1002 01:28:23.898972 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.899040 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.899105 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1002 01:28:23.899173 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.899248 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.899335 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.899404 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.899474 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.899554 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.899621 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.899690 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.899759 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.899828 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.899897 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1002 01:28:23.899960 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1002 01:28:23.900025 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.900089 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1002 01:28:23.900160 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.900229 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.900299 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1002 01:28:23.900368 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.900441 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.900511 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.900589 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.900659 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.900731 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.900800 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.900873 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.900943 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.901011 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.901079 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1002 01:28:23.901148 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1002 01:28:23.901220 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.901290 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1002 01:28:23.901363 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.901433 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.901502 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1002 01:28:23.901571 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.901652 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.901722 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.901796 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.901864 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.901937 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.902006 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.902079 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.902149 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.902218 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.902287 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1002 01:28:23.902356 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1002 01:28:23.902429 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.902498 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1002 01:28:23.902571 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.902669 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.902798 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1002 01:28:23.902911 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.903030 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.903142 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.903248 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.903336 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.903435 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.903520 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.903607 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.903689 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.903781 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.903872 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1002 01:28:23.903979 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1002 01:28:23.904096 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.904194 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1002 01:28:23.904270 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.904354 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.904445 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1002 01:28:23.904528 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.904631 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.904740 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.904844 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.904939 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.905055 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.905162 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.905261 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.905364 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.905449 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.905539 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1002 01:28:23.905642 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1002 01:28:23.905728 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.905819 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1002 01:28:23.905906 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.905991 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.906074 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1002 01:28:23.906153 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.906225 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.906316 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.906399 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.906465 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.906572 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.906655 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.906753 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.906840 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.906913 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.907002 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1002 01:28:23.907079 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1002 01:28:23.907152 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.907222 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1002 01:28:23.907327 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.907402 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.907482 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1002 01:28:23.907575 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.907660 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.907747 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.907842 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.907923 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.908019 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.908096 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.908188 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.908280 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.908356 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.908435 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1002 01:28:23.908522 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1002 01:28:23.908620 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.908694 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1002 01:28:23.908777 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.908864 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.908934 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1002 01:28:23.909016 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.909105 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.909195 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.909276 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.909373 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.909460 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.909545 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.909628 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.909705 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.909795 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.909882 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1002 01:28:23.909965 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1002 01:28:23.910058 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.910130 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1002 01:28:23.910227 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.910310 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.910394 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1002 01:28:23.910482 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.910558 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.910658 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.910743 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.910835 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.910932 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.911019 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.911102 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.911197 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.911278 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.911387 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1002 01:28:23.911470 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1002 01:28:23.911555 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.911644 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1002 01:28:23.911741 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.911824 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.911900 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1002 01:28:23.911986 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.912075 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.912173 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.912249 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.912337 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.912425 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.912508 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.912584 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.912667 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.912746 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.912834 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1002 01:28:23.912929 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1002 01:28:23.913013 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.913094 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1002 01:28:23.913199 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.913288 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.913382 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1002 01:28:23.913454 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.913529 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.913602 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.913700 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.913784 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.913859 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.913960 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.914047 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.914132 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.914223 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.914305 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1002 01:28:23.914390 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1002 01:28:23.914478 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.914571 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1002 01:28:23.914654 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.914733 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.914825 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1002 01:28:23.914918 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.915005 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.915088 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.915167 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.915263 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.915356 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.915427 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.915512 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.915589 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.915668 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.915756 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1002 01:28:23.915847 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1002 01:28:23.915939 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.916024 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1002 01:28:23.916110 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.916197 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.916263 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1002 01:28:23.916351 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.916434 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.916526 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.916605 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.916693 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.916782 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.916865 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.916963 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.917036 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.917106 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.917191 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1002 01:28:23.917278 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1002 01:28:23.917371 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.917443 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1002 01:28:23.917536 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.917621 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.917705 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1002 01:28:23.917788 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.917873 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.917945 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.918042 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.918140 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.918228 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.918315 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.918407 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.918489 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.918585 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.918662 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1002 01:28:23.918752 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1002 01:28:23.918841 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.918922 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1002 01:28:23.919027 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.919110 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.919187 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1002 01:28:23.919279 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.919386 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.919462 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.919551 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.919635 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.919725 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.919814 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.919900 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.919982 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.920063 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.920155 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1002 01:28:23.920242 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1002 01:28:23.920330 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.920411 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1002 01:28:23.920493 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.920581 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.920668 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1002 01:28:23.920760 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.920850 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.920923 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.921025 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.921102 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.921195 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.921288 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.921381 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.921458 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.921534 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.921616 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1002 01:28:23.921708 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1002 01:28:23.921786 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.921862 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1002 01:28:23.921937 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.922027 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.922116 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1002 01:28:23.922204 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.922290 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.922363 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.922456 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.922541 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.922635 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.922714 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.922788 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.922863 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.922950 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.923042 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1002 01:28:23.923116 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1002 01:28:23.923208 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.923291 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1002 01:28:23.923399 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.923494 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.923575 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1002 01:28:23.923676 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.923753 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.923836 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.923925 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.924005 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.924097 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.924190 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.924283 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.924355 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.924448 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.924525 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1002 01:28:23.924596 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1002 01:28:23.924677 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.924761 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1002 01:28:23.924834 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.924911 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.925005 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1002 01:28:23.925102 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.925187 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.925268 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.925358 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.925434 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.925507 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.925597 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.925695 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.925769 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.925860 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.925937 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1002 01:28:23.926013 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1002 01:28:23.926098 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.926193 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1002 01:28:23.926288 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.926359 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.926440 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1002 01:28:23.926531 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.926620 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.926693 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.926791 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.926864 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.926959 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.927047 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.927139 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.927219 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.927314 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.927400 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1002 01:28:23.927482 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1002 01:28:23.927578 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.927661 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1002 01:28:23.927741 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.927829 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.927910 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1002 01:28:23.927982 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.928057 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.928134 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.928223 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.928308 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.928398 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.928472 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.928566 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.928646 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.928732 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.928813 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1002 01:28:23.928910 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1002 01:28:23.928993 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.929073 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1002 01:28:23.929163 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.929248 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.929319 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1002 01:28:23.929421 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.929514 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.929585 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.929692 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.929780 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.929867 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.929951 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.930026 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.930118 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.930204 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.930269 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1002 01:28:23.930352 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1002 01:28:23.930438 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.930511 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1002 01:28:23.930600 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.930694 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.930775 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1002 01:28:23.930858 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.930955 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.931037 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.931108 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.931176 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.931249 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.931359 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.931440 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.931518 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.931606 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.931689 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1002 01:28:23.931771 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1002 01:28:23.931857 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.931961 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1002 01:28:23.932051 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.932124 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.932204 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1002 01:28:23.932284 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.932372 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.932459 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.932540 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.932627 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.932712 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.932802 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.932889 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.932969 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.933051 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.933139 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1002 01:28:23.933223 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1002 01:28:23.933295 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.933385 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1002 01:28:23.933474 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.933546 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.933616 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1002 01:28:23.933685 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.933775 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.933851 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.933954 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.934042 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.934127 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.934209 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.934292 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.934364 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.934454 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.934540 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1002 01:28:23.934620 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1002 01:28:23.934713 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.934788 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1002 01:28:23.934860 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.934950 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.935019 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1002 01:28:23.935110 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.935202 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.935281 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.935398 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.935474 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.935582 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.935664 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.935765 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.935848 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.935934 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.936016 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1002 01:28:23.936103 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1002 01:28:23.936194 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.936283 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1002 01:28:23.936379 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.936464 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.936549 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1002 01:28:23.936637 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.936733 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.936820 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.936920 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.937005 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.937102 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.937190 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.937282 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.937368 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.937447 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.937538 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1002 01:28:23.937623 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1002 01:28:23.937713 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.937796 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1002 01:28:23.937894 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.937981 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.938065 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1002 01:28:23.938152 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.938252 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.938337 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.938432 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.938523 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.938612 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.938698 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.938786 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.938865 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.938957 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.939033 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1002 01:28:23.939115 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1002 01:28:23.939212 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.939292 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1002 01:28:23.939404 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.939488 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.939572 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1002 01:28:23.939665 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.939762 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.939850 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.939943 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.940035 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.940126 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.940210 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.940294 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.940385 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.940470 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.940556 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1002 01:28:23.940642 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1002 01:28:23.940727 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.940830 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1002 01:28:23.940922 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.941006 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.941100 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1002 01:28:23.941184 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.941270 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.941359 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.941458 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.941540 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.941630 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.941715 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.941807 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.941891 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.941982 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.942067 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1002 01:28:23.942155 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1002 01:28:23.942245 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.942328 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1002 01:28:23.942414 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.942493 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.942586 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1002 01:28:23.942673 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.942763 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.942856 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.942943 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.943034 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.943125 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.943210 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:28:23.943318 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.943412 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.943493 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:28:23.943581 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_0: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var:0' shape=(2000,) dtype=float32_ref>
I1002 01:28:23.943666 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_1: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var:0' shape=(2000,) dtype=float32_ref>
I1002 01:28:23.943758 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_10: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var:0' shape=(2000,) dtype=float32_ref>
I1002 01:28:23.943841 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_11: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var:0' shape=(2000,) dtype=float32_ref>
I1002 01:28:23.943943 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_12: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var:0' shape=(2000,) dtype=float32_ref>
I1002 01:28:23.944029 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_13: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var:0' shape=(2000,) dtype=float32_ref>
I1002 01:28:23.944120 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_14: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var:0' shape=(2000,) dtype=float32_ref>
I1002 01:28:23.944204 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_15: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var:0' shape=(2000,) dtype=float32_ref>
I1002 01:28:23.944300 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_2: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var:0' shape=(2000,) dtype=float32_ref>
I1002 01:28:23.944386 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_3: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var:0' shape=(2000,) dtype=float32_ref>
I1002 01:28:23.944475 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_4: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var:0' shape=(2000,) dtype=float32_ref>
I1002 01:28:23.944565 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_5: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var:0' shape=(2000,) dtype=float32_ref>
I1002 01:28:23.944646 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_6: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var:0' shape=(2000,) dtype=float32_ref>
I1002 01:28:23.944733 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_7: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var:0' shape=(2000,) dtype=float32_ref>
I1002 01:28:23.944810 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_8: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var:0' shape=(2000,) dtype=float32_ref>
I1002 01:28:23.944888 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_9: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var:0' shape=(2000,) dtype=float32_ref>
I1002 01:28:23.944978 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_0: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var:0' shape=(2048, 2000) dtype=float32_ref>
I1002 01:28:23.945066 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_1: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var:0' shape=(2048, 2000) dtype=float32_ref>
I1002 01:28:23.945161 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_10: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var:0' shape=(2048, 2000) dtype=float32_ref>
I1002 01:28:23.945255 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_11: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var:0' shape=(2048, 2000) dtype=float32_ref>
I1002 01:28:23.945348 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_12: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var:0' shape=(2048, 2000) dtype=float32_ref>
I1002 01:28:23.945445 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_13: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var:0' shape=(2048, 2000) dtype=float32_ref>
I1002 01:28:23.945544 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_14: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var:0' shape=(2048, 2000) dtype=float32_ref>
I1002 01:28:23.945653 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_15: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var:0' shape=(2048, 2000) dtype=float32_ref>
I1002 01:28:23.945730 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_2: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var:0' shape=(2048, 2000) dtype=float32_ref>
I1002 01:28:23.945823 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_3: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var:0' shape=(2048, 2000) dtype=float32_ref>
I1002 01:28:23.945919 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_4: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var:0' shape=(2048, 2000) dtype=float32_ref>
I1002 01:28:23.946017 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_5: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var:0' shape=(2048, 2000) dtype=float32_ref>
I1002 01:28:23.946120 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_6: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var:0' shape=(2048, 2000) dtype=float32_ref>
I1002 01:28:23.946201 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_7: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var:0' shape=(2048, 2000) dtype=float32_ref>
I1002 01:28:23.946294 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_8: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var:0' shape=(2048, 2000) dtype=float32_ref>
I1002 01:28:23.946384 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_9: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var:0' shape=(2048, 2000) dtype=float32_ref>
I1002 01:28:33.835618 139650252134208 learner.py:279] gradient_adjuster=<bound method LanguageModel.AdjustGradients of <lingvo.tasks.lm.model.FixedShapeInputLanguageModel object at 0x7f02502fbbe0>>
I1002 01:28:39.117217 139650252134208 cluster.py:515] Place variable beta1_power on /job:local/replica:0/task:0/device:CPU:0 6970291220
I1002 01:28:39.120275 139650252134208 cluster.py:515] Place variable beta2_power on /job:local/replica:0/task:0/device:CPU:0 6970291224
I1002 01:28:39.125504 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7232435224
I1002 01:28:39.130622 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7494579224
I1002 01:28:39.135821 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7494611992
I1002 01:28:39.140883 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7494644760
I1002 01:28:39.145943 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7561753624
I1002 01:28:39.150999 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7628862488
I1002 01:28:39.155995 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7628870680
I1002 01:28:39.161064 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7628878872
I1002 01:28:39.166048 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7695987736
I1002 01:28:39.171105 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7763096600
I1002 01:28:39.176099 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7763104792
I1002 01:28:39.181154 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7763112984
I1002 01:28:39.186152 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7763121176
I1002 01:28:39.191211 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7763129368
I1002 01:28:39.195032 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7763129880
I1002 01:28:39.198823 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7763130392
I1002 01:28:39.203765 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7779907608
I1002 01:28:39.208837 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7796684824
I1002 01:28:39.213830 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7796693016
I1002 01:28:39.218916 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7796701208
I1002 01:28:39.223943 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7813478424
I1002 01:28:39.229604 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7830255640
I1002 01:28:39.234638 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7830263832
I1002 01:28:39.239611 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7830272024
I1002 01:28:39.244683 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7847049240
I1002 01:28:39.249668 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7863826456
I1002 01:28:39.254763 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7863834648
I1002 01:28:39.259762 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7863842840
I1002 01:28:39.264859 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7880620056
I1002 01:28:39.269832 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7897397272
I1002 01:28:39.274913 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7897405464
I1002 01:28:39.279984 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7897413656
I1002 01:28:39.285073 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7897421848
I1002 01:28:39.290150 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7897430040
I1002 01:28:39.295127 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7897438232
I1002 01:28:39.300229 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7897446424
I1002 01:28:39.305219 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7897479192
I1002 01:28:39.310279 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7897511960
I1002 01:28:39.315291 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7964620824
I1002 01:28:39.320394 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8031729688
I1002 01:28:39.325374 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8031737880
I1002 01:28:39.330510 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8031746072
I1002 01:28:39.335516 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8098854936
I1002 01:28:39.341101 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8165963800
I1002 01:28:39.346170 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8165971992
I1002 01:28:39.351161 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8165980184
I1002 01:28:39.356273 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8165988376
I1002 01:28:39.361248 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8165996568
I1002 01:28:39.365076 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8165997080
I1002 01:28:39.368936 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8165997592
I1002 01:28:39.373835 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8182774808
I1002 01:28:39.378919 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8199552024
I1002 01:28:39.384047 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8199560216
I1002 01:28:39.389032 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8199568408
I1002 01:28:39.394138 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8216345624
I1002 01:28:39.399143 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8233122840
I1002 01:28:39.404245 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8233131032
I1002 01:28:39.409325 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8233139224
I1002 01:28:39.414424 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8249916440
I1002 01:28:39.419449 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8266693656
I1002 01:28:39.424524 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8266701848
I1002 01:28:39.429532 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8266710040
I1002 01:28:39.434630 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8283487256
I1002 01:28:39.439748 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8300264472
I1002 01:28:39.444705 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8300272664
I1002 01:28:39.450286 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8300280856
I1002 01:28:39.455284 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8300289048
I1002 01:28:39.460437 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8300297240
I1002 01:28:39.465580 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8300305432
I1002 01:28:39.470885 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8300313624
I1002 01:28:39.475982 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8300346392
I1002 01:28:39.481082 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8300379160
I1002 01:28:39.486101 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8367488024
I1002 01:28:39.491209 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8434596888
I1002 01:28:39.496279 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8434605080
I1002 01:28:39.501321 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8434613272
I1002 01:28:39.506727 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8501722136
I1002 01:28:39.511883 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8568831000
I1002 01:28:39.517008 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8568839192
I1002 01:28:39.521998 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8568847384
I1002 01:28:39.527153 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8568855576
I1002 01:28:39.532162 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8568863768
I1002 01:28:39.536118 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8568864280
I1002 01:28:39.539944 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8568864792
I1002 01:28:39.544891 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8585642008
I1002 01:28:39.550016 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8602419224
I1002 01:28:39.555112 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8602427416
I1002 01:28:39.560100 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8602435608
I1002 01:28:39.565821 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8619212824
I1002 01:28:39.570821 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8635990040
I1002 01:28:39.575958 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8635998232
I1002 01:28:39.580953 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8636006424
I1002 01:28:39.586047 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8652783640
I1002 01:28:39.591084 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8669560856
I1002 01:28:39.596233 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8669569048
I1002 01:28:39.601321 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8669577240
I1002 01:28:39.606332 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8686354456
I1002 01:28:39.611443 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8703131672
I1002 01:28:39.616450 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8703139864
I1002 01:28:39.621538 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8703148056
I1002 01:28:39.626509 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8703156248
I1002 01:28:39.631648 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8703164440
I1002 01:28:39.636683 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8703172632
I1002 01:28:39.641799 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8703180824
I1002 01:28:39.647059 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8703213592
I1002 01:28:39.652241 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8703246360
I1002 01:28:39.657275 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8770355224
I1002 01:28:39.662404 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8837464088
I1002 01:28:39.667586 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8837472280
I1002 01:28:39.672621 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8837480472
I1002 01:28:39.678458 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8904589336
I1002 01:28:39.683506 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8971698200
I1002 01:28:39.688621 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8971706392
I1002 01:28:39.693652 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8971714584
I1002 01:28:39.698773 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8971722776
I1002 01:28:39.703804 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8971730968
I1002 01:28:39.707747 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8971731480
I1002 01:28:39.711512 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8971731992
I1002 01:28:39.716451 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8988509208
I1002 01:28:39.721553 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9005286424
I1002 01:28:39.726692 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9005294616
I1002 01:28:39.731703 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9005302808
I1002 01:28:39.736818 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9022080024
I1002 01:28:39.741849 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9038857240
I1002 01:28:39.747061 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9038865432
I1002 01:28:39.752203 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9038873624
I1002 01:28:39.757366 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9055650840
I1002 01:28:39.762661 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9072428056
I1002 01:28:39.768017 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9072436248
I1002 01:28:39.773165 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9072444440
I1002 01:28:39.778247 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9089221656
I1002 01:28:39.783515 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9105998872
I1002 01:28:39.788598 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9106007064
I1002 01:28:39.794292 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9106015256
I1002 01:28:39.799336 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9106023448
I1002 01:28:39.804508 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9106031640
I1002 01:28:39.809521 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9106039832
I1002 01:28:39.814651 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9106048024
I1002 01:28:39.819696 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9106080792
I1002 01:28:39.824816 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9106113560
I1002 01:28:39.829815 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9173222424
I1002 01:28:39.834937 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9240331288
I1002 01:28:39.840069 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9240339480
I1002 01:28:39.845094 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9240347672
I1002 01:28:39.850265 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9307456536
I1002 01:28:39.855287 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9374565400
I1002 01:28:39.860417 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9374573592
I1002 01:28:39.865427 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9374581784
I1002 01:28:39.870523 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9374589976
I1002 01:28:39.875589 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9374598168
I1002 01:28:39.879595 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9374598680
I1002 01:28:39.883402 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9374599192
I1002 01:28:39.888442 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9391376408
I1002 01:28:39.893704 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9408153624
I1002 01:28:39.898821 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9408161816
I1002 01:28:39.903886 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9408170008
I1002 01:28:39.909588 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9424947224
I1002 01:28:39.914661 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9441724440
I1002 01:28:39.919893 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9441732632
I1002 01:28:39.924902 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9441740824
I1002 01:28:39.930070 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9458518040
I1002 01:28:39.935118 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9475295256
I1002 01:28:39.940298 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9475303448
I1002 01:28:39.945417 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9475311640
I1002 01:28:39.950513 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9492088856
I1002 01:28:39.955716 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9508866072
I1002 01:28:39.960736 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9508874264
I1002 01:28:39.965877 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9508882456
I1002 01:28:39.970909 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9508890648
I1002 01:28:39.976134 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9508898840
I1002 01:28:39.981173 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9508907032
I1002 01:28:39.986291 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9508915224
I1002 01:28:39.991337 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9508947992
I1002 01:28:39.996437 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9508980760
I1002 01:28:40.001475 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9576089624
I1002 01:28:40.006664 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9643198488
I1002 01:28:40.011809 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9643206680
I1002 01:28:40.016840 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9643214872
I1002 01:28:40.022526 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9710323736
I1002 01:28:40.027586 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9777432600
I1002 01:28:40.032723 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9777440792
I1002 01:28:40.037761 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9777448984
I1002 01:28:40.042922 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9777457176
I1002 01:28:40.047988 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9777465368
I1002 01:28:40.052018 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9777465880
I1002 01:28:40.055771 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9777466392
I1002 01:28:40.060741 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9794243608
I1002 01:28:40.065864 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9811020824
I1002 01:28:40.070987 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9811029016
I1002 01:28:40.076074 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9811037208
I1002 01:28:40.081195 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9827814424
I1002 01:28:40.086231 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9844591640
I1002 01:28:40.091362 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9844599832
I1002 01:28:40.096393 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9844608024
I1002 01:28:40.101495 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9861385240
I1002 01:28:40.106522 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9878162456
I1002 01:28:40.111676 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9878170648
I1002 01:28:40.116833 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9878178840
I1002 01:28:40.121860 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9894956056
I1002 01:28:40.126989 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9911733272
I1002 01:28:40.132116 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9911741464
I1002 01:28:40.137843 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9911749656
I1002 01:28:40.142848 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9911757848
I1002 01:28:40.148054 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9911766040
I1002 01:28:40.153090 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9911774232
I1002 01:28:40.158185 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9911782424
I1002 01:28:40.163204 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9911815192
I1002 01:28:40.168351 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9911847960
I1002 01:28:40.173355 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9978956824
I1002 01:28:40.178488 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10046065688
I1002 01:28:40.183618 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10046073880
I1002 01:28:40.188642 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10046082072
I1002 01:28:40.193734 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10113190936
I1002 01:28:40.198763 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10180299800
I1002 01:28:40.203934 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10180307992
I1002 01:28:40.208967 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10180316184
I1002 01:28:40.214087 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10180324376
I1002 01:28:40.219115 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10180332568
I1002 01:28:40.223067 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10180333080
I1002 01:28:40.226855 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10180333592
I1002 01:28:40.231824 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10197110808
I1002 01:28:40.236944 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10213888024
I1002 01:28:40.242084 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10213896216
I1002 01:28:40.247118 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10213904408
I1002 01:28:40.252800 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10230681624
I1002 01:28:40.257840 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10247458840
I1002 01:28:40.262970 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10247467032
I1002 01:28:40.268072 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10247475224
I1002 01:28:40.273196 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10264252440
I1002 01:28:40.278232 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10281029656
I1002 01:28:40.283388 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10281037848
I1002 01:28:40.288531 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10281046040
I1002 01:28:40.293609 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10297823256
I1002 01:28:40.298756 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10314600472
I1002 01:28:40.303817 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10314608664
I1002 01:28:40.308932 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10314616856
I1002 01:28:40.313959 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10314625048
I1002 01:28:40.319131 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10314633240
I1002 01:28:40.324219 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10314641432
I1002 01:28:40.329339 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10314649624
I1002 01:28:40.334377 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10314682392
I1002 01:28:40.339513 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10314715160
I1002 01:28:40.344516 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10381824024
I1002 01:28:40.349645 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10448932888
I1002 01:28:40.354782 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10448941080
I1002 01:28:40.359820 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10448949272
I1002 01:28:40.365479 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10516058136
I1002 01:28:40.370487 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10583167000
I1002 01:28:40.375732 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10583175192
I1002 01:28:40.380760 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10583183384
I1002 01:28:40.385914 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10583191576
I1002 01:28:40.390964 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10583199768
I1002 01:28:40.394922 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10583200280
I1002 01:28:40.398709 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10583200792
I1002 01:28:40.403740 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10599978008
I1002 01:28:40.408916 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10616755224
I1002 01:28:40.414076 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10616763416
I1002 01:28:40.419130 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10616771608
I1002 01:28:40.424319 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10633548824
I1002 01:28:40.429369 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10650326040
I1002 01:28:40.434506 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10650334232
I1002 01:28:40.439563 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10650342424
I1002 01:28:40.444720 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10667119640
I1002 01:28:40.449832 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10683896856
I1002 01:28:40.454973 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10683905048
I1002 01:28:40.460138 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10683913240
I1002 01:28:40.465162 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10700690456
I1002 01:28:40.470329 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10717467672
I1002 01:28:40.475424 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10717475864
I1002 01:28:40.481087 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10717484056
I1002 01:28:40.486129 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10717492248
I1002 01:28:40.491336 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10717500440
I1002 01:28:40.496384 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10717508632
I1002 01:28:40.501526 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10717516824
I1002 01:28:40.506577 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10717549592
I1002 01:28:40.511728 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10717582360
I1002 01:28:40.516736 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10784691224
I1002 01:28:40.521911 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10851800088
I1002 01:28:40.527031 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10851808280
I1002 01:28:40.532107 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10851816472
I1002 01:28:40.537251 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10918925336
I1002 01:28:40.542281 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10986034200
I1002 01:28:40.547425 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10986042392
I1002 01:28:40.552482 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10986050584
I1002 01:28:40.557616 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10986058776
I1002 01:28:40.562674 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10986066968
I1002 01:28:40.566642 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10986067480
I1002 01:28:40.570441 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10986067992
I1002 01:28:40.575415 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11002845208
I1002 01:28:40.580596 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11019622424
I1002 01:28:40.585704 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11019630616
I1002 01:28:40.590732 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11019638808
I1002 01:28:40.596427 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11036416024
I1002 01:28:40.601469 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11053193240
I1002 01:28:40.606634 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11053201432
I1002 01:28:40.611696 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11053209624
I1002 01:28:40.616832 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11069986840
I1002 01:28:40.621888 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11086764056
I1002 01:28:40.627070 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11086772248
I1002 01:28:40.632241 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11086780440
I1002 01:28:40.637302 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11103557656
I1002 01:28:40.642453 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11120334872
I1002 01:28:40.647507 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11120343064
I1002 01:28:40.652635 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11120351256
I1002 01:28:40.657701 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11120359448
I1002 01:28:40.662872 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11120367640
I1002 01:28:40.667980 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11120375832
I1002 01:28:40.673095 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11120384024
I1002 01:28:40.678123 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11120416792
I1002 01:28:40.683268 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11120449560
I1002 01:28:40.688343 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11187558424
I1002 01:28:40.693467 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11254667288
I1002 01:28:40.698609 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11254675480
I1002 01:28:40.703670 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11254683672
I1002 01:28:40.709308 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11321792536
I1002 01:28:40.714351 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11388901400
I1002 01:28:40.719516 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11388909592
I1002 01:28:40.724557 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11388917784
I1002 01:28:40.729672 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11388925976
I1002 01:28:40.734764 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11388934168
I1002 01:28:40.738758 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11388934680
I1002 01:28:40.742582 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11388935192
I1002 01:28:40.747579 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11405712408
I1002 01:28:40.752717 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11422489624
I1002 01:28:40.757865 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11422497816
I1002 01:28:40.762907 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11422506008
I1002 01:28:40.768085 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11439283224
I1002 01:28:40.773136 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11456060440
I1002 01:28:40.778295 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11456068632
I1002 01:28:40.783359 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11456076824
I1002 01:28:40.788506 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11472854040
I1002 01:28:40.793550 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11489631256
I1002 01:28:40.798650 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11489639448
I1002 01:28:40.803785 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11489647640
I1002 01:28:40.808845 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11506424856
I1002 01:28:40.814003 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11523202072
I1002 01:28:40.819040 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11523210264
I1002 01:28:40.824731 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11523218456
I1002 01:28:40.829758 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11523226648
I1002 01:28:40.834929 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11523234840
I1002 01:28:40.840024 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11523243032
I1002 01:28:40.845147 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11523251224
I1002 01:28:40.850166 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11523283992
I1002 01:28:40.855283 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11523316760
I1002 01:28:40.860318 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11590425624
I1002 01:28:40.865485 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11657534488
I1002 01:28:40.870600 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11657542680
I1002 01:28:40.875627 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11657550872
I1002 01:28:40.880751 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11724659736
I1002 01:28:40.885746 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11791768600
I1002 01:28:40.890892 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11791776792
I1002 01:28:40.895958 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11791784984
I1002 01:28:40.901067 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11791793176
I1002 01:28:40.906107 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11791801368
I1002 01:28:40.910064 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11791801880
I1002 01:28:40.913838 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11791802392
I1002 01:28:40.918847 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11808579608
I1002 01:28:40.924034 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11825356824
I1002 01:28:40.929192 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11825365016
I1002 01:28:40.934225 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11825373208
I1002 01:28:40.939862 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11842150424
I1002 01:28:40.944891 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11858927640
I1002 01:28:40.950002 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11858935832
I1002 01:28:40.955034 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11858944024
I1002 01:28:40.960181 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11875721240
I1002 01:28:40.965245 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11892498456
I1002 01:28:40.970373 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11892506648
I1002 01:28:40.975547 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11892514840
I1002 01:28:40.980565 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11909292056
I1002 01:28:40.985715 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11926069272
I1002 01:28:40.990736 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11926077464
I1002 01:28:40.995901 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11926085656
I1002 01:28:41.000925 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11926093848
I1002 01:28:41.006087 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11926102040
I1002 01:28:41.011147 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11926110232
I1002 01:28:41.016298 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11926118424
I1002 01:28:41.021356 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11926151192
I1002 01:28:41.026463 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11926183960
I1002 01:28:41.031499 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11993292824
I1002 01:28:41.036638 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12060401688
I1002 01:28:41.041775 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12060409880
I1002 01:28:41.046946 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12060418072
I1002 01:28:41.052580 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12127526936
I1002 01:28:41.057598 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12194635800
I1002 01:28:41.062737 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12194643992
I1002 01:28:41.067792 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12194652184
I1002 01:28:41.073030 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12194660376
I1002 01:28:41.078072 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12194668568
I1002 01:28:41.082050 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12194669080
I1002 01:28:41.085813 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12194669592
I1002 01:28:41.090770 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12211446808
I1002 01:28:41.095958 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12228224024
I1002 01:28:41.101140 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12228232216
I1002 01:28:41.106182 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12228240408
I1002 01:28:41.111334 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12245017624
I1002 01:28:41.116371 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12261794840
I1002 01:28:41.121495 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12261803032
I1002 01:28:41.126527 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12261811224
I1002 01:28:41.131789 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12278588440
I1002 01:28:41.136848 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12295365656
I1002 01:28:41.141968 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12295373848
I1002 01:28:41.147110 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12295382040
I1002 01:28:41.152173 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12312159256
I1002 01:28:41.157288 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12328936472
I1002 01:28:41.162312 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12328944664
I1002 01:28:41.167976 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12328952856
I1002 01:28:41.173001 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12328961048
I1002 01:28:41.178140 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12328969240
I1002 01:28:41.183187 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12328977432
I1002 01:28:41.188326 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12328985624
I1002 01:28:41.193343 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12329018392
I1002 01:28:41.198522 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12329051160
I1002 01:28:41.203612 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12396160024
I1002 01:28:41.208746 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12463268888
I1002 01:28:41.213877 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12463277080
I1002 01:28:41.218927 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12463285272
I1002 01:28:41.224096 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12530394136
I1002 01:28:41.229162 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12597503000
I1002 01:28:41.234293 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12597511192
I1002 01:28:41.239379 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12597519384
I1002 01:28:41.244493 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12597527576
I1002 01:28:41.249532 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12597535768
I1002 01:28:41.253518 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12597536280
I1002 01:28:41.257333 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12597536792
I1002 01:28:41.262267 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12614314008
I1002 01:28:41.267452 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12631091224
I1002 01:28:41.272600 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12631099416
I1002 01:28:41.277629 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12631107608
I1002 01:28:41.283247 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12647884824
I1002 01:28:41.288321 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12664662040
I1002 01:28:41.293468 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12664670232
I1002 01:28:41.298521 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12664678424
I1002 01:28:41.303708 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12681455640
I1002 01:28:41.308786 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12698232856
I1002 01:28:41.313949 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12698241048
I1002 01:28:41.319075 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12698249240
I1002 01:28:41.324141 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12715026456
I1002 01:28:41.329362 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12731803672
I1002 01:28:41.334414 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12731811864
I1002 01:28:41.339579 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12731820056
I1002 01:28:41.344616 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12731828248
I1002 01:28:41.349800 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12731836440
I1002 01:28:41.354870 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12731844632
I1002 01:28:41.360092 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12731852824
I1002 01:28:41.365123 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12731885592
I1002 01:28:41.370238 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12731918360
I1002 01:28:41.375265 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12799027224
I1002 01:28:41.380470 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12866136088
I1002 01:28:41.385595 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12866144280
I1002 01:28:41.390633 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12866152472
I1002 01:28:41.396331 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12933261336
I1002 01:28:41.401392 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13000370200
I1002 01:28:41.406575 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13000378392
I1002 01:28:41.411645 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13000386584
I1002 01:28:41.416779 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13000394776
I1002 01:28:41.421818 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13000402968
I1002 01:28:41.425788 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13000403480
I1002 01:28:41.429601 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13000403992
I1002 01:28:41.434547 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13017181208
I1002 01:28:41.439740 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13033958424
I1002 01:28:41.444876 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13033966616
I1002 01:28:41.449931 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13033974808
I1002 01:28:41.455084 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13050752024
I1002 01:28:41.460235 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13067529240
I1002 01:28:41.465385 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13067537432
I1002 01:28:41.470436 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13067545624
I1002 01:28:41.475600 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13084322840
I1002 01:28:41.480669 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13101100056
I1002 01:28:41.485827 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13101108248
I1002 01:28:41.490974 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13101116440
I1002 01:28:41.496048 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13117893656
I1002 01:28:41.501180 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13134670872
I1002 01:28:41.506233 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13134679064
I1002 01:28:41.511856 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13134687256
I1002 01:28:41.516891 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13134695448
I1002 01:28:41.522055 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13134703640
I1002 01:28:41.527173 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13134711832
I1002 01:28:41.532337 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13134720024
I1002 01:28:41.537408 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13134752792
I1002 01:28:41.542547 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13134785560
I1002 01:28:41.547608 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13201894424
I1002 01:28:41.552758 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13269003288
I1002 01:28:41.557903 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13269011480
I1002 01:28:41.562949 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13269019672
I1002 01:28:41.568130 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13336128536
I1002 01:28:41.573205 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13403237400
I1002 01:28:41.578354 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13403245592
I1002 01:28:41.583412 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13403253784
I1002 01:28:41.588587 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13403261976
I1002 01:28:41.593648 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13403270168
I1002 01:28:41.597650 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13403270680
I1002 01:28:41.601434 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13403271192
I1002 01:28:41.606410 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13420048408
I1002 01:28:41.611629 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13436825624
I1002 01:28:41.616986 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13436833816
I1002 01:28:41.622051 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13436842008
I1002 01:28:41.627691 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13453619224
I1002 01:28:41.632736 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13470396440
I1002 01:28:41.637884 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13470404632
I1002 01:28:41.642968 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13470412824
I1002 01:28:41.648160 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13487190040
I1002 01:28:41.653204 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13503967256
I1002 01:28:41.658380 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13503975448
I1002 01:28:41.663547 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13503983640
I1002 01:28:41.668625 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13520760856
I1002 01:28:41.673768 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13537538072
I1002 01:28:41.678847 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13537546264
I1002 01:28:41.684032 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13537554456
I1002 01:28:41.689074 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13537562648
I1002 01:28:41.694228 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13537570840
I1002 01:28:41.699273 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13537579032
I1002 01:28:41.704432 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13537587224
I1002 01:28:41.709470 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13537619992
I1002 01:28:41.714614 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13537652760
I1002 01:28:41.719707 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13604761624
I1002 01:28:41.724848 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13671870488
I1002 01:28:41.729999 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13671878680
I1002 01:28:41.735074 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13671886872
I1002 01:28:41.740758 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13738995736
I1002 01:28:41.745869 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13806104600
I1002 01:28:41.751032 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13806112792
I1002 01:28:41.756128 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13806120984
I1002 01:28:41.761281 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13806129176
I1002 01:28:41.766319 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13806137368
I1002 01:28:41.770323 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13806137880
I1002 01:28:41.774139 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13806138392
I1002 01:28:41.779104 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13822915608
I1002 01:28:41.784289 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13839692824
I1002 01:28:41.789454 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13839701016
I1002 01:28:41.794534 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13839709208
I1002 01:28:41.799735 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13856486424
I1002 01:28:41.804793 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13873263640
I1002 01:28:41.809963 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13873271832
I1002 01:28:41.815000 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13873280024
I1002 01:28:41.820192 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13890057240
I1002 01:28:41.825275 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13906834456
I1002 01:28:41.830475 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13906842648
I1002 01:28:41.835686 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13906850840
I1002 01:28:41.840774 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13923628056
I1002 01:28:41.845986 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13940405272
I1002 01:28:41.851115 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13940413464
I1002 01:28:41.856803 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13940421656
I1002 01:28:41.861866 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13940429848
I1002 01:28:41.867045 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13940438040
I1002 01:28:41.872131 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13940446232
I1002 01:28:41.877285 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13940454424
I1002 01:28:41.882336 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13940487192
I1002 01:28:41.887536 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13940519960
I1002 01:28:41.892593 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14007628824
I1002 01:28:41.897764 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14074737688
I1002 01:28:41.902920 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14074745880
I1002 01:28:41.908009 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14074754072
I1002 01:28:41.913188 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14141862936
I1002 01:28:41.918269 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14208971800
I1002 01:28:41.923467 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14208979992
I1002 01:28:41.928551 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14208988184
I1002 01:28:41.933704 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14208996376
I1002 01:28:41.938810 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14209004568
I1002 01:28:41.942784 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14209005080
I1002 01:28:41.946613 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14209005592
I1002 01:28:41.951585 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14225782808
I1002 01:28:41.956801 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14242560024
I1002 01:28:41.961978 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14242568216
I1002 01:28:41.967066 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14242576408
I1002 01:28:41.972713 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14259353624
I1002 01:28:41.977875 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14276130840
I1002 01:28:41.983086 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14276139032
I1002 01:28:41.988200 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14276147224
I1002 01:28:41.993358 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14292924440
I1002 01:28:41.998444 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14309701656
I1002 01:28:42.003661 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14309709848
I1002 01:28:42.008878 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14309718040
I1002 01:28:42.013935 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14326495256
I1002 01:28:42.019121 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14343272472
I1002 01:28:42.024219 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14343280664
I1002 01:28:42.029394 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14343288856
I1002 01:28:42.034463 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14343297048
I1002 01:28:42.039666 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14343305240
I1002 01:28:42.044749 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14343313432
I1002 01:28:42.049907 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14343321624
I1002 01:28:42.054977 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14343354392
I1002 01:28:42.060172 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14343387160
I1002 01:28:42.065238 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14410496024
I1002 01:28:42.070431 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14477604888
I1002 01:28:42.075627 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14477613080
I1002 01:28:42.080708 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14477621272
I1002 01:28:42.086465 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14544730136
I1002 01:28:42.091569 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14611839000
I1002 01:28:42.096754 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14611847192
I1002 01:28:42.101820 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14611855384
I1002 01:28:42.106986 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14611863576
I1002 01:28:42.112187 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14611871768
I1002 01:28:42.116179 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14611872280
I1002 01:28:42.120016 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14611872792
I1002 01:28:42.125026 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14628650008
I1002 01:28:42.130225 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14645427224
I1002 01:28:42.135513 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14645435416
I1002 01:28:42.140589 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14645443608
I1002 01:28:42.145788 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14662220824
I1002 01:28:42.150860 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14678998040
I1002 01:28:42.156070 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14679006232
I1002 01:28:42.161197 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14679014424
I1002 01:28:42.166354 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14695791640
I1002 01:28:42.171462 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14712568856
I1002 01:28:42.176641 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14712577048
I1002 01:28:42.181802 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14712585240
I1002 01:28:42.186952 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14729362456
I1002 01:28:42.192166 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14746139672
I1002 01:28:42.197245 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14746147864
I1002 01:28:42.202877 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14746156056
I1002 01:28:42.207969 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14746164248
I1002 01:28:42.213157 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14746172440
I1002 01:28:42.218251 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14746180632
I1002 01:28:42.223485 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14746188824
I1002 01:28:42.228568 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14746221592
I1002 01:28:42.233718 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14746254360
I1002 01:28:42.238831 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14813363224
I1002 01:28:42.244016 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14880472088
I1002 01:28:42.249224 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14880480280
I1002 01:28:42.254296 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14880488472
I1002 01:28:42.259521 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14947597336
I1002 01:28:42.264668 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15014706200
I1002 01:28:42.269839 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15014714392
I1002 01:28:42.274966 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15014722584
I1002 01:28:42.280144 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15014730776
I1002 01:28:42.285235 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15014738968
I1002 01:28:42.289266 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15014739480
I1002 01:28:42.293095 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15014739992
I1002 01:28:42.298074 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15031517208
I1002 01:28:42.303275 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15048294424
I1002 01:28:42.308471 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15048302616
I1002 01:28:42.313542 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15048310808
I1002 01:28:42.319265 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15065088024
I1002 01:28:42.324376 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15081865240
I1002 01:28:42.329548 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15081873432
I1002 01:28:42.334630 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15081881624
I1002 01:28:42.339842 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15098658840
I1002 01:28:42.345007 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15115436056
I1002 01:28:42.350190 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15115444248
I1002 01:28:42.355404 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15115452440
I1002 01:28:42.360488 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15132229656
I1002 01:28:42.365665 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15149006872
I1002 01:28:42.370756 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15149015064
I1002 01:28:42.375940 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15149023256
I1002 01:28:42.381014 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15149031448
I1002 01:28:42.386230 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15149039640
I1002 01:28:42.391357 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15149047832
I1002 01:28:42.396604 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15149056024
I1002 01:28:42.401695 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15149088792
I1002 01:28:42.406872 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15149121560
I1002 01:28:42.411977 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15216230424
I1002 01:28:42.417187 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15283339288
I1002 01:28:42.422381 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15283347480
I1002 01:28:42.427491 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15283355672
I1002 01:28:42.433130 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15350464536
I1002 01:28:42.438235 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15417573400
I1002 01:28:42.443449 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15417581592
I1002 01:28:42.448565 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15417589784
I1002 01:28:42.453774 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15417597976
I1002 01:28:42.458887 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15417606168
I1002 01:28:42.462900 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15417606680
I1002 01:28:42.466740 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15417607192
I1002 01:28:42.471745 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15434384408
I1002 01:28:42.477022 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15451161624
I1002 01:28:42.482200 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15451169816
I1002 01:28:42.487277 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15451178008
I1002 01:28:42.492496 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15467955224
I1002 01:28:42.497560 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15484732440
I1002 01:28:42.502750 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15484740632
I1002 01:28:42.507853 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15484748824
I1002 01:28:42.513041 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15501526040
I1002 01:28:42.518154 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15518303256
I1002 01:28:42.523400 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15518311448
I1002 01:28:42.528647 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15518319640
I1002 01:28:42.533748 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15535096856
I1002 01:28:42.538954 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15551874072
I1002 01:28:42.544130 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15551882264
I1002 01:28:42.549879 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15551890456
I1002 01:28:42.554976 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15551898648
I1002 01:28:42.560247 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15551906840
I1002 01:28:42.565343 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15551915032
I1002 01:28:42.570555 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15551923224
I1002 01:28:42.575684 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15551955992
I1002 01:28:42.580921 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15551988760
I1002 01:28:42.586028 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15619097624
I1002 01:28:42.591233 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15686206488
I1002 01:28:42.596440 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15686214680
I1002 01:28:42.601540 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15686222872
I1002 01:28:42.606741 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15753331736
I1002 01:28:42.611886 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15820440600
I1002 01:28:42.617074 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15820448792
I1002 01:28:42.622211 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15820456984
I1002 01:28:42.627416 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15820465176
I1002 01:28:42.632529 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15820473368
I1002 01:28:42.636567 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15820473880
I1002 01:28:42.640407 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15820474392
I1002 01:28:42.645382 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15837251608
I1002 01:28:42.650582 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15854028824
I1002 01:28:42.655811 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15854037016
I1002 01:28:42.660893 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15854045208
I1002 01:28:42.666663 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15870822424
I1002 01:28:42.671857 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15887599640
I1002 01:28:42.677063 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15887607832
I1002 01:28:42.682157 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15887616024
I1002 01:28:42.687433 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15904393240
I1002 01:28:42.692555 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15921170456
I1002 01:28:42.697730 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15921178648
I1002 01:28:42.702931 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15921186840
I1002 01:28:42.708078 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15937964056
I1002 01:28:42.713314 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15954741272
I1002 01:28:42.718400 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15954749464
I1002 01:28:42.723625 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15954757656
I1002 01:28:42.728763 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15954765848
I1002 01:28:42.734030 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15954774040
I1002 01:28:42.739153 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15954782232
I1002 01:28:42.744418 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15954790424
I1002 01:28:42.749495 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15954823192
I1002 01:28:42.754701 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15954855960
I1002 01:28:42.759831 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16021964824
I1002 01:28:42.765045 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16089073688
I1002 01:28:42.770260 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16089081880
I1002 01:28:42.775422 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16089090072
I1002 01:28:42.781200 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16156198936
I1002 01:28:42.786332 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16223307800
I1002 01:28:42.791613 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16223315992
I1002 01:28:42.796784 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16223324184
I1002 01:28:42.802012 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16223332376
I1002 01:28:42.807116 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16223340568
I1002 01:28:42.811215 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16223341080
I1002 01:28:42.815098 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16223341592
I1002 01:28:42.820227 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16240118808
I1002 01:28:42.825494 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16256896024
I1002 01:28:42.830724 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16256904216
I1002 01:28:42.835880 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16256912408
I1002 01:28:42.841136 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16273689624
I1002 01:28:42.846251 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16290466840
I1002 01:28:42.851531 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16290475032
I1002 01:28:42.856676 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16290483224
I1002 01:28:42.861881 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16307260440
I1002 01:28:42.867030 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16324037656
I1002 01:28:42.872312 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16324045848
I1002 01:28:42.877512 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16324054040
I1002 01:28:42.882654 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16340831256
I1002 01:28:42.887942 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16357608472
I1002 01:28:42.893172 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16357616664
I1002 01:28:42.899059 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16357624856
I1002 01:28:42.904217 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16357633048
I1002 01:28:42.909575 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16357641240
I1002 01:28:42.914754 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16357649432
I1002 01:28:42.920004 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16357657624
I1002 01:28:42.925176 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16357690392
I1002 01:28:42.930417 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16357723160
I1002 01:28:42.935595 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16424832024
I1002 01:28:42.940840 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16491940888
I1002 01:28:42.946078 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16491949080
I1002 01:28:42.951209 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16491957272
I1002 01:28:42.956484 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16559066136
I1002 01:28:42.961609 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16626175000
I1002 01:28:42.966875 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16626183192
I1002 01:28:42.972123 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16626191384
I1002 01:28:42.977430 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16626199576
I1002 01:28:42.982634 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16626207768
I1002 01:28:42.986728 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16626208280
I1002 01:28:42.990615 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16626208792
I1002 01:28:42.995715 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16642986008
I1002 01:28:43.001041 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16659763224
I1002 01:28:43.006294 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16659771416
I1002 01:28:43.011469 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16659779608
I1002 01:28:43.017360 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16676556824
I1002 01:28:43.022555 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16693334040
I1002 01:28:43.027875 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16693342232
I1002 01:28:43.033055 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16693350424
I1002 01:28:43.038292 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16710127640
I1002 01:28:43.043495 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16726904856
I1002 01:28:43.048753 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16726913048
I1002 01:28:43.053985 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16726921240
I1002 01:28:43.059170 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16743698456
I1002 01:28:43.064464 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16760475672
I1002 01:28:43.069603 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16760483864
I1002 01:28:43.074876 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16760492056
I1002 01:28:43.080049 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16760500248
I1002 01:28:43.085374 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16760508440
I1002 01:28:43.090562 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16760516632
I1002 01:28:43.095852 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16760524824
I1002 01:28:43.100990 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16760557592
I1002 01:28:43.106292 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16760590360
I1002 01:28:43.111472 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16827699224
I1002 01:28:43.116780 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16894808088
I1002 01:28:43.122035 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16894816280
I1002 01:28:43.127199 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16894824472
I1002 01:28:43.133152 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16961933336
I1002 01:28:43.138320 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17029042200
I1002 01:28:43.143570 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17029050392
I1002 01:28:43.148760 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17029058584
I1002 01:28:43.153990 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17029066776
I1002 01:28:43.159158 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17029074968
I1002 01:28:43.163358 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17029075480
I1002 01:28:43.167218 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17029075992
I1002 01:28:43.172302 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17045853208
I1002 01:28:43.177566 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17062630424
I1002 01:28:43.182823 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17062638616
I1002 01:28:43.188099 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17062646808
I1002 01:28:43.193376 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17079424024
I1002 01:28:43.198587 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17096201240
I1002 01:28:43.203847 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17096209432
I1002 01:28:43.209005 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17096217624
I1002 01:28:43.214308 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17112994840
I1002 01:28:43.219552 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17129772056
I1002 01:28:43.224811 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17129780248
I1002 01:28:43.230065 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17129788440
I1002 01:28:43.235254 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17146565656
I1002 01:28:43.240533 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17163342872
I1002 01:28:43.245706 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17163351064
I1002 01:28:43.251536 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17163359256
I1002 01:28:43.256692 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17163367448
I1002 01:28:43.262032 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17163375640
I1002 01:28:43.267351 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17163383832
I1002 01:28:43.272645 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17163392024
I1002 01:28:43.277827 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17163424792
I1002 01:28:43.283105 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17163457560
I1002 01:28:43.288305 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17230566424
I1002 01:28:43.293592 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17297675288
I1002 01:28:43.298837 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17297683480
I1002 01:28:43.304019 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17297691672
I1002 01:28:43.309265 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17364800536
I1002 01:28:43.314438 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17431909400
I1002 01:28:43.319701 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17431917592
I1002 01:28:43.324845 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17431925784
I1002 01:28:43.330112 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17431933976
I1002 01:28:43.335273 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17431942168
I1002 01:28:43.339368 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17431942680
I1002 01:28:43.343250 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17431943192
I1002 01:28:43.348383 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17448720408
I1002 01:28:43.353668 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17465497624
I1002 01:28:43.358928 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17465505816
I1002 01:28:43.364109 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17465514008
I1002 01:28:43.369900 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17482291224
I1002 01:28:43.375050 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17499068440
I1002 01:28:43.380335 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17499076632
I1002 01:28:43.385496 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17499084824
I1002 01:28:43.390726 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17515862040
I1002 01:28:43.395926 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17532639256
I1002 01:28:43.401335 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17532647448
I1002 01:28:43.406563 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17532655640
I1002 01:28:43.411753 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17549432856
I1002 01:28:43.417012 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17566210072
I1002 01:28:43.422196 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17566218264
I1002 01:28:43.427557 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17566226456
I1002 01:28:43.432701 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17566234648
I1002 01:28:43.438005 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17566242840
I1002 01:28:43.443160 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17566251032
I1002 01:28:43.448443 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17566259224
I1002 01:28:43.453582 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17566291992
I1002 01:28:43.458848 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17566324760
I1002 01:28:43.464034 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17633433624
I1002 01:28:43.469317 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17700542488
I1002 01:28:43.474574 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17700550680
I1002 01:28:43.479824 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17700558872
I1002 01:28:43.485655 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17767667736
I1002 01:28:43.490821 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17834776600
I1002 01:28:43.496069 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17834784792
I1002 01:28:43.501231 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17834792984
I1002 01:28:43.506470 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17834801176
I1002 01:28:43.511653 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17834809368
I1002 01:28:43.515733 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17834809880
I1002 01:28:43.519642 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17834810392
I1002 01:28:43.524697 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17851587608
I1002 01:28:43.529959 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17868364824
I1002 01:28:43.535209 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17868373016
I1002 01:28:43.540338 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17868381208
I1002 01:28:43.545575 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17885158424
I1002 01:28:43.550712 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17901935640
I1002 01:28:43.555961 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17901943832
I1002 01:28:43.561099 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17901952024
I1002 01:28:43.566329 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17918729240
I1002 01:28:43.571507 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17935506456
I1002 01:28:43.576747 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17935514648
I1002 01:28:43.581967 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17935522840
I1002 01:28:43.587121 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17952300056
I1002 01:28:43.592355 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17969077272
I1002 01:28:43.597498 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17969085464
I1002 01:28:43.603217 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17969093656
I1002 01:28:43.608358 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17969101848
I1002 01:28:43.613613 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17969110040
I1002 01:28:43.618754 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17969118232
I1002 01:28:43.624041 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17969126424
I1002 01:28:43.629191 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17969159192
I1002 01:28:43.634406 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17969191960
I1002 01:28:43.639600 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18036300824
I1002 01:28:43.644861 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18103409688
I1002 01:28:43.650084 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18103417880
I1002 01:28:43.655226 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18103426072
I1002 01:28:43.660466 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18170534936
I1002 01:28:43.665640 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18237643800
I1002 01:28:43.670865 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18237651992
I1002 01:28:43.676075 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18237660184
I1002 01:28:43.681298 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18237668376
I1002 01:28:43.686438 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18237676568
I1002 01:28:43.690560 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18237677080
I1002 01:28:43.694434 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18237677592
I1002 01:28:43.699513 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18254454808
I1002 01:28:43.704737 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18271232024
I1002 01:28:43.709984 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18271240216
I1002 01:28:43.715161 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18271248408
I1002 01:28:43.720895 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18288025624
I1002 01:28:43.726041 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18304802840
I1002 01:28:44.470777 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18304811032
I1002 01:28:44.476630 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18304819224
I1002 01:28:44.481857 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18321596440
I1002 01:28:44.487062 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18338373656
I1002 01:28:44.492274 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18338381848
I1002 01:28:44.497494 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18338390040
I1002 01:28:44.502670 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18355167256
I1002 01:28:44.507963 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18371944472
I1002 01:28:44.513109 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18371952664
I1002 01:28:44.518372 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18371960856
I1002 01:28:44.523518 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18371969048
I1002 01:28:44.528847 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18371977240
I1002 01:28:44.534011 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18371985432
I1002 01:28:44.539268 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18371993624
I1002 01:28:44.544558 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18372026392
I1002 01:28:44.549706 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18372059160
I1002 01:28:44.554971 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18439168024
I1002 01:28:44.560148 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18506276888
I1002 01:28:44.565404 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18506285080
I1002 01:28:44.570553 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18506293272
I1002 01:28:44.575849 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18573402136
I1002 01:28:44.581002 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18640511000
I1002 01:28:44.586774 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18640519192
I1002 01:28:44.591967 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18640527384
I1002 01:28:44.597211 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18640535576
I1002 01:28:44.602449 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18640543768
I1002 01:28:44.606483 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18640544280
I1002 01:28:44.610374 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18640544792
I1002 01:28:44.615666 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18657322008
I1002 01:28:44.620829 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18674099224
I1002 01:28:44.626091 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18674107416
I1002 01:28:44.631218 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18674115608
I1002 01:28:44.636495 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18690892824
I1002 01:28:44.641753 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18707670040
I1002 01:28:44.646890 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18707678232
I1002 01:28:44.652180 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18707686424
I1002 01:28:44.657306 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18724463640
I1002 01:28:44.662587 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18741240856
I1002 01:28:44.667807 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18741249048
I1002 01:28:44.673050 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18741257240
I1002 01:28:44.678210 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18758034456
I1002 01:28:44.683489 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18774811672
I1002 01:28:44.688632 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18774819864
I1002 01:28:44.693923 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18774828056
I1002 01:28:44.699689 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18774836248
I1002 01:28:44.704878 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18774844440
I1002 01:28:44.710152 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18774852632
I1002 01:28:44.715353 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18774860824
I1002 01:28:44.720601 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18774893592
I1002 01:28:44.725736 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18774926360
I1002 01:28:44.730979 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18842035224
I1002 01:28:44.736171 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18909144088
I1002 01:28:44.741424 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18909152280
I1002 01:28:44.746672 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18909160472
I1002 01:28:44.751952 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18976269336
I1002 01:28:44.757105 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19043378200
I1002 01:28:44.762345 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19043386392
I1002 01:28:44.767637 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19043394584
I1002 01:28:44.773098 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19043402776
I1002 01:28:44.778346 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19043410968
I1002 01:28:44.782349 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19043411480
I1002 01:28:44.786251 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19043411992
I1002 01:28:44.791446 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19060189208
I1002 01:28:44.796635 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19076966424
I1002 01:28:44.801919 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19076974616
I1002 01:28:44.807161 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19076982808
I1002 01:28:44.812351 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19093760024
I1002 01:28:44.818093 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19110537240
I1002 01:28:44.823252 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19110545432
I1002 01:28:44.828572 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19110553624
I1002 01:28:44.833749 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19127330840
I1002 01:28:44.839008 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19144108056
I1002 01:28:44.844182 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19144116248
I1002 01:28:44.849411 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19144124440
I1002 01:28:44.854565 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19160901656
I1002 01:28:44.859844 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19177678872
I1002 01:28:44.865114 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19177687064
I1002 01:28:44.870250 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19177695256
I1002 01:28:44.875530 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19177703448
I1002 01:28:44.880746 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19177711640
I1002 01:28:44.886006 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19177719832
I1002 01:28:44.891156 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19177728024
I1002 01:28:44.896456 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19177760792
I1002 01:28:44.901619 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19177793560
I1002 01:28:44.906951 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19244902424
I1002 01:28:44.912259 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19312011288
I1002 01:28:44.917531 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19312019480
I1002 01:28:44.922788 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19312027672
I1002 01:28:44.927975 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19379136536
I1002 01:28:44.933800 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19446245400
I1002 01:28:44.938960 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19446253592
I1002 01:28:44.944275 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19446261784
I1002 01:28:44.949445 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19446269976
I1002 01:28:44.954708 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19446278168
I1002 01:28:44.958715 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19446278680
I1002 01:28:44.962623 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19446279192
I1002 01:28:44.967843 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19463056408
I1002 01:28:44.973120 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19479833624
I1002 01:28:44.978259 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19479841816
I1002 01:28:44.983544 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19479850008
I1002 01:28:44.988712 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19496627224
I1002 01:28:44.993978 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19513404440
I1002 01:28:44.999121 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19513412632
I1002 01:28:45.004407 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19513420824
I1002 01:28:45.009570 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19530198040
I1002 01:28:45.014944 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19546975256
I1002 01:28:45.020125 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19546983448
I1002 01:28:45.025386 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19546991640
I1002 01:28:45.030647 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19563768856
I1002 01:28:45.035870 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19580546072
I1002 01:28:45.041137 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19580554264
I1002 01:28:45.046329 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19580562456
I1002 01:28:45.052066 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19580570648
I1002 01:28:45.057258 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19580578840
I1002 01:28:45.062534 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19580587032
I1002 01:28:45.067852 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19580595224
I1002 01:28:45.073126 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19580627992
I1002 01:28:45.078280 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19580660760
I1002 01:28:45.083585 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19647769624
I1002 01:28:45.088796 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19714878488
I1002 01:28:45.094096 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19714886680
I1002 01:28:45.099397 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19714894872
I1002 01:28:45.104581 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19782003736
I1002 01:28:45.109858 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19849112600
I1002 01:28:45.115039 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19849120792
I1002 01:28:45.120369 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19849128984
I1002 01:28:45.125520 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19849137176
I1002 01:28:45.130773 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19849145368
I1002 01:28:45.134872 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19849145880
I1002 01:28:45.138821 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19849146392
I1002 01:28:45.144032 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19865923608
I1002 01:28:45.149304 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19882700824
I1002 01:28:45.154476 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19882709016
I1002 01:28:45.159751 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19882717208
I1002 01:28:45.164907 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19899494424
I1002 01:28:45.170669 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19916271640
I1002 01:28:45.175881 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19916279832
I1002 01:28:45.181183 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19916288024
I1002 01:28:45.186353 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19933065240
I1002 01:28:45.191655 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19949842456
I1002 01:28:45.196845 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19949850648
I1002 01:28:45.202110 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19949858840
I1002 01:28:45.207404 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19966636056
I1002 01:28:45.212587 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19983413272
I1002 01:28:45.217865 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19983421464
I1002 01:28:45.223048 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19983429656
I1002 01:28:45.228326 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19983437848
I1002 01:28:45.233506 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19983446040
I1002 01:28:45.238818 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19983454232
I1002 01:28:45.244005 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19983462424
I1002 01:28:45.249289 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19983495192
I1002 01:28:45.254481 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19983527960
I1002 01:28:45.259753 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20050636824
I1002 01:28:45.264919 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20117745688
I1002 01:28:45.270174 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20117753880
I1002 01:28:45.275475 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20117762072
I1002 01:28:45.280674 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20184870936
I1002 01:28:45.286449 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20251979800
I1002 01:28:45.291646 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20251987992
I1002 01:28:45.296921 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20251996184
I1002 01:28:45.302090 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20252004376
I1002 01:28:45.307444 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20252012568
I1002 01:28:45.311475 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20252013080
I1002 01:28:45.315402 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20252013592
I1002 01:28:45.320623 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20268790808
I1002 01:28:45.325912 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20285568024
I1002 01:28:45.331188 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20285576216
I1002 01:28:45.336486 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20285584408
I1002 01:28:45.341661 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20302361624
I1002 01:28:45.346949 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20319138840
I1002 01:28:45.352138 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20319147032
I1002 01:28:45.357464 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20319155224
I1002 01:28:45.362651 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20335932440
I1002 01:28:45.367955 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20352709656
I1002 01:28:45.373141 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20352717848
I1002 01:28:45.378391 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20352726040
I1002 01:28:45.383714 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20369503256
I1002 01:28:45.388897 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386280472
I1002 01:28:45.394209 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386288664
I1002 01:28:45.399389 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386296856
I1002 01:28:45.405169 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386305048
I1002 01:28:45.410450 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386313240
I1002 01:28:45.415800 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386321432
I1002 01:28:45.420978 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386329624
I1002 01:28:45.426251 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386337624
I1002 01:28:45.431491 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386345624
I1002 01:28:45.436775 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386353624
I1002 01:28:45.441964 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386361624
I1002 01:28:45.447212 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386369624
I1002 01:28:45.452505 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386377624
I1002 01:28:45.457679 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386385624
I1002 01:28:45.462959 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386393624
I1002 01:28:45.468188 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386401624
I1002 01:28:45.473460 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386409624
I1002 01:28:45.478641 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386417624
I1002 01:28:45.483926 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386425624
I1002 01:28:45.489111 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386433624
I1002 01:28:45.494460 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386441624
I1002 01:28:45.499660 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386449624
I1002 01:28:45.504959 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386457624
I1002 01:28:45.510114 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386465624
I1002 01:28:45.515425 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386473624
I1002 01:28:45.521178 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386481624
I1002 01:28:45.526383 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386489624
I1002 01:28:45.531685 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386497624
I1002 01:28:45.536840 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386505624
I1002 01:28:45.542139 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386513624
I1002 01:28:45.547419 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386521624
I1002 01:28:45.552679 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386529624
I1002 01:28:45.557871 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386537624
I1002 01:28:45.563145 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386545624
I1002 01:28:45.568333 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386553624
I1002 01:28:45.573582 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386561624
I1002 01:28:45.578762 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386569624
I1002 01:28:45.584087 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386577624
I1002 01:28:45.589364 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386585624
I1002 01:28:45.594570 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20402969624
I1002 01:28:45.599900 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20419353624
I1002 01:28:45.605138 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20435737624
I1002 01:28:45.610405 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20452121624
I1002 01:28:45.615623 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20468505624
I1002 01:28:45.620927 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20484889624
I1002 01:28:45.626149 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20501273624
I1002 01:28:45.631558 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20517657624
I1002 01:28:45.636750 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20534041624
I1002 01:28:45.642514 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20550425624
I1002 01:28:45.647858 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20566809624
I1002 01:28:45.653049 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20583193624
I1002 01:28:45.658360 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20599577624
I1002 01:28:45.663587 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20615961624
I1002 01:28:45.668884 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20632345624
I1002 01:28:45.674116 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20648729624
I1002 01:28:45.679457 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20665113624
I1002 01:28:45.684699 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20681497624
I1002 01:28:45.689989 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20697881624
I1002 01:28:45.695183 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20714265624
I1002 01:28:45.700487 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20730649624
I1002 01:28:45.705785 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20747033624
I1002 01:28:45.711009 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20763417624
I1002 01:28:45.716367 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20779801624
I1002 01:28:45.721589 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20796185624
I1002 01:28:45.726873 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20812569624
I1002 01:28:45.732126 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20828953624
I1002 01:28:45.737423 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20845337624
I1002 01:28:45.742609 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20861721624
I1002 01:28:45.747960 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20878105624
I1002 01:28:45.753166 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20894489624
I1002 01:28:45.759023 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20910873624
I1002 01:28:46.497384 139650252134208 cluster.py:515] Place variable total_nan_gradients/var on /job:local/replica:0/task:0/device:CPU:0 20910873632
I1002 01:28:46.499581 139650252134208 py_utils.py:1389] Creating var total_nan_gradients/var:0 shape=() on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:28:49.147726 139650252134208 py_utils.py:1474] MODEL ANALYSIS: 
I1002 01:28:49.147944 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.emb.src_token_emb.wm                               (32000, 2048)          65536000 1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var
I1002 01:28:49.148022 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[0].b                  (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var
I1002 01:28:49.148090 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[0].w                  (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var
I1002 01:28:49.148149 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[1].b                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var
I1002 01:28:49.148207 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[1].w                  (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var
I1002 01:28:49.148263 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.fflayer.layer_norm.bias                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var
I1002 01:28:49.148320 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.fflayer.layer_norm.scale                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var
I1002 01:28:49.148375 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.self_atten.atten.atten.per_dim_scale     (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1002 01:28:49.148444 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.self_atten.atten.ctx_post_proj           (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1002 01:28:49.148503 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.self_atten.atten.ctx_post_proj_b         (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1002 01:28:49.148559 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.self_atten.atten.ctx_proj                (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var
I1002 01:28:49.148614 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.self_atten.atten.ctx_proj_b              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1002 01:28:49.148670 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.self_atten.atten.query_proj              (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var
I1002 01:28:49.148726 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.self_atten.atten.query_proj_b            (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var
I1002 01:28:49.148781 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.self_atten.atten.source_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var
I1002 01:28:49.148837 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.self_atten.atten.source_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var
I1002 01:28:49.148892 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.self_atten.layer_norm.bias               (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var
I1002 01:28:49.148948 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.self_atten.layer_norm.scale              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var
I1002 01:28:49.149003 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[0].b                  (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var
I1002 01:28:49.149059 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[0].w                  (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var
I1002 01:28:49.149114 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[1].b                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var
I1002 01:28:49.149168 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[1].w                  (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var
I1002 01:28:49.149223 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.fflayer.layer_norm.bias                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var
I1002 01:28:49.149283 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.fflayer.layer_norm.scale                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var
I1002 01:28:49.149338 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.self_atten.atten.atten.per_dim_scale     (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1002 01:28:49.149393 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.self_atten.atten.ctx_post_proj           (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1002 01:28:49.149447 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.self_atten.atten.ctx_post_proj_b         (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1002 01:28:49.149502 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.self_atten.atten.ctx_proj                (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var
I1002 01:28:49.149556 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.self_atten.atten.ctx_proj_b              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1002 01:28:49.149611 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.self_atten.atten.query_proj              (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var
I1002 01:28:49.149666 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.self_atten.atten.query_proj_b            (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var
I1002 01:28:49.149721 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.self_atten.atten.source_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var
I1002 01:28:49.149775 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.self_atten.atten.source_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var
I1002 01:28:49.149829 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.self_atten.layer_norm.bias               (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var
I1002 01:28:49.149884 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.self_atten.layer_norm.scale              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var
I1002 01:28:49.149938 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[0].b                  (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var
I1002 01:28:49.149992 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[0].w                  (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var
I1002 01:28:49.150046 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[1].b                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var
I1002 01:28:49.150101 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[1].w                  (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var
I1002 01:28:49.150159 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.fflayer.layer_norm.bias                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var
I1002 01:28:49.150214 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.fflayer.layer_norm.scale                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var
I1002 01:28:49.150269 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.self_atten.atten.atten.per_dim_scale     (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1002 01:28:49.150323 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.self_atten.atten.ctx_post_proj           (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1002 01:28:49.150378 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.self_atten.atten.ctx_post_proj_b         (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1002 01:28:49.150433 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.self_atten.atten.ctx_proj                (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var
I1002 01:28:49.150488 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.self_atten.atten.ctx_proj_b              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1002 01:28:49.150543 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.self_atten.atten.query_proj              (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var
I1002 01:28:49.150597 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.self_atten.atten.query_proj_b            (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var
I1002 01:28:49.150653 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.self_atten.atten.source_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var
I1002 01:28:49.150707 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.self_atten.atten.source_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var
I1002 01:28:49.150763 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.self_atten.layer_norm.bias               (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var
I1002 01:28:49.150817 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.self_atten.layer_norm.scale              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var
I1002 01:28:49.150871 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[0].b                  (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var
I1002 01:28:49.150926 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[0].w                  (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var
I1002 01:28:49.150985 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[1].b                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var
I1002 01:28:49.151040 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[1].w                  (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var
I1002 01:28:49.151095 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.fflayer.layer_norm.bias                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var
I1002 01:28:49.151149 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.fflayer.layer_norm.scale                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var
I1002 01:28:49.151203 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.self_atten.atten.atten.per_dim_scale     (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1002 01:28:49.151258 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.self_atten.atten.ctx_post_proj           (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1002 01:28:49.151327 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.self_atten.atten.ctx_post_proj_b         (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1002 01:28:49.151385 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.self_atten.atten.ctx_proj                (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var
I1002 01:28:49.151441 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.self_atten.atten.ctx_proj_b              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1002 01:28:49.151496 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.self_atten.atten.query_proj              (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var
I1002 01:28:49.151554 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.self_atten.atten.query_proj_b            (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var
I1002 01:28:49.151609 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.self_atten.atten.source_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var
I1002 01:28:49.151663 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.self_atten.atten.source_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var
I1002 01:28:49.151717 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.self_atten.layer_norm.bias               (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var
I1002 01:28:49.151772 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.self_atten.layer_norm.scale              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var
I1002 01:28:49.151832 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[0].b                  (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var
I1002 01:28:49.151888 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[0].w                  (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var
I1002 01:28:49.151943 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[1].b                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var
I1002 01:28:49.151997 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[1].w                  (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var
I1002 01:28:49.152051 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.fflayer.layer_norm.bias                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var
I1002 01:28:49.152105 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.fflayer.layer_norm.scale                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var
I1002 01:28:49.152159 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.self_atten.atten.atten.per_dim_scale     (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1002 01:28:49.152213 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.self_atten.atten.ctx_post_proj           (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1002 01:28:49.152268 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.self_atten.atten.ctx_post_proj_b         (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1002 01:28:49.152322 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.self_atten.atten.ctx_proj                (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var
I1002 01:28:49.152377 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.self_atten.atten.ctx_proj_b              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1002 01:28:49.152431 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.self_atten.atten.query_proj              (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var
I1002 01:28:49.152485 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.self_atten.atten.query_proj_b            (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var
I1002 01:28:49.152540 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.self_atten.atten.source_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var
I1002 01:28:49.152594 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.self_atten.atten.source_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var
I1002 01:28:49.152648 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.self_atten.layer_norm.bias               (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var
I1002 01:28:49.152708 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.self_atten.layer_norm.scale              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var
I1002 01:28:49.152763 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[0].b                  (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var
I1002 01:28:49.152818 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[0].w                  (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var
I1002 01:28:49.152873 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[1].b                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var
I1002 01:28:49.152928 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[1].w                  (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var
I1002 01:28:49.152982 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.fflayer.layer_norm.bias                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var
I1002 01:28:49.153037 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.fflayer.layer_norm.scale                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var
I1002 01:28:49.153090 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.self_atten.atten.atten.per_dim_scale     (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1002 01:28:49.153145 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.self_atten.atten.ctx_post_proj           (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1002 01:28:49.153199 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.self_atten.atten.ctx_post_proj_b         (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1002 01:28:49.153253 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.self_atten.atten.ctx_proj                (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var
I1002 01:28:49.153308 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.self_atten.atten.ctx_proj_b              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1002 01:28:49.153362 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.self_atten.atten.query_proj              (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var
I1002 01:28:49.153417 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.self_atten.atten.query_proj_b            (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var
I1002 01:28:49.153472 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.self_atten.atten.source_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var
I1002 01:28:49.153531 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.self_atten.atten.source_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var
I1002 01:28:49.153586 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.self_atten.layer_norm.bias               (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var
I1002 01:28:49.153641 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.self_atten.layer_norm.scale              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var
I1002 01:28:49.153696 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[0].b                  (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var
I1002 01:28:49.153751 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[0].w                  (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var
I1002 01:28:49.153805 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[1].b                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var
I1002 01:28:49.153860 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[1].w                  (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var
I1002 01:28:49.153915 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.fflayer.layer_norm.bias                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var
I1002 01:28:49.153970 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.fflayer.layer_norm.scale                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var
I1002 01:28:49.154025 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.self_atten.atten.atten.per_dim_scale     (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1002 01:28:49.154079 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.self_atten.atten.ctx_post_proj           (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1002 01:28:49.154134 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.self_atten.atten.ctx_post_proj_b         (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1002 01:28:49.154188 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.self_atten.atten.ctx_proj                (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var
I1002 01:28:49.154242 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.self_atten.atten.ctx_proj_b              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1002 01:28:49.154297 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.self_atten.atten.query_proj              (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var
I1002 01:28:49.154351 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.self_atten.atten.query_proj_b            (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var
I1002 01:28:49.154410 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.self_atten.atten.source_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var
I1002 01:28:49.154465 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.self_atten.atten.source_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var
I1002 01:28:49.154520 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.self_atten.layer_norm.bias               (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var
I1002 01:28:49.154574 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.self_atten.layer_norm.scale              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var
I1002 01:28:49.154629 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[0].b                  (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var
I1002 01:28:49.154683 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[0].w                  (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var
I1002 01:28:49.154737 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[1].b                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var
I1002 01:28:49.154791 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[1].w                  (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var
I1002 01:28:49.154844 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.fflayer.layer_norm.bias                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var
I1002 01:28:49.154899 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.fflayer.layer_norm.scale                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var
I1002 01:28:49.154952 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.self_atten.atten.atten.per_dim_scale     (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1002 01:28:49.155007 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.self_atten.atten.ctx_post_proj           (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1002 01:28:49.155061 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.self_atten.atten.ctx_post_proj_b         (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1002 01:28:49.155116 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.self_atten.atten.ctx_proj                (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var
I1002 01:28:49.155170 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.self_atten.atten.ctx_proj_b              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1002 01:28:49.155230 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.self_atten.atten.query_proj              (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var
I1002 01:28:49.155286 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.self_atten.atten.query_proj_b            (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var
I1002 01:28:49.155355 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.self_atten.atten.source_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var
I1002 01:28:49.155411 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.self_atten.atten.source_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var
I1002 01:28:49.155465 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.self_atten.layer_norm.bias               (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var
I1002 01:28:49.155519 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.self_atten.layer_norm.scale              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var
I1002 01:28:49.155574 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var
I1002 01:28:49.155629 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var
I1002 01:28:49.155683 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var
I1002 01:28:49.155737 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var
I1002 01:28:49.155791 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var
I1002 01:28:49.155844 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var
I1002 01:28:49.155898 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1002 01:28:49.155953 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1002 01:28:49.156007 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1002 01:28:49.156061 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var
I1002 01:28:49.156121 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1002 01:28:49.156176 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var
I1002 01:28:49.156232 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var
I1002 01:28:49.156286 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var
I1002 01:28:49.156342 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var
I1002 01:28:49.156396 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var
I1002 01:28:49.156451 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var
I1002 01:28:49.156506 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var
I1002 01:28:49.156561 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var
I1002 01:28:49.156616 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var
I1002 01:28:49.156670 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var
I1002 01:28:49.156725 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var
I1002 01:28:49.156779 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var
I1002 01:28:49.156834 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1002 01:28:49.156889 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1002 01:28:49.156948 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1002 01:28:49.157004 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var
I1002 01:28:49.157059 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1002 01:28:49.157114 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var
I1002 01:28:49.157168 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var
I1002 01:28:49.157224 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var
I1002 01:28:49.157278 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var
I1002 01:28:49.157333 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var
I1002 01:28:49.157387 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var
I1002 01:28:49.157443 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var
I1002 01:28:49.157498 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var
I1002 01:28:49.157552 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var
I1002 01:28:49.157607 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var
I1002 01:28:49.157661 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var
I1002 01:28:49.157716 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var
I1002 01:28:49.157770 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1002 01:28:49.157829 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1002 01:28:49.157884 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1002 01:28:49.157939 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var
I1002 01:28:49.157994 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1002 01:28:49.158049 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var
I1002 01:28:49.158103 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var
I1002 01:28:49.158157 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var
I1002 01:28:49.158210 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var
I1002 01:28:49.158265 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var
I1002 01:28:49.158319 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var
I1002 01:28:49.158374 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var
I1002 01:28:49.158427 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var
I1002 01:28:49.158480 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var
I1002 01:28:49.158535 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var
I1002 01:28:49.158588 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var
I1002 01:28:49.158647 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var
I1002 01:28:49.158702 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1002 01:28:49.158756 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1002 01:28:49.158811 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1002 01:28:49.158865 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var
I1002 01:28:49.158920 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1002 01:28:49.158973 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var
I1002 01:28:49.159028 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var
I1002 01:28:49.159082 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var
I1002 01:28:49.159136 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var
I1002 01:28:49.159191 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var
I1002 01:28:49.159246 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var
I1002 01:28:49.159316 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var
I1002 01:28:49.159375 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var
I1002 01:28:49.159429 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var
I1002 01:28:49.159488 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var
I1002 01:28:49.159543 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var
I1002 01:28:49.159597 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var
I1002 01:28:49.159651 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1002 01:28:49.159706 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1002 01:28:49.159760 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1002 01:28:49.159815 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var
I1002 01:28:49.159869 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1002 01:28:49.159924 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var
I1002 01:28:49.159978 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var
I1002 01:28:49.160032 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var
I1002 01:28:49.160087 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var
I1002 01:28:49.160140 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var
I1002 01:28:49.160195 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var
I1002 01:28:49.160250 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var
I1002 01:28:49.160309 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var
I1002 01:28:49.160365 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var
I1002 01:28:49.160419 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var
I1002 01:28:49.160474 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var
I1002 01:28:49.160528 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var
I1002 01:28:49.160581 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1002 01:28:49.160635 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1002 01:28:49.160690 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1002 01:28:49.160745 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var
I1002 01:28:49.160800 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1002 01:28:49.160855 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var
I1002 01:28:49.160909 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var
I1002 01:28:49.160964 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var
I1002 01:28:49.161019 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var
I1002 01:28:49.161073 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var
I1002 01:28:49.161129 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var
I1002 01:28:49.161190 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[0].b                  (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var
I1002 01:28:49.161246 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[0].w                  (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var
I1002 01:28:49.161300 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[1].b                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var
I1002 01:28:49.161355 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[1].w                  (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var
I1002 01:28:49.161409 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.fflayer.layer_norm.bias                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var
I1002 01:28:49.161464 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.fflayer.layer_norm.scale                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var
I1002 01:28:49.161518 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.self_atten.atten.atten.per_dim_scale     (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1002 01:28:49.161575 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.self_atten.atten.ctx_post_proj           (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1002 01:28:49.161630 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.self_atten.atten.ctx_post_proj_b         (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1002 01:28:49.161685 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.self_atten.atten.ctx_proj                (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var
I1002 01:28:49.161740 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.self_atten.atten.ctx_proj_b              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1002 01:28:49.161795 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.self_atten.atten.query_proj              (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var
I1002 01:28:49.161850 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.self_atten.atten.query_proj_b            (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var
I1002 01:28:49.161906 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.self_atten.atten.source_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var
I1002 01:28:49.161961 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.self_atten.atten.source_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var
I1002 01:28:49.162021 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.self_atten.layer_norm.bias               (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var
I1002 01:28:49.162077 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.self_atten.layer_norm.scale              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var
I1002 01:28:49.162132 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[0].b                  (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var
I1002 01:28:49.162188 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[0].w                  (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var
I1002 01:28:49.162243 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[1].b                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var
I1002 01:28:49.162297 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[1].w                  (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var
I1002 01:28:49.162352 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.fflayer.layer_norm.bias                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var
I1002 01:28:49.162406 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.fflayer.layer_norm.scale                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var
I1002 01:28:49.162461 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.self_atten.atten.atten.per_dim_scale     (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1002 01:28:49.162515 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.self_atten.atten.ctx_post_proj           (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1002 01:28:49.162571 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.self_atten.atten.ctx_post_proj_b         (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1002 01:28:49.162625 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.self_atten.atten.ctx_proj                (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var
I1002 01:28:49.162680 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.self_atten.atten.ctx_proj_b              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1002 01:28:49.162736 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.self_atten.atten.query_proj              (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var
I1002 01:28:49.162791 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.self_atten.atten.query_proj_b            (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var
I1002 01:28:49.162846 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.self_atten.atten.source_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var
I1002 01:28:49.162905 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.self_atten.atten.source_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var
I1002 01:28:49.162960 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.self_atten.layer_norm.bias               (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var
I1002 01:28:49.163015 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.self_atten.layer_norm.scale              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var
I1002 01:28:49.163070 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var
I1002 01:28:49.163124 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var
I1002 01:28:49.163178 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var
I1002 01:28:49.163233 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var
I1002 01:28:49.163288 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var
I1002 01:28:49.163356 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var
I1002 01:28:49.163411 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1002 01:28:49.163466 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1002 01:28:49.163521 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1002 01:28:49.163576 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var
I1002 01:28:49.163631 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1002 01:28:49.163686 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var
I1002 01:28:49.163745 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var
I1002 01:28:49.163801 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var
I1002 01:28:49.163856 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var
I1002 01:28:49.163911 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var
I1002 01:28:49.163966 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var
I1002 01:28:49.164022 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var
I1002 01:28:49.164077 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var
I1002 01:28:49.164131 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var
I1002 01:28:49.164185 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var
I1002 01:28:49.164240 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var
I1002 01:28:49.164294 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var
I1002 01:28:49.164347 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1002 01:28:49.164402 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1002 01:28:49.164456 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1002 01:28:49.164510 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var
I1002 01:28:49.164569 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1002 01:28:49.164625 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var
I1002 01:28:49.164680 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var
I1002 01:28:49.164734 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var
I1002 01:28:49.164788 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var
I1002 01:28:49.164843 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var
I1002 01:28:49.164897 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var
I1002 01:28:49.164951 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var
I1002 01:28:49.165006 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var
I1002 01:28:49.165060 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var
I1002 01:28:49.165114 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var
I1002 01:28:49.165169 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var
I1002 01:28:49.165223 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var
I1002 01:28:49.165278 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1002 01:28:49.165332 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1002 01:28:49.165387 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1002 01:28:49.165447 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var
I1002 01:28:49.165503 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1002 01:28:49.165558 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var
I1002 01:28:49.165613 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var
I1002 01:28:49.165668 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var
I1002 01:28:49.165722 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var
I1002 01:28:49.165777 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var
I1002 01:28:49.165832 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var
I1002 01:28:49.165887 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var
I1002 01:28:49.165942 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var
I1002 01:28:49.165997 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var
I1002 01:28:49.166051 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var
I1002 01:28:49.166105 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var
I1002 01:28:49.166160 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var
I1002 01:28:49.166214 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1002 01:28:49.166275 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1002 01:28:49.166331 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1002 01:28:49.166386 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var
I1002 01:28:49.166441 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1002 01:28:49.166496 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var
I1002 01:28:49.166550 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var
I1002 01:28:49.166605 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var
I1002 01:28:49.166660 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var
I1002 01:28:49.166715 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var
I1002 01:28:49.166770 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var
I1002 01:28:49.166826 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var
I1002 01:28:49.166881 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var
I1002 01:28:49.166936 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var
I1002 01:28:49.166990 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var
I1002 01:28:49.167044 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var
I1002 01:28:49.167099 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var
I1002 01:28:49.167157 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1002 01:28:49.167213 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1002 01:28:49.167269 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1002 01:28:49.167336 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var
I1002 01:28:49.167393 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1002 01:28:49.167447 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var
I1002 01:28:49.167502 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var
I1002 01:28:49.167557 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var
I1002 01:28:49.167611 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var
I1002 01:28:49.167666 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var
I1002 01:28:49.167720 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var
I1002 01:28:49.167775 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var
I1002 01:28:49.167829 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var
I1002 01:28:49.167884 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var
I1002 01:28:49.167937 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var
I1002 01:28:49.167996 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var
I1002 01:28:49.168051 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var
I1002 01:28:49.168106 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1002 01:28:49.168160 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1002 01:28:49.168215 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1002 01:28:49.168270 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var
I1002 01:28:49.168324 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1002 01:28:49.168379 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var
I1002 01:28:49.168433 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var
I1002 01:28:49.168488 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var
I1002 01:28:49.168542 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var
I1002 01:28:49.168596 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var
I1002 01:28:49.168651 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var
I1002 01:28:49.168706 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var
I1002 01:28:49.168760 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var
I1002 01:28:49.168822 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var
I1002 01:28:49.168876 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var
I1002 01:28:49.168931 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var
I1002 01:28:49.168985 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var
I1002 01:28:49.169040 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1002 01:28:49.169095 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1002 01:28:49.169150 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1002 01:28:49.169204 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var
I1002 01:28:49.169260 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1002 01:28:49.169314 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var
I1002 01:28:49.169369 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var
I1002 01:28:49.169424 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var
I1002 01:28:49.169479 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var
I1002 01:28:49.169534 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var
I1002 01:28:49.169588 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var
I1002 01:28:49.169643 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var
I1002 01:28:49.169702 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var
I1002 01:28:49.169757 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var
I1002 01:28:49.169810 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var
I1002 01:28:49.169864 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var
I1002 01:28:49.169918 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var
I1002 01:28:49.169971 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1002 01:28:49.170025 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1002 01:28:49.170079 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1002 01:28:49.170134 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var
I1002 01:28:49.170188 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1002 01:28:49.170243 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var
I1002 01:28:49.170298 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var
I1002 01:28:49.170352 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var
I1002 01:28:49.170408 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var
I1002 01:28:49.170462 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var
I1002 01:28:49.170521 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var
I1002 01:28:49.170576 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var
I1002 01:28:49.170630 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var
I1002 01:28:49.170684 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var
I1002 01:28:49.170739 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var
I1002 01:28:49.170794 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var
I1002 01:28:49.170848 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var
I1002 01:28:49.170902 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1002 01:28:49.170957 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1002 01:28:49.171012 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1002 01:28:49.171066 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var
I1002 01:28:49.171121 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1002 01:28:49.171176 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var
I1002 01:28:49.171231 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var
I1002 01:28:49.171286 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var
I1002 01:28:49.171363 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var
I1002 01:28:49.171419 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var
I1002 01:28:49.171474 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var
I1002 01:28:49.171530 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var
I1002 01:28:49.171584 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var
I1002 01:28:49.171642 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var
I1002 01:28:49.171697 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var
I1002 01:28:49.171751 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var
I1002 01:28:49.171806 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var
I1002 01:28:49.171860 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1002 01:28:49.171915 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1002 01:28:49.171970 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1002 01:28:49.172024 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var
I1002 01:28:49.172079 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1002 01:28:49.172133 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var
I1002 01:28:49.172188 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var
I1002 01:28:49.172248 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var
I1002 01:28:49.172303 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var
I1002 01:28:49.172357 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var
I1002 01:28:49.172412 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var
I1002 01:28:49.172467 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var
I1002 01:28:49.172521 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var
I1002 01:28:49.172576 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var
I1002 01:28:49.172630 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var
I1002 01:28:49.172685 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var
I1002 01:28:49.172739 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var
I1002 01:28:49.172793 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1002 01:28:49.172847 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1002 01:28:49.172902 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1002 01:28:49.172956 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var
I1002 01:28:49.173010 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1002 01:28:49.173070 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var
I1002 01:28:49.173125 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var
I1002 01:28:49.173179 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var
I1002 01:28:49.173234 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var
I1002 01:28:49.173288 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var
I1002 01:28:49.173342 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var
I1002 01:28:49.173397 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var
I1002 01:28:49.173451 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var
I1002 01:28:49.173505 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var
I1002 01:28:49.173559 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var
I1002 01:28:49.173613 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var
I1002 01:28:49.173667 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var
I1002 01:28:49.173721 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1002 01:28:49.173776 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1002 01:28:49.173830 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1002 01:28:49.173888 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var
I1002 01:28:49.173944 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1002 01:28:49.173999 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var
I1002 01:28:49.174053 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var
I1002 01:28:49.174108 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var
I1002 01:28:49.174163 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var
I1002 01:28:49.174217 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var
I1002 01:28:49.174271 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var
I1002 01:28:49.174325 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var
I1002 01:28:49.174380 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var
I1002 01:28:49.174433 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var
I1002 01:28:49.174488 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var
I1002 01:28:49.174543 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var
I1002 01:28:49.174597 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var
I1002 01:28:49.174651 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1002 01:28:49.174705 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1002 01:28:49.174765 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1002 01:28:49.174820 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var
I1002 01:28:49.174874 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1002 01:28:49.174929 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var
I1002 01:28:49.174984 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var
I1002 01:28:49.175039 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var
I1002 01:28:49.175093 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var
I1002 01:28:49.175147 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var
I1002 01:28:49.175202 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var
I1002 01:28:49.175256 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var
I1002 01:28:49.175322 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var
I1002 01:28:49.175379 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var
I1002 01:28:49.175433 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var
I1002 01:28:49.175487 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var
I1002 01:28:49.175541 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var
I1002 01:28:49.175601 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1002 01:28:49.175656 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1002 01:28:49.175710 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1002 01:28:49.175765 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var
I1002 01:28:49.175819 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1002 01:28:49.175874 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var
I1002 01:28:49.175928 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var
I1002 01:28:49.175983 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var
I1002 01:28:49.176037 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var
I1002 01:28:49.176091 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var
I1002 01:28:49.176146 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var
I1002 01:28:49.176200 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var
I1002 01:28:49.176254 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var
I1002 01:28:49.176309 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var
I1002 01:28:49.176363 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var
I1002 01:28:49.176417 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var
I1002 01:28:49.176480 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var
I1002 01:28:49.176535 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1002 01:28:49.176590 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1002 01:28:49.176645 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1002 01:28:49.176699 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var
I1002 01:28:49.176753 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1002 01:28:49.176807 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var
I1002 01:28:49.176862 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var
I1002 01:28:49.176916 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var
I1002 01:28:49.176970 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var
I1002 01:28:49.177024 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var
I1002 01:28:49.177078 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var
I1002 01:28:49.177133 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var
I1002 01:28:49.177187 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var
I1002 01:28:49.177240 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var
I1002 01:28:49.177299 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var
I1002 01:28:49.177354 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var
I1002 01:28:49.177408 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var
I1002 01:28:49.177462 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1002 01:28:49.177517 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1002 01:28:49.177571 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1002 01:28:49.177626 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var
I1002 01:28:49.177680 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1002 01:28:49.177735 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var
I1002 01:28:49.177789 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var
I1002 01:28:49.177844 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var
I1002 01:28:49.177898 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var
I1002 01:28:49.177953 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var
I1002 01:28:49.178008 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var
I1002 01:28:49.178061 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.bias_0                                     (2000,)                    2000 1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var
I1002 01:28:49.178121 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.bias_1                                     (2000,)                    2000 1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var
I1002 01:28:49.178176 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.bias_10                                    (2000,)                    2000 1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var
I1002 01:28:49.178230 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.bias_11                                    (2000,)                    2000 1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var
I1002 01:28:49.178284 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.bias_12                                    (2000,)                    2000 1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var
I1002 01:28:49.178338 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.bias_13                                    (2000,)                    2000 1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var
I1002 01:28:49.178392 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.bias_14                                    (2000,)                    2000 1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var
I1002 01:28:49.178446 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.bias_15                                    (2000,)                    2000 1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var
I1002 01:28:49.178500 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.bias_2                                     (2000,)                    2000 1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var
I1002 01:28:49.178555 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.bias_3                                     (2000,)                    2000 1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var
I1002 01:28:49.178609 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.bias_4                                     (2000,)                    2000 1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var
I1002 01:28:49.178663 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.bias_5                                     (2000,)                    2000 1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var
I1002 01:28:49.178718 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.bias_6                                     (2000,)                    2000 1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var
I1002 01:28:49.178771 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.bias_7                                     (2000,)                    2000 1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var
I1002 01:28:49.178825 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.bias_8                                     (2000,)                    2000 1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var
I1002 01:28:49.178879 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.bias_9                                     (2000,)                    2000 1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var
I1002 01:28:49.178933 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.weight_0                                   (2048, 2000)            4096000 1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var
I1002 01:28:49.178988 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.weight_1                                   (2048, 2000)            4096000 1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var
I1002 01:28:49.179042 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.weight_10                                  (2048, 2000)            4096000 1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var
I1002 01:28:49.179100 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.weight_11                                  (2048, 2000)            4096000 1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var
I1002 01:28:49.179155 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.weight_12                                  (2048, 2000)            4096000 1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var
I1002 01:28:49.179209 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.weight_13                                  (2048, 2000)            4096000 1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var
I1002 01:28:49.179263 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.weight_14                                  (2048, 2000)            4096000 1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var
I1002 01:28:49.179328 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.weight_15                                  (2048, 2000)            4096000 1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var
I1002 01:28:49.179385 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.weight_2                                   (2048, 2000)            4096000 1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var
I1002 01:28:49.179439 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.weight_3                                   (2048, 2000)            4096000 1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var
I1002 01:28:49.179493 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.weight_4                                   (2048, 2000)            4096000 1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var
I1002 01:28:49.179548 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.weight_5                                   (2048, 2000)            4096000 1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var
I1002 01:28:49.179602 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.weight_6                                   (2048, 2000)            4096000 1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var
I1002 01:28:49.179656 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.weight_7                                   (2048, 2000)            4096000 1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var
I1002 01:28:49.179710 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.weight_8                                   (2048, 2000)            4096000 1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var
I1002 01:28:49.179764 139650252134208 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.weight_9                                   (2048, 2000)            4096000 1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var
I1002 01:28:49.179819 139650252134208 py_utils.py:1474] MODEL ANALYSIS: ====================================================================================================
I1002 01:28:49.179872 139650252134208 py_utils.py:1474] MODEL ANALYSIS: total #params: 1742572800
I1002 01:28:49.179927 139650252134208 py_utils.py:1474] MODEL ANALYSIS: 
I1002 01:29:08.561513 139650252134208 trainer.py:1515] Job trainer_client start
I1002 01:29:08.575267 139650252134208 base_runner.py:57] ============================================================
I1002 01:29:08.580930 139650252134208 base_runner.py:59] allow_implicit_capture : NoneType
I1002 01:29:08.581032 139650252134208 base_runner.py:59] cls : type/lingvo.core.base_model/SingleTaskModel
I1002 01:29:08.581109 139650252134208 base_runner.py:59] cluster.add_summary : NoneType
I1002 01:29:08.581174 139650252134208 base_runner.py:59] cluster.cls : type/lingvo.core.cluster/_Cluster
I1002 01:29:08.581237 139650252134208 base_runner.py:59] cluster.controller.cpus_per_replica : 1
I1002 01:29:08.581310 139650252134208 base_runner.py:59] cluster.controller.devices_per_split : 1
I1002 01:29:08.581371 139650252134208 base_runner.py:59] cluster.controller.gpus_per_replica : 0
I1002 01:29:08.581431 139650252134208 base_runner.py:59] cluster.controller.name : '/job:local'
I1002 01:29:08.581490 139650252134208 base_runner.py:59] cluster.controller.num_tpu_hosts : 0
I1002 01:29:08.581549 139650252134208 base_runner.py:59] cluster.controller.replicas : 1
I1002 01:29:08.581608 139650252134208 base_runner.py:59] cluster.controller.targets : ''
I1002 01:29:08.581667 139650252134208 base_runner.py:59] cluster.controller.tpus_per_replica : 0
I1002 01:29:08.581725 139650252134208 base_runner.py:59] cluster.decoder.cpus_per_replica : 1
I1002 01:29:08.581784 139650252134208 base_runner.py:59] cluster.decoder.devices_per_split : 1
I1002 01:29:08.581843 139650252134208 base_runner.py:59] cluster.decoder.gpus_per_replica : 1
I1002 01:29:08.581902 139650252134208 base_runner.py:59] cluster.decoder.name : '/job:local'
I1002 01:29:08.581961 139650252134208 base_runner.py:59] cluster.decoder.num_tpu_hosts : 0
I1002 01:29:08.582020 139650252134208 base_runner.py:59] cluster.decoder.replicas : 1
I1002 01:29:08.582079 139650252134208 base_runner.py:59] cluster.decoder.targets : ''
I1002 01:29:08.582137 139650252134208 base_runner.py:59] cluster.decoder.tpus_per_replica : 0
I1002 01:29:08.582195 139650252134208 base_runner.py:59] cluster.evaler.cpus_per_replica : 1
I1002 01:29:08.582253 139650252134208 base_runner.py:59] cluster.evaler.devices_per_split : 1
I1002 01:29:08.582312 139650252134208 base_runner.py:59] cluster.evaler.gpus_per_replica : 1
I1002 01:29:08.582371 139650252134208 base_runner.py:59] cluster.evaler.name : '/job:local'
I1002 01:29:08.582429 139650252134208 base_runner.py:59] cluster.evaler.num_tpu_hosts : 0
I1002 01:29:08.582487 139650252134208 base_runner.py:59] cluster.evaler.replicas : 1
I1002 01:29:08.582545 139650252134208 base_runner.py:59] cluster.evaler.targets : ''
I1002 01:29:08.582602 139650252134208 base_runner.py:59] cluster.evaler.tpus_per_replica : 0
I1002 01:29:08.582661 139650252134208 base_runner.py:59] cluster.input.cpus_per_replica : 1
I1002 01:29:08.582718 139650252134208 base_runner.py:59] cluster.input.devices_per_split : 1
I1002 01:29:08.582776 139650252134208 base_runner.py:59] cluster.input.gpus_per_replica : 0
I1002 01:29:08.582834 139650252134208 base_runner.py:59] cluster.input.name : '/job:local'
I1002 01:29:08.582892 139650252134208 base_runner.py:59] cluster.input.num_tpu_hosts : 0
I1002 01:29:08.582951 139650252134208 base_runner.py:59] cluster.input.replicas : 0
I1002 01:29:08.583010 139650252134208 base_runner.py:59] cluster.input.targets : ''
I1002 01:29:08.583067 139650252134208 base_runner.py:59] cluster.input.tpus_per_replica : 0
I1002 01:29:08.583125 139650252134208 base_runner.py:59] cluster.job : 'trainer_client'
I1002 01:29:08.583183 139650252134208 base_runner.py:59] cluster.logdir : ''
I1002 01:29:08.583240 139650252134208 base_runner.py:59] cluster.mode : 'sync'
I1002 01:29:08.583313 139650252134208 base_runner.py:59] cluster.ps.cpus_per_replica : 1
I1002 01:29:08.583375 139650252134208 base_runner.py:59] cluster.ps.devices_per_split : 1
I1002 01:29:08.583436 139650252134208 base_runner.py:59] cluster.ps.gpus_per_replica : 0
I1002 01:29:08.583494 139650252134208 base_runner.py:59] cluster.ps.name : '/job:local'
I1002 01:29:08.583553 139650252134208 base_runner.py:59] cluster.ps.num_tpu_hosts : 0
I1002 01:29:08.583611 139650252134208 base_runner.py:59] cluster.ps.replicas : 1
I1002 01:29:08.583669 139650252134208 base_runner.py:59] cluster.ps.targets : ''
I1002 01:29:08.583727 139650252134208 base_runner.py:59] cluster.ps.tpus_per_replica : 0
I1002 01:29:08.583785 139650252134208 base_runner.py:59] cluster.task : 0
I1002 01:29:08.583842 139650252134208 base_runner.py:59] cluster.worker.cpus_per_replica : 1
I1002 01:29:08.583900 139650252134208 base_runner.py:59] cluster.worker.devices_per_split : 1
I1002 01:29:08.583958 139650252134208 base_runner.py:59] cluster.worker.gpus_per_replica : 1
I1002 01:29:08.584022 139650252134208 base_runner.py:59] cluster.worker.name : '/job:local'
I1002 01:29:08.584081 139650252134208 base_runner.py:59] cluster.worker.num_tpu_hosts : 0
I1002 01:29:08.584139 139650252134208 base_runner.py:59] cluster.worker.replicas : 1
I1002 01:29:08.584197 139650252134208 base_runner.py:59] cluster.worker.targets : ''
I1002 01:29:08.584255 139650252134208 base_runner.py:59] cluster.worker.tpus_per_replica : 0
I1002 01:29:08.584313 139650252134208 base_runner.py:59] dtype : float32
I1002 01:29:08.584371 139650252134208 base_runner.py:59] fprop_dtype : NoneType
I1002 01:29:08.584429 139650252134208 base_runner.py:59] inference_driver_name : NoneType
I1002 01:29:08.584487 139650252134208 base_runner.py:59] input.allow_implicit_capture : NoneType
I1002 01:29:08.584545 139650252134208 base_runner.py:59] input.bucket_adjust_every_n : 0
I1002 01:29:08.584602 139650252134208 base_runner.py:59] input.bucket_batch_limit : [32]
I1002 01:29:08.584660 139650252134208 base_runner.py:59] input.bucket_upper_bound : [1024]
I1002 01:29:08.584718 139650252134208 base_runner.py:59] input.cls : type/lingvo.tasks.lm.input_generator/LmInput
I1002 01:29:08.584777 139650252134208 base_runner.py:59] input.dtype : float32
I1002 01:29:08.584835 139650252134208 base_runner.py:59] input.file_buffer_size : 10000000
I1002 01:29:08.584892 139650252134208 base_runner.py:59] input.file_datasource : NoneType
I1002 01:29:08.584950 139650252134208 base_runner.py:59] input.file_parallelism : 10
I1002 01:29:08.585008 139650252134208 base_runner.py:59] input.file_pattern : 'text:/tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en*'
I1002 01:29:08.585068 139650252134208 base_runner.py:59] input.file_random_seed : 301
I1002 01:29:08.585127 139650252134208 base_runner.py:59] input.fixed_input_shape : True
I1002 01:29:08.585184 139650252134208 base_runner.py:59] input.flush_every_n : 0
I1002 01:29:08.585242 139650252134208 base_runner.py:59] input.fprop_dtype : NoneType
I1002 01:29:08.585300 139650252134208 base_runner.py:59] input.inference_driver_name : NoneType
I1002 01:29:08.585358 139650252134208 base_runner.py:59] input.is_eval : NoneType
I1002 01:29:08.585416 139650252134208 base_runner.py:59] input.is_inference : NoneType
I1002 01:29:08.585474 139650252134208 base_runner.py:59] input.name : '1bwds_train_set'
I1002 01:29:08.585531 139650252134208 base_runner.py:59] input.num_batcher_threads : 16
I1002 01:29:08.585589 139650252134208 base_runner.py:59] input.num_samples : 0
I1002 01:29:08.585647 139650252134208 base_runner.py:59] input.pad_to_max_seq_length : False
I1002 01:29:08.585705 139650252134208 base_runner.py:59] input.params_init.method : 'xavier'
I1002 01:29:08.585762 139650252134208 base_runner.py:59] input.params_init.scale : 1.000001
I1002 01:29:08.585820 139650252134208 base_runner.py:59] input.params_init.seed : NoneType
I1002 01:29:08.585877 139650252134208 base_runner.py:59] input.random_seed : NoneType
I1002 01:29:08.585934 139650252134208 base_runner.py:59] input.remote.max_inflights_per_target : 32
I1002 01:29:08.585992 139650252134208 base_runner.py:59] input.remote.shardable_batch : False
I1002 01:29:08.586049 139650252134208 base_runner.py:59] input.require_sequential_order : False
I1002 01:29:08.586106 139650252134208 base_runner.py:59] input.skip_lp_regularization : NoneType
I1002 01:29:08.586164 139650252134208 base_runner.py:59] input.source_max_length : NoneType
I1002 01:29:08.586221 139650252134208 base_runner.py:59] input.target_max_length : 1024
I1002 01:29:08.586279 139650252134208 base_runner.py:59] input.tokenizer.allow_implicit_capture : NoneType
I1002 01:29:08.586337 139650252134208 base_runner.py:59] input.tokenizer.append_eos : True
I1002 01:29:08.586395 139650252134208 base_runner.py:59] input.tokenizer.cls : type/lingvo.core.tokenizers/AsciiTokenizer
I1002 01:29:08.586453 139650252134208 base_runner.py:59] input.tokenizer.dtype : float32
I1002 01:29:08.586510 139650252134208 base_runner.py:59] input.tokenizer.fprop_dtype : NoneType
I1002 01:29:08.586572 139650252134208 base_runner.py:59] input.tokenizer.inference_driver_name : NoneType
I1002 01:29:08.586631 139650252134208 base_runner.py:59] input.tokenizer.is_eval : NoneType
I1002 01:29:08.586688 139650252134208 base_runner.py:59] input.tokenizer.is_inference : NoneType
I1002 01:29:08.586746 139650252134208 base_runner.py:59] input.tokenizer.name : 'tokenizer'
I1002 01:29:08.586804 139650252134208 base_runner.py:59] input.tokenizer.pad_to_max_length : True
I1002 01:29:08.586862 139650252134208 base_runner.py:59] input.tokenizer.params_init.method : 'xavier'
I1002 01:29:08.586919 139650252134208 base_runner.py:59] input.tokenizer.params_init.scale : 1.000001
I1002 01:29:08.586977 139650252134208 base_runner.py:59] input.tokenizer.params_init.seed : NoneType
I1002 01:29:08.587035 139650252134208 base_runner.py:59] input.tokenizer.random_seed : NoneType
I1002 01:29:08.587092 139650252134208 base_runner.py:59] input.tokenizer.skip_lp_regularization : NoneType
I1002 01:29:08.587150 139650252134208 base_runner.py:59] input.tokenizer.target_eos_id : 2
I1002 01:29:08.587208 139650252134208 base_runner.py:59] input.tokenizer.target_sos_id : 1
I1002 01:29:08.587265 139650252134208 base_runner.py:59] input.tokenizer.target_unk_id : 0
I1002 01:29:08.587341 139650252134208 base_runner.py:59] input.tokenizer.vn.global_vn : False
I1002 01:29:08.587402 139650252134208 base_runner.py:59] input.tokenizer.vn.per_step_vn : False
I1002 01:29:08.587460 139650252134208 base_runner.py:59] input.tokenizer.vn.scale : NoneType
I1002 01:29:08.587518 139650252134208 base_runner.py:59] input.tokenizer.vn.seed : NoneType
I1002 01:29:08.587574 139650252134208 base_runner.py:59] input.tokenizer.vocab_size : 32000
I1002 01:29:08.587632 139650252134208 base_runner.py:59] input.tokenizer_dict : {}
I1002 01:29:08.587689 139650252134208 base_runner.py:59] input.tpu_infeed_parallelism : 1
I1002 01:29:08.587746 139650252134208 base_runner.py:59] input.use_chaining : False
I1002 01:29:08.587803 139650252134208 base_runner.py:59] input.use_per_host_infeed : False
I1002 01:29:08.587861 139650252134208 base_runner.py:59] input.use_within_batch_mixing : False
I1002 01:29:08.587918 139650252134208 base_runner.py:59] input.vn.global_vn : False
I1002 01:29:08.587976 139650252134208 base_runner.py:59] input.vn.per_step_vn : False
I1002 01:29:08.588034 139650252134208 base_runner.py:59] input.vn.scale : NoneType
I1002 01:29:08.588092 139650252134208 base_runner.py:59] input.vn.seed : NoneType
I1002 01:29:08.588150 139650252134208 base_runner.py:59] is_eval : NoneType
I1002 01:29:08.588208 139650252134208 base_runner.py:59] is_inference : NoneType
I1002 01:29:08.588266 139650252134208 base_runner.py:59] model : 'lm.one_billion_wds.OneBWdsGPipeTransformerWPM@/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/tasks/lm/params/one_billion_wds.py:188'
I1002 01:29:08.588325 139650252134208 base_runner.py:59] name : ''
I1002 01:29:08.588382 139650252134208 base_runner.py:59] params_init.method : 'xavier'
I1002 01:29:08.588440 139650252134208 base_runner.py:59] params_init.scale : 1.000001
I1002 01:29:08.588498 139650252134208 base_runner.py:59] params_init.seed : NoneType
I1002 01:29:08.588555 139650252134208 base_runner.py:59] random_seed : NoneType
I1002 01:29:08.588613 139650252134208 base_runner.py:59] skip_lp_regularization : NoneType
I1002 01:29:08.588671 139650252134208 base_runner.py:59] task.allow_implicit_capture : NoneType
I1002 01:29:08.588729 139650252134208 base_runner.py:59] task.cls : type/lingvo.tasks.lm.model/FixedShapeInputLanguageModel
I1002 01:29:08.588786 139650252134208 base_runner.py:59] task.decoder : NoneType
I1002 01:29:08.588844 139650252134208 base_runner.py:59] task.dtype : float32
I1002 01:29:08.588902 139650252134208 base_runner.py:59] task.encoder : NoneType
I1002 01:29:08.588960 139650252134208 base_runner.py:59] task.eval.decoder_samples_per_summary : 0
I1002 01:29:08.589018 139650252134208 base_runner.py:59] task.eval.load_checkpoint_from : NoneType
I1002 01:29:08.589081 139650252134208 base_runner.py:59] task.eval.samples_per_summary : 0
I1002 01:29:08.589140 139650252134208 base_runner.py:59] task.eval.start_decoder_after : 0
I1002 01:29:08.589198 139650252134208 base_runner.py:59] task.eval.start_eval_after : 0
I1002 01:29:08.589256 139650252134208 base_runner.py:59] task.fprop_dtype : NoneType
I1002 01:29:08.589313 139650252134208 base_runner.py:59] task.inference_driver_name : NoneType
I1002 01:29:08.589371 139650252134208 base_runner.py:59] task.input : NoneType
I1002 01:29:08.589428 139650252134208 base_runner.py:59] task.is_eval : NoneType
I1002 01:29:08.589486 139650252134208 base_runner.py:59] task.is_inference : NoneType
I1002 01:29:08.589543 139650252134208 base_runner.py:59] task.lm.allow_implicit_capture : NoneType
I1002 01:29:08.589601 139650252134208 base_runner.py:59] task.lm.cls : type/lingvo.tasks.lm.layers/GPipeTransformerLm
I1002 01:29:08.589658 139650252134208 base_runner.py:59] task.lm.dtype : float32
I1002 01:29:08.589722 139650252134208 base_runner.py:59] task.lm.fprop_dtype : NoneType
I1002 01:29:08.589780 139650252134208 base_runner.py:59] task.lm.inference_driver_name : NoneType
I1002 01:29:08.589838 139650252134208 base_runner.py:59] task.lm.is_eval : NoneType
I1002 01:29:08.589896 139650252134208 base_runner.py:59] task.lm.is_inference : NoneType
I1002 01:29:08.589953 139650252134208 base_runner.py:59] task.lm.name : 'transformerlm'
I1002 01:29:08.590011 139650252134208 base_runner.py:59] task.lm.params_init.method : 'xavier'
I1002 01:29:08.590069 139650252134208 base_runner.py:59] task.lm.params_init.scale : 1.000001
I1002 01:29:08.590126 139650252134208 base_runner.py:59] task.lm.params_init.seed : NoneType
I1002 01:29:08.590183 139650252134208 base_runner.py:59] task.lm.random_seed : NoneType
I1002 01:29:08.590240 139650252134208 base_runner.py:59] task.lm.skip_lp_regularization : NoneType
I1002 01:29:08.590297 139650252134208 base_runner.py:59] task.lm.stack.allow_implicit_capture : NoneType
I1002 01:29:08.590354 139650252134208 base_runner.py:59] task.lm.stack.batch_dim : 1
I1002 01:29:08.590411 139650252134208 base_runner.py:59] task.lm.stack.cls : type/lingvo.core.layers_with_gpipe/GPipeTransformerStack
I1002 01:29:08.590468 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.allow_implicit_capture : NoneType
I1002 01:29:08.590525 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.cls : type/lingvo.core.layers_with_gpipe/GPipeTransformerLayer
I1002 01:29:08.590583 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.dtype : float32
I1002 01:29:08.590641 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.final_enc_layer : False
I1002 01:29:08.590698 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.fprop_dtype : NoneType
I1002 01:29:08.590756 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.has_aux_atten : True
I1002 01:29:08.590814 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.inference_driver_name : NoneType
I1002 01:29:08.590871 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.is_decoder : False
I1002 01:29:08.590929 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.is_eval : NoneType
I1002 01:29:08.590986 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.is_inference : NoneType
I1002 01:29:08.591044 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.is_transparent : False
I1002 01:29:08.591101 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.allow_implicit_capture : NoneType
I1002 01:29:08.591158 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.cls : type/lingvo.core.layers/LayerNorm
I1002 01:29:08.591216 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.dtype : float32
I1002 01:29:08.591274 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.epsilon : 1e-06
I1002 01:29:08.591346 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.fprop_dtype : NoneType
I1002 01:29:08.591406 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.inference_driver_name : NoneType
I1002 01:29:08.591468 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.input_dim : 0
I1002 01:29:08.591527 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.is_eval : NoneType
I1002 01:29:08.591586 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.is_inference : NoneType
I1002 01:29:08.591643 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.name : ''
I1002 01:29:08.591707 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.params_init.method : 'xavier'
I1002 01:29:08.591764 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.params_init.scale : 1.000001
I1002 01:29:08.591822 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.params_init.seed : NoneType
I1002 01:29:08.591880 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.random_seed : NoneType
I1002 01:29:08.591938 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.skip_lp_regularization : NoneType
I1002 01:29:08.591996 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.vn.global_vn : False
I1002 01:29:08.592053 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.vn.per_step_vn : False
I1002 01:29:08.592110 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.vn.scale : NoneType
I1002 01:29:08.592168 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.vn.seed : NoneType
I1002 01:29:08.592226 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.mask_self_atten : True
I1002 01:29:08.592283 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.name : ''
I1002 01:29:08.592341 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.normalize_output : False
I1002 01:29:08.592398 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.output_dim : 0
I1002 01:29:08.592454 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.packed_input : False
I1002 01:29:08.592512 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.params_init.method : 'xavier'
I1002 01:29:08.592569 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.params_init.scale : 1.000001
I1002 01:29:08.592627 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.params_init.seed : NoneType
I1002 01:29:08.592684 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.random_seed : NoneType
I1002 01:29:08.592741 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.skip_lp_regularization : NoneType
I1002 01:29:08.592798 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.source_dim : 0
I1002 01:29:08.592856 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.add_unnormalized_input : False
I1002 01:29:08.592913 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.allow_implicit_capture : NoneType
I1002 01:29:08.592970 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_dropout_prob : 0.0
I1002 01:29:08.593028 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_hidden_dim : 0
I1002 01:29:08.593085 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.allow_implicit_capture : NoneType
I1002 01:29:08.593144 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.atten_dropout_deterministic : False
I1002 01:29:08.593202 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.atten_dropout_prob : 0.0
I1002 01:29:08.593260 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.cls : type/lingvo.core.attention/MultiHeadedAttention
I1002 01:29:08.593317 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.context_dim : 0
I1002 01:29:08.593374 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.ctx_post_proj_dim : 0
I1002 01:29:08.593433 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.dtype : float32
I1002 01:29:08.593497 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.enable_ctx_post_proj : True
I1002 01:29:08.593556 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.enable_ctx_pre_proj : False
I1002 01:29:08.593614 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.enable_query_proj : True
I1002 01:29:08.593672 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.enable_source_proj : True
I1002 01:29:08.593730 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.fprop_dtype : NoneType
I1002 01:29:08.593787 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.hidden_dim : 0
I1002 01:29:08.593845 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inference_driver_name : NoneType
I1002 01:29:08.593903 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.allow_implicit_capture : NoneType
I1002 01:29:08.593962 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.atten_dropout_deterministic : False
I1002 01:29:08.594020 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.atten_dropout_prob : 0.0
I1002 01:29:08.594078 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.cls : type/lingvo.core.attention/DotProductAttention
I1002 01:29:08.594135 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.dtype : float32
I1002 01:29:08.594193 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.fprop_dtype : NoneType
I1002 01:29:08.594250 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.hidden_dim : 0
I1002 01:29:08.594308 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.inference_driver_name : NoneType
I1002 01:29:08.594366 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.is_eval : NoneType
I1002 01:29:08.594423 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.is_inference : NoneType
I1002 01:29:08.594481 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.name : ''
I1002 01:29:08.594538 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.packed_input : False
I1002 01:29:08.594596 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.params_init.method : 'xavier'
I1002 01:29:08.594653 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.params_init.scale : 1.000001
I1002 01:29:08.594710 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.params_init.seed : NoneType
I1002 01:29:08.594768 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.qdomain.default : NoneType
I1002 01:29:08.594826 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.qdomain.fullyconnected : NoneType
I1002 01:29:08.594883 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.qdomain.softmax : NoneType
I1002 01:29:08.594941 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.query_dim : 0
I1002 01:29:08.594999 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.random_seed : NoneType
I1002 01:29:08.595057 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.skip_lp_regularization : NoneType
I1002 01:29:08.595119 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.source_dim : 0
I1002 01:29:08.595178 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.global_vn : False
I1002 01:29:08.595244 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.per_step_vn : False
I1002 01:29:08.595312 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.scale : NoneType
I1002 01:29:08.595373 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.seed : NoneType
I1002 01:29:08.595430 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.is_eval : NoneType
I1002 01:29:08.595487 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.is_inference : NoneType
I1002 01:29:08.595545 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.name : ''
I1002 01:29:08.595602 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.num_attention_heads : 2
I1002 01:29:08.595660 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.packed_input : False
I1002 01:29:08.595718 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.params_init.method : 'xavier'
I1002 01:29:08.595776 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.params_init.scale : 1.0
I1002 01:29:08.595833 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.params_init.seed : NoneType
I1002 01:29:08.595891 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.qdomain.atten_context : NoneType
I1002 01:29:08.595949 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.qdomain.default : NoneType
I1002 01:29:08.596007 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.qdomain.fullyconnected : NoneType
I1002 01:29:08.596064 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.qdomain.softmax : NoneType
I1002 01:29:08.596122 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.query_dim : 0
I1002 01:29:08.596179 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.random_seed : NoneType
I1002 01:29:08.596237 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.skip_lp_regularization : NoneType
I1002 01:29:08.596295 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.source_dim : 0
I1002 01:29:08.596352 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.use_source_vec_as_attention_value : False
I1002 01:29:08.596410 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.vn.global_vn : False
I1002 01:29:08.596467 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.vn.per_step_vn : False
I1002 01:29:08.596525 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.vn.scale : NoneType
I1002 01:29:08.596583 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.vn.seed : NoneType
I1002 01:29:08.596641 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.cls : type/lingvo.core.layers_with_attention/TransformerAttentionLayer
I1002 01:29:08.596699 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.context_dim : 0
I1002 01:29:08.596756 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.dtype : float32
I1002 01:29:08.596814 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.fprop_dtype : NoneType
I1002 01:29:08.596872 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.inference_driver_name : NoneType
I1002 01:29:08.596934 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.is_eval : NoneType
I1002 01:29:08.596993 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.is_inference : NoneType
I1002 01:29:08.597050 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.is_masked : False
I1002 01:29:08.597108 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.allow_implicit_capture : NoneType
I1002 01:29:08.597165 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.cls : type/lingvo.core.layers/LayerNorm
I1002 01:29:08.597222 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.dtype : float32
I1002 01:29:08.597280 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.epsilon : 1e-06
I1002 01:29:08.597337 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.fprop_dtype : NoneType
I1002 01:29:08.597394 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.inference_driver_name : NoneType
I1002 01:29:08.597452 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.input_dim : 0
I1002 01:29:08.597509 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.is_eval : NoneType
I1002 01:29:08.597567 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.is_inference : NoneType
I1002 01:29:08.597624 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.name : ''
I1002 01:29:08.597682 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.params_init.method : 'xavier'
I1002 01:29:08.597739 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.params_init.scale : 1.000001
I1002 01:29:08.597797 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.params_init.seed : NoneType
I1002 01:29:08.597855 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.random_seed : NoneType
I1002 01:29:08.597912 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.skip_lp_regularization : NoneType
I1002 01:29:08.597970 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.vn.global_vn : False
I1002 01:29:08.598028 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.vn.per_step_vn : False
I1002 01:29:08.598085 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.vn.scale : NoneType
I1002 01:29:08.598143 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.vn.seed : NoneType
I1002 01:29:08.598201 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.mask_type : 'future'
I1002 01:29:08.598258 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.name : ''
I1002 01:29:08.598316 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.num_attention_heads : 8
I1002 01:29:08.598373 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.packed_input : False
I1002 01:29:08.598431 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.params_init.method : 'xavier'
I1002 01:29:08.598488 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.params_init.scale : 1.000001
I1002 01:29:08.598546 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.params_init.seed : NoneType
I1002 01:29:08.598603 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.random_seed : NoneType
I1002 01:29:08.598661 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_prob : 0.0
I1002 01:29:08.598718 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.allow_implicit_capture : NoneType
I1002 01:29:08.598777 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.cls : type/lingvo.core.layers/DropoutLayer
I1002 01:29:08.598840 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.dropout_at_eval : False
I1002 01:29:08.598900 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.dtype : float32
I1002 01:29:08.598958 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.fprop_dtype : NoneType
I1002 01:29:08.599016 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.inference_driver_name : NoneType
I1002 01:29:08.599074 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.is_eval : NoneType
I1002 01:29:08.599132 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.is_inference : NoneType
I1002 01:29:08.599190 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.keep_prob : 1.0
I1002 01:29:08.599248 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.name : ''
I1002 01:29:08.599321 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.noise_shape : NoneType
I1002 01:29:08.599383 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.noise_shape_broadcast_dims : NoneType
I1002 01:29:08.599441 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.params_init.method : 'xavier'
I1002 01:29:08.599498 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.params_init.scale : 1.000001
I1002 01:29:08.599556 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.params_init.seed : NoneType
I1002 01:29:08.599614 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.random_seed : NoneType
I1002 01:29:08.599672 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.skip_lp_regularization : NoneType
I1002 01:29:08.599730 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.global_vn : False
I1002 01:29:08.599788 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.per_step_vn : False
I1002 01:29:08.599846 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.scale : NoneType
I1002 01:29:08.599904 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.seed : NoneType
I1002 01:29:08.599961 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.skip_lp_regularization : NoneType
I1002 01:29:08.600019 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.source_dim : 0
I1002 01:29:08.600077 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.vn.global_vn : False
I1002 01:29:08.600135 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.vn.per_step_vn : False
I1002 01:29:08.600193 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.vn.scale : NoneType
I1002 01:29:08.600259 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.vn.seed : NoneType
I1002 01:29:08.600317 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_aux_atten_tpl : NoneType
I1002 01:29:08.600376 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.activation : 'RELU'
I1002 01:29:08.600434 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.allow_implicit_capture : NoneType
I1002 01:29:08.600492 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.cls : type/lingvo.core.layers_with_attention/TransformerFeedForwardLayer
I1002 01:29:08.600551 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.dtype : float32
I1002 01:29:08.600614 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.activation : ['RELU', 'NONE']
I1002 01:29:08.600676 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.allow_implicit_capture : NoneType
I1002 01:29:08.600735 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.batch_norm : False
I1002 01:29:08.600794 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.bn_fold_weights : NoneType
I1002 01:29:08.600851 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.cls : type/lingvo.core.layers/FeedForwardNet
I1002 01:29:08.600909 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.allow_implicit_capture : NoneType
I1002 01:29:08.600967 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.cls : type/lingvo.core.layers/DropoutLayer
I1002 01:29:08.601025 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.dropout_at_eval : False
I1002 01:29:08.601082 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.dtype : float32
I1002 01:29:08.601140 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.fprop_dtype : NoneType
I1002 01:29:08.601198 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.inference_driver_name : NoneType
I1002 01:29:08.601256 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.is_eval : NoneType
I1002 01:29:08.601314 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.is_inference : NoneType
I1002 01:29:08.601372 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.keep_prob : 1.0
I1002 01:29:08.601429 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.name : ''
I1002 01:29:08.601488 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.noise_shape : NoneType
I1002 01:29:08.601546 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.noise_shape_broadcast_dims : NoneType
I1002 01:29:08.601604 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.params_init.method : 'xavier'
I1002 01:29:08.601662 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.params_init.scale : 1.000001
I1002 01:29:08.601720 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.params_init.seed : NoneType
I1002 01:29:08.601778 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.random_seed : NoneType
I1002 01:29:08.601835 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.skip_lp_regularization : NoneType
I1002 01:29:08.601894 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.global_vn : False
I1002 01:29:08.601952 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.per_step_vn : False
I1002 01:29:08.602010 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.scale : NoneType
I1002 01:29:08.602068 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.seed : NoneType
I1002 01:29:08.602127 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dtype : float32
I1002 01:29:08.602185 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.fprop_dtype : NoneType
I1002 01:29:08.602243 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.inference_driver_name : NoneType
I1002 01:29:08.602306 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.input_dim : 0
I1002 01:29:08.602365 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.is_eval : NoneType
I1002 01:29:08.602422 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.is_inference : NoneType
I1002 01:29:08.602481 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.name : ''
I1002 01:29:08.602539 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.params_init.method : 'xavier'
I1002 01:29:08.602598 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.params_init.scale : 1.000001
I1002 01:29:08.602656 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.params_init.seed : NoneType
I1002 01:29:08.602714 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.activation : 'RELU'
I1002 01:29:08.602772 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.affine_last : False
I1002 01:29:08.602830 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.allow_implicit_capture : NoneType
I1002 01:29:08.602889 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.batch_norm : True
I1002 01:29:08.602947 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.bias_init : 0.0
I1002 01:29:08.603005 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.bn_fold_weights : NoneType
I1002 01:29:08.603063 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.cls : type/lingvo.core.layers/ProjectionLayer
I1002 01:29:08.603121 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.dtype : float32
I1002 01:29:08.603179 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.fprop_dtype : NoneType
I1002 01:29:08.603238 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.has_bias : False
I1002 01:29:08.603307 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.inference_driver_name : NoneType
I1002 01:29:08.603368 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.input_dim : 0
I1002 01:29:08.603428 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.is_eval : NoneType
I1002 01:29:08.603487 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.is_inference : NoneType
I1002 01:29:08.603548 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.name : ''
I1002 01:29:08.603606 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.output_dim : 0
I1002 01:29:08.603665 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.params_init.method : 'xavier'
I1002 01:29:08.603724 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.params_init.scale : 1.000001
I1002 01:29:08.603782 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.params_init.seed : NoneType
I1002 01:29:08.603840 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.qdomain.default : NoneType
I1002 01:29:08.603898 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.random_seed : NoneType
I1002 01:29:08.603956 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.skip_lp_regularization : NoneType
I1002 01:29:08.604020 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.global_vn : False
I1002 01:29:08.604079 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.per_step_vn : False
I1002 01:29:08.604137 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.scale : NoneType
I1002 01:29:08.604196 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.seed : NoneType
I1002 01:29:08.604254 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.weight_norm : False
I1002 01:29:08.604311 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.qdomain.default : NoneType
I1002 01:29:08.604369 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.random_seed : NoneType
I1002 01:29:08.604427 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.skip_connections : NoneType
I1002 01:29:08.604485 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.skip_lp_regularization : NoneType
I1002 01:29:08.604542 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.global_vn : False
I1002 01:29:08.604600 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.per_step_vn : False
I1002 01:29:08.604658 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.scale : NoneType
I1002 01:29:08.604716 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.seed : NoneType
I1002 01:29:08.604774 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.weight_norm : False
I1002 01:29:08.604832 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fprop_dtype : NoneType
I1002 01:29:08.604890 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.hidden_dim : 2048
I1002 01:29:08.604948 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.inference_driver_name : NoneType
I1002 01:29:08.605005 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.input_dim : 0
I1002 01:29:08.605063 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.is_eval : NoneType
I1002 01:29:08.605121 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.is_inference : NoneType
I1002 01:29:08.605178 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.allow_implicit_capture : NoneType
I1002 01:29:08.605237 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.cls : type/lingvo.core.layers/LayerNorm
I1002 01:29:08.605294 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.dtype : float32
I1002 01:29:08.605352 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.epsilon : 1e-06
I1002 01:29:08.605410 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.fprop_dtype : NoneType
I1002 01:29:08.605468 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.inference_driver_name : NoneType
I1002 01:29:08.605525 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.input_dim : 0
I1002 01:29:08.605584 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.is_eval : NoneType
I1002 01:29:08.605642 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.is_inference : NoneType
I1002 01:29:08.605700 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.name : ''
I1002 01:29:08.605758 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.params_init.method : 'xavier'
I1002 01:29:08.605821 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.params_init.scale : 1.000001
I1002 01:29:08.605880 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.params_init.seed : NoneType
I1002 01:29:08.605938 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.random_seed : NoneType
I1002 01:29:08.605996 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.skip_lp_regularization : NoneType
I1002 01:29:08.606055 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.vn.global_vn : False
I1002 01:29:08.606112 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.vn.per_step_vn : False
I1002 01:29:08.606170 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.vn.scale : NoneType
I1002 01:29:08.606229 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.vn.seed : NoneType
I1002 01:29:08.606286 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.name : ''
I1002 01:29:08.606344 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.output_dim : 0
I1002 01:29:08.606402 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.params_init.method : 'xavier'
I1002 01:29:08.606460 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.params_init.scale : 1.000001
I1002 01:29:08.606518 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.params_init.seed : NoneType
I1002 01:29:08.606576 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.random_seed : NoneType
I1002 01:29:08.606633 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.relu_dropout_prob : 0.0
I1002 01:29:08.606695 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.activation : 'RELU'
I1002 01:29:08.606754 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.affine_last : False
I1002 01:29:08.606812 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.allow_implicit_capture : NoneType
I1002 01:29:08.606870 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.batch_norm : True
I1002 01:29:08.606928 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.bias_init : 0.0
I1002 01:29:08.606986 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.bn_fold_weights : NoneType
I1002 01:29:08.607044 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.cls : type/lingvo.core.layers/ProjectionLayer
I1002 01:29:08.607102 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.dtype : float32
I1002 01:29:08.607160 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.fprop_dtype : NoneType
I1002 01:29:08.607218 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.has_bias : False
I1002 01:29:08.607277 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.inference_driver_name : NoneType
I1002 01:29:08.607348 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.input_dim : 0
I1002 01:29:08.607407 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.is_eval : NoneType
I1002 01:29:08.607466 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.is_inference : NoneType
I1002 01:29:08.607523 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.name : ''
I1002 01:29:08.607581 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.output_dim : 0
I1002 01:29:08.607640 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.params_init.method : 'xavier'
I1002 01:29:08.607703 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.params_init.scale : 1.000001
I1002 01:29:08.607762 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.params_init.seed : NoneType
I1002 01:29:08.607820 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.qdomain.default : NoneType
I1002 01:29:08.607878 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.random_seed : NoneType
I1002 01:29:08.607936 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.skip_lp_regularization : NoneType
I1002 01:29:08.607993 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.global_vn : False
I1002 01:29:08.608051 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.per_step_vn : False
I1002 01:29:08.608109 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.scale : NoneType
I1002 01:29:08.608166 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.seed : NoneType
I1002 01:29:08.608224 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.weight_norm : False
I1002 01:29:08.608282 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_prob : 0.0
I1002 01:29:08.608340 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.allow_implicit_capture : NoneType
I1002 01:29:08.608398 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.cls : type/lingvo.core.layers/DropoutLayer
I1002 01:29:08.608456 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.dropout_at_eval : False
I1002 01:29:08.608514 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.dtype : float32
I1002 01:29:08.608573 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.fprop_dtype : NoneType
I1002 01:29:08.608631 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.inference_driver_name : NoneType
I1002 01:29:08.608689 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.is_eval : NoneType
I1002 01:29:08.608747 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.is_inference : NoneType
I1002 01:29:08.608805 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.keep_prob : 1.0
I1002 01:29:08.608863 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.name : ''
I1002 01:29:08.608921 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.noise_shape : NoneType
I1002 01:29:08.608978 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.noise_shape_broadcast_dims : NoneType
I1002 01:29:08.609036 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.params_init.method : 'xavier'
I1002 01:29:08.609094 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.params_init.scale : 1.000001
I1002 01:29:08.609151 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.params_init.seed : NoneType
I1002 01:29:08.609208 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.random_seed : NoneType
I1002 01:29:08.609266 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.skip_lp_regularization : NoneType
I1002 01:29:08.609323 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.global_vn : False
I1002 01:29:08.609387 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.per_step_vn : False
I1002 01:29:08.609446 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.scale : NoneType
I1002 01:29:08.609504 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.seed : NoneType
I1002 01:29:08.609562 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.skip_lp_regularization : NoneType
I1002 01:29:08.609620 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.vn.global_vn : False
I1002 01:29:08.609678 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.vn.per_step_vn : False
I1002 01:29:08.609735 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.vn.scale : NoneType
I1002 01:29:08.609792 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.vn.seed : NoneType
I1002 01:29:08.609850 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.transparent_merger_tpl : NoneType
I1002 01:29:08.609907 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.vn.global_vn : False
I1002 01:29:08.609965 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.vn.per_step_vn : False
I1002 01:29:08.610023 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.vn.scale : NoneType
I1002 01:29:08.610079 139650252134208 base_runner.py:59] task.lm.stack.decoder_tpl.vn.seed : NoneType
I1002 01:29:08.610136 139650252134208 base_runner.py:59] task.lm.stack.dtype : float32
I1002 01:29:08.610194 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.add_tgt_embedding_layer : False
I1002 01:29:08.610251 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.allow_implicit_capture : NoneType
I1002 01:29:08.610308 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.batch_dim : 1
I1002 01:29:08.610366 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.cls : type/lingvo.core.layers_with_gpipe/GPipeTransformerEmbeddingLayer
I1002 01:29:08.610423 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.dec_task_emb : NoneType
I1002 01:29:08.610481 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.allow_implicit_capture : NoneType
I1002 01:29:08.610538 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.cls : type/lingvo.core.layers/DropoutLayer
I1002 01:29:08.610596 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.dropout_at_eval : False
I1002 01:29:08.610653 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.dtype : float32
I1002 01:29:08.610711 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.fprop_dtype : NoneType
I1002 01:29:08.610768 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.inference_driver_name : NoneType
I1002 01:29:08.610826 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.is_eval : NoneType
I1002 01:29:08.610884 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.is_inference : NoneType
I1002 01:29:08.610942 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.keep_prob : 1.0
I1002 01:29:08.610999 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.name : ''
I1002 01:29:08.611057 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.noise_shape : NoneType
I1002 01:29:08.611115 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.noise_shape_broadcast_dims : NoneType
I1002 01:29:08.611173 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.params_init.method : 'xavier'
I1002 01:29:08.611230 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.params_init.scale : 1.000001
I1002 01:29:08.611288 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.params_init.seed : NoneType
I1002 01:29:08.611364 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.random_seed : NoneType
I1002 01:29:08.611428 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.skip_lp_regularization : NoneType
I1002 01:29:08.611487 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.vn.global_vn : False
I1002 01:29:08.611545 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.vn.per_step_vn : False
I1002 01:29:08.611604 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.vn.scale : NoneType
I1002 01:29:08.611662 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.vn.seed : NoneType
I1002 01:29:08.611721 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.dtype : float32
I1002 01:29:08.611778 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.enc_task_emb : NoneType
I1002 01:29:08.611837 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.fprop_dtype : NoneType
I1002 01:29:08.611895 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.inference_driver_name : NoneType
I1002 01:29:08.611952 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.input_dropout_prob : 0.0
I1002 01:29:08.612009 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.is_eval : NoneType
I1002 01:29:08.612066 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.is_inference : NoneType
I1002 01:29:08.612124 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.is_transparent : False
I1002 01:29:08.612181 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.max_seq_len : 300
I1002 01:29:08.612238 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.name : ''
I1002 01:29:08.612296 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.packed_input : False
I1002 01:29:08.612354 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.params_init.method : 'xavier'
I1002 01:29:08.612411 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.params_init.scale : 1.000001
I1002 01:29:08.612469 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.params_init.seed : NoneType
I1002 01:29:08.612528 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.allow_implicit_capture : NoneType
I1002 01:29:08.612585 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.cls : type/lingvo.core.layers/PositionalEmbeddingLayer
I1002 01:29:08.612648 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.dtype : float32
I1002 01:29:08.612706 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.embedding_dim : 2048
I1002 01:29:08.612764 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.fprop_dtype : NoneType
I1002 01:29:08.612823 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.inference_driver_name : NoneType
I1002 01:29:08.612880 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.is_eval : NoneType
I1002 01:29:08.612939 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.is_inference : NoneType
I1002 01:29:08.612997 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.max_timescale : 10000
I1002 01:29:08.613055 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.min_timescale : 1
I1002 01:29:08.613113 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.name : ''
I1002 01:29:08.613171 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.params_init.method : 'xavier'
I1002 01:29:08.613229 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.params_init.scale : 1.000001
I1002 01:29:08.613287 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.params_init.seed : NoneType
I1002 01:29:08.613345 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.random_seed : NoneType
I1002 01:29:08.613403 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.skip_lp_regularization : NoneType
I1002 01:29:08.613461 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.trainable_scaling : False
I1002 01:29:08.613519 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.trainable_scaling_init : 1.0
I1002 01:29:08.613584 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.vn.global_vn : False
I1002 01:29:08.613644 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.vn.per_step_vn : False
I1002 01:29:08.613702 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.vn.scale : NoneType
I1002 01:29:08.613760 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.vn.seed : NoneType
I1002 01:29:08.613819 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.random_seed : NoneType
I1002 01:29:08.613877 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.skip_lp_regularization : NoneType
I1002 01:29:08.613934 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.allow_implicit_capture : NoneType
I1002 01:29:08.613992 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.apply_pruning : False
I1002 01:29:08.614050 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.cls : type/lingvo.core.layers/SimpleEmbeddingLayer
I1002 01:29:08.614108 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.dtype : float32
I1002 01:29:08.614166 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.embedding_dim : 2048
I1002 01:29:08.614224 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.fprop_dtype : NoneType
I1002 01:29:08.614283 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.fprop_mode : NoneType
I1002 01:29:08.614341 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.inference_driver_name : NoneType
I1002 01:29:08.614399 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.is_eval : NoneType
I1002 01:29:08.614457 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.is_inference : NoneType
I1002 01:29:08.614516 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.name : ''
I1002 01:29:08.614573 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.params_init.method : 'gaussian'
I1002 01:29:08.614631 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.params_init.scale : 0.022097086912079608
I1002 01:29:08.614690 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.params_init.seed : NoneType
I1002 01:29:08.614749 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.qdomain.default : NoneType
I1002 01:29:08.614807 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.random_seed : NoneType
I1002 01:29:08.614865 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.skip_lp_regularization : NoneType
I1002 01:29:08.614923 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.use_3d_weight_tensor : False
I1002 01:29:08.614982 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.use_matmul : False
I1002 01:29:08.615040 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.vn.global_vn : False
I1002 01:29:08.615098 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.vn.per_step_vn : False
I1002 01:29:08.615157 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.vn.scale : NoneType
I1002 01:29:08.615215 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.vn.seed : NoneType
I1002 01:29:08.615273 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.vocab_size : 32000
I1002 01:29:08.615345 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.vn.global_vn : False
I1002 01:29:08.615405 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.vn.per_step_vn : False
I1002 01:29:08.615463 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.vn.scale : NoneType
I1002 01:29:08.615522 139650252134208 base_runner.py:59] task.lm.stack.emb_tpl.vn.seed : NoneType
I1002 01:29:08.615579 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.allow_implicit_capture : NoneType
I1002 01:29:08.615637 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.cls : type/lingvo.core.layers_with_gpipe/GPipeTransformerLayer
I1002 01:29:08.615700 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.dtype : float32
I1002 01:29:08.615760 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.final_enc_layer : False
I1002 01:29:08.615818 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.fprop_dtype : NoneType
I1002 01:29:08.615877 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.has_aux_atten : False
I1002 01:29:08.615935 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.inference_driver_name : NoneType
I1002 01:29:08.615993 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.is_decoder : False
I1002 01:29:08.616052 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.is_eval : NoneType
I1002 01:29:08.616110 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.is_inference : NoneType
I1002 01:29:08.616168 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.is_transparent : False
I1002 01:29:08.616226 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.allow_implicit_capture : NoneType
I1002 01:29:08.616284 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.cls : type/lingvo.core.layers/LayerNorm
I1002 01:29:08.616342 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.dtype : float32
I1002 01:29:08.616400 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.epsilon : 1e-06
I1002 01:29:08.616458 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.fprop_dtype : NoneType
I1002 01:29:08.616517 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.inference_driver_name : NoneType
I1002 01:29:08.616575 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.input_dim : 0
I1002 01:29:08.616632 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.is_eval : NoneType
I1002 01:29:08.616690 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.is_inference : NoneType
I1002 01:29:08.616748 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.name : ''
I1002 01:29:08.616807 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.params_init.method : 'xavier'
I1002 01:29:08.616864 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.params_init.scale : 1.000001
I1002 01:29:08.616923 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.params_init.seed : NoneType
I1002 01:29:08.616981 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.random_seed : NoneType
I1002 01:29:08.617039 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.skip_lp_regularization : NoneType
I1002 01:29:08.617096 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.vn.global_vn : False
I1002 01:29:08.617154 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.vn.per_step_vn : False
I1002 01:29:08.617212 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.vn.scale : NoneType
I1002 01:29:08.617270 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.vn.seed : NoneType
I1002 01:29:08.617328 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.mask_self_atten : True
I1002 01:29:08.617386 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.name : ''
I1002 01:29:08.617444 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.normalize_output : False
I1002 01:29:08.617502 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.output_dim : 0
I1002 01:29:08.617559 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.packed_input : False
I1002 01:29:08.617617 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.params_init.method : 'xavier'
I1002 01:29:08.617675 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.params_init.scale : 1.000001
I1002 01:29:08.617733 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.params_init.seed : NoneType
I1002 01:29:08.617791 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.random_seed : NoneType
I1002 01:29:08.617854 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.skip_lp_regularization : NoneType
I1002 01:29:08.617913 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.source_dim : 2048
I1002 01:29:08.617972 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.add_unnormalized_input : False
I1002 01:29:08.618030 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.allow_implicit_capture : NoneType
I1002 01:29:08.618089 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_dropout_prob : 0.0
I1002 01:29:08.618148 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_hidden_dim : 0
I1002 01:29:08.618206 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.allow_implicit_capture : NoneType
I1002 01:29:08.618263 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.atten_dropout_deterministic : False
I1002 01:29:08.618322 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.atten_dropout_prob : 0.0
I1002 01:29:08.618380 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.cls : type/lingvo.core.attention/MultiHeadedAttention
I1002 01:29:08.618438 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.context_dim : 0
I1002 01:29:08.618497 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.ctx_post_proj_dim : 0
I1002 01:29:08.618554 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.dtype : float32
I1002 01:29:08.618612 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.enable_ctx_post_proj : True
I1002 01:29:08.618669 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.enable_ctx_pre_proj : True
I1002 01:29:08.618727 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.enable_query_proj : True
I1002 01:29:08.618785 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.enable_source_proj : True
I1002 01:29:08.618844 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.fprop_dtype : NoneType
I1002 01:29:08.618902 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.hidden_dim : 0
I1002 01:29:08.618960 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inference_driver_name : NoneType
I1002 01:29:08.619018 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.allow_implicit_capture : NoneType
I1002 01:29:08.619076 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.atten_dropout_deterministic : False
I1002 01:29:08.619133 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.atten_dropout_prob : 0.0
I1002 01:29:08.619190 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.cls : type/lingvo.core.attention/DotProductAttention
I1002 01:29:08.619248 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.dtype : float32
I1002 01:29:08.619317 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.fprop_dtype : NoneType
I1002 01:29:08.619379 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.hidden_dim : 0
I1002 01:29:08.619437 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.inference_driver_name : NoneType
I1002 01:29:08.619495 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.is_eval : NoneType
I1002 01:29:08.619553 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.is_inference : NoneType
I1002 01:29:08.619616 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.name : ''
I1002 01:29:08.619675 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.packed_input : False
I1002 01:29:08.619734 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.params_init.method : 'xavier'
I1002 01:29:08.619792 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.params_init.scale : 1.000001
I1002 01:29:08.619850 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.params_init.seed : NoneType
I1002 01:29:08.619909 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.qdomain.default : NoneType
I1002 01:29:08.619967 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.qdomain.fullyconnected : NoneType
I1002 01:29:08.620025 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.qdomain.softmax : NoneType
I1002 01:29:08.620084 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.query_dim : 0
I1002 01:29:08.620141 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.random_seed : NoneType
I1002 01:29:08.620199 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.skip_lp_regularization : NoneType
I1002 01:29:08.620257 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.source_dim : 0
I1002 01:29:08.620314 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.global_vn : False
I1002 01:29:08.620372 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.per_step_vn : False
I1002 01:29:08.620431 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.scale : NoneType
I1002 01:29:08.620489 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.seed : NoneType
I1002 01:29:08.620547 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.is_eval : NoneType
I1002 01:29:08.620605 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.is_inference : NoneType
I1002 01:29:08.620663 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.name : ''
I1002 01:29:08.620721 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.num_attention_heads : 2
I1002 01:29:08.620779 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.packed_input : False
I1002 01:29:08.620837 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.params_init.method : 'xavier'
I1002 01:29:08.620895 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.params_init.scale : 1.0
I1002 01:29:08.620953 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.params_init.seed : NoneType
I1002 01:29:08.621011 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.qdomain.atten_context : NoneType
I1002 01:29:08.621069 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.qdomain.default : NoneType
I1002 01:29:08.621127 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.qdomain.fullyconnected : NoneType
I1002 01:29:08.621191 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.qdomain.softmax : NoneType
I1002 01:29:08.621249 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.query_dim : 0
I1002 01:29:08.621311 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.random_seed : NoneType
I1002 01:29:08.621370 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.skip_lp_regularization : NoneType
I1002 01:29:08.621428 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.source_dim : 0
I1002 01:29:08.621486 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.use_source_vec_as_attention_value : False
I1002 01:29:08.621544 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.vn.global_vn : False
I1002 01:29:08.621602 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.vn.per_step_vn : False
I1002 01:29:08.621660 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.vn.scale : NoneType
I1002 01:29:08.621719 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.vn.seed : NoneType
I1002 01:29:08.621777 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.cls : type/lingvo.core.layers_with_attention/TransformerAttentionLayer
I1002 01:29:08.621835 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.context_dim : 0
I1002 01:29:08.621893 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.dtype : float32
I1002 01:29:08.621951 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.fprop_dtype : NoneType
I1002 01:29:08.622009 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.inference_driver_name : NoneType
I1002 01:29:08.622068 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.is_eval : NoneType
I1002 01:29:08.622126 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.is_inference : NoneType
I1002 01:29:08.622184 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.is_masked : True
I1002 01:29:08.622243 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.allow_implicit_capture : NoneType
I1002 01:29:08.622302 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.cls : type/lingvo.core.layers/LayerNorm
I1002 01:29:08.622359 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.dtype : float32
I1002 01:29:08.622418 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.epsilon : 1e-06
I1002 01:29:08.622477 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.fprop_dtype : NoneType
I1002 01:29:08.622535 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.inference_driver_name : NoneType
I1002 01:29:08.622593 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.input_dim : 0
I1002 01:29:08.622652 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.is_eval : NoneType
I1002 01:29:08.622710 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.is_inference : NoneType
I1002 01:29:08.622769 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.name : ''
I1002 01:29:08.622827 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.params_init.method : 'xavier'
I1002 01:29:08.622886 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.params_init.scale : 1.000001
I1002 01:29:08.622945 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.params_init.seed : NoneType
I1002 01:29:08.623003 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.random_seed : NoneType
I1002 01:29:08.623062 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.skip_lp_regularization : NoneType
I1002 01:29:08.623121 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.vn.global_vn : False
I1002 01:29:08.623179 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.vn.per_step_vn : False
I1002 01:29:08.623241 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.vn.scale : NoneType
I1002 01:29:08.623316 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.vn.seed : NoneType
I1002 01:29:08.623378 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.mask_type : 'future'
I1002 01:29:08.623437 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.name : ''
I1002 01:29:08.623496 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.num_attention_heads : 16
I1002 01:29:08.623554 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.packed_input : False
I1002 01:29:08.623615 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.params_init.method : 'xavier'
I1002 01:29:08.623674 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.params_init.scale : 1.000001
I1002 01:29:08.623733 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.params_init.seed : NoneType
I1002 01:29:08.623791 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.random_seed : NoneType
I1002 01:29:08.623850 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_prob : 0.0
I1002 01:29:08.623909 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.allow_implicit_capture : NoneType
I1002 01:29:08.623968 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.cls : type/lingvo.core.layers/DropoutLayer
I1002 01:29:08.624026 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.dropout_at_eval : False
I1002 01:29:08.624085 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.dtype : float32
I1002 01:29:08.624144 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.fprop_dtype : NoneType
I1002 01:29:08.624202 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.inference_driver_name : NoneType
I1002 01:29:08.624260 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.is_eval : NoneType
I1002 01:29:08.624319 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.is_inference : NoneType
I1002 01:29:08.624377 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.keep_prob : 1.0
I1002 01:29:08.624437 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.name : ''
I1002 01:29:08.624495 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.noise_shape : NoneType
I1002 01:29:08.624554 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.noise_shape_broadcast_dims : NoneType
I1002 01:29:08.624613 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.params_init.method : 'xavier'
I1002 01:29:08.624671 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.params_init.scale : 1.000001
I1002 01:29:08.624730 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.params_init.seed : NoneType
I1002 01:29:08.624789 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.random_seed : NoneType
I1002 01:29:08.624847 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.skip_lp_regularization : NoneType
I1002 01:29:08.624906 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.global_vn : False
I1002 01:29:08.624965 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.per_step_vn : False
I1002 01:29:08.625028 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.scale : NoneType
I1002 01:29:08.625087 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.seed : NoneType
I1002 01:29:08.625146 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.skip_lp_regularization : NoneType
I1002 01:29:08.625205 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.source_dim : 0
I1002 01:29:08.625263 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.vn.global_vn : False
I1002 01:29:08.625322 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.vn.per_step_vn : False
I1002 01:29:08.625381 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.vn.scale : NoneType
I1002 01:29:08.625440 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.vn.seed : NoneType
I1002 01:29:08.625498 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_aux_atten_tpl : NoneType
I1002 01:29:08.625557 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.activation : 'RELU'
I1002 01:29:08.625615 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.allow_implicit_capture : NoneType
I1002 01:29:08.625674 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.cls : type/lingvo.core.layers_with_attention/TransformerFeedForwardLayer
I1002 01:29:08.625733 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.dtype : float32
I1002 01:29:08.625792 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.activation : ['RELU', 'NONE']
I1002 01:29:08.625851 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.allow_implicit_capture : NoneType
I1002 01:29:08.625910 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.batch_norm : False
I1002 01:29:08.625969 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.bn_fold_weights : NoneType
I1002 01:29:08.626028 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.cls : type/lingvo.core.layers/FeedForwardNet
I1002 01:29:08.626086 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.allow_implicit_capture : NoneType
I1002 01:29:08.626145 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.cls : type/lingvo.core.layers/DropoutLayer
I1002 01:29:08.626204 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.dropout_at_eval : False
I1002 01:29:08.626262 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.dtype : float32
I1002 01:29:08.626321 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.fprop_dtype : NoneType
I1002 01:29:08.626379 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.inference_driver_name : NoneType
I1002 01:29:08.626437 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.is_eval : NoneType
I1002 01:29:08.626494 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.is_inference : NoneType
I1002 01:29:08.626554 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.keep_prob : 1.0
I1002 01:29:08.626612 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.name : ''
I1002 01:29:08.626671 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.noise_shape : NoneType
I1002 01:29:08.626728 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.noise_shape_broadcast_dims : NoneType
I1002 01:29:08.626791 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.params_init.method : 'xavier'
I1002 01:29:08.626850 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.params_init.scale : 1.000001
I1002 01:29:08.626907 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.params_init.seed : NoneType
I1002 01:29:08.626965 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.random_seed : NoneType
I1002 01:29:08.627023 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.skip_lp_regularization : NoneType
I1002 01:29:08.627080 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.global_vn : False
I1002 01:29:08.627139 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.per_step_vn : False
I1002 01:29:08.627197 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.scale : NoneType
I1002 01:29:08.627255 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.seed : NoneType
I1002 01:29:08.627324 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dtype : float32
I1002 01:29:08.627384 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.fprop_dtype : NoneType
I1002 01:29:08.627442 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.inference_driver_name : NoneType
I1002 01:29:08.627501 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.input_dim : 0
I1002 01:29:08.627559 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.is_eval : NoneType
I1002 01:29:08.627617 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.is_inference : NoneType
I1002 01:29:08.627676 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.name : ''
I1002 01:29:08.627734 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.params_init.method : 'xavier'
I1002 01:29:08.627791 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.params_init.scale : 1.000001
I1002 01:29:08.627850 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.params_init.seed : NoneType
I1002 01:29:08.627907 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.activation : 'RELU'
I1002 01:29:08.627965 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.affine_last : False
I1002 01:29:08.628023 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.allow_implicit_capture : NoneType
I1002 01:29:08.628082 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.batch_norm : True
I1002 01:29:08.628139 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.bias_init : 0.0
I1002 01:29:08.628197 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.bn_fold_weights : NoneType
I1002 01:29:08.628255 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.cls : type/lingvo.core.layers/ProjectionLayer
I1002 01:29:08.628313 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.dtype : float32
I1002 01:29:08.628371 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.fprop_dtype : NoneType
I1002 01:29:08.628430 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.has_bias : False
I1002 01:29:08.628488 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.inference_driver_name : NoneType
I1002 01:29:08.628551 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.input_dim : 0
I1002 01:29:08.628609 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.is_eval : NoneType
I1002 01:29:08.628668 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.is_inference : NoneType
I1002 01:29:08.628726 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.name : ''
I1002 01:29:08.628784 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.output_dim : 0
I1002 01:29:08.628842 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.params_init.method : 'xavier'
I1002 01:29:08.628899 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.params_init.scale : 1.000001
I1002 01:29:08.628957 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.params_init.seed : NoneType
I1002 01:29:08.629014 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.qdomain.default : NoneType
I1002 01:29:08.629073 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.random_seed : NoneType
I1002 01:29:08.629131 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.skip_lp_regularization : NoneType
I1002 01:29:08.629189 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.global_vn : False
I1002 01:29:08.629247 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.per_step_vn : False
I1002 01:29:08.629305 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.scale : NoneType
I1002 01:29:08.629363 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.seed : NoneType
I1002 01:29:08.629420 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.weight_norm : False
I1002 01:29:08.629478 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.qdomain.default : NoneType
I1002 01:29:08.629537 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.random_seed : NoneType
I1002 01:29:08.629595 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.skip_connections : NoneType
I1002 01:29:08.629652 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.skip_lp_regularization : NoneType
I1002 01:29:08.629710 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.global_vn : False
I1002 01:29:08.629768 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.per_step_vn : False
I1002 01:29:08.629826 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.scale : NoneType
I1002 01:29:08.629884 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.seed : NoneType
I1002 01:29:08.629941 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.weight_norm : False
I1002 01:29:08.629999 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fprop_dtype : NoneType
I1002 01:29:08.630058 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.hidden_dim : 8192
I1002 01:29:08.630116 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.inference_driver_name : NoneType
I1002 01:29:08.630174 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.input_dim : 0
I1002 01:29:08.630236 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.is_eval : NoneType
I1002 01:29:08.630296 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.is_inference : NoneType
I1002 01:29:08.630354 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.allow_implicit_capture : NoneType
I1002 01:29:08.630412 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.cls : type/lingvo.core.layers/LayerNorm
I1002 01:29:08.630469 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.dtype : float32
I1002 01:29:08.630527 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.epsilon : 1e-06
I1002 01:29:08.630584 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.fprop_dtype : NoneType
I1002 01:29:08.630641 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.inference_driver_name : NoneType
I1002 01:29:08.630699 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.input_dim : 0
I1002 01:29:08.630762 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.is_eval : NoneType
I1002 01:29:08.630820 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.is_inference : NoneType
I1002 01:29:08.630879 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.name : ''
I1002 01:29:08.630936 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.params_init.method : 'xavier'
I1002 01:29:08.630994 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.params_init.scale : 1.000001
I1002 01:29:08.631051 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.params_init.seed : NoneType
I1002 01:29:08.631109 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.random_seed : NoneType
I1002 01:29:08.631167 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.skip_lp_regularization : NoneType
I1002 01:29:08.631225 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.vn.global_vn : False
I1002 01:29:08.631284 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.vn.per_step_vn : False
I1002 01:29:08.631358 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.vn.scale : NoneType
I1002 01:29:08.631417 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.vn.seed : NoneType
I1002 01:29:08.631476 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.name : ''
I1002 01:29:08.631535 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.output_dim : 0
I1002 01:29:08.631593 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.params_init.method : 'xavier'
I1002 01:29:08.631652 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.params_init.scale : 1.000001
I1002 01:29:08.631711 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.params_init.seed : NoneType
I1002 01:29:08.631769 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.random_seed : NoneType
I1002 01:29:08.631827 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.relu_dropout_prob : 0.0
I1002 01:29:08.631886 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.activation : 'RELU'
I1002 01:29:08.631946 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.affine_last : False
I1002 01:29:08.632004 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.allow_implicit_capture : NoneType
I1002 01:29:08.632062 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.batch_norm : True
I1002 01:29:08.632121 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.bias_init : 0.0
I1002 01:29:08.632184 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.bn_fold_weights : NoneType
I1002 01:29:08.632243 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.cls : type/lingvo.core.layers/ProjectionLayer
I1002 01:29:08.632301 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.dtype : float32
I1002 01:29:08.632360 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.fprop_dtype : NoneType
I1002 01:29:08.632418 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.has_bias : False
I1002 01:29:08.632477 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.inference_driver_name : NoneType
I1002 01:29:08.632534 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.input_dim : 0
I1002 01:29:08.632592 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.is_eval : NoneType
I1002 01:29:08.632650 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.is_inference : NoneType
I1002 01:29:08.632709 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.name : ''
I1002 01:29:08.632767 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.output_dim : 0
I1002 01:29:08.632825 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.params_init.method : 'xavier'
I1002 01:29:08.632883 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.params_init.scale : 1.000001
I1002 01:29:08.632941 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.params_init.seed : NoneType
I1002 01:29:08.632999 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.qdomain.default : NoneType
I1002 01:29:08.633057 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.random_seed : NoneType
I1002 01:29:08.633116 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.skip_lp_regularization : NoneType
I1002 01:29:08.633174 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.global_vn : False
I1002 01:29:08.633233 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.per_step_vn : False
I1002 01:29:08.633290 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.scale : NoneType
I1002 01:29:08.633349 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.seed : NoneType
I1002 01:29:08.633407 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.weight_norm : False
I1002 01:29:08.633465 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_prob : 0.0
I1002 01:29:08.633523 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.allow_implicit_capture : NoneType
I1002 01:29:08.633582 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.cls : type/lingvo.core.layers/DropoutLayer
I1002 01:29:08.633640 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.dropout_at_eval : False
I1002 01:29:08.633702 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.dtype : float32
I1002 01:29:08.633760 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.fprop_dtype : NoneType
I1002 01:29:08.633819 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.inference_driver_name : NoneType
I1002 01:29:08.633877 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.is_eval : NoneType
I1002 01:29:08.633941 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.is_inference : NoneType
I1002 01:29:08.634000 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.keep_prob : 1.0
I1002 01:29:08.634059 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.name : ''
I1002 01:29:08.634117 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.noise_shape : NoneType
I1002 01:29:08.634175 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.noise_shape_broadcast_dims : NoneType
I1002 01:29:08.634232 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.params_init.method : 'xavier'
I1002 01:29:08.634291 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.params_init.scale : 1.000001
I1002 01:29:08.634349 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.params_init.seed : NoneType
I1002 01:29:08.634406 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.random_seed : NoneType
I1002 01:29:08.634463 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.skip_lp_regularization : NoneType
I1002 01:29:08.634521 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.global_vn : False
I1002 01:29:08.634578 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.per_step_vn : False
I1002 01:29:08.634637 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.scale : NoneType
I1002 01:29:08.634695 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.seed : NoneType
I1002 01:29:08.634754 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.skip_lp_regularization : NoneType
I1002 01:29:08.634812 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.vn.global_vn : False
I1002 01:29:08.634871 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.vn.per_step_vn : False
I1002 01:29:08.634929 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.vn.scale : NoneType
I1002 01:29:08.634988 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.vn.seed : NoneType
I1002 01:29:08.635046 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.transparent_merger_tpl : NoneType
I1002 01:29:08.635105 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.vn.global_vn : False
I1002 01:29:08.635163 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.vn.per_step_vn : False
I1002 01:29:08.635221 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.vn.scale : NoneType
I1002 01:29:08.635280 139650252134208 base_runner.py:59] task.lm.stack.encoder_tpl.vn.seed : NoneType
I1002 01:29:08.635355 139650252134208 base_runner.py:59] task.lm.stack.fprop_dtype : NoneType
I1002 01:29:08.635414 139650252134208 base_runner.py:59] task.lm.stack.inference_driver_name : NoneType
I1002 01:29:08.635472 139650252134208 base_runner.py:59] task.lm.stack.is_eval : NoneType
I1002 01:29:08.635530 139650252134208 base_runner.py:59] task.lm.stack.is_inference : NoneType
I1002 01:29:08.635588 139650252134208 base_runner.py:59] task.lm.stack.is_transparent : False
I1002 01:29:08.635646 139650252134208 base_runner.py:59] task.lm.stack.label_smoothing : NoneType
I1002 01:29:08.635704 139650252134208 base_runner.py:59] task.lm.stack.model_dim : 2048
I1002 01:29:08.635763 139650252134208 base_runner.py:59] task.lm.stack.name : ''
I1002 01:29:08.635821 139650252134208 base_runner.py:59] task.lm.stack.normalize_encoder : False
I1002 01:29:08.635885 139650252134208 base_runner.py:59] task.lm.stack.num_decoder_layers : 0
I1002 01:29:08.635945 139650252134208 base_runner.py:59] task.lm.stack.num_encoder_layers : 32
I1002 01:29:08.636004 139650252134208 base_runner.py:59] task.lm.stack.num_micro_batches : 32
I1002 01:29:08.636061 139650252134208 base_runner.py:59] task.lm.stack.packed_input : False
I1002 01:29:08.636119 139650252134208 base_runner.py:59] task.lm.stack.params_init.method : 'xavier'
I1002 01:29:08.636178 139650252134208 base_runner.py:59] task.lm.stack.params_init.scale : 1.000001
I1002 01:29:08.636236 139650252134208 base_runner.py:59] task.lm.stack.params_init.seed : NoneType
I1002 01:29:08.636294 139650252134208 base_runner.py:59] task.lm.stack.random_seed : NoneType
I1002 01:29:08.636352 139650252134208 base_runner.py:59] task.lm.stack.skip_lp_regularization : NoneType
I1002 01:29:08.636410 139650252134208 base_runner.py:59] task.lm.stack.softmax_tpl.allow_implicit_capture : NoneType
I1002 01:29:08.636469 139650252134208 base_runner.py:59] task.lm.stack.softmax_tpl.apply_pruning : False
I1002 01:29:08.636527 139650252134208 base_runner.py:59] task.lm.stack.softmax_tpl.chunk_size : 4194
I1002 01:29:08.636586 139650252134208 base_runner.py:59] task.lm.stack.softmax_tpl.cls : type/lingvo.core.layers_with_gpipe/GPipeTransformerSoftmaxLayer
I1002 01:29:08.636644 139650252134208 base_runner.py:59] task.lm.stack.softmax_tpl.dtype : float32
I1002 01:29:08.636702 139650252134208 base_runner.py:59] task.lm.stack.softmax_tpl.fprop_dtype : NoneType
I1002 01:29:08.636760 139650252134208 base_runner.py:59] task.lm.stack.softmax_tpl.inference_driver_name : NoneType
I1002 01:29:08.636818 139650252134208 base_runner.py:59] task.lm.stack.softmax_tpl.input_dim : 2048
I1002 01:29:08.636876 139650252134208 base_runner.py:59] task.lm.stack.softmax_tpl.inputs_from_decoder : False
I1002 01:29:08.636934 139650252134208 base_runner.py:59] task.lm.stack.softmax_tpl.is_eval : NoneType
I1002 01:29:08.636992 139650252134208 base_runner.py:59] task.lm.stack.softmax_tpl.is_inference : NoneType
I1002 01:29:08.637051 139650252134208 base_runner.py:59] task.lm.stack.softmax_tpl.logits_abs_max : NoneType
I1002 01:29:08.637109 139650252134208 base_runner.py:59] task.lm.stack.softmax_tpl.name : ''
I1002 01:29:08.637166 139650252134208 base_runner.py:59] task.lm.stack.softmax_tpl.num_classes : 32000
I1002 01:29:08.637225 139650252134208 base_runner.py:59] task.lm.stack.softmax_tpl.num_sampled : 0
I1002 01:29:08.637283 139650252134208 base_runner.py:59] task.lm.stack.softmax_tpl.num_shards : 16
I1002 01:29:08.637341 139650252134208 base_runner.py:59] task.lm.stack.softmax_tpl.params_init.method : 'xavier'
I1002 01:29:08.637399 139650252134208 base_runner.py:59] task.lm.stack.softmax_tpl.params_init.scale : 1.000001
I1002 01:29:08.637457 139650252134208 base_runner.py:59] task.lm.stack.softmax_tpl.params_init.seed : NoneType
I1002 01:29:08.637515 139650252134208 base_runner.py:59] task.lm.stack.softmax_tpl.qdomain.default : NoneType
I1002 01:29:08.637573 139650252134208 base_runner.py:59] task.lm.stack.softmax_tpl.random_seed : NoneType
I1002 01:29:08.637631 139650252134208 base_runner.py:59] task.lm.stack.softmax_tpl.skip_lp_regularization : NoneType
I1002 01:29:08.637689 139650252134208 base_runner.py:59] task.lm.stack.softmax_tpl.vn.global_vn : False
I1002 01:29:08.637747 139650252134208 base_runner.py:59] task.lm.stack.softmax_tpl.vn.per_step_vn : False
I1002 01:29:08.637805 139650252134208 base_runner.py:59] task.lm.stack.softmax_tpl.vn.scale : NoneType
I1002 01:29:08.637864 139650252134208 base_runner.py:59] task.lm.stack.softmax_tpl.vn.seed : NoneType
I1002 01:29:08.637921 139650252134208 base_runner.py:59] task.lm.stack.splits : [8, 16, 24, 32]
I1002 01:29:08.637979 139650252134208 base_runner.py:59] task.lm.stack.state_dtype : float32
I1002 01:29:08.638037 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_dropout_prob : 0.1
I1002 01:29:08.638095 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.allow_implicit_capture : NoneType
I1002 01:29:08.638158 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.cls : type/lingvo.core.layers_with_gpipe/DeterministicWeightsLayer
I1002 01:29:08.638216 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.allow_implicit_capture : NoneType
I1002 01:29:08.638275 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.cls : type/lingvo.core.layers/DeterministicDropoutLayer
I1002 01:29:08.638332 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.dropout_at_eval : False
I1002 01:29:08.638390 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.dtype : float32
I1002 01:29:08.638448 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.fprop_dtype : NoneType
I1002 01:29:08.638507 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.inference_driver_name : NoneType
I1002 01:29:08.638565 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.is_eval : NoneType
I1002 01:29:08.638623 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.is_inference : NoneType
I1002 01:29:08.638683 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.keep_prob : 1.0
I1002 01:29:08.638741 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.name : ''
I1002 01:29:08.638799 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.noise_shape : NoneType
I1002 01:29:08.638857 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.noise_shape_broadcast_dims : NoneType
I1002 01:29:08.638915 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.params_init.method : 'xavier'
I1002 01:29:08.638973 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.params_init.scale : 1.000001
I1002 01:29:08.639031 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.params_init.seed : NoneType
I1002 01:29:08.639089 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.random_seed : NoneType
I1002 01:29:08.639147 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.skip_lp_regularization : NoneType
I1002 01:29:08.639205 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.vn.global_vn : False
I1002 01:29:08.639263 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.vn.per_step_vn : False
I1002 01:29:08.639337 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.vn.scale : NoneType
I1002 01:29:08.639397 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.vn.seed : NoneType
I1002 01:29:08.639455 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dtype : float32
I1002 01:29:08.639513 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.fprop_dtype : NoneType
I1002 01:29:08.639572 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.global_weight_scale : 1.0
I1002 01:29:08.639630 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.inference_driver_name : NoneType
I1002 01:29:08.639688 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.is_eval : NoneType
I1002 01:29:08.639745 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.is_inference : NoneType
I1002 01:29:08.639804 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.minimal_prob : 0.0
I1002 01:29:08.639863 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.name : ''
I1002 01:29:08.639921 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.num_sources : 0
I1002 01:29:08.639980 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.params_init.method : 'xavier'
I1002 01:29:08.640043 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.params_init.scale : 1.000001
I1002 01:29:08.640101 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.params_init.seed : NoneType
I1002 01:29:08.640160 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.random_seed : NoneType
I1002 01:29:08.640218 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.skip_lp_regularization : NoneType
I1002 01:29:08.640276 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.vn.global_vn : False
I1002 01:29:08.640334 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.vn.per_step_vn : False
I1002 01:29:08.640393 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.vn.scale : NoneType
I1002 01:29:08.640451 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.vn.seed : NoneType
I1002 01:29:08.640509 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.weighted_merger_dropout_prob : 0.0
I1002 01:29:08.640568 139650252134208 base_runner.py:59] task.lm.stack.transparent_merger_tpl.weighted_merger_softmax : True
I1002 01:29:08.640626 139650252134208 base_runner.py:59] task.lm.stack.use_pipelined_embeddings : True
I1002 01:29:08.640684 139650252134208 base_runner.py:59] task.lm.stack.vn.global_vn : False
I1002 01:29:08.640743 139650252134208 base_runner.py:59] task.lm.stack.vn.per_step_vn : False
I1002 01:29:08.640800 139650252134208 base_runner.py:59] task.lm.stack.vn.scale : NoneType
I1002 01:29:08.640859 139650252134208 base_runner.py:59] task.lm.stack.vn.seed : NoneType
I1002 01:29:08.640917 139650252134208 base_runner.py:59] task.lm.vn.global_vn : False
I1002 01:29:08.640976 139650252134208 base_runner.py:59] task.lm.vn.per_step_vn : False
I1002 01:29:08.641033 139650252134208 base_runner.py:59] task.lm.vn.scale : NoneType
I1002 01:29:08.641091 139650252134208 base_runner.py:59] task.lm.vn.seed : NoneType
I1002 01:29:08.641149 139650252134208 base_runner.py:59] task.lm.vocab_size : 32000
I1002 01:29:08.641207 139650252134208 base_runner.py:59] task.name : '1bwds_wpm_level_lm'
I1002 01:29:08.641265 139650252134208 base_runner.py:59] task.online_encoder : NoneType
I1002 01:29:08.641323 139650252134208 base_runner.py:59] task.params_init.method : 'xavier'
I1002 01:29:08.641381 139650252134208 base_runner.py:59] task.params_init.scale : 1.000001
I1002 01:29:08.641438 139650252134208 base_runner.py:59] task.params_init.seed : NoneType
I1002 01:29:08.641496 139650252134208 base_runner.py:59] task.random_seed : NoneType
I1002 01:29:08.641555 139650252134208 base_runner.py:59] task.skip_lp_regularization : NoneType
I1002 01:29:08.641613 139650252134208 base_runner.py:59] task.train.bprop_variable_exclusion : NoneType
I1002 01:29:08.641671 139650252134208 base_runner.py:59] task.train.bprop_variable_filter : NoneType
I1002 01:29:08.641729 139650252134208 base_runner.py:59] task.train.clip_gradient_norm_to_value : 0.0
I1002 01:29:08.641787 139650252134208 base_runner.py:59] task.train.clip_gradient_single_norm_to_value : 0.0
I1002 01:29:08.641844 139650252134208 base_runner.py:59] task.train.colocate_gradients_with_ops : True
I1002 01:29:08.641902 139650252134208 base_runner.py:59] task.train.early_stop.metric_history.jobname : 'eval_dev'
I1002 01:29:08.641959 139650252134208 base_runner.py:59] task.train.early_stop.metric_history.local_filesystem : False
I1002 01:29:08.642017 139650252134208 base_runner.py:59] task.train.early_stop.metric_history.logdir : ''
I1002 01:29:08.642074 139650252134208 base_runner.py:59] task.train.early_stop.metric_history.metric : 'log_pplx'
I1002 01:29:08.642144 139650252134208 base_runner.py:59] task.train.early_stop.metric_history.minimize : True
I1002 01:29:08.642202 139650252134208 base_runner.py:59] task.train.early_stop.metric_history.name : 'MetricHistory'
I1002 01:29:08.642260 139650252134208 base_runner.py:59] task.train.early_stop.metric_history.tfevent_file : False
I1002 01:29:08.642317 139650252134208 base_runner.py:59] task.train.early_stop.min_steps : 0
I1002 01:29:08.642386 139650252134208 base_runner.py:59] task.train.early_stop.name : 'EarlyStop'
I1002 01:29:08.642444 139650252134208 base_runner.py:59] task.train.early_stop.tolerance : 0.0
I1002 01:29:08.642503 139650252134208 base_runner.py:59] task.train.early_stop.verbose : True
I1002 01:29:08.642560 139650252134208 base_runner.py:59] task.train.early_stop.window : 0
I1002 01:29:08.642617 139650252134208 base_runner.py:59] task.train.ema_decay : 0.0
I1002 01:29:08.642674 139650252134208 base_runner.py:59] task.train.enqueue_max_steps : -1
I1002 01:29:08.642732 139650252134208 base_runner.py:59] task.train.gate_gradients : False
I1002 01:29:08.642790 139650252134208 base_runner.py:59] task.train.grad_aggregation_method : 1
I1002 01:29:08.642847 139650252134208 base_runner.py:59] task.train.grad_norm_to_clip_to_zero : 0.0
I1002 01:29:08.642905 139650252134208 base_runner.py:59] task.train.grad_norm_tracker : NoneType
I1002 01:29:08.642963 139650252134208 base_runner.py:59] task.train.init_from_checkpoint_rules : {}
I1002 01:29:08.643020 139650252134208 base_runner.py:59] task.train.l1_regularizer_weight : NoneType
I1002 01:29:08.643077 139650252134208 base_runner.py:59] task.train.l2_regularizer_weight : 1e-06
I1002 01:29:08.643135 139650252134208 base_runner.py:59] task.train.learner : NoneType
I1002 01:29:08.643193 139650252134208 base_runner.py:59] task.train.learning_rate : 0.5
I1002 01:29:08.643250 139650252134208 base_runner.py:59] task.train.lr_schedule.allow_implicit_capture : NoneType
I1002 01:29:08.643320 139650252134208 base_runner.py:59] task.train.lr_schedule.cls : type/lingvo.core.schedule/TransformerLearningRateSchedule
I1002 01:29:08.643380 139650252134208 base_runner.py:59] task.train.lr_schedule.decay_end : NoneType
I1002 01:29:08.643438 139650252134208 base_runner.py:59] task.train.lr_schedule.dtype : float32
I1002 01:29:08.643496 139650252134208 base_runner.py:59] task.train.lr_schedule.fprop_dtype : NoneType
I1002 01:29:08.643553 139650252134208 base_runner.py:59] task.train.lr_schedule.inference_driver_name : NoneType
I1002 01:29:08.643611 139650252134208 base_runner.py:59] task.train.lr_schedule.is_eval : NoneType
I1002 01:29:08.643669 139650252134208 base_runner.py:59] task.train.lr_schedule.is_inference : NoneType
I1002 01:29:08.643729 139650252134208 base_runner.py:59] task.train.lr_schedule.model_dim : 2048
I1002 01:29:08.643786 139650252134208 base_runner.py:59] task.train.lr_schedule.name : 'LRSched'
I1002 01:29:08.643844 139650252134208 base_runner.py:59] task.train.lr_schedule.params_init.method : 'xavier'
I1002 01:29:08.643902 139650252134208 base_runner.py:59] task.train.lr_schedule.params_init.scale : 1.000001
I1002 01:29:08.643959 139650252134208 base_runner.py:59] task.train.lr_schedule.params_init.seed : NoneType
I1002 01:29:08.644017 139650252134208 base_runner.py:59] task.train.lr_schedule.random_seed : NoneType
I1002 01:29:08.644075 139650252134208 base_runner.py:59] task.train.lr_schedule.skip_lp_regularization : NoneType
I1002 01:29:08.644133 139650252134208 base_runner.py:59] task.train.lr_schedule.vn.global_vn : False
I1002 01:29:08.644190 139650252134208 base_runner.py:59] task.train.lr_schedule.vn.per_step_vn : False
I1002 01:29:08.644251 139650252134208 base_runner.py:59] task.train.lr_schedule.vn.scale : NoneType
I1002 01:29:08.644309 139650252134208 base_runner.py:59] task.train.lr_schedule.vn.seed : NoneType
I1002 01:29:08.644367 139650252134208 base_runner.py:59] task.train.lr_schedule.warmup_steps : 40000
I1002 01:29:08.644424 139650252134208 base_runner.py:59] task.train.lr_schedule.worker_replicas : 1
I1002 01:29:08.644482 139650252134208 base_runner.py:59] task.train.max_lstm_gradient_norm : 0.0
I1002 01:29:08.644539 139650252134208 base_runner.py:59] task.train.max_steps : 4000000
I1002 01:29:08.644597 139650252134208 base_runner.py:59] task.train.optimizer.allow_implicit_capture : NoneType
I1002 01:29:08.644655 139650252134208 base_runner.py:59] task.train.optimizer.beta1 : 0.9
I1002 01:29:08.644713 139650252134208 base_runner.py:59] task.train.optimizer.beta2 : 0.997
I1002 01:29:08.644776 139650252134208 base_runner.py:59] task.train.optimizer.cls : type/lingvo.core.optimizer/Adam
I1002 01:29:08.644836 139650252134208 base_runner.py:59] task.train.optimizer.dtype : float32
I1002 01:29:08.644894 139650252134208 base_runner.py:59] task.train.optimizer.epsilon : 1e-09
I1002 01:29:08.644952 139650252134208 base_runner.py:59] task.train.optimizer.fprop_dtype : NoneType
I1002 01:29:08.645010 139650252134208 base_runner.py:59] task.train.optimizer.inference_driver_name : NoneType
I1002 01:29:08.645068 139650252134208 base_runner.py:59] task.train.optimizer.is_eval : NoneType
I1002 01:29:08.645127 139650252134208 base_runner.py:59] task.train.optimizer.is_inference : NoneType
I1002 01:29:08.645185 139650252134208 base_runner.py:59] task.train.optimizer.name : 'Adam'
I1002 01:29:08.645243 139650252134208 base_runner.py:59] task.train.optimizer.params_init.method : 'xavier'
I1002 01:29:08.645301 139650252134208 base_runner.py:59] task.train.optimizer.params_init.scale : 1.000001
I1002 01:29:08.645358 139650252134208 base_runner.py:59] task.train.optimizer.params_init.seed : NoneType
I1002 01:29:08.645416 139650252134208 base_runner.py:59] task.train.optimizer.random_seed : NoneType
I1002 01:29:08.645475 139650252134208 base_runner.py:59] task.train.optimizer.skip_lp_regularization : NoneType
I1002 01:29:08.645533 139650252134208 base_runner.py:59] task.train.optimizer.vn.global_vn : False
I1002 01:29:08.645591 139650252134208 base_runner.py:59] task.train.optimizer.vn.per_step_vn : False
I1002 01:29:08.645649 139650252134208 base_runner.py:59] task.train.optimizer.vn.scale : NoneType
I1002 01:29:08.645706 139650252134208 base_runner.py:59] task.train.optimizer.vn.seed : NoneType
I1002 01:29:08.645764 139650252134208 base_runner.py:59] task.train.pruning_hparams_dict : NoneType
I1002 01:29:08.645822 139650252134208 base_runner.py:59] task.train.save_interval_seconds : 600
I1002 01:29:08.645879 139650252134208 base_runner.py:59] task.train.save_keep_checkpoint_every_n_hours : 0.5
I1002 01:29:08.645937 139650252134208 base_runner.py:59] task.train.save_max_to_keep : 100
I1002 01:29:08.645995 139650252134208 base_runner.py:59] task.train.start_up_delay_steps : 200
I1002 01:29:08.646053 139650252134208 base_runner.py:59] task.train.sum_loss_across_tokens_in_batch : False
I1002 01:29:08.646111 139650252134208 base_runner.py:59] task.train.summary_interval_steps : 100
I1002 01:29:08.646168 139650252134208 base_runner.py:59] task.train.tpu_steps_per_loop : 100
I1002 01:29:08.646226 139650252134208 base_runner.py:59] task.train.vn_start_step : 20000
I1002 01:29:08.646283 139650252134208 base_runner.py:59] task.train.vn_std : 0.0
I1002 01:29:08.646341 139650252134208 base_runner.py:59] task.vn.global_vn : False
I1002 01:29:08.646399 139650252134208 base_runner.py:59] task.vn.per_step_vn : False
I1002 01:29:08.646457 139650252134208 base_runner.py:59] task.vn.scale : NoneType
I1002 01:29:08.646515 139650252134208 base_runner.py:59] task.vn.seed : NoneType
I1002 01:29:08.646573 139650252134208 base_runner.py:59] train.early_stop.metric_history.jobname : 'eval_dev'
I1002 01:29:08.646631 139650252134208 base_runner.py:59] train.early_stop.metric_history.local_filesystem : False
I1002 01:29:08.646689 139650252134208 base_runner.py:59] train.early_stop.metric_history.logdir : ''
I1002 01:29:08.646747 139650252134208 base_runner.py:59] train.early_stop.metric_history.metric : 'log_pplx'
I1002 01:29:08.646805 139650252134208 base_runner.py:59] train.early_stop.metric_history.minimize : True
I1002 01:29:08.646863 139650252134208 base_runner.py:59] train.early_stop.metric_history.name : 'MetricHistory'
I1002 01:29:08.646920 139650252134208 base_runner.py:59] train.early_stop.metric_history.tfevent_file : False
I1002 01:29:08.646979 139650252134208 base_runner.py:59] train.early_stop.min_steps : 0
I1002 01:29:08.647037 139650252134208 base_runner.py:59] train.early_stop.name : 'EarlyStop'
I1002 01:29:08.647095 139650252134208 base_runner.py:59] train.early_stop.tolerance : 0.0
I1002 01:29:08.647157 139650252134208 base_runner.py:59] train.early_stop.verbose : True
I1002 01:29:08.647216 139650252134208 base_runner.py:59] train.early_stop.window : 0
I1002 01:29:08.647274 139650252134208 base_runner.py:59] train.ema_decay : 0.0
I1002 01:29:08.647351 139650252134208 base_runner.py:59] train.enqueue_max_steps : -1
I1002 01:29:08.647409 139650252134208 base_runner.py:59] train.init_from_checkpoint_rules : {}
I1002 01:29:08.647468 139650252134208 base_runner.py:59] train.max_steps : 4000000
I1002 01:29:08.647526 139650252134208 base_runner.py:59] train.save_interval_seconds : 600
I1002 01:29:08.647584 139650252134208 base_runner.py:59] train.save_keep_checkpoint_every_n_hours : 0.5
I1002 01:29:08.647643 139650252134208 base_runner.py:59] train.save_max_to_keep : 100
I1002 01:29:08.647700 139650252134208 base_runner.py:59] train.start_up_delay_steps : 200
I1002 01:29:08.647758 139650252134208 base_runner.py:59] train.summary_interval_steps : 100
I1002 01:29:08.647816 139650252134208 base_runner.py:59] train.tpu_steps_per_loop : 100
I1002 01:29:08.647874 139650252134208 base_runner.py:59] vn.global_vn : False
I1002 01:29:08.647932 139650252134208 base_runner.py:59] vn.per_step_vn : False
I1002 01:29:08.647991 139650252134208 base_runner.py:59] vn.scale : NoneType
I1002 01:29:08.648049 139650252134208 base_runner.py:59] vn.seed : NoneType
I1002 01:29:08.648108 139650252134208 base_runner.py:59] 
I1002 01:29:08.648195 139650252134208 base_runner.py:60] ============================================================
I1002 01:29:08.650285 139650252134208 base_runner.py:106] Starting ...
I1002 01:29:08.650525 139650252134208 cluster.py:497] _LeastLoadedPlacer : ['/job:local/replica:0/task:0/device:CPU:0']
I1002 01:29:08.659737 139650252134208 cluster.py:515] Place variable global_step on /job:local/replica:0/task:0/device:CPU:0 8
I1002 01:29:08.673287 139650252134208 base_model.py:1093] Training parameters for <class 'lingvo.core.base_model.SingleTaskModel'>: {
  early_stop: {
    metric_history: {
"eval_dev"
      local_filesystem: False
"/tmp/mnist/log"
"log_pplx"
      minimize: True
"MetricHistory"
      tfevent_file: False
    }
    min_steps: 0
"EarlyStop"
    tolerance: 0.0
    verbose: True
    window: 0
  }
  ema_decay: 0.0
  enqueue_max_steps: -1
  init_from_checkpoint_rules: {}
  max_steps: 4000000
  save_interval_seconds: 600
  save_keep_checkpoint_every_n_hours: 0.5
  save_max_to_keep: 100
  start_up_delay_steps: 200
  summary_interval_steps: 100
  tpu_steps_per_loop: 100
}
I1002 01:29:08.689777 139650252134208 base_model.py:301] input_params: {
  allow_implicit_capture: None
  bucket_adjust_every_n: 0
  bucket_batch_limit: [32]
  bucket_upper_bound: [1024]
  cls: <class 'lingvo.tasks.lm.input_generator.LmInput'>
  dtype: <dtype: 'float32'>
  file_buffer_size: 10000000
  file_datasource: None
  file_parallelism: 10
"text:/tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en*"
  file_random_seed: 301
  fixed_input_shape: True
  flush_every_n: 0
  fprop_dtype: None
  inference_driver_name: None
  is_eval: None
  is_inference: None
"1bwds_train_set"
  num_batcher_threads: 16
  num_samples: 0
  pad_to_max_seq_length: False
  params_init: {
"xavier"
    scale: 1.000001
    seed: None
  }
  random_seed: None
  remote: {
    max_inflights_per_target: 32
    shardable_batch: False
  }
  require_sequential_order: False
  skip_lp_regularization: None
  source_max_length: None
  target_max_length: 1024
  tokenizer: {
    allow_implicit_capture: None
    append_eos: True
    cls: <class 'lingvo.core.tokenizers.AsciiTokenizer'>
    dtype: <dtype: 'float32'>
    fprop_dtype: None
    inference_driver_name: None
    is_eval: None
    is_inference: None
"tokenizer"
    pad_to_max_length: True
    params_init: {
"xavier"
      scale: 1.000001
      seed: None
    }
    random_seed: None
    skip_lp_regularization: None
    target_eos_id: 2
    target_sos_id: 1
    target_unk_id: 0
    vn: {
      global_vn: False
      per_step_vn: False
      scale: None
      seed: None
    }
    vocab_size: 32000
  }
  tokenizer_dict: {}
  tpu_infeed_parallelism: 1
  use_chaining: False
  use_per_host_infeed: False
  use_within_batch_mixing: False
  vn: {
    global_vn: False
    per_step_vn: False
    scale: None
    seed: None
  }
}
I1002 01:29:08.693227 139650252134208 base_input_generator.py:624] bucket_batch_limit [32]
I1002 01:29:08.742117 139650252134208 learner.py:351] Ignoring legacy param start_up_delay_steps=200 for optimization program
I1002 01:29:08.742243 139650252134208 learner.py:351] Ignoring legacy param max_steps=4000000 for optimization program
I1002 01:29:08.742319 139650252134208 learner.py:351] Ignoring legacy param tpu_steps_per_loop=100 for optimization program
I1002 01:29:08.742384 139650252134208 learner.py:351] Ignoring legacy param vn_start_step=20000 for optimization program
I1002 01:29:08.742445 139650252134208 learner.py:351] Ignoring legacy param vn_std=0.0 for optimization program
I1002 01:29:08.742510 139650252134208 learner.py:351] Ignoring legacy param early_stop={
  metric_history: {
"eval_dev"
    local_filesystem: False
"/tmp/mnist/log"
"log_pplx"
    minimize: True
"MetricHistory"
    tfevent_file: False
  }
  min_steps: 0
"EarlyStop"
  tolerance: 0.0
  verbose: True
  window: 0
} for optimization program
I1002 01:29:08.742631 139650252134208 learner.py:351] Ignoring legacy param ema_decay=0.0 for optimization program
I1002 01:29:08.742696 139650252134208 learner.py:351] Ignoring legacy param init_from_checkpoint_rules={} for optimization program
I1002 01:29:08.742766 139650252134208 learner.py:351] Ignoring legacy param pruning_hparams_dict=None for optimization program
I1002 01:29:08.742827 139650252134208 learner.py:351] Ignoring legacy param enqueue_max_steps=-1 for optimization program
I1002 01:29:08.742885 139650252134208 learner.py:351] Ignoring legacy param save_interval_seconds=600 for optimization program
I1002 01:29:08.742942 139650252134208 learner.py:351] Ignoring legacy param save_max_to_keep=100 for optimization program
I1002 01:29:08.743000 139650252134208 learner.py:351] Ignoring legacy param save_keep_checkpoint_every_n_hours=0.5 for optimization program
I1002 01:29:08.743061 139650252134208 learner.py:351] Ignoring legacy param summary_interval_steps=100 for optimization program
I1002 01:29:08.743119 139650252134208 learner.py:351] Ignoring legacy param learner=None for optimization program
I1002 01:29:08.743211 139650252134208 learner.py:351] Ignoring legacy param max_lstm_gradient_norm=0.0 for optimization program
I1002 01:29:08.743273 139650252134208 learner.py:351] Ignoring legacy param sum_loss_across_tokens_in_batch=False for optimization program
I1002 01:29:08.743760 139650252134208 learner.py:356] Learner params: allow_implicit_capture : NoneType
I1002 01:29:08.743853 139650252134208 learner.py:356] Learner params: bprop_variable_exclusion : NoneType
I1002 01:29:08.743924 139650252134208 learner.py:356] Learner params: bprop_variable_filter : NoneType
I1002 01:29:08.743989 139650252134208 learner.py:356] Learner params: clip_gradient_norm_to_value : 0.0
I1002 01:29:08.744051 139650252134208 learner.py:356] Learner params: clip_gradient_single_norm_to_value : 0.0
I1002 01:29:08.744112 139650252134208 learner.py:356] Learner params: cls : type/lingvo.core.learner/Learner
I1002 01:29:08.744172 139650252134208 learner.py:356] Learner params: colocate_gradients_with_ops : True
I1002 01:29:08.744231 139650252134208 learner.py:356] Learner params: dtype : float32
I1002 01:29:08.744294 139650252134208 learner.py:356] Learner params: fprop_dtype : NoneType
I1002 01:29:08.744354 139650252134208 learner.py:356] Learner params: gate_gradients : False
I1002 01:29:08.744414 139650252134208 learner.py:356] Learner params: grad_aggregation_method : 1
I1002 01:29:08.744472 139650252134208 learner.py:356] Learner params: grad_norm_to_clip_to_zero : 0.0
I1002 01:29:08.744530 139650252134208 learner.py:356] Learner params: grad_norm_tracker : NoneType
I1002 01:29:08.744589 139650252134208 learner.py:356] Learner params: inference_driver_name : NoneType
I1002 01:29:08.744655 139650252134208 learner.py:356] Learner params: is_eval : NoneType
I1002 01:29:08.744715 139650252134208 learner.py:356] Learner params: is_inference : NoneType
I1002 01:29:08.744774 139650252134208 learner.py:356] Learner params: l1_regularizer_weight : NoneType
I1002 01:29:08.744833 139650252134208 learner.py:356] Learner params: l2_regularizer_weight : 1e-06
I1002 01:29:08.744892 139650252134208 learner.py:356] Learner params: learning_rate : 0.5
I1002 01:29:08.744951 139650252134208 learner.py:356] Learner params: lr_schedule.allow_implicit_capture : NoneType
I1002 01:29:08.745010 139650252134208 learner.py:356] Learner params: lr_schedule.cls : type/lingvo.core.schedule/TransformerLearningRateSchedule
I1002 01:29:08.745069 139650252134208 learner.py:356] Learner params: lr_schedule.decay_end : NoneType
I1002 01:29:08.745127 139650252134208 learner.py:356] Learner params: lr_schedule.dtype : float32
I1002 01:29:08.745186 139650252134208 learner.py:356] Learner params: lr_schedule.fprop_dtype : NoneType
I1002 01:29:08.745280 139650252134208 learner.py:356] Learner params: lr_schedule.inference_driver_name : NoneType
I1002 01:29:08.745339 139650252134208 learner.py:356] Learner params: lr_schedule.is_eval : NoneType
I1002 01:29:08.745398 139650252134208 learner.py:356] Learner params: lr_schedule.is_inference : NoneType
I1002 01:29:08.745457 139650252134208 learner.py:356] Learner params: lr_schedule.model_dim : 2048
I1002 01:29:08.745516 139650252134208 learner.py:356] Learner params: lr_schedule.name : 'LRSched'
I1002 01:29:08.745574 139650252134208 learner.py:356] Learner params: lr_schedule.params_init.method : 'xavier'
I1002 01:29:08.745634 139650252134208 learner.py:356] Learner params: lr_schedule.params_init.scale : 1.000001
I1002 01:29:08.745692 139650252134208 learner.py:356] Learner params: lr_schedule.params_init.seed : NoneType
I1002 01:29:08.745751 139650252134208 learner.py:356] Learner params: lr_schedule.random_seed : NoneType
I1002 01:29:08.745810 139650252134208 learner.py:356] Learner params: lr_schedule.skip_lp_regularization : NoneType
I1002 01:29:08.745869 139650252134208 learner.py:356] Learner params: lr_schedule.vn.global_vn : False
I1002 01:29:08.745928 139650252134208 learner.py:356] Learner params: lr_schedule.vn.per_step_vn : False
I1002 01:29:08.745986 139650252134208 learner.py:356] Learner params: lr_schedule.vn.scale : NoneType
I1002 01:29:08.746051 139650252134208 learner.py:356] Learner params: lr_schedule.vn.seed : NoneType
I1002 01:29:08.746108 139650252134208 learner.py:356] Learner params: lr_schedule.warmup_steps : 40000
I1002 01:29:08.746166 139650252134208 learner.py:356] Learner params: lr_schedule.worker_replicas : 1
I1002 01:29:08.746224 139650252134208 learner.py:356] Learner params: name : 'loss'
I1002 01:29:08.746282 139650252134208 learner.py:356] Learner params: optimizer.allow_implicit_capture : NoneType
I1002 01:29:08.746340 139650252134208 learner.py:356] Learner params: optimizer.beta1 : 0.9
I1002 01:29:08.746398 139650252134208 learner.py:356] Learner params: optimizer.beta2 : 0.997
I1002 01:29:08.746456 139650252134208 learner.py:356] Learner params: optimizer.cls : type/lingvo.core.optimizer/Adam
I1002 01:29:08.746515 139650252134208 learner.py:356] Learner params: optimizer.dtype : float32
I1002 01:29:08.746572 139650252134208 learner.py:356] Learner params: optimizer.epsilon : 1e-09
I1002 01:29:08.746631 139650252134208 learner.py:356] Learner params: optimizer.fprop_dtype : NoneType
I1002 01:29:08.746689 139650252134208 learner.py:356] Learner params: optimizer.inference_driver_name : NoneType
I1002 01:29:08.746747 139650252134208 learner.py:356] Learner params: optimizer.is_eval : NoneType
I1002 01:29:08.746805 139650252134208 learner.py:356] Learner params: optimizer.is_inference : NoneType
I1002 01:29:08.746864 139650252134208 learner.py:356] Learner params: optimizer.name : 'Adam'
I1002 01:29:08.746922 139650252134208 learner.py:356] Learner params: optimizer.params_init.method : 'xavier'
I1002 01:29:08.746987 139650252134208 learner.py:356] Learner params: optimizer.params_init.scale : 1.000001
I1002 01:29:08.747046 139650252134208 learner.py:356] Learner params: optimizer.params_init.seed : NoneType
I1002 01:29:08.747105 139650252134208 learner.py:356] Learner params: optimizer.random_seed : NoneType
I1002 01:29:08.747164 139650252134208 learner.py:356] Learner params: optimizer.skip_lp_regularization : NoneType
I1002 01:29:08.747222 139650252134208 learner.py:356] Learner params: optimizer.vn.global_vn : False
I1002 01:29:08.747280 139650252134208 learner.py:356] Learner params: optimizer.vn.per_step_vn : False
I1002 01:29:08.747354 139650252134208 learner.py:356] Learner params: optimizer.vn.scale : NoneType
I1002 01:29:08.747415 139650252134208 learner.py:356] Learner params: optimizer.vn.seed : NoneType
I1002 01:29:08.747474 139650252134208 learner.py:356] Learner params: params_init.method : 'xavier'
I1002 01:29:08.747534 139650252134208 learner.py:356] Learner params: params_init.scale : 1.000001
I1002 01:29:08.747592 139650252134208 learner.py:356] Learner params: params_init.seed : NoneType
I1002 01:29:08.747651 139650252134208 learner.py:356] Learner params: random_seed : NoneType
I1002 01:29:08.747710 139650252134208 learner.py:356] Learner params: skip_lp_regularization : NoneType
I1002 01:29:08.747769 139650252134208 learner.py:356] Learner params: vn.global_vn : False
I1002 01:29:08.747827 139650252134208 learner.py:356] Learner params: vn.per_step_vn : False
I1002 01:29:08.747886 139650252134208 learner.py:356] Learner params: vn.scale : NoneType
I1002 01:29:08.747945 139650252134208 learner.py:356] Learner params: vn.seed : NoneType
I1002 01:29:08.748004 139650252134208 learner.py:356] Learner params: 
I1002 01:29:08.994469 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var on /job:local/replica:0/task:0/device:CPU:0 262144008
I1002 01:29:08.996488 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var:0 shape=(32000, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.015697 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 278921224
I1002 01:29:09.017632 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.020229 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 278929416
I1002 01:29:09.021967 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.028861 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 295706632
I1002 01:29:09.030794 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.033484 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 295714824
I1002 01:29:09.035122 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.042043 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 312492040
I1002 01:29:09.044074 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.046636 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 312500232
I1002 01:29:09.048306 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.055242 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 329277448
I1002 01:29:09.057183 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.059803 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 329285640
I1002 01:29:09.061541 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.065270 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 329286152
I1002 01:29:09.066906 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.070778 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 329294344
I1002 01:29:09.072447 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.075577 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 329302536
I1002 01:29:09.077212 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:29:09.086850 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:29:09.093071 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 396411400
I1002 01:29:09.095070 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.097682 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 396444168
I1002 01:29:09.099334 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:29:09.101269 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:29:09.107551 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 463553032
I1002 01:29:09.109486 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.112176 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 463561224
I1002 01:29:09.113893 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.118571 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 463569416
I1002 01:29:09.120228 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.122942 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 463577608
I1002 01:29:09.124599 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.145209 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 480354824
I1002 01:29:09.147128 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.149712 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 480363016
I1002 01:29:09.151491 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.158406 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 497140232
I1002 01:29:09.160345 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.163025 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 497148424
I1002 01:29:09.164693 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.171657 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 513925640
I1002 01:29:09.173641 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.176253 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 513933832
I1002 01:29:09.177893 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.185282 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 530711048
I1002 01:29:09.187202 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.189840 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 530719240
I1002 01:29:09.191611 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.195290 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 530719752
I1002 01:29:09.196960 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.200883 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 530727944
I1002 01:29:09.202530 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.205232 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 530736136
I1002 01:29:09.206874 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:29:09.216612 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:29:09.222847 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 597845000
I1002 01:29:09.224875 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.227493 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 597877768
I1002 01:29:09.229144 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:29:09.231117 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:29:09.237881 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 664986632
I1002 01:29:09.239885 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.242553 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 664994824
I1002 01:29:09.244233 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.248936 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 665003016
I1002 01:29:09.250574 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.253316 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 665011208
I1002 01:29:09.254958 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.275173 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 681788424
I1002 01:29:09.277134 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.279760 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 681796616
I1002 01:29:09.281526 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.289105 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 698573832
I1002 01:29:09.291033 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.293751 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 698582024
I1002 01:29:09.295429 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.302578 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 715359240
I1002 01:29:09.304586 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.307195 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 715367432
I1002 01:29:09.308865 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.315887 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 732144648
I1002 01:29:09.317834 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.320486 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 732152840
I1002 01:29:09.322254 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.326055 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 732153352
I1002 01:29:09.327740 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.331721 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 732161544
I1002 01:29:09.333356 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.336105 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 732169736
I1002 01:29:09.337757 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:29:09.348203 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:29:09.354513 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 799278600
I1002 01:29:09.356547 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.359156 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 799311368
I1002 01:29:09.360818 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:29:09.362813 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:29:09.369093 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 866420232
I1002 01:29:09.371027 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.373710 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 866428424
I1002 01:29:09.375389 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.380136 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 866436616
I1002 01:29:09.381797 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.384548 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 866444808
I1002 01:29:09.386258 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.407184 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 883222024
I1002 01:29:09.409165 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.411776 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 883230216
I1002 01:29:09.413533 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.420513 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 900007432
I1002 01:29:09.422447 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.425182 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 900015624
I1002 01:29:09.426830 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.433800 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 916792840
I1002 01:29:09.435830 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.438434 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 916801032
I1002 01:29:09.440115 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.447140 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 933578248
I1002 01:29:09.449113 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.451774 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 933586440
I1002 01:29:09.453544 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.457320 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 933586952
I1002 01:29:09.458982 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.463000 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 933595144
I1002 01:29:09.464687 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.467908 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 933603336
I1002 01:29:09.469567 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:29:09.479578 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:29:09.485848 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 1000712200
I1002 01:29:09.487895 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.490489 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 1000744968
I1002 01:29:09.492176 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:29:09.494186 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:29:09.500430 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 1067853832
I1002 01:29:09.502396 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.505094 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 1067862024
I1002 01:29:09.506765 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.511548 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1067870216
I1002 01:29:09.513199 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.515945 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1067878408
I1002 01:29:09.517618 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.538563 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 1084655624
I1002 01:29:09.540531 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.543124 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1084663816
I1002 01:29:09.544906 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.551883 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 1101441032
I1002 01:29:09.553820 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.556529 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1101449224
I1002 01:29:09.558196 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.565190 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 1118226440
I1002 01:29:09.567229 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.569888 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1118234632
I1002 01:29:09.571567 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.579231 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 1135011848
I1002 01:29:09.581200 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.583870 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1135020040
I1002 01:29:09.585670 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.589502 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 1135020552
I1002 01:29:09.591193 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.595207 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1135028744
I1002 01:29:09.596882 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.599612 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1135036936
I1002 01:29:09.601276 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:29:09.611234 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:29:09.617586 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 1202145800
I1002 01:29:09.619645 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.622247 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 1202178568
I1002 01:29:09.623946 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:29:09.625950 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:29:09.632767 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 1269287432
I1002 01:29:09.634745 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.637476 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 1269295624
I1002 01:29:09.639166 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.643934 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1269303816
I1002 01:29:09.645594 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.648368 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1269312008
I1002 01:29:09.650031 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.670387 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 1286089224
I1002 01:29:09.672551 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.675140 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1286097416
I1002 01:29:09.676927 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.684443 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 1302874632
I1002 01:29:09.686393 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.689133 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1302882824
I1002 01:29:09.690804 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.697779 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 1319660040
I1002 01:29:09.699834 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.702432 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1319668232
I1002 01:29:09.704132 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.711159 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 1336445448
I1002 01:29:09.713138 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.715800 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1336453640
I1002 01:29:09.717564 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.721337 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 1336454152
I1002 01:29:09.723010 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.726995 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1336462344
I1002 01:29:09.728697 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.731447 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1336470536
I1002 01:29:09.733117 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:29:09.743610 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:29:09.749960 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 1403579400
I1002 01:29:09.752024 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.754629 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 1403612168
I1002 01:29:09.756351 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:29:09.758381 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:29:09.764706 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 1470721032
I1002 01:29:09.766663 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.769396 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 1470729224
I1002 01:29:09.771091 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.775904 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1470737416
I1002 01:29:09.777570 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.780342 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1470745608
I1002 01:29:09.782019 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.802971 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 1487522824
I1002 01:29:09.804961 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.807589 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1487531016
I1002 01:29:09.809377 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.816370 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 1504308232
I1002 01:29:09.818321 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.821120 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1504316424
I1002 01:29:09.822796 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.829857 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 1521093640
I1002 01:29:09.831898 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.834513 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1521101832
I1002 01:29:09.836212 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.843363 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 1537879048
I1002 01:29:09.845319 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.847981 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1537887240
I1002 01:29:09.849759 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.853541 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 1537887752
I1002 01:29:09.855226 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.859241 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1537895944
I1002 01:29:09.860939 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.864156 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1537904136
I1002 01:29:09.865838 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:29:09.875791 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:29:09.882054 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 1605013000
I1002 01:29:09.884121 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.886739 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 1605045768
I1002 01:29:09.888516 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:29:09.890617 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:29:09.896916 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 1672154632
I1002 01:29:09.898882 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.901614 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 1672162824
I1002 01:29:09.903324 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.908130 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1672171016
I1002 01:29:09.909810 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.912585 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1672179208
I1002 01:29:09.914274 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.935161 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 1688956424
I1002 01:29:09.937146 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.939769 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1688964616
I1002 01:29:09.941551 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.948574 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 1705741832
I1002 01:29:09.950553 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.953304 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1705750024
I1002 01:29:09.954990 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.962003 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 1722527240
I1002 01:29:09.964059 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.966684 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1722535432
I1002 01:29:09.968388 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.975883 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 1739312648
I1002 01:29:09.977841 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.980511 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1739320840
I1002 01:29:09.982317 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.986108 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 1739321352
I1002 01:29:09.987840 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.992039 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1739329544
I1002 01:29:09.993724 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:09.996469 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1739337736
I1002 01:29:09.998156 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:29:10.008304 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:29:10.014692 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 1806446600
I1002 01:29:10.016782 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.019432 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 1806479368
I1002 01:29:10.021137 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:29:10.023192 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:29:10.030067 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 1873588232
I1002 01:29:10.032108 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.034822 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 1873596424
I1002 01:29:10.036529 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.041378 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1873604616
I1002 01:29:10.043069 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.045855 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1873612808
I1002 01:29:10.047561 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.121873 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 1890390024
I1002 01:29:10.123896 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.126535 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1890398216
I1002 01:29:10.128346 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.135432 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 1907175432
I1002 01:29:10.137434 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.140575 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1907183624
I1002 01:29:10.142284 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.149272 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 1923960840
I1002 01:29:10.151316 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.154030 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1923969032
I1002 01:29:10.155744 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.162754 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 1940746248
I1002 01:29:10.164749 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.167446 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1940754440
I1002 01:29:10.169243 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.173071 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 1940754952
I1002 01:29:10.174783 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.178849 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1940763144
I1002 01:29:10.180548 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.183292 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1940771336
I1002 01:29:10.185003 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:29:10.194947 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:29:10.201777 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 2007880200
I1002 01:29:10.203838 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.206482 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 2007912968
I1002 01:29:10.208219 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:29:10.210270 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:29:10.216565 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 2075021832
I1002 01:29:10.218536 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.221289 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 2075030024
I1002 01:29:10.222991 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.227859 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2075038216
I1002 01:29:10.229554 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.232343 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2075046408
I1002 01:29:10.234041 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.254893 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 2091823624
I1002 01:29:10.256902 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.259545 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2091831816
I1002 01:29:10.261328 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.268366 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 2108609032
I1002 01:29:10.270358 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.273119 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2108617224
I1002 01:29:10.274829 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.281841 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 2125394440
I1002 01:29:10.283933 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.286592 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2125402632
I1002 01:29:10.288312 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.295385 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 2142179848
I1002 01:29:10.297400 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.300087 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2142188040
I1002 01:29:10.301875 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.305754 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 2142188552
I1002 01:29:10.307474 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.311590 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2142196744
I1002 01:29:10.313295 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.316078 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2142204936
I1002 01:29:10.317775 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:29:10.328396 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:29:10.334712 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 2209313800
I1002 01:29:10.336790 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.339451 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 2209346568
I1002 01:29:10.341154 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:29:10.343233 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:29:10.349564 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 2276455432
I1002 01:29:10.351565 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.354318 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 2276463624
I1002 01:29:10.356045 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.360905 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2276471816
I1002 01:29:10.362613 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.365407 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2276480008
I1002 01:29:10.367104 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.388141 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 2293257224
I1002 01:29:10.390128 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.392800 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2293265416
I1002 01:29:10.394594 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.401585 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 2310042632
I1002 01:29:10.403600 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.406363 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2310050824
I1002 01:29:10.408097 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.415122 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 2326828040
I1002 01:29:10.417204 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.419893 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2326836232
I1002 01:29:10.421616 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.429299 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 2343613448
I1002 01:29:10.431288 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.434019 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2343621640
I1002 01:29:10.435846 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.439695 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 2343622152
I1002 01:29:10.441400 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.445593 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2343630344
I1002 01:29:10.447324 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.450075 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2343638536
I1002 01:29:10.451801 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:29:10.461867 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:29:10.468300 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 2410747400
I1002 01:29:10.470371 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.473109 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 2410780168
I1002 01:29:10.474816 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:29:10.476917 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:29:10.483267 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 2477889032
I1002 01:29:10.485289 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.488628 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 2477897224
I1002 01:29:10.490346 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.495240 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2477905416
I1002 01:29:10.496970 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.499803 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2477913608
I1002 01:29:10.501980 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.522574 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 2494690824
I1002 01:29:10.524643 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.527291 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2494699016
I1002 01:29:10.529119 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.536165 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 2511476232
I1002 01:29:10.538254 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.541634 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2511484424
I1002 01:29:10.543352 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.550359 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 2528261640
I1002 01:29:10.552441 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.555104 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2528269832
I1002 01:29:10.556858 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.563973 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 2545047048
I1002 01:29:10.565974 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.568674 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2545055240
I1002 01:29:10.570483 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.574422 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 2545055752
I1002 01:29:10.576164 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.580327 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2545063944
I1002 01:29:10.582033 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.584819 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2545072136
I1002 01:29:10.586542 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:29:10.596657 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:29:10.603607 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 2612181000
I1002 01:29:10.605678 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.608412 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 2612213768
I1002 01:29:10.610129 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:29:10.612257 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:29:10.618581 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 2679322632
I1002 01:29:10.620605 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.623382 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 2679330824
I1002 01:29:10.625139 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.630107 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2679339016
I1002 01:29:10.631838 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.634638 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2679347208
I1002 01:29:10.636379 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.657728 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 2696124424
I1002 01:29:10.659771 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.662433 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2696132616
I1002 01:29:10.664294 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.671442 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 2712909832
I1002 01:29:10.673434 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.676237 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2712918024
I1002 01:29:10.677958 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.685015 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 2729695240
I1002 01:29:10.687065 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.689754 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2729703432
I1002 01:29:10.691482 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.698568 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 2746480648
I1002 01:29:10.700633 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.703364 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2746488840
I1002 01:29:10.705198 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.709135 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 2746489352
I1002 01:29:10.710855 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.715070 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2746497544
I1002 01:29:10.716819 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.719612 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2746505736
I1002 01:29:10.721323 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:29:10.732082 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:29:10.738481 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 2813614600
I1002 01:29:10.740582 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.743247 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 2813647368
I1002 01:29:10.744991 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:29:10.747131 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:29:10.753461 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 2880756232
I1002 01:29:10.755493 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.758252 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 2880764424
I1002 01:29:10.759999 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.764950 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2880772616
I1002 01:29:10.766657 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.769492 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2880780808
I1002 01:29:10.771205 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.792487 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 2897558024
I1002 01:29:10.794505 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.797200 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2897566216
I1002 01:29:10.799029 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.806086 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 2914343432
I1002 01:29:10.808119 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.810868 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2914351624
I1002 01:29:10.812614 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.819689 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 2931128840
I1002 01:29:10.821751 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.824440 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2931137032
I1002 01:29:10.826164 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.833865 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 2947914248
I1002 01:29:10.835902 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.838600 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2947922440
I1002 01:29:10.840444 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.844421 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 2947922952
I1002 01:29:10.846149 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.850486 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2947931144
I1002 01:29:10.852222 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.855003 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2947939336
I1002 01:29:10.856751 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:29:10.866895 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:29:10.873329 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 3015048200
I1002 01:29:10.875433 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.878141 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 3015080968
I1002 01:29:10.879910 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:29:10.882019 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:29:10.888399 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 3082189832
I1002 01:29:10.890417 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.893764 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 3082198024
I1002 01:29:10.895513 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.900453 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3082206216
I1002 01:29:10.902213 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.905052 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3082214408
I1002 01:29:10.906780 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.927617 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 3098991624
I1002 01:29:10.929644 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.932330 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3098999816
I1002 01:29:10.934176 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.941234 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 3115777032
I1002 01:29:10.943240 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.946600 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3115785224
I1002 01:29:10.948350 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.955394 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 3132562440
I1002 01:29:10.957473 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.960172 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3132570632
I1002 01:29:10.961897 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.969040 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 3149347848
I1002 01:29:10.971062 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.973785 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3149356040
I1002 01:29:10.975665 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.979655 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 3149356552
I1002 01:29:10.981379 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.985583 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3149364744
I1002 01:29:10.987336 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:10.990134 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3149372936
I1002 01:29:10.991896 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:29:11.002163 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:29:11.009129 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 3216481800
I1002 01:29:11.011471 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.014192 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 3216514568
I1002 01:29:11.015956 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:29:11.018096 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:29:11.024472 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 3283623432
I1002 01:29:11.026525 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.029357 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 3283631624
I1002 01:29:11.031100 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.036125 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3283639816
I1002 01:29:11.037849 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.040687 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3283648008
I1002 01:29:11.042412 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.063876 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 3300425224
I1002 01:29:11.065940 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.068656 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3300433416
I1002 01:29:11.070489 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.077586 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 3317210632
I1002 01:29:11.079633 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.082415 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3317218824
I1002 01:29:11.084166 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.091215 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 3333996040
I1002 01:29:11.093316 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.096041 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3334004232
I1002 01:29:11.097770 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.104928 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 3350781448
I1002 01:29:11.106952 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.109704 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3350789640
I1002 01:29:11.111557 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.115457 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 3350790152
I1002 01:29:11.117218 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.121432 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3350798344
I1002 01:29:11.123168 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.125999 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3350806536
I1002 01:29:11.127778 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:29:11.138596 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:29:11.145001 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 3417915400
I1002 01:29:11.147094 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.149819 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 3417948168
I1002 01:29:11.151584 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:29:11.153727 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:29:11.160109 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 3485057032
I1002 01:29:11.162140 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.164927 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 3485065224
I1002 01:29:11.166685 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.171697 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3485073416
I1002 01:29:11.173435 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.176282 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3485081608
I1002 01:29:11.178038 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.251965 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 3501858824
I1002 01:29:11.254039 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.256757 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3501867016
I1002 01:29:11.258617 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.265730 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 3518644232
I1002 01:29:11.267799 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.270577 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3518652424
I1002 01:29:11.272346 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.279455 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 3535429640
I1002 01:29:11.281484 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.284183 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3535437832
I1002 01:29:11.285932 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.293039 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 3552215048
I1002 01:29:11.295068 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.297863 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3552223240
I1002 01:29:11.300249 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.304217 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 3552223752
I1002 01:29:11.305970 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.310213 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3552231944
I1002 01:29:11.311981 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.314776 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3552240136
I1002 01:29:11.316553 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:29:11.326745 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:29:11.333174 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 3619349000
I1002 01:29:11.335273 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.338008 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 3619381768
I1002 01:29:11.339778 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:29:11.341931 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:29:11.348316 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 3686490632
I1002 01:29:11.350363 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.353172 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 3686498824
I1002 01:29:11.354917 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.359983 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3686507016
I1002 01:29:11.361729 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.365107 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3686515208
I1002 01:29:11.366852 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.387578 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 3703292424
I1002 01:29:11.389637 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.392339 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3703300616
I1002 01:29:11.394205 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.401323 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 3720077832
I1002 01:29:11.403387 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.406193 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3720086024
I1002 01:29:11.407978 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.415532 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 3736863240
I1002 01:29:11.417645 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.420377 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3736871432
I1002 01:29:11.422118 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.429317 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 3753648648
I1002 01:29:11.431384 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.434119 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3753656840
I1002 01:29:11.435984 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.439918 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 3753657352
I1002 01:29:11.441663 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.445968 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3753665544
I1002 01:29:11.447783 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.450598 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3753673736
I1002 01:29:11.452373 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:29:11.462507 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:29:11.468889 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 3820782600
I1002 01:29:11.471559 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.474257 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 3820815368
I1002 01:29:11.476024 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:29:11.478192 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:29:11.484553 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 3887924232
I1002 01:29:11.486602 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.489427 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 3887932424
I1002 01:29:11.491190 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.496201 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3887940616
I1002 01:29:11.497948 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.500818 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3887948808
I1002 01:29:11.502578 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.523828 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 3904726024
I1002 01:29:11.525861 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.528580 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3904734216
I1002 01:29:11.530433 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.537508 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 3921511432
I1002 01:29:11.539578 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.542617 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3921519624
I1002 01:29:11.544409 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.551496 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 3938296840
I1002 01:29:11.553589 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.556316 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3938305032
I1002 01:29:11.558148 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.565286 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 3955082248
I1002 01:29:11.567341 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.570087 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3955090440
I1002 01:29:11.571961 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.575998 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 3955090952
I1002 01:29:11.577754 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.582035 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3955099144
I1002 01:29:11.583805 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.586619 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3955107336
I1002 01:29:11.588402 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:29:11.599211 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:29:11.605616 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 4022216200
I1002 01:29:11.607752 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.610498 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 4022248968
I1002 01:29:11.612295 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:29:11.614480 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:29:11.620858 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 4089357832
I1002 01:29:11.622902 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.625734 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 4089366024
I1002 01:29:11.627552 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.632597 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4089374216
I1002 01:29:11.634362 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.637247 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4089382408
I1002 01:29:11.638997 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.660439 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 4106159624
I1002 01:29:11.662511 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.665232 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4106167816
I1002 01:29:11.667098 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.674210 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 4122945032
I1002 01:29:11.676275 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.679071 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4122953224
I1002 01:29:11.680852 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.687972 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 4139730440
I1002 01:29:11.690096 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.692826 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4139738632
I1002 01:29:11.694573 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.701754 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 4156515848
I1002 01:29:11.703835 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.706591 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4156524040
I1002 01:29:11.708979 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.712961 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 4156524552
I1002 01:29:11.714722 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.719018 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4156532744
I1002 01:29:11.720808 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.723637 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4156540936
I1002 01:29:11.725396 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:29:11.735656 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:29:11.742089 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 4223649800
I1002 01:29:11.744234 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.746992 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 4223682568
I1002 01:29:11.748789 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:29:11.750995 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:29:11.757508 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 4290791432
I1002 01:29:11.759568 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.762390 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 4290799624
I1002 01:29:11.764190 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.769282 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4290807816
I1002 01:29:11.771056 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.774449 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4290816008
I1002 01:29:11.776249 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.797161 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 4307593224
I1002 01:29:11.799204 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.801953 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4307601416
I1002 01:29:11.803860 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.810940 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 4324378632
I1002 01:29:11.813025 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.815884 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4324386824
I1002 01:29:11.817649 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.825438 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 4341164040
I1002 01:29:11.827569 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.830302 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4341172232
I1002 01:29:11.832107 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.839272 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 4357949448
I1002 01:29:11.841356 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.844132 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4357957640
I1002 01:29:11.845986 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.849950 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 4357958152
I1002 01:29:11.851759 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.856067 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4357966344
I1002 01:29:11.857847 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.860705 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4357974536
I1002 01:29:11.862487 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:29:11.872728 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:29:11.879141 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 4425083400
I1002 01:29:11.881839 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.884594 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 4425116168
I1002 01:29:11.886366 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:29:11.888607 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:29:11.895011 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 4492225032
I1002 01:29:11.897104 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.899945 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 4492233224
I1002 01:29:11.901732 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.906817 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4492241416
I1002 01:29:11.908598 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.911501 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4492249608
I1002 01:29:11.913269 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.934817 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 4509026824
I1002 01:29:11.936928 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.939668 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4509035016
I1002 01:29:11.941540 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.948655 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 4525812232
I1002 01:29:11.950721 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.953573 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4525820424
I1002 01:29:11.955359 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.962460 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 4542597640
I1002 01:29:11.964600 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.967337 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4542605832
I1002 01:29:11.969147 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.976318 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 4559383048
I1002 01:29:11.978388 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.981167 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4559391240
I1002 01:29:11.983053 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.987053 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 4559391752
I1002 01:29:11.988856 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.993183 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4559399944
I1002 01:29:11.994951 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:11.997817 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4559408136
I1002 01:29:11.999627 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:29:12.010467 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:29:12.016897 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 4626517000
I1002 01:29:12.019026 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.021809 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 4626549768
I1002 01:29:12.023619 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:29:12.025835 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:29:12.032224 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 4693658632
I1002 01:29:12.034291 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.037139 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 4693666824
I1002 01:29:12.038930 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.044072 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4693675016
I1002 01:29:12.045850 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.048743 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4693683208
I1002 01:29:12.050524 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.071944 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 4710460424
I1002 01:29:12.074008 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.076749 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4710468616
I1002 01:29:12.078630 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.085766 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 4727245832
I1002 01:29:12.087858 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.090703 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4727254024
I1002 01:29:12.092513 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.099655 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 4744031240
I1002 01:29:12.101777 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.104542 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4744039432
I1002 01:29:12.106312 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.113491 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 4760816648
I1002 01:29:12.115619 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.118392 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4760824840
I1002 01:29:12.120804 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.124833 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 4760825352
I1002 01:29:12.126636 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.131004 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4760833544
I1002 01:29:12.132904 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.135794 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4760841736
I1002 01:29:12.137578 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:29:12.147971 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:29:12.154408 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 4827950600
I1002 01:29:12.156567 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.159334 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 4827983368
I1002 01:29:12.161137 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:29:12.163406 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:29:12.169800 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 4895092232
I1002 01:29:12.171899 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.174756 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 4895100424
I1002 01:29:12.176581 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.181734 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4895108616
I1002 01:29:12.183546 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.187006 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4895116808
I1002 01:29:12.188814 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.209884 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 4911894024
I1002 01:29:12.211994 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.214744 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4911902216
I1002 01:29:12.216647 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.223774 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 4928679432
I1002 01:29:12.225847 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.228702 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4928687624
I1002 01:29:12.230501 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.238212 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 4945464840
I1002 01:29:12.240392 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.243140 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4945473032
I1002 01:29:12.244946 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.252209 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 4962250248
I1002 01:29:12.254293 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.257112 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4962258440
I1002 01:29:12.259024 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.263183 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 4962258952
I1002 01:29:12.265039 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.269608 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4962267144
I1002 01:29:12.271452 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.274333 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4962275336
I1002 01:29:12.276162 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:29:12.286815 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:29:12.293714 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 5029384200
I1002 01:29:12.296755 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.299651 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 5029416968
I1002 01:29:12.301455 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:29:12.303844 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:29:12.310438 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 5096525832
I1002 01:29:12.312656 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.316830 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 5096534024
I1002 01:29:12.318651 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.324180 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5096542216
I1002 01:29:12.326005 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.329083 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5096550408
I1002 01:29:12.330880 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.409250 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 5113327624
I1002 01:29:12.411544 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.415092 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5113335816
I1002 01:29:12.417020 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.424358 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 5130113032
I1002 01:29:12.426587 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.429507 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5130121224
I1002 01:29:12.431358 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.438695 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 5146898440
I1002 01:29:12.440894 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.443773 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5146906632
I1002 01:29:12.445674 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.452946 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 5163683848
I1002 01:29:12.455074 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.458050 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5163692040
I1002 01:29:12.459855 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.463996 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 5163692552
I1002 01:29:12.465791 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.470335 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5163700744
I1002 01:29:12.472175 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.475062 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5163708936
I1002 01:29:12.476897 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:29:12.488219 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:29:12.495187 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 5230817800
I1002 01:29:12.497484 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.500353 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 5230850568
I1002 01:29:12.502153 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:29:12.504573 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:29:12.511178 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 5297959432
I1002 01:29:12.513330 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.516292 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 5297967624
I1002 01:29:12.518086 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.523568 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5297975816
I1002 01:29:12.525399 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.528408 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5297984008
I1002 01:29:12.530235 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.553041 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 5314761224
I1002 01:29:12.555333 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.558154 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5314769416
I1002 01:29:12.560095 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.567254 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 5331546632
I1002 01:29:12.569381 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.572306 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5331554824
I1002 01:29:12.574111 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.581300 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 5348332040
I1002 01:29:12.583501 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.586321 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5348340232
I1002 01:29:12.588159 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.595532 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 5365117448
I1002 01:29:12.597760 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.600721 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5365125640
I1002 01:29:12.602623 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.607432 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 5365126152
I1002 01:29:12.609242 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.613721 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5365134344
I1002 01:29:12.615558 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.618426 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5365142536
I1002 01:29:12.620261 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:29:12.630813 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:29:12.637325 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 5432251400
I1002 01:29:12.639521 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.642303 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 5432284168
I1002 01:29:12.644135 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:29:12.646407 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:29:12.652820 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 5499393032
I1002 01:29:12.654937 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.657862 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 5499401224
I1002 01:29:12.659722 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.665082 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5499409416
I1002 01:29:12.666876 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.669809 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5499417608
I1002 01:29:12.671625 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.693603 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 5516194824
I1002 01:29:12.695744 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.698509 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5516203016
I1002 01:29:12.700431 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.707558 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 5532980232
I1002 01:29:12.709648 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.712542 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5532988424
I1002 01:29:12.714350 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.721540 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 5549765640
I1002 01:29:12.724202 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.726981 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5549773832
I1002 01:29:12.728804 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.735998 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 5566551048
I1002 01:29:12.738079 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.740922 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5566559240
I1002 01:29:12.742856 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.746916 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 5566559752
I1002 01:29:12.748749 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.753190 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5566567944
I1002 01:29:12.754993 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.757886 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5566576136
I1002 01:29:12.759744 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:29:12.770100 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:29:12.776604 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 5633685000
I1002 01:29:12.778791 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.781592 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 5633717768
I1002 01:29:12.783429 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:29:12.786249 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:29:12.792709 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 5700826632
I1002 01:29:12.794824 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.797719 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 5700834824
I1002 01:29:12.799556 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.804755 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5700843016
I1002 01:29:12.806567 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.809507 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5700851208
I1002 01:29:12.811348 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.832421 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 5717628424
I1002 01:29:12.834527 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.837336 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5717636616
I1002 01:29:12.839782 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.846932 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 5734413832
I1002 01:29:12.849071 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.852097 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5734422024
I1002 01:29:12.853907 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.861044 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 5751199240
I1002 01:29:12.863184 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.865977 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5751207432
I1002 01:29:12.867799 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.874977 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 5767984648
I1002 01:29:12.877112 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.879937 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5767992840
I1002 01:29:12.881852 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.885936 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 5767993352
I1002 01:29:12.887784 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.892220 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5768001544
I1002 01:29:12.894020 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.896916 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5768009736
I1002 01:29:12.898746 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:29:12.909677 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:29:12.916186 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 5835118600
I1002 01:29:12.918372 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.921191 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 5835151368
I1002 01:29:12.923004 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:29:12.925325 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:29:12.931807 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 5902260232
I1002 01:29:12.933919 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.936813 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 5902268424
I1002 01:29:12.938632 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.943888 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5902276616
I1002 01:29:12.945701 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.948716 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5902284808
I1002 01:29:12.950544 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.972165 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 5919062024
I1002 01:29:12.974270 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.977086 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5919070216
I1002 01:29:12.978987 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.986168 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 5935847432
I1002 01:29:12.988299 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:12.991172 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5935855624
I1002 01:29:12.993019 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:13.000171 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 5952632840
I1002 01:29:13.002330 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:13.005150 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5952641032
I1002 01:29:13.006974 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:13.014185 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 5969418248
I1002 01:29:13.016325 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:13.019148 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5969426440
I1002 01:29:13.021099 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:13.025755 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 5969426952
I1002 01:29:13.027619 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:13.032089 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5969435144
I1002 01:29:13.033900 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:13.036816 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5969443336
I1002 01:29:13.038621 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:29:13.049126 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:29:13.055644 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 6036552200
I1002 01:29:13.057811 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:13.060650 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 6036584968
I1002 01:29:13.062485 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:29:13.064817 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:29:13.071242 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 6103693832
I1002 01:29:13.073381 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:13.076288 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 6103702024
I1002 01:29:13.078155 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:13.083431 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 6103710216
I1002 01:29:13.085263 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:13.088221 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 6103718408
I1002 01:29:13.090039 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:13.112046 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 6120495624
I1002 01:29:13.114267 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:13.117125 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6120503816
I1002 01:29:13.119042 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:13.126306 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 6137281032
I1002 01:29:13.128449 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:13.131446 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6137289224
I1002 01:29:13.133266 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:13.140540 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 6154066440
I1002 01:29:13.143396 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:13.146193 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6154074632
I1002 01:29:13.148036 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:13.155232 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 6170851848
I1002 01:29:13.157379 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:13.160226 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6170860040
I1002 01:29:13.162130 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:13.166282 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 6170860552
I1002 01:29:13.168161 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:13.172694 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 6170868744
I1002 01:29:13.174504 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:13.177411 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 6170876936
I1002 01:29:13.179238 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:29:13.189788 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:29:13.196292 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 6237985800
I1002 01:29:13.198469 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:13.201281 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 6238018568
I1002 01:29:13.203107 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:29:13.205966 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:29:13.212440 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 6305127432
I1002 01:29:13.214573 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:13.217483 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 6305135624
I1002 01:29:13.219342 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:13.224596 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 6305143816
I1002 01:29:13.226424 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:13.229367 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 6305152008
I1002 01:29:13.231205 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:13.252442 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 6321929224
I1002 01:29:13.254559 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:13.257455 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6321937416
I1002 01:29:13.259942 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:13.267090 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 6338714632
I1002 01:29:13.269251 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:13.272160 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6338722824
I1002 01:29:13.274003 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:13.281188 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 6355500040
I1002 01:29:13.283382 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:13.286179 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6355508232
I1002 01:29:13.288032 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:13.295270 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 6372285448
I1002 01:29:13.297403 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:13.300255 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6372293640
I1002 01:29:13.302172 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:13.306288 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 6372294152
I1002 01:29:13.308145 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:13.312706 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 6372302344
I1002 01:29:13.314536 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:13.317459 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 6372310536
I1002 01:29:13.319322 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:29:14.228988 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:29:14.236630 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 6439419400
I1002 01:29:14.238872 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:14.241722 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 6439452168
I1002 01:29:14.243584 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:29:14.246010 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:29:14.252501 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 6506561032
I1002 01:29:14.254728 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:14.257582 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 6506569224
I1002 01:29:14.259477 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:14.264912 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 6506577416
I1002 01:29:14.266974 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:14.269804 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 6506585608
I1002 01:29:14.271684 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:14.294200 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 6523362824
I1002 01:29:14.296380 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:14.299294 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6523371016
I1002 01:29:14.301158 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:14.308406 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 6540148232
I1002 01:29:14.310622 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:14.313473 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6540156424
I1002 01:29:14.315354 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:14.322663 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 6556933640
I1002 01:29:14.324843 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:14.327713 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6556941832
I1002 01:29:14.329680 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:14.336926 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 6573719048
I1002 01:29:14.339071 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:14.342053 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6573727240
I1002 01:29:14.343909 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:14.348102 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 6573727752
I1002 01:29:14.349971 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:14.354597 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 6573735944
I1002 01:29:14.356475 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:14.360018 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 6573744136
I1002 01:29:14.361870 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:29:14.372643 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:29:14.379158 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 6640853000
I1002 01:29:14.381390 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:14.384232 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 6640885768
I1002 01:29:14.386101 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:29:14.388486 139650252134208 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1002 01:29:14.394946 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 6707994632
I1002 01:29:14.397131 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:14.400053 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 6708002824
I1002 01:29:14.401916 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:14.407269 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 6708011016
I1002 01:29:14.409155 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:14.412141 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 6708019208
I1002 01:29:14.413981 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:29:14.419634 139650252134208 py_utils.py:1229] WARNING!!! var weight_0 is using the default xavier initializer. Make sure this is intended.
I1002 01:29:14.426111 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var on /job:local/replica:0/task:0/device:CPU:0 6724403208
I1002 01:29:14.428265 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:29:14.429136 139650252134208 py_utils.py:1229] WARNING!!! var weight_1 is using the default xavier initializer. Make sure this is intended.
I1002 01:29:14.436084 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var on /job:local/replica:0/task:0/device:CPU:0 6740787208
I1002 01:29:14.438212 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:29:14.439074 139650252134208 py_utils.py:1229] WARNING!!! var weight_2 is using the default xavier initializer. Make sure this is intended.
I1002 01:29:14.445481 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var on /job:local/replica:0/task:0/device:CPU:0 6757171208
I1002 01:29:14.447633 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:29:14.448507 139650252134208 py_utils.py:1229] WARNING!!! var weight_3 is using the default xavier initializer. Make sure this is intended.
I1002 01:29:14.455074 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var on /job:local/replica:0/task:0/device:CPU:0 6773555208
I1002 01:29:14.457245 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:29:14.458119 139650252134208 py_utils.py:1229] WARNING!!! var weight_4 is using the default xavier initializer. Make sure this is intended.
I1002 01:29:14.464484 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var on /job:local/replica:0/task:0/device:CPU:0 6789939208
I1002 01:29:14.466602 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:29:14.467493 139650252134208 py_utils.py:1229] WARNING!!! var weight_5 is using the default xavier initializer. Make sure this is intended.
I1002 01:29:14.473931 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var on /job:local/replica:0/task:0/device:CPU:0 6806323208
I1002 01:29:14.476077 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:29:14.476949 139650252134208 py_utils.py:1229] WARNING!!! var weight_6 is using the default xavier initializer. Make sure this is intended.
I1002 01:29:14.483312 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var on /job:local/replica:0/task:0/device:CPU:0 6822707208
I1002 01:29:14.485428 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:29:14.486298 139650252134208 py_utils.py:1229] WARNING!!! var weight_7 is using the default xavier initializer. Make sure this is intended.
I1002 01:29:14.492726 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var on /job:local/replica:0/task:0/device:CPU:0 6839091208
I1002 01:29:14.494881 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:29:14.495780 139650252134208 py_utils.py:1229] WARNING!!! var weight_8 is using the default xavier initializer. Make sure this is intended.
I1002 01:29:14.502094 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var on /job:local/replica:0/task:0/device:CPU:0 6855475208
I1002 01:29:14.504251 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:29:14.505119 139650252134208 py_utils.py:1229] WARNING!!! var weight_9 is using the default xavier initializer. Make sure this is intended.
I1002 01:29:14.512088 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var on /job:local/replica:0/task:0/device:CPU:0 6871859208
I1002 01:29:14.514211 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:29:14.515084 139650252134208 py_utils.py:1229] WARNING!!! var weight_10 is using the default xavier initializer. Make sure this is intended.
I1002 01:29:14.521436 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var on /job:local/replica:0/task:0/device:CPU:0 6888243208
I1002 01:29:14.523578 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:29:14.524449 139650252134208 py_utils.py:1229] WARNING!!! var weight_11 is using the default xavier initializer. Make sure this is intended.
I1002 01:29:14.530877 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var on /job:local/replica:0/task:0/device:CPU:0 6904627208
I1002 01:29:14.533015 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:29:14.533881 139650252134208 py_utils.py:1229] WARNING!!! var weight_12 is using the default xavier initializer. Make sure this is intended.
I1002 01:29:14.540229 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var on /job:local/replica:0/task:0/device:CPU:0 6921011208
I1002 01:29:14.542331 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:29:14.543186 139650252134208 py_utils.py:1229] WARNING!!! var weight_13 is using the default xavier initializer. Make sure this is intended.
I1002 01:29:14.549578 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var on /job:local/replica:0/task:0/device:CPU:0 6937395208
I1002 01:29:14.551703 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:29:14.552570 139650252134208 py_utils.py:1229] WARNING!!! var weight_14 is using the default xavier initializer. Make sure this is intended.
I1002 01:29:14.558889 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var on /job:local/replica:0/task:0/device:CPU:0 6953779208
I1002 01:29:14.561033 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1002 01:29:14.561895 139650252134208 py_utils.py:1229] WARNING!!! var weight_15 is using the default xavier initializer. Make sure this is intended.
I1002 01:29:14.568312 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var on /job:local/replica:0/task:0/device:CPU:0 6970163208
I1002 01:29:14.570426 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:14.573309 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var on /job:local/replica:0/task:0/device:CPU:0 6970171208
I1002 01:29:14.575267 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:14.578076 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var on /job:local/replica:0/task:0/device:CPU:0 6970179208
I1002 01:29:14.579933 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:14.583325 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var on /job:local/replica:0/task:0/device:CPU:0 6970187208
I1002 01:29:14.585167 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:14.587981 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var on /job:local/replica:0/task:0/device:CPU:0 6970195208
I1002 01:29:14.589820 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:14.592753 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var on /job:local/replica:0/task:0/device:CPU:0 6970203208
I1002 01:29:14.594587 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:14.597413 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var on /job:local/replica:0/task:0/device:CPU:0 6970211208
I1002 01:29:14.599209 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:14.602104 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var on /job:local/replica:0/task:0/device:CPU:0 6970219208
I1002 01:29:14.603960 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:14.606761 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var on /job:local/replica:0/task:0/device:CPU:0 6970227208
I1002 01:29:14.608720 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:14.611661 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var on /job:local/replica:0/task:0/device:CPU:0 6970235208
I1002 01:29:14.613496 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:14.616425 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var on /job:local/replica:0/task:0/device:CPU:0 6970243208
I1002 01:29:14.618265 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:14.621079 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var on /job:local/replica:0/task:0/device:CPU:0 6970251208
I1002 01:29:14.622915 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:14.625830 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var on /job:local/replica:0/task:0/device:CPU:0 6970259208
I1002 01:29:14.627688 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:14.630463 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var on /job:local/replica:0/task:0/device:CPU:0 6970267208
I1002 01:29:14.632308 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:14.635248 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var on /job:local/replica:0/task:0/device:CPU:0 6970275208
I1002 01:29:14.637103 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:14.639914 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var on /job:local/replica:0/task:0/device:CPU:0 6970283208
I1002 01:29:14.641845 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:14.644667 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var on /job:local/replica:0/task:0/device:CPU:0 6970291208
I1002 01:29:14.646496 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:15.314599 139650252134208 py_utils.py:1484] === worker 0 ===
I1002 01:29:15.331215 139650252134208 py_utils.py:1474] worker 0: global_step                                                           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.331412 139650252134208 py_utils.py:1474] worker 0: input._tokenizer_default.global_step                                  /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.331503 139650252134208 py_utils.py:1474] worker 0: input.global_step                                                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.331582 139650252134208 py_utils.py:1474] worker 0: learners[0].global_step                                               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.331658 139650252134208 py_utils.py:1474] worker 0: learners[0].lr_schedule.global_step                                   /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.331731 139650252134208 py_utils.py:1474] worker 0: learners[0].optimizer.global_step                                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.331813 139650252134208 py_utils.py:1474] worker 0: lm.global_step                                                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.331885 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.emb.global_step                                       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.331957 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.emb.src_dropout.global_step                           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.332028 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.emb.src_pos_emb.global_step                           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.332098 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.emb.src_token_emb.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.332169 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.emb.src_token_emb.wm                                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.332244 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.332314 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.332390 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.332461 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.332531 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.332602 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.332672 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.332743 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.332812 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.332883 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.332952 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.333023 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.333094 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.333169 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.333241 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.global_step                                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.333312 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.333382 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.333452 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.333522 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.333593 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.333663 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.333733 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.333803 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.333874 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.333945 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.334015 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.334085 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.global_step                      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.334156 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.334225 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.334295 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.334366 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.334440 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.334511 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.334581 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.334651 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.334721 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.334790 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.334860 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.334930 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.335001 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.335071 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.335141 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.335211 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.335281 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.335366 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.335438 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.global_step                                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.335508 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.335579 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.335649 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.335718 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.335792 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.335862 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.335932 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.336002 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.336072 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.336141 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.336211 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.336280 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.global_step                      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.336349 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.336419 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.336488 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.336558 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.336627 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.336696 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.336765 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.336835 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.336904 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.336974 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.337048 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.337118 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.337189 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.337259 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.337330 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.337400 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.337470 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.337540 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.337610 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.global_step                                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.337679 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.337749 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.337819 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.337888 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.337958 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.338028 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.338100 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.338176 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.338245 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.338315 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.338388 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.338459 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.global_step                      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.338529 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.338599 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.338670 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.338739 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.338809 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.338879 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.338958 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.339028 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.339098 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.339167 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.339237 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.339320 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.339393 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.339463 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.339533 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.339603 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.339673 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.339748 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.339819 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.global_step                                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.339889 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.339959 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.340029 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.340099 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.340173 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.340242 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.340312 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.340381 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.340450 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.340520 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.340589 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.340658 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.global_step                      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.340728 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.340798 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.340867 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.340937 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.341010 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.341079 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.341149 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.341218 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.341297 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.341366 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.341435 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.341504 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.341573 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.341643 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.341712 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.341781 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.341851 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.341920 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.341989 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.global_step                                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.342058 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.342128 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.342198 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.342267 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.342340 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.342409 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.342478 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.342548 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.342617 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.342686 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.342755 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.342824 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.global_step                      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.342894 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.342963 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.343032 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.343101 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.343171 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.343240 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.343327 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.343400 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.343470 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.343539 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.343612 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.343683 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.343754 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.343823 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.343893 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.343962 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.344031 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.344100 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.344169 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.global_step                                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.344238 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.344307 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.344377 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.344446 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.344515 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.344585 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.344654 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.344724 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.344793 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.344862 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.344934 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.345004 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.global_step                      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.345073 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.345143 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.345212 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.345282 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.345351 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.345421 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.345491 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.345560 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.345629 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.345698 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.345767 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.345836 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.345906 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.345975 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.346045 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.346114 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.346187 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.346257 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.346327 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.global_step                                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.346397 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.346467 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.346537 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.346606 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.346677 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.346746 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.346817 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.346886 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.346956 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.347025 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.347095 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.347164 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.global_step                      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.347234 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.347316 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.347388 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.347458 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.347531 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.347600 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.347670 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.347739 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.347809 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.347878 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.347947 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.348028 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.348097 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.348169 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.348238 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.348308 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.348377 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.348446 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.348516 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.global_step                                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.348586 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.348655 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.348725 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.348798 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.348869 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.348938 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.349008 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.349077 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.349147 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.349216 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.349285 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.349354 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.global_step                      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.349424 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.349493 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.349562 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.349632 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.349702 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_0.global_step                                           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.349771 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.349840 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.349910 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.349980 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.350050 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.350124 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.350194 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.350264 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.350335 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.350404 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.350474 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.350544 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.350614 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.350684 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.350754 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.350824 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.350893 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.350963 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.351033 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.351104 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.351173 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.351243 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.351324 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.351401 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.351472 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.351543 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.351613 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.351688 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.351758 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.351828 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.351898 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.351969 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.352039 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.352109 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.352179 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.352249 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.352319 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.352389 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.352459 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.352529 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.352599 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.352670 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.352744 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.352814 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.352885 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.352955 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.353025 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.353096 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.353166 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.353236 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.353306 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.353376 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.353446 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.353516 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.353587 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.353656 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.353726 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.353796 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.353866 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.353935 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.354009 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.354081 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.354152 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.354222 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.354293 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.354363 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.354433 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.354503 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.354573 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.354644 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.354714 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.354784 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.354854 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.354924 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.354993 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.355062 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.355132 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.355202 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.355273 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.355365 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.355437 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.355508 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.355578 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.355648 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.355718 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.355788 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.355859 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.355928 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.355998 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.356069 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.356139 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.356209 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.356279 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.356350 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.356420 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.356491 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.356561 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.356635 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.356706 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.356777 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.356847 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.356917 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.356988 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.357058 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.357128 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.357198 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.357268 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.357338 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.357409 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.357479 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.357549 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.357619 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.357689 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.357760 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.357831 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.357901 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.357976 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.358047 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.358118 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.358191 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.358262 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.358333 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.358405 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.358476 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.358547 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.358617 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.358686 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.358757 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.358828 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.358897 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.358968 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.359039 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.359109 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.359179 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.359250 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.359336 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.359409 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.359480 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.359551 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.359622 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.359692 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.359762 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.359833 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.359904 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.359975 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.360044 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.360115 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.360185 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.360255 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.360324 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.360395 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.360465 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.360535 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.360609 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.360679 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.360749 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.360819 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.360890 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.360960 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.361030 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.361100 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.361170 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.361240 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.361310 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.361381 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.361451 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.361521 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.361593 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.361663 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.361734 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.361804 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.361875 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.361949 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.362020 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.362090 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.362160 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.362230 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.362300 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.362370 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.362445 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.362515 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.362585 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.362654 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.362725 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.362795 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.362864 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.362934 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.363003 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.363073 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.363143 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.363218 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.363289 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.363375 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.363446 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.363517 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.363587 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.363658 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.363729 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.363799 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.363869 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.363940 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.global_step                                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.364011 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.364081 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.364151 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.364220 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.364291 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.364361 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.364430 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.364501 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.364575 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.364646 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.364716 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.364786 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.global_step                      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.364857 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.364927 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.364998 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.365067 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.365137 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.365208 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.365277 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.365347 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.365417 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.365487 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.365557 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.365626 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.365696 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.365766 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.365841 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.365911 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.365981 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.366052 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.366122 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.global_step                                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.366192 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.366262 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.366333 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.366403 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.366473 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.366543 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.366613 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.366683 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.366753 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.366824 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.366894 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.366965 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.global_step                      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.367035 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.367105 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.367178 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.367249 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.367335 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_1.global_step                                           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.367409 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.367479 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.367550 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.367619 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.367689 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.367758 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.367828 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.367898 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.367969 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.368039 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.368109 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.368179 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.368251 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.368321 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.368391 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.368465 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.368535 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.368605 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.368676 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.368746 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.368816 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.368886 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.368956 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.369025 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.369095 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.369165 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.369235 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.369304 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.369375 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.369445 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.369516 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.369585 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.369655 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.369725 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.369799 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.369869 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.369939 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.370009 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.370079 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.370148 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.370218 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.370288 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.370357 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.370426 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.370496 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.370566 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.370636 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.370706 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.370775 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.370845 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.370915 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.370985 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.371058 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.371128 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.371198 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.371268 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.371351 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.371423 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.371493 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.371563 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.371634 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.371703 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.371773 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.371842 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.371912 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.371981 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.372051 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.372121 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.372190 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.372260 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.372329 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.372403 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.372474 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.372544 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.372614 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.372684 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.372754 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.372823 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.372894 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.372964 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.373034 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.373104 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.373174 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.373244 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.373314 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.373384 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.373454 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.373524 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.373594 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.373668 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.373739 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.373810 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.373881 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.373951 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.374022 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.374092 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.374163 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.374234 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.374304 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.374374 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.374445 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.374516 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.374586 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.374655 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.374727 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.374797 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.374867 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.374938 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.375012 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.375083 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.375157 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.375228 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.375309 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.375384 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.375454 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.375525 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.375595 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.375665 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.375735 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.375805 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.375875 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.375945 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.376015 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.376085 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.376155 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.376225 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.376300 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.376371 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.376441 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.376512 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.376582 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.376652 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.376722 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.376793 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.376862 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.376932 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.377002 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.377072 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.377141 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.377213 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.377282 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.377352 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.377422 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.377492 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.377563 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.377638 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.377708 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.377779 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.377849 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.377918 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.377988 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.378058 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.378127 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.378197 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.378266 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.378339 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.378409 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.378479 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.378549 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.378618 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.378689 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.378759 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.378828 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.378898 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.378973 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.379044 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.379114 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.379185 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.379255 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.379341 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.379414 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.379484 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.379554 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.379624 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.379694 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.379765 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.379835 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.379905 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.379974 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.380045 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.380115 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.380185 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.380258 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.380329 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.380399 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.380468 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.380538 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.380607 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.380677 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.380747 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.380816 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.380885 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.380955 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.381024 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.381094 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.381164 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.381233 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.381303 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.381373 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.381442 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.381512 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.381588 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.381659 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.381729 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.381799 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.381869 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.381938 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.382008 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.382077 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.382147 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.382216 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.382286 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.382356 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.382426 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.382496 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.382565 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.382635 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.382704 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.382774 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.382848 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.382918 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.382988 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.383058 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.383127 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.383197 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.383267 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.383351 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.383423 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.383492 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.383562 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.383632 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.383701 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.383771 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.383841 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.383910 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.383980 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.384049 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.384119 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.384193 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.384263 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.384332 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.384402 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.384471 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.384541 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.384609 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.384680 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.384749 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.384818 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.384888 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_2.global_step                                           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.384958 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.385028 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.385098 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.385168 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.385238 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.385308 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.385379 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.385453 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.385524 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.385595 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.385665 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.385735 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.385805 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.385875 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.385946 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.386016 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.386085 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.386155 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.386225 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.386295 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.386364 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.386434 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.386503 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.386573 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.386643 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.386713 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.386787 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.386858 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.386928 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.386998 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.387068 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.387138 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.387207 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.387277 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.387362 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.387433 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.387503 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.387573 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.387643 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.387713 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.387783 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.387853 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.387923 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.387994 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.388068 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.388140 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.388211 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.388281 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.388353 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.388424 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.388494 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.388565 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.388634 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.388704 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.388773 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.388843 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.388912 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.388981 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.389051 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.389121 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.389191 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.389261 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.389330 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.389404 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.389473 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.389544 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.389613 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.389683 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.389753 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.389822 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.389891 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.389961 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.390030 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.390099 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.390168 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.390238 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.390307 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.390377 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.390447 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.390517 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.390588 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.390663 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.390734 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.390805 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.390876 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.390946 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.391016 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.391086 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.391156 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.391227 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.391311 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.391386 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.391458 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.391527 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.391597 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.391668 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.391737 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.391808 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.391878 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.391947 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.392022 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.392092 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.392162 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.392231 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.392301 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.392370 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.392444 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.392514 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.392584 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.392653 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.392723 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.392793 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.392863 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.392932 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.393002 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.393072 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.393142 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.393211 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.393286 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.393357 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.393427 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.393497 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.393567 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.393635 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.393705 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.393775 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.393845 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.393914 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.393984 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.394053 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.394123 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.394192 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.394261 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.394331 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.394401 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.394470 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.394539 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.394613 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.394683 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.394752 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.394822 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.394891 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.394960 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.395029 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.395098 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.395167 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.395237 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.395318 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.395391 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.395460 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.395530 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.395600 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.395670 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.395740 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.395810 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.395883 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.395953 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.396023 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.396092 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.396161 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.396230 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.396299 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.396369 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.396439 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.396509 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.396579 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.396648 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.396717 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.396787 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.396856 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.396925 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.396995 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.397065 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.397135 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.397209 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.397279 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.397348 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.397418 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.397488 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.397557 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.397627 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.397696 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.397765 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.397835 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.397905 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.397974 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.398044 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.398113 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.398183 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.398252 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.398321 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.398393 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.398463 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.398538 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.398609 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.398680 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.398750 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.398820 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.398889 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.398959 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.399029 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.399098 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.399168 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.399238 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.399320 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.399393 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.399463 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.399532 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.399602 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.399672 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.399742 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.399817 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.399888 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.399958 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.400029 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.400098 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.400168 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.400238 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.400309 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.400379 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.400449 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.400519 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.400590 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.400660 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.400730 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.400801 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.400871 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.400941 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.401011 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.401080 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.401155 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.401225 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.401296 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.401366 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.401436 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.401506 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.401576 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.401646 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.401716 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.401786 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.401856 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.401926 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.401996 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.402066 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.402136 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.402207 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.402277 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.402347 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.402422 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.global_step                                           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.402493 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_0                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.402564 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_1                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.402634 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_10                                       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.402704 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_11                                       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.402774 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_12                                       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.402843 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_13                                       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.402914 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_14                                       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.402983 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_15                                       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.403053 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_2                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.403123 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_3                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.403193 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_4                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.403264 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_5                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.403351 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_6                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.403424 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_7                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.403494 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_8                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.403564 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_9                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.403635 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.global_step                                   /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.403705 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_0                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.403781 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_1                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.403852 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_10                                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.403921 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_11                                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.403991 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_12                                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.404061 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_13                                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.404131 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_14                                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.404200 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_15                                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.404270 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_2                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.404340 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_3                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.404410 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_4                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.404480 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_5                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.404551 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_6                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.404621 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_7                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.404691 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_8                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.404760 139650252134208 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_9                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.404829 139650252134208 py_utils.py:1474] worker 0: lm.stack.global_step                                                  /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1002 01:29:15.404929 139650252134208 py_utils.py:1490] ==========
I1002 01:29:17.636196 139650252134208 gpipe.py:457] cell 0 input [<tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/GatherV2_1:0' shape=(1024, 1) dtype=int32>, <tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/GatherV2_2:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None, None, None]
I1002 01:29:19.829243 139650252134208 gpipe.py:457] cell 1 input [<tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/encoder_7/add:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/GatherV2_2:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I1002 01:29:21.960101 139650252134208 gpipe.py:457] cell 2 input [<tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/encoder_15/add:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/GatherV2_2:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I1002 01:29:24.098898 139650252134208 gpipe.py:457] cell 3 input [<tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/encoder_23/add:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/GatherV2_2:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I1002 01:29:26.365289 139650252134208 gpipe.py:457] cell 0 input [<tf.Tensor 'arg259:0' shape=(1024, 1) dtype=int32>, <tf.Tensor 'arg260:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None, None, None]
W1002 01:29:29.350097 139650252134208 recurrent.py:886] cell_fn contains stateful ops: [('emb/Assert/Assert', 'Assert'), ('emb/Assert_1/Assert', 'Assert'), ('encoder_0/fflayer_0/encoder_0/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_0/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_0/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_1/fflayer_0/encoder_1/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_1/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_1/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_2/fflayer_0/encoder_2/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_2/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_2/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_3/fflayer_0/encoder_3/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_3/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_3/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_4/fflayer_0/encoder_4/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_4/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_4/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_5/fflayer_0/encoder_5/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_5/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_5/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_6/fflayer_0/encoder_6/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_6/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_6/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_7/fflayer_0/encoder_7/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_7/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_7/fflayer_1/Assert/AssertGuard/Assert', 'Assert')]
I1002 01:29:29.820389 139650252134208 gpipe.py:457] cell 1 input [<tf.Tensor 'arg254:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'arg255:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
W1002 01:29:31.678462 139650252134208 recurrent.py:886] cell_fn contains stateful ops: [('encoder_8/fflayer_0/encoder_8/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_8/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_8/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_9/fflayer_0/encoder_9/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_9/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_9/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_10/fflayer_0/encoder_10/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_10/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_10/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_11/fflayer_0/encoder_11/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_11/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_11/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_12/fflayer_0/encoder_12/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_12/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_12/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_13/fflayer_0/encoder_13/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_13/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_13/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_14/fflayer_0/encoder_14/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_14/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_14/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_15/fflayer_0/encoder_15/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_15/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_15/fflayer_1/Assert/AssertGuard/Assert', 'Assert')]
I1002 01:29:32.170320 139650252134208 gpipe.py:457] cell 2 input [<tf.Tensor 'arg254:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'arg255:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
W1002 01:29:34.047741 139650252134208 recurrent.py:886] cell_fn contains stateful ops: [('encoder_16/fflayer_0/encoder_16/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_16/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_16/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_17/fflayer_0/encoder_17/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_17/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_17/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_18/fflayer_0/encoder_18/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_18/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_18/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_19/fflayer_0/encoder_19/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_19/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_19/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_20/fflayer_0/encoder_20/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_20/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_20/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_21/fflayer_0/encoder_21/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_21/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_21/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_22/fflayer_0/encoder_22/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_22/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_22/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_23/fflayer_0/encoder_23/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_23/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_23/fflayer_1/Assert/AssertGuard/Assert', 'Assert')]
I1002 01:29:34.586078 139650252134208 gpipe.py:457] cell 3 input [<tf.Tensor 'arg286:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'arg287:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
W1002 01:29:36.451467 139650252134208 recurrent.py:886] cell_fn contains stateful ops: [('encoder_24/fflayer_0/encoder_24/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_24/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_24/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_25/fflayer_0/encoder_25/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_25/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_25/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_26/fflayer_0/encoder_26/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_26/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_26/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_27/fflayer_0/encoder_27/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_27/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_27/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_28/fflayer_0/encoder_28/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_28/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_28/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_29/fflayer_0/encoder_29/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_29/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_29/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_30/fflayer_0/encoder_30/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_30/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_30/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_31/fflayer_0/encoder_31/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_31/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_31/fflayer_1/Assert/AssertGuard/Assert', 'Assert')]
I1002 01:29:37.377543 139650252134208 gpipe.py:457] cell 0 input [<tf.Tensor 'arg259:0' shape=(1024, 1) dtype=int32>, <tf.Tensor 'arg260:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None, None, None]
I1002 01:29:39.944487 139650252134208 gpipe.py:457] cell 1 input [<tf.Tensor 'Recv_1:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'Recv_2:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I1002 01:29:42.431179 139650252134208 gpipe.py:457] cell 2 input [<tf.Tensor 'Recv_1:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'Recv_2:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I1002 01:29:46.801060 139650252134208 gpipe.py:457] cell 3 input [<tf.Tensor 'Recv_1:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'Recv_2:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I1002 01:29:48.876809 139650252134208 gpipe.py:548] pipeline output = [<tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/Reshape_2:0' shape=(1024, 32, 32000) dtype=float32>]
I1002 01:29:48.881210 139650252134208 layers.py:2786] Using sparse_softmax_cross_entropy_with_logits() in SimpleFullSoftmax::_FProp2D logits_shape=[32768, 32000]
I1002 01:29:48.971573 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/total_samples/var on /job:local/replica:0/task:0/device:CPU:0 6970291216
I1002 01:29:48.973486 139650252134208 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/total_samples/var:0 shape=() on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:29:48.981668 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var:0
I1002 01:29:48.981783 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var:0
I1002 01:29:48.981861 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var:0
I1002 01:29:48.981929 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var:0
I1002 01:29:48.981993 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var:0
I1002 01:29:48.982055 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var:0
I1002 01:29:48.982115 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var:0
I1002 01:29:48.982176 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1002 01:29:48.982238 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1002 01:29:48.982298 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1002 01:29:48.982359 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1002 01:29:48.982420 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1002 01:29:48.982480 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var:0
I1002 01:29:48.982551 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1002 01:29:48.982612 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var:0
I1002 01:29:48.982672 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1002 01:29:48.982732 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var:0
I1002 01:29:48.982791 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var:0
I1002 01:29:48.982851 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var:0
I1002 01:29:48.982910 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var:0
I1002 01:29:48.982969 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var:0
I1002 01:29:48.983029 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var:0
I1002 01:29:48.983088 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var:0
I1002 01:29:48.983148 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var:0
I1002 01:29:48.983207 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1002 01:29:48.983267 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1002 01:29:48.983344 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1002 01:29:48.983407 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1002 01:29:48.983466 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1002 01:29:48.983525 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var:0
I1002 01:29:48.983585 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1002 01:29:48.983644 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var:0
I1002 01:29:48.983704 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1002 01:29:48.983763 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var:0
I1002 01:29:48.983822 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var:0
I1002 01:29:48.983881 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var:0
I1002 01:29:48.983941 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var:0
I1002 01:29:48.984001 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var:0
I1002 01:29:48.984066 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var:0
I1002 01:29:48.984127 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var:0
I1002 01:29:48.984188 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var:0
I1002 01:29:48.984248 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1002 01:29:48.984313 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1002 01:29:48.984372 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1002 01:29:48.984432 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1002 01:29:48.984492 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1002 01:29:48.984551 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var:0
I1002 01:29:48.984610 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1002 01:29:48.984670 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var:0
I1002 01:29:48.984729 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1002 01:29:48.984788 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var:0
I1002 01:29:48.984848 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var:0
I1002 01:29:48.984908 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var:0
I1002 01:29:48.984967 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var:0
I1002 01:29:48.985027 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var:0
I1002 01:29:48.985086 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var:0
I1002 01:29:48.985145 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var:0
I1002 01:29:48.985203 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var:0
I1002 01:29:48.985263 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1002 01:29:48.985322 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1002 01:29:48.985382 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1002 01:29:48.985441 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1002 01:29:48.985506 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1002 01:29:48.985566 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var:0
I1002 01:29:48.985626 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1002 01:29:48.985686 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var:0
I1002 01:29:48.985745 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1002 01:29:48.985804 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var:0
I1002 01:29:48.985863 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var:0
I1002 01:29:48.985922 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var:0
I1002 01:29:48.985981 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var:0
I1002 01:29:48.986040 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var:0
I1002 01:29:48.986099 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var:0
I1002 01:29:48.986158 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var:0
I1002 01:29:48.986217 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var:0
I1002 01:29:48.986275 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1002 01:29:48.986335 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1002 01:29:48.986394 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1002 01:29:48.986453 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1002 01:29:48.986512 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1002 01:29:48.986571 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var:0
I1002 01:29:48.986630 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1002 01:29:48.986689 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var:0
I1002 01:29:48.986748 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1002 01:29:48.986806 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var:0
I1002 01:29:48.986865 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var:0
I1002 01:29:48.986924 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var:0
I1002 01:29:48.986989 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var:0
I1002 01:29:48.987050 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var:0
I1002 01:29:48.987108 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var:0
I1002 01:29:48.987167 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var:0
I1002 01:29:48.987226 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var:0
I1002 01:29:48.987285 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1002 01:29:48.987359 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1002 01:29:48.987420 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1002 01:29:48.987480 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1002 01:29:48.987539 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1002 01:29:48.987598 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var:0
I1002 01:29:48.987658 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1002 01:29:48.987716 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var:0
I1002 01:29:48.987775 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1002 01:29:48.987834 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var:0
I1002 01:29:48.987893 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var:0
I1002 01:29:48.987952 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var:0
I1002 01:29:48.988011 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var:0
I1002 01:29:48.988070 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var:0
I1002 01:29:48.988130 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var:0
I1002 01:29:48.988189 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var:0
I1002 01:29:48.988248 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var:0
I1002 01:29:48.988307 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1002 01:29:48.988366 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1002 01:29:48.988425 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1002 01:29:48.988489 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1002 01:29:48.988549 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1002 01:29:48.988609 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var:0
I1002 01:29:48.988667 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1002 01:29:48.988726 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var:0
I1002 01:29:48.988784 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1002 01:29:48.988843 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var:0
I1002 01:29:48.988901 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var:0
I1002 01:29:48.988960 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var:0
I1002 01:29:48.989019 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var:0
I1002 01:29:48.989078 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var:0
I1002 01:29:48.989136 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var:0
I1002 01:29:48.989195 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var:0
I1002 01:29:48.989253 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var:0
I1002 01:29:48.989312 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1002 01:29:48.989371 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1002 01:29:48.989430 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1002 01:29:48.989489 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1002 01:29:48.989547 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1002 01:29:48.989606 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var:0
I1002 01:29:48.989665 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1002 01:29:48.989724 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var:0
I1002 01:29:48.989782 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1002 01:29:48.989840 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var:0
I1002 01:29:48.989905 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var:0
I1002 01:29:48.989965 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var:0
I1002 01:29:48.990025 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var:0
I1002 01:29:48.990083 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var:0
I1002 01:29:48.990141 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var:0
I1002 01:29:48.990200 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var:0
I1002 01:29:48.990258 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var:0
I1002 01:29:48.990317 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1002 01:29:48.990376 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1002 01:29:48.990436 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1002 01:29:48.990495 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1002 01:29:48.990553 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1002 01:29:48.990612 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var:0
I1002 01:29:48.990671 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1002 01:29:48.990730 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var:0
I1002 01:29:48.990789 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1002 01:29:48.990852 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var:0
I1002 01:29:48.990911 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var:0
I1002 01:29:48.990970 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var:0
I1002 01:29:48.991029 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var:0
I1002 01:29:48.991088 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var:0
I1002 01:29:48.991146 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var:0
I1002 01:29:48.991205 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var:0
I1002 01:29:48.991263 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var:0
I1002 01:29:48.991339 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1002 01:29:48.991407 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1002 01:29:48.991470 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1002 01:29:48.991531 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1002 01:29:48.991591 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1002 01:29:48.991650 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var:0
I1002 01:29:48.991709 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1002 01:29:48.991768 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var:0
I1002 01:29:48.991827 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1002 01:29:48.991885 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var:0
I1002 01:29:48.991945 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var:0
I1002 01:29:48.992004 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var:0
I1002 01:29:48.992063 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var:0
I1002 01:29:48.992122 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var:0
I1002 01:29:48.992181 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var:0
I1002 01:29:48.992239 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var:0
I1002 01:29:48.992298 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var:0
I1002 01:29:48.992358 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1002 01:29:48.992418 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1002 01:29:48.992478 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1002 01:29:48.992537 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1002 01:29:48.992596 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1002 01:29:48.992655 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var:0
I1002 01:29:48.992714 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1002 01:29:48.992773 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var:0
I1002 01:29:48.992838 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1002 01:29:48.992899 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var:0
I1002 01:29:48.992958 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var:0
I1002 01:29:48.993018 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var:0
I1002 01:29:48.993082 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var:0
I1002 01:29:48.993143 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var:0
I1002 01:29:48.993202 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var:0
I1002 01:29:48.993261 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var:0
I1002 01:29:48.993320 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var:0
I1002 01:29:48.993380 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1002 01:29:48.993440 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1002 01:29:48.993499 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1002 01:29:48.993559 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1002 01:29:48.993619 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1002 01:29:48.993678 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var:0
I1002 01:29:48.993745 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1002 01:29:48.993804 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var:0
I1002 01:29:48.993864 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1002 01:29:48.993924 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var:0
I1002 01:29:48.993983 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var:0
I1002 01:29:48.994043 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var:0
I1002 01:29:48.994102 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var:0
I1002 01:29:48.994163 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var:0
I1002 01:29:48.994222 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var:0
I1002 01:29:48.994288 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var:0
I1002 01:29:48.994348 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var:0
I1002 01:29:48.994407 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1002 01:29:48.994467 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1002 01:29:48.994526 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1002 01:29:48.994586 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1002 01:29:48.994645 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1002 01:29:48.994704 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var:0
I1002 01:29:48.994764 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1002 01:29:48.994823 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var:0
I1002 01:29:48.994882 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1002 01:29:48.994942 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var:0
I1002 01:29:48.995000 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var:0
I1002 01:29:48.995059 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var:0
I1002 01:29:48.995118 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var:0
I1002 01:29:48.995177 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var:0
I1002 01:29:48.995236 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var:0
I1002 01:29:48.995308 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var:0
I1002 01:29:48.995372 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var:0
I1002 01:29:48.995433 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1002 01:29:48.995493 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1002 01:29:48.995553 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1002 01:29:48.995612 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1002 01:29:48.995671 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1002 01:29:48.995731 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var:0
I1002 01:29:48.995795 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1002 01:29:48.995855 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var:0
I1002 01:29:48.995914 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1002 01:29:48.995974 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var:0
I1002 01:29:48.996033 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var:0
I1002 01:29:48.996092 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var:0
I1002 01:29:48.996152 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var:0
I1002 01:29:48.996211 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var:0
I1002 01:29:48.996270 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var:0
I1002 01:29:48.996328 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var:0
I1002 01:29:48.996387 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var:0
I1002 01:29:48.996447 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1002 01:29:48.996511 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1002 01:29:48.996572 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1002 01:29:48.996631 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1002 01:29:48.996690 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1002 01:29:48.996749 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var:0
I1002 01:29:48.996816 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1002 01:29:48.996875 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var:0
I1002 01:29:48.996934 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1002 01:29:48.996994 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var:0
I1002 01:29:48.997053 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var:0
I1002 01:29:48.997114 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var:0
I1002 01:29:48.997173 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var:0
I1002 01:29:48.997238 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var:0
I1002 01:29:48.997298 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var:0
I1002 01:29:48.997357 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var:0
I1002 01:29:48.997416 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var:0
I1002 01:29:48.997475 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1002 01:29:48.997535 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1002 01:29:48.997594 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1002 01:29:48.997654 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1002 01:29:48.997714 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1002 01:29:48.997773 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var:0
I1002 01:29:48.997832 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1002 01:29:48.997891 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var:0
I1002 01:29:48.997950 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1002 01:29:48.998009 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var:0
I1002 01:29:48.998068 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var:0
I1002 01:29:48.998128 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var:0
I1002 01:29:48.998187 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var:0
I1002 01:29:48.998247 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var:0
I1002 01:29:48.998306 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var:0
I1002 01:29:48.998366 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var:0
I1002 01:29:48.998425 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var:0
I1002 01:29:48.998484 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1002 01:29:48.998544 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1002 01:29:48.998604 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1002 01:29:48.998663 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1002 01:29:48.998727 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1002 01:29:48.998799 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var:0
I1002 01:29:48.998858 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1002 01:29:48.998917 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var:0
I1002 01:29:48.998976 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1002 01:29:48.999036 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var:0
I1002 01:29:48.999095 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var:0
I1002 01:29:48.999156 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var:0
I1002 01:29:48.999215 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var:0
I1002 01:29:48.999274 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var:0
I1002 01:29:48.999350 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var:0
I1002 01:29:48.999412 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var:0
I1002 01:29:48.999472 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var:0
I1002 01:29:48.999531 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1002 01:29:48.999592 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1002 01:29:48.999652 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1002 01:29:48.999712 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1002 01:29:48.999783 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1002 01:29:48.999844 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var:0
I1002 01:29:48.999904 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1002 01:29:48.999964 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var:0
I1002 01:29:49.000024 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1002 01:29:49.000084 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var:0
I1002 01:29:49.000144 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var:0
I1002 01:29:49.000209 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var:0
I1002 01:29:49.000272 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var:0
I1002 01:29:49.000331 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var:0
I1002 01:29:49.000391 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var:0
I1002 01:29:49.000450 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var:0
I1002 01:29:49.000510 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var:0
I1002 01:29:49.000570 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1002 01:29:49.000631 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1002 01:29:49.000692 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1002 01:29:49.000753 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1002 01:29:49.000813 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1002 01:29:49.000873 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var:0
I1002 01:29:49.000934 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1002 01:29:49.000994 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var:0
I1002 01:29:49.001054 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1002 01:29:49.001114 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var:0
I1002 01:29:49.001173 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var:0
I1002 01:29:49.001233 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var:0
I1002 01:29:49.001292 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var:0
I1002 01:29:49.001351 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var:0
I1002 01:29:49.001411 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var:0
I1002 01:29:49.001471 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var:0
I1002 01:29:49.001532 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var:0
I1002 01:29:49.001592 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1002 01:29:49.001652 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1002 01:29:49.001718 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1002 01:29:49.001779 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1002 01:29:49.001839 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1002 01:29:49.001899 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var:0
I1002 01:29:49.001959 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1002 01:29:49.002019 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var:0
I1002 01:29:49.002078 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1002 01:29:49.002138 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var:0
I1002 01:29:49.002197 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var:0
I1002 01:29:49.002256 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var:0
I1002 01:29:49.002315 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var:0
I1002 01:29:49.002374 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var:0
I1002 01:29:49.002434 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var:0
I1002 01:29:49.002493 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var:0
I1002 01:29:49.002552 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var:0
I1002 01:29:49.002611 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1002 01:29:49.002671 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1002 01:29:49.002731 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1002 01:29:49.002792 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1002 01:29:49.002851 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1002 01:29:49.002910 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var:0
I1002 01:29:49.002970 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1002 01:29:49.003029 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var:0
I1002 01:29:49.003087 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1002 01:29:49.003151 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var:0
I1002 01:29:49.003211 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var:0
I1002 01:29:49.003270 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var:0
I1002 01:29:49.003347 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var:0
I1002 01:29:49.003409 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var:0
I1002 01:29:49.003468 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var:0
I1002 01:29:49.003528 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var:0
I1002 01:29:49.003587 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var:0
I1002 01:29:49.003647 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1002 01:29:49.003708 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1002 01:29:49.003767 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1002 01:29:49.003828 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1002 01:29:49.003888 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1002 01:29:49.003947 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var:0
I1002 01:29:49.004007 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1002 01:29:49.004067 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var:0
I1002 01:29:49.004127 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1002 01:29:49.004188 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var:0
I1002 01:29:49.004248 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var:0
I1002 01:29:49.004308 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var:0
I1002 01:29:49.004368 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var:0
I1002 01:29:49.004428 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var:0
I1002 01:29:49.004487 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var:0
I1002 01:29:49.004547 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var:0
I1002 01:29:49.004606 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var:0
I1002 01:29:49.004671 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1002 01:29:49.004732 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1002 01:29:49.004792 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1002 01:29:49.004851 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1002 01:29:49.004910 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1002 01:29:49.004970 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var:0
I1002 01:29:49.005029 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1002 01:29:49.005089 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var:0
I1002 01:29:49.005148 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1002 01:29:49.005207 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var:0
I1002 01:29:49.005266 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var:0
I1002 01:29:49.005326 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var:0
I1002 01:29:49.005385 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var:0
I1002 01:29:49.005445 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var:0
I1002 01:29:49.005504 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var:0
I1002 01:29:49.005563 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var:0
I1002 01:29:49.005622 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var:0
I1002 01:29:49.005682 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1002 01:29:49.005742 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1002 01:29:49.005801 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1002 01:29:49.005860 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1002 01:29:49.005919 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1002 01:29:49.005979 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var:0
I1002 01:29:49.006037 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1002 01:29:49.006102 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var:0
I1002 01:29:49.006162 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1002 01:29:49.006222 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var:0
I1002 01:29:49.006281 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var:0
I1002 01:29:49.006340 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var:0
I1002 01:29:49.006399 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var:0
I1002 01:29:49.006458 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var:0
I1002 01:29:49.006518 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var:0
I1002 01:29:49.006577 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var:0
I1002 01:29:49.006636 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var:0
I1002 01:29:49.006695 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1002 01:29:49.006759 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1002 01:29:49.006818 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1002 01:29:49.006879 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1002 01:29:49.006937 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1002 01:29:49.006996 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var:0
I1002 01:29:49.007055 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1002 01:29:49.007115 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var:0
I1002 01:29:49.007174 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1002 01:29:49.007234 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var:0
I1002 01:29:49.007295 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var:0
I1002 01:29:49.007370 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var:0
I1002 01:29:49.007431 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var:0
I1002 01:29:49.007490 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var:0
I1002 01:29:49.007550 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var:0
I1002 01:29:49.007614 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var:0
I1002 01:29:49.007675 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var:0
I1002 01:29:49.007734 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1002 01:29:49.007794 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1002 01:29:49.007854 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1002 01:29:49.007914 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1002 01:29:49.007973 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1002 01:29:49.008033 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var:0
I1002 01:29:49.008093 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1002 01:29:49.008153 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var:0
I1002 01:29:49.008214 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1002 01:29:49.008278 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var:0
I1002 01:29:49.008338 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var:0
I1002 01:29:49.008398 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var:0
I1002 01:29:49.008457 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var:0
I1002 01:29:49.008517 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var:0
I1002 01:29:49.008577 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var:0
I1002 01:29:49.008637 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var:0
I1002 01:29:49.008696 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var:0
I1002 01:29:49.008756 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1002 01:29:49.008816 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1002 01:29:49.008876 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1002 01:29:49.008937 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1002 01:29:49.008997 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1002 01:29:49.009068 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var:0
I1002 01:29:49.009130 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1002 01:29:49.009190 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var:0
I1002 01:29:49.009249 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1002 01:29:49.009309 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var:0
I1002 01:29:49.009369 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var:0
I1002 01:29:49.009429 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var:0
I1002 01:29:49.009489 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var:0
I1002 01:29:49.009549 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var:0
I1002 01:29:49.009608 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var:0
I1002 01:29:49.009669 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var:0
I1002 01:29:49.009728 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var:0
I1002 01:29:49.009788 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1002 01:29:49.009848 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1002 01:29:49.009908 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1002 01:29:49.009968 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1002 01:29:49.010028 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1002 01:29:49.010088 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var:0
I1002 01:29:49.010147 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1002 01:29:49.010206 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var:0
I1002 01:29:49.010266 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1002 01:29:49.010326 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var:0
I1002 01:29:49.010386 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var:0
I1002 01:29:49.010446 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var:0
I1002 01:29:49.010512 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var:0
I1002 01:29:49.010573 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var:0
I1002 01:29:49.010633 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var:0
I1002 01:29:49.010693 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var:0
I1002 01:29:49.010753 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var:0
I1002 01:29:49.010812 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1002 01:29:49.010872 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1002 01:29:49.010931 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1002 01:29:49.010991 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1002 01:29:49.011050 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1002 01:29:49.011110 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var:0
I1002 01:29:49.011170 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1002 01:29:49.011229 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var:0
I1002 01:29:49.011289 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1002 01:29:49.011363 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var:0
I1002 01:29:49.011424 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var:0
I1002 01:29:49.011484 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var:0
I1002 01:29:49.011543 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var:0
I1002 01:29:49.011605 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var:0
I1002 01:29:49.011665 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var:0
I1002 01:29:49.011724 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var:0
I1002 01:29:49.011783 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var:0
I1002 01:29:49.011843 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1002 01:29:49.011903 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1002 01:29:49.011963 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1002 01:29:49.012027 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1002 01:29:49.012089 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1002 01:29:49.012148 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var:0
I1002 01:29:49.012207 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1002 01:29:49.012267 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var:0
I1002 01:29:49.012327 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1002 01:29:49.012386 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var:0
I1002 01:29:49.012446 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var:0
I1002 01:29:49.012505 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var:0
I1002 01:29:49.012565 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var:0
I1002 01:29:49.012625 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var:0
I1002 01:29:49.012684 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var:0
I1002 01:29:49.012744 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var:0
I1002 01:29:49.012804 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var:0
I1002 01:29:49.012863 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1002 01:29:49.012923 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1002 01:29:49.012983 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1002 01:29:49.013042 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1002 01:29:49.013102 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1002 01:29:49.013161 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var:0
I1002 01:29:49.013220 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1002 01:29:49.013278 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var:0
I1002 01:29:49.013337 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1002 01:29:49.013397 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var:0
I1002 01:29:49.013462 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var:0
I1002 01:29:49.013522 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var:0
I1002 01:29:49.013581 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var:0
I1002 01:29:49.013641 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var:0
I1002 01:29:49.013701 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var:0
I1002 01:29:49.013760 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var:0
I1002 01:29:49.013820 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var:0
I1002 01:29:49.013879 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1002 01:29:49.013939 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1002 01:29:49.013998 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1002 01:29:49.014057 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1002 01:29:49.014117 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1002 01:29:49.014176 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var:0
I1002 01:29:49.014236 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1002 01:29:49.014296 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var:0
I1002 01:29:49.014355 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1002 01:29:49.014414 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var:0
I1002 01:29:49.014473 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var:0
I1002 01:29:49.014533 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var:0
I1002 01:29:49.014593 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var:0
I1002 01:29:49.014652 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var:0
I1002 01:29:49.014711 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var:0
I1002 01:29:49.014771 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var:0
I1002 01:29:49.014830 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var:0
I1002 01:29:49.014889 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var:0
I1002 01:29:49.014949 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var:0
I1002 01:29:49.015014 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var:0
I1002 01:29:49.015074 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var:0
I1002 01:29:49.015134 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var:0
I1002 01:29:49.015193 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var:0
I1002 01:29:49.015252 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var:0
I1002 01:29:49.015327 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var:0
I1002 01:29:49.015389 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var:0
I1002 01:29:49.015449 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var:0
I1002 01:29:49.015507 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var:0
I1002 01:29:49.015567 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var:0
I1002 01:29:49.015625 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var:0
I1002 01:29:49.015685 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var:0
I1002 01:29:49.015743 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var:0
I1002 01:29:49.015802 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var:0
I1002 01:29:49.015861 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var:0
I1002 01:29:49.015920 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var:0
I1002 01:29:49.015979 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var:0
I1002 01:29:49.016037 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var:0
I1002 01:29:49.016097 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var:0
I1002 01:29:49.016156 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var:0
I1002 01:29:49.016215 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var:0
I1002 01:29:49.016274 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var:0
I1002 01:29:49.016333 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var:0
I1002 01:29:49.016391 139650252134208 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var:0
I1002 01:29:53.609080 139650252134208 gpipe.py:457] cell 3 input [<tf.Tensor 'arg287:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'arg288:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I1002 01:30:00.689625 139650252134208 gpipe.py:457] cell 2 input [<tf.Tensor 'arg255:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'arg256:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I1002 01:30:05.658538 139650252134208 gpipe.py:457] cell 1 input [<tf.Tensor 'arg255:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'arg256:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I1002 01:30:12.635489 139650252134208 gpipe.py:457] cell 0 input [<tf.Tensor 'arg259:0' shape=(1024, 1) dtype=int32>, <tf.Tensor 'arg260:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None, None, None]
I1002 01:30:22.671686 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.emb.src_token_emb.wm: <tf.Variable '1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var:0' shape=(32000, 2048) dtype=float32_ref>
I1002 01:30:22.671978 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1002 01:30:22.672076 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1002 01:30:22.672165 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.672242 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1002 01:30:22.672320 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.672392 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.672462 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1002 01:30:22.672533 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.672609 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.672678 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.672751 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.672821 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.672893 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.672961 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.673035 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.673113 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.673184 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.673254 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1002 01:30:22.673323 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1002 01:30:22.673395 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.673465 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1002 01:30:22.673549 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.673619 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.673688 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1002 01:30:22.673758 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.673831 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.673901 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.673973 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.674042 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.674120 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.674189 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.674261 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.674329 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.674398 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.674467 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1002 01:30:22.674539 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1002 01:30:22.674611 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.674680 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1002 01:30:22.674752 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.674821 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.674889 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1002 01:30:22.674958 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.675031 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.675101 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.675179 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.675249 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.675342 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.675415 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.675488 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.675557 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.675627 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.675697 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1002 01:30:22.675765 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1002 01:30:22.675837 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.675907 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1002 01:30:22.675980 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.676063 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.676132 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1002 01:30:22.676202 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.676280 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.676351 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.676424 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.676494 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.676567 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.676637 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.676709 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.676778 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.676847 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.676915 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1002 01:30:22.676985 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1002 01:30:22.677057 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.677127 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1002 01:30:22.677200 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.677270 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.677345 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1002 01:30:22.677414 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.677487 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.677557 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.677630 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.677700 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.677772 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.677841 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.677914 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.677984 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.678052 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.678121 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1002 01:30:22.678190 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1002 01:30:22.678262 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.678340 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1002 01:30:22.678414 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.678484 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.678554 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1002 01:30:22.678623 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.678695 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.678765 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.678838 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.678907 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.678979 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.679052 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.679124 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.679193 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.679262 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.679345 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1002 01:30:22.679421 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1002 01:30:22.679496 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.679565 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1002 01:30:22.679638 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.679708 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.679777 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1002 01:30:22.679847 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.679919 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.679989 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.680062 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.680130 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.680202 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.680271 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.680344 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.680413 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.680487 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.680558 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1002 01:30:22.680628 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1002 01:30:22.680700 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.680770 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1002 01:30:22.680843 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.680912 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.680980 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1002 01:30:22.681050 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.681121 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.681191 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.681263 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.681332 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.681411 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.681484 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.681557 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.681626 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.681695 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.681764 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1002 01:30:22.681833 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1002 01:30:22.681906 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.681975 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1002 01:30:22.682048 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.682117 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.682188 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1002 01:30:22.682258 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.682331 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.682401 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.682474 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.682549 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.682622 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.682691 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.682764 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.682832 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.682901 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.682972 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1002 01:30:22.683041 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1002 01:30:22.683112 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.683182 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1002 01:30:22.683254 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.683338 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.683409 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1002 01:30:22.683478 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.683550 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.683624 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.683697 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.683766 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.683837 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.683906 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.683978 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.684045 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.684114 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.684183 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1002 01:30:22.684250 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1002 01:30:22.684322 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.684391 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1002 01:30:22.684461 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.684532 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.684600 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1002 01:30:22.684674 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.684747 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.684816 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.684887 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.684956 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.685028 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.685096 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.685167 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.685235 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.685302 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.685370 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1002 01:30:22.685439 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1002 01:30:22.685510 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.685578 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1002 01:30:22.685658 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.685728 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.685796 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1002 01:30:22.685865 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.685938 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.686007 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.686080 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.686150 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.686223 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.686294 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.686365 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.686434 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.686502 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.686571 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1002 01:30:22.686640 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1002 01:30:22.686716 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.686784 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1002 01:30:22.686856 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.686925 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.686994 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1002 01:30:22.687062 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.687135 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.687204 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.687276 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.687365 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.687439 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.687509 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.687581 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.687651 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.687726 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.687796 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1002 01:30:22.687866 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1002 01:30:22.687938 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.688008 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1002 01:30:22.688079 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.688148 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.688217 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1002 01:30:22.688286 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.688359 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.688429 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.688502 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.688571 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.688644 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.688714 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.688792 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.688862 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.688932 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.689001 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1002 01:30:22.689070 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1002 01:30:22.689142 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.689212 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1002 01:30:22.689284 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.689353 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.689422 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1002 01:30:22.689490 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.689562 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.689631 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.689704 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.689773 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.689850 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.689920 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.689992 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.690062 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.690131 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.690200 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1002 01:30:22.690269 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1002 01:30:22.690350 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.690419 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1002 01:30:22.690491 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.690561 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.690629 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1002 01:30:22.690698 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.690771 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.690840 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.690918 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.690987 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.691060 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.691129 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.691201 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.691270 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.691352 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.691422 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1002 01:30:22.691491 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1002 01:30:22.691565 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.691634 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1002 01:30:22.691705 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.691775 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.691845 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1002 01:30:22.691919 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.691993 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.692062 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.692134 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.692204 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.692276 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.692345 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.692416 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.692485 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.692554 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.692624 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1002 01:30:22.692693 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1002 01:30:22.692765 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.692833 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1002 01:30:22.692904 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.692979 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.693048 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1002 01:30:22.693116 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.693190 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.693259 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.693332 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.693401 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.693473 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.693542 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.693614 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.693681 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.693750 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.693818 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1002 01:30:22.693887 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1002 01:30:22.693964 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.694033 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1002 01:30:22.694106 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.694175 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.694250 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1002 01:30:22.694319 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.694392 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.694461 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.694533 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.694604 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.694676 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.694746 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.694818 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.694887 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.694957 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.695031 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1002 01:30:22.695104 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1002 01:30:22.695177 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.695247 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1002 01:30:22.695331 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.695403 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.695472 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1002 01:30:22.695542 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.695615 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.695684 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.695757 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.695825 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.695897 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.695966 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.696037 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.696111 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.696182 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.696250 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1002 01:30:22.696320 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1002 01:30:22.696391 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.696461 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1002 01:30:22.696533 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.696603 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.696672 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1002 01:30:22.696741 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.696814 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.696883 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.696955 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.697024 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.697101 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.697172 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.697244 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.697313 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.697382 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.697452 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1002 01:30:22.697521 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1002 01:30:22.697594 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.697663 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1002 01:30:22.697736 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.697805 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.697874 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1002 01:30:22.697943 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.698015 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.698085 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.698163 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.698232 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.698308 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.698376 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.698447 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.698515 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.698584 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.698652 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1002 01:30:22.698720 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1002 01:30:22.698793 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.698861 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1002 01:30:22.698933 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.699002 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.699070 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1002 01:30:22.699138 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.699216 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.699285 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.699382 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.699452 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.699524 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.699593 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.699666 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.699735 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.699803 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.699872 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1002 01:30:22.699940 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1002 01:30:22.700012 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.700081 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1002 01:30:22.700152 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.700226 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.700294 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1002 01:30:22.700362 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.700435 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.700504 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.700575 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.700643 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.700716 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.700785 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.700857 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.700925 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.700994 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.701063 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1002 01:30:22.701132 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1002 01:30:22.701205 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.701280 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1002 01:30:22.701351 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.701420 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.701489 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1002 01:30:22.701559 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.701631 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.701699 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.701772 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.701842 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.701915 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.701983 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.702056 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.702126 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.702195 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.702271 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1002 01:30:22.702340 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1002 01:30:22.702414 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.702484 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1002 01:30:22.702555 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.702624 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.702693 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1002 01:30:22.702761 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.702834 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.702903 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.702976 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.703045 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.703117 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.703185 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.703257 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.703344 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.703415 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.703484 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1002 01:30:22.703554 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1002 01:30:22.703626 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.703695 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1002 01:30:22.703767 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.703837 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.703906 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1002 01:30:22.703975 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.704048 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.704116 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.704188 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.704257 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.704328 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.704401 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.704473 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.704541 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.704609 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.704680 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1002 01:30:22.704749 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1002 01:30:22.704820 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.704889 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1002 01:30:22.704961 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.705029 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.705097 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1002 01:30:22.705165 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.705239 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.705307 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.705384 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.705453 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.705525 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.705592 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.705663 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.705731 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.705799 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.705867 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1002 01:30:22.705935 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1002 01:30:22.706007 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.706075 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1002 01:30:22.706147 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.706216 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.706284 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1002 01:30:22.706352 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.706429 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.706499 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.706572 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.706640 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.706712 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.706781 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.706853 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.706922 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.706990 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.707059 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1002 01:30:22.707128 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1002 01:30:22.707200 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.707267 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1002 01:30:22.707353 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.707424 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.707503 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1002 01:30:22.707573 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.707645 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.707715 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.707787 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.707856 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.707928 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.707997 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.708069 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.708139 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.708208 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.708277 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1002 01:30:22.708347 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1002 01:30:22.708420 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.708505 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1002 01:30:22.708578 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.708648 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.708718 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1002 01:30:22.708787 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.708860 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.708930 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.709002 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.709071 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.709148 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.709218 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.709290 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.709358 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.709428 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.709497 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1002 01:30:22.709571 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1002 01:30:22.709643 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.709711 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1002 01:30:22.709782 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.709850 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.709918 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1002 01:30:22.709988 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.710061 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.710130 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.710203 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.710273 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.710345 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.710414 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1002 01:30:22.710485 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.710559 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.710628 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1002 01:30:22.710697 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_0: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var:0' shape=(2000,) dtype=float32_ref>
I1002 01:30:22.710767 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_1: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var:0' shape=(2000,) dtype=float32_ref>
I1002 01:30:22.710835 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_10: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var:0' shape=(2000,) dtype=float32_ref>
I1002 01:30:22.710902 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_11: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var:0' shape=(2000,) dtype=float32_ref>
I1002 01:30:22.710972 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_12: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var:0' shape=(2000,) dtype=float32_ref>
I1002 01:30:22.711040 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_13: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var:0' shape=(2000,) dtype=float32_ref>
I1002 01:30:22.711103 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_14: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var:0' shape=(2000,) dtype=float32_ref>
I1002 01:30:22.711171 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_15: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var:0' shape=(2000,) dtype=float32_ref>
I1002 01:30:22.711239 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_2: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var:0' shape=(2000,) dtype=float32_ref>
I1002 01:30:22.711323 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_3: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var:0' shape=(2000,) dtype=float32_ref>
I1002 01:30:22.711395 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_4: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var:0' shape=(2000,) dtype=float32_ref>
I1002 01:30:22.711463 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_5: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var:0' shape=(2000,) dtype=float32_ref>
I1002 01:30:22.711531 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_6: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var:0' shape=(2000,) dtype=float32_ref>
I1002 01:30:22.711600 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_7: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var:0' shape=(2000,) dtype=float32_ref>
I1002 01:30:22.711668 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_8: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var:0' shape=(2000,) dtype=float32_ref>
I1002 01:30:22.711735 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_9: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var:0' shape=(2000,) dtype=float32_ref>
I1002 01:30:22.711803 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_0: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var:0' shape=(2048, 2000) dtype=float32_ref>
I1002 01:30:22.711881 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_1: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var:0' shape=(2048, 2000) dtype=float32_ref>
I1002 01:30:22.711955 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_10: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var:0' shape=(2048, 2000) dtype=float32_ref>
I1002 01:30:22.712027 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_11: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var:0' shape=(2048, 2000) dtype=float32_ref>
I1002 01:30:22.712099 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_12: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var:0' shape=(2048, 2000) dtype=float32_ref>
I1002 01:30:22.712170 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_13: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var:0' shape=(2048, 2000) dtype=float32_ref>
I1002 01:30:22.712242 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_14: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var:0' shape=(2048, 2000) dtype=float32_ref>
I1002 01:30:22.712313 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_15: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var:0' shape=(2048, 2000) dtype=float32_ref>
I1002 01:30:22.712384 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_2: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var:0' shape=(2048, 2000) dtype=float32_ref>
I1002 01:30:22.712457 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_3: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var:0' shape=(2048, 2000) dtype=float32_ref>
I1002 01:30:22.712529 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_4: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var:0' shape=(2048, 2000) dtype=float32_ref>
I1002 01:30:22.712601 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_5: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var:0' shape=(2048, 2000) dtype=float32_ref>
I1002 01:30:22.712673 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_6: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var:0' shape=(2048, 2000) dtype=float32_ref>
I1002 01:30:22.712744 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_7: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var:0' shape=(2048, 2000) dtype=float32_ref>
I1002 01:30:22.712816 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_8: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var:0' shape=(2048, 2000) dtype=float32_ref>
I1002 01:30:22.712887 139650252134208 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_9: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var:0' shape=(2048, 2000) dtype=float32_ref>
I1002 01:30:32.523236 139650252134208 learner.py:279] gradient_adjuster=<bound method LanguageModel.AdjustGradients of <lingvo.tasks.lm.model.FixedShapeInputLanguageModel object at 0x7f00c54f16a0>>
I1002 01:30:36.119831 139650252134208 cluster.py:515] Place variable beta1_power on /job:local/replica:0/task:0/device:CPU:0 6970291220
I1002 01:30:36.122910 139650252134208 cluster.py:515] Place variable beta2_power on /job:local/replica:0/task:0/device:CPU:0 6970291224
I1002 01:30:36.127692 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7232435224
I1002 01:30:36.132987 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7494579224
I1002 01:30:36.138076 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7494611992
I1002 01:30:36.143083 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7494644760
I1002 01:30:36.148239 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7561753624
I1002 01:30:36.153205 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7628862488
I1002 01:30:36.158285 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7628870680
I1002 01:30:36.163244 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7628878872
I1002 01:30:36.168342 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7695987736
I1002 01:30:36.173325 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7763096600
I1002 01:30:36.179108 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7763104792
I1002 01:30:36.184102 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7763112984
I1002 01:30:36.189169 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7763121176
I1002 01:30:36.194249 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7763129368
I1002 01:30:36.198047 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7763129880
I1002 01:30:36.201769 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7763130392
I1002 01:30:36.206735 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7779907608
I1002 01:30:36.211732 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7796684824
I1002 01:30:36.240042 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7796693016
I1002 01:30:36.245152 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7796701208
I1002 01:30:36.250235 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7813478424
I1002 01:30:36.255292 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7830255640
I1002 01:30:36.260294 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7830263832
I1002 01:30:36.265325 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7830272024
I1002 01:30:36.270274 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7847049240
I1002 01:30:36.275353 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7863826456
I1002 01:30:36.280322 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7863834648
I1002 01:30:36.285397 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7863842840
I1002 01:30:36.290435 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7880620056
I1002 01:30:36.295536 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7897397272
I1002 01:30:36.300497 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7897405464
I1002 01:30:36.305547 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7897413656
I1002 01:30:36.311073 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7897421848
I1002 01:30:36.316220 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7897430040
I1002 01:30:36.321293 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7897438232
I1002 01:30:36.326248 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7897446424
I1002 01:30:36.331284 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7897479192
I1002 01:30:36.336263 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7897511960
I1002 01:30:36.341338 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7964620824
I1002 01:30:36.346321 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8031729688
I1002 01:30:36.351411 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8031737880
I1002 01:30:36.356385 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8031746072
I1002 01:30:36.361428 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8098854936
I1002 01:30:36.366423 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8165963800
I1002 01:30:36.371491 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8165971992
I1002 01:30:36.376579 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8165980184
I1002 01:30:36.381530 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8165988376
I1002 01:30:36.386584 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8165996568
I1002 01:30:36.390383 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8165997080
I1002 01:30:36.394149 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8165997592
I1002 01:30:36.399118 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8182774808
I1002 01:30:36.404090 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8199552024
I1002 01:30:36.409131 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8199560216
I1002 01:30:36.414176 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8199568408
I1002 01:30:36.419154 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8216345624
I1002 01:30:36.424745 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8233122840
I1002 01:30:36.429702 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8233131032
I1002 01:30:36.434783 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8233139224
I1002 01:30:36.439751 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8249916440
I1002 01:30:36.444849 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8266693656
I1002 01:30:36.449818 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8266701848
I1002 01:30:36.454877 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8266710040
I1002 01:30:36.459889 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8283487256
I1002 01:30:36.464969 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8300264472
I1002 01:30:36.470044 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8300272664
I1002 01:30:36.474999 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8300280856
I1002 01:30:36.480135 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8300289048
I1002 01:30:36.485127 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8300297240
I1002 01:30:36.490188 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8300305432
I1002 01:30:36.495146 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8300313624
I1002 01:30:36.500255 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8300346392
I1002 01:30:36.505226 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8300379160
I1002 01:30:36.510318 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8367488024
I1002 01:30:36.515316 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8434596888
I1002 01:30:36.520399 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8434605080
I1002 01:30:36.525475 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8434613272
I1002 01:30:36.530455 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8501722136
I1002 01:30:36.536087 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8568831000
I1002 01:30:36.541057 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8568839192
I1002 01:30:36.546149 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8568847384
I1002 01:30:36.551264 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8568855576
I1002 01:30:36.556368 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8568863768
I1002 01:30:36.560202 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8568864280
I1002 01:30:36.563917 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8568864792
I1002 01:30:36.568909 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8585642008
I1002 01:30:36.573982 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8602419224
I1002 01:30:36.579052 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8602427416
I1002 01:30:36.584215 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8602435608
I1002 01:30:36.589250 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8619212824
I1002 01:30:36.594381 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8635990040
I1002 01:30:36.599420 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8635998232
I1002 01:30:36.604623 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8636006424
I1002 01:30:36.609635 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8652783640
I1002 01:30:36.614722 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8669560856
I1002 01:30:36.619746 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8669569048
I1002 01:30:36.624830 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8669577240
I1002 01:30:36.630000 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8686354456
I1002 01:30:36.635028 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8703131672
I1002 01:30:36.640155 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8703139864
I1002 01:30:36.645137 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8703148056
I1002 01:30:36.650812 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8703156248
I1002 01:30:36.655899 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8703164440
I1002 01:30:36.660967 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8703172632
I1002 01:30:36.665934 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8703180824
I1002 01:30:36.671042 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8703213592
I1002 01:30:36.676083 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8703246360
I1002 01:30:36.681169 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8770355224
I1002 01:30:36.686164 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8837464088
I1002 01:30:36.691259 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8837472280
I1002 01:30:36.696365 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8837480472
I1002 01:30:36.701335 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8904589336
I1002 01:30:36.706501 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8971698200
I1002 01:30:36.711513 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8971706392
I1002 01:30:36.716637 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8971714584
I1002 01:30:36.721659 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8971722776
I1002 01:30:36.726761 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8971730968
I1002 01:30:36.730614 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8971731480
I1002 01:30:36.734348 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8971731992
I1002 01:30:36.739431 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8988509208
I1002 01:30:36.744557 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9005286424
I1002 01:30:36.749527 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9005294616
I1002 01:30:36.754647 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9005302808
I1002 01:30:36.759679 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9022080024
I1002 01:30:36.765403 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9038857240
I1002 01:30:36.770408 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9038865432
I1002 01:30:36.775556 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9038873624
I1002 01:30:36.780564 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9055650840
I1002 01:30:36.785695 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9072428056
I1002 01:30:36.790684 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9072436248
I1002 01:30:36.795812 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9072444440
I1002 01:30:36.801270 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9089221656
I1002 01:30:36.806611 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9105998872
I1002 01:30:36.811806 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9106007064
I1002 01:30:36.816820 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9106015256
I1002 01:30:36.821875 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9106023448
I1002 01:30:36.826927 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9106031640
I1002 01:30:36.832050 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9106039832
I1002 01:30:36.837077 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9106048024
I1002 01:30:36.842217 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9106080792
I1002 01:30:36.847234 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9106113560
I1002 01:30:36.852411 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9173222424
I1002 01:30:36.857438 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9240331288
I1002 01:30:36.862540 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9240339480
I1002 01:30:36.867660 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9240347672
I1002 01:30:36.872662 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9307456536
I1002 01:30:36.878319 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9374565400
I1002 01:30:36.883343 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9374573592
I1002 01:30:36.888452 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9374581784
I1002 01:30:36.893438 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9374589976
I1002 01:30:36.898529 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9374598168
I1002 01:30:36.902385 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9374598680
I1002 01:30:36.906157 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9374599192
I1002 01:30:36.911233 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9391376408
I1002 01:30:36.916385 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9408153624
I1002 01:30:36.921406 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9408161816
I1002 01:30:36.926515 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9408170008
I1002 01:30:36.931563 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9424947224
I1002 01:30:36.936671 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9441724440
I1002 01:30:36.941733 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9441732632
I1002 01:30:36.946843 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9441740824
I1002 01:30:36.951857 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9458518040
I1002 01:30:36.956963 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9475295256
I1002 01:30:36.961944 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9475303448
I1002 01:30:36.967045 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9475311640
I1002 01:30:36.972167 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9492088856
I1002 01:30:36.977175 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9508866072
I1002 01:30:36.982294 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9508874264
I1002 01:30:36.987290 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9508882456
I1002 01:30:36.993014 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9508890648
I1002 01:30:36.998028 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9508898840
I1002 01:30:37.003134 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9508907032
I1002 01:30:37.008165 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9508915224
I1002 01:30:37.013290 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9508947992
I1002 01:30:37.018383 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9508980760
I1002 01:30:37.023495 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9576089624
I1002 01:30:37.028505 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9643198488
I1002 01:30:37.033595 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9643206680
I1002 01:30:37.038714 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9643214872
I1002 01:30:37.043738 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9710323736
I1002 01:30:37.048866 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9777432600
I1002 01:30:37.053861 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9777440792
I1002 01:30:37.058967 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9777448984
I1002 01:30:37.063987 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9777457176
I1002 01:30:37.069109 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9777465368
I1002 01:30:37.072945 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9777465880
I1002 01:30:37.076706 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9777466392
I1002 01:30:37.081744 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9794243608
I1002 01:30:37.086838 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9811020824
I1002 01:30:37.091850 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9811029016
I1002 01:30:37.096996 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9811037208
I1002 01:30:37.102009 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9827814424
I1002 01:30:37.107671 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9844591640
I1002 01:30:37.112679 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9844599832
I1002 01:30:37.117766 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9844608024
I1002 01:30:37.122777 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9861385240
I1002 01:30:37.127895 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9878162456
I1002 01:30:37.132983 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9878170648
I1002 01:30:37.138103 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9878178840
I1002 01:30:37.143216 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9894956056
I1002 01:30:37.148327 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9911733272
I1002 01:30:37.153441 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9911741464
I1002 01:30:37.158421 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9911749656
I1002 01:30:37.163525 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9911757848
I1002 01:30:37.168535 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9911766040
I1002 01:30:37.173673 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9911774232
I1002 01:30:37.178665 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9911782424
I1002 01:30:37.183841 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9911815192
I1002 01:30:37.188856 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9911847960
I1002 01:30:37.193920 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9978956824
I1002 01:30:37.198986 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10046065688
I1002 01:30:37.204099 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10046073880
I1002 01:30:37.209171 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10046082072
I1002 01:30:37.214141 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10113190936
I1002 01:30:37.219745 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10180299800
I1002 01:30:37.224884 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10180307992
I1002 01:30:37.229987 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10180316184
I1002 01:30:37.235004 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10180324376
I1002 01:30:37.240133 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10180332568
I1002 01:30:37.243992 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10180333080
I1002 01:30:37.247729 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10180333592
I1002 01:30:37.252784 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10197110808
I1002 01:30:37.257891 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10213888024
I1002 01:30:37.262894 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10213896216
I1002 01:30:37.268075 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10213904408
I1002 01:30:37.273081 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10230681624
I1002 01:30:37.278200 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10247458840
I1002 01:30:37.283216 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10247467032
I1002 01:30:37.288352 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10247475224
I1002 01:30:37.293368 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10264252440
I1002 01:30:37.298475 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10281029656
I1002 01:30:37.303521 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10281037848
I1002 01:30:37.308599 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10281046040
I1002 01:30:37.313730 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10297823256
I1002 01:30:37.318760 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10314600472
I1002 01:30:37.323877 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10314608664
I1002 01:30:37.328988 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10314616856
I1002 01:30:37.334597 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10314625048
I1002 01:30:37.339642 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10314633240
I1002 01:30:37.344758 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10314641432
I1002 01:30:37.349777 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10314649624
I1002 01:30:37.354877 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10314682392
I1002 01:30:37.359898 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10314715160
I1002 01:30:37.365008 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10381824024
I1002 01:30:37.370025 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10448932888
I1002 01:30:37.375117 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10448941080
I1002 01:30:37.380305 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10448949272
I1002 01:30:37.385292 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10516058136
I1002 01:30:37.390403 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10583167000
I1002 01:30:37.395444 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10583175192
I1002 01:30:37.400594 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10583183384
I1002 01:30:37.405596 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10583191576
I1002 01:30:37.410683 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10583199768
I1002 01:30:37.414536 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10583200280
I1002 01:30:37.418301 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10583200792
I1002 01:30:37.423344 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10599978008
I1002 01:30:37.428474 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10616755224
I1002 01:30:37.433487 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10616763416
I1002 01:30:37.438591 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10616771608
I1002 01:30:37.443634 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10633548824
I1002 01:30:37.449275 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10650326040
I1002 01:30:37.454304 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10650334232
I1002 01:30:37.459463 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10650342424
I1002 01:30:37.464484 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10667119640
I1002 01:30:37.469619 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10683896856
I1002 01:30:37.474651 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10683905048
I1002 01:30:37.479803 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10683913240
I1002 01:30:37.484953 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10700690456
I1002 01:30:37.490005 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10717467672
I1002 01:30:37.495115 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10717475864
I1002 01:30:37.500178 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10717484056
I1002 01:30:37.505300 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10717492248
I1002 01:30:37.510388 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10717500440
I1002 01:30:37.515547 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10717508632
I1002 01:30:37.520553 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10717516824
I1002 01:30:37.525676 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10717549592
I1002 01:30:37.530723 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10717582360
I1002 01:30:37.535847 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10784691224
I1002 01:30:37.540860 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10851800088
I1002 01:30:37.546000 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10851808280
I1002 01:30:37.551118 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10851816472
I1002 01:30:37.556309 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10918925336
I1002 01:30:37.561968 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10986034200
I1002 01:30:37.566991 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10986042392
I1002 01:30:37.572123 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10986050584
I1002 01:30:37.577131 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10986058776
I1002 01:30:37.582248 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10986066968
I1002 01:30:37.586138 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10986067480
I1002 01:30:37.589900 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10986067992
I1002 01:30:37.594959 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11002845208
I1002 01:30:37.600124 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11019622424
I1002 01:30:37.605147 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11019630616
I1002 01:30:37.610303 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11019638808
I1002 01:30:37.615366 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11036416024
I1002 01:30:37.620494 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11053193240
I1002 01:30:37.625514 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11053201432
I1002 01:30:37.630630 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11053209624
I1002 01:30:37.635734 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11069986840
I1002 01:30:37.640823 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11086764056
I1002 01:30:37.645914 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11086772248
I1002 01:30:37.651005 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11086780440
I1002 01:30:37.656179 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11103557656
I1002 01:30:37.661268 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11120334872
I1002 01:30:37.666409 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11120343064
I1002 01:30:37.671452 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11120351256
I1002 01:30:37.677051 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11120359448
I1002 01:30:37.682076 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11120367640
I1002 01:30:37.687223 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11120375832
I1002 01:30:37.692274 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11120384024
I1002 01:30:37.697398 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11120416792
I1002 01:30:37.702409 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11120449560
I1002 01:30:37.707581 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11187558424
I1002 01:30:37.712624 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11254667288
I1002 01:30:37.717733 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11254675480
I1002 01:30:37.722842 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11254683672
I1002 01:30:37.727866 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11321792536
I1002 01:30:37.732993 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11388901400
I1002 01:30:37.738026 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11388909592
I1002 01:30:37.743146 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11388917784
I1002 01:30:37.748183 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11388925976
I1002 01:30:37.753304 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11388934168
I1002 01:30:37.757190 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11388934680
I1002 01:30:37.760974 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11388935192
I1002 01:30:37.766024 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11405712408
I1002 01:30:37.771126 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11422489624
I1002 01:30:37.776180 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11422497816
I1002 01:30:37.781286 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11422506008
I1002 01:30:37.786307 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11439283224
I1002 01:30:37.791985 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11456060440
I1002 01:30:37.797018 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11456068632
I1002 01:30:37.802132 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11456076824
I1002 01:30:37.807153 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11472854040
I1002 01:30:37.812319 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11489631256
I1002 01:30:37.817346 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11489639448
I1002 01:30:37.822733 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11489647640
I1002 01:30:37.827990 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11506424856
I1002 01:30:37.833056 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11523202072
I1002 01:30:37.838178 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11523210264
I1002 01:30:37.843249 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11523218456
I1002 01:30:37.848384 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11523226648
I1002 01:30:37.853458 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11523234840
I1002 01:30:37.858595 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11523243032
I1002 01:30:37.863642 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11523251224
I1002 01:30:37.868782 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11523283992
I1002 01:30:37.873821 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11523316760
I1002 01:30:37.878936 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11590425624
I1002 01:30:37.884012 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11657534488
I1002 01:30:37.889216 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11657542680
I1002 01:30:37.894464 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11657550872
I1002 01:30:37.899529 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11724659736
I1002 01:30:37.905289 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11791768600
I1002 01:30:37.910350 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11791776792
I1002 01:30:37.915566 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11791784984
I1002 01:30:37.920623 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11791793176
I1002 01:30:37.925760 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11791801368
I1002 01:30:37.929685 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11791801880
I1002 01:30:37.933491 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11791802392
I1002 01:30:37.938610 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11808579608
I1002 01:30:37.943850 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11825356824
I1002 01:30:37.948925 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11825365016
I1002 01:30:37.954116 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11825373208
I1002 01:30:37.959185 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11842150424
I1002 01:30:37.964444 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11858927640
I1002 01:30:37.969897 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11858935832
I1002 01:30:37.975605 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11858944024
I1002 01:30:37.980754 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11875721240
I1002 01:30:37.986048 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11892498456
I1002 01:30:37.991189 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11892506648
I1002 01:30:37.996509 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11892514840
I1002 01:30:38.001740 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11909292056
I1002 01:30:38.006880 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11926069272
I1002 01:30:38.012130 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11926077464
I1002 01:30:38.017232 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11926085656
I1002 01:30:38.023015 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11926093848
I1002 01:30:38.028195 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11926102040
I1002 01:30:38.033364 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11926110232
I1002 01:30:38.038454 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11926118424
I1002 01:30:38.043717 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11926151192
I1002 01:30:38.048823 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11926183960
I1002 01:30:38.054011 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11993292824
I1002 01:30:38.059132 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12060401688
I1002 01:30:38.064343 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12060409880
I1002 01:30:38.069541 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12060418072
I1002 01:30:38.074660 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12127526936
I1002 01:30:38.079905 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12194635800
I1002 01:30:38.085511 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12194643992
I1002 01:30:38.091156 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12194652184
I1002 01:30:38.096414 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12194660376
I1002 01:30:38.101768 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12194668568
I1002 01:30:38.105713 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12194669080
I1002 01:30:38.109487 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12194669592
I1002 01:30:38.114605 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12211446808
I1002 01:30:38.119881 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12228224024
I1002 01:30:38.125009 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12228232216
I1002 01:30:38.130266 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12228240408
I1002 01:30:38.135493 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12245017624
I1002 01:30:38.141390 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12261794840
I1002 01:30:38.146664 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12261803032
I1002 01:30:38.152012 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12261811224
I1002 01:30:38.157257 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12278588440
I1002 01:30:38.162435 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12295365656
I1002 01:30:38.167533 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12295373848
I1002 01:30:38.172674 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12295382040
I1002 01:30:38.177836 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12312159256
I1002 01:30:38.182997 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12328936472
I1002 01:30:38.188159 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12328944664
I1002 01:30:38.193397 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12328952856
I1002 01:30:38.198880 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12328961048
I1002 01:30:38.204225 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12328969240
I1002 01:30:38.209730 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12328977432
I1002 01:30:38.214950 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12328985624
I1002 01:30:38.220167 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12329018392
I1002 01:30:38.225215 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12329051160
I1002 01:30:38.230384 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12396160024
I1002 01:30:38.235491 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12463268888
I1002 01:30:38.240625 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12463277080
I1002 01:30:38.245785 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12463285272
I1002 01:30:38.250832 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12530394136
I1002 01:30:38.256628 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12597503000
I1002 01:30:38.261679 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12597511192
I1002 01:30:38.266865 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12597519384
I1002 01:30:38.271966 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12597527576
I1002 01:30:38.277149 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12597535768
I1002 01:30:38.281038 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12597536280
I1002 01:30:38.284837 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12597536792
I1002 01:30:38.289940 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12614314008
I1002 01:30:38.295123 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12631091224
I1002 01:30:38.300216 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12631099416
I1002 01:30:38.305619 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12631107608
I1002 01:30:38.310688 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12647884824
I1002 01:30:38.315959 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12664662040
I1002 01:30:38.321014 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12664670232
I1002 01:30:38.326196 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12664678424
I1002 01:30:38.331243 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12681455640
I1002 01:30:38.336455 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12698232856
I1002 01:30:38.341547 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12698241048
I1002 01:30:38.346692 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12698249240
I1002 01:30:38.351885 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12715026456
I1002 01:30:38.356942 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12731803672
I1002 01:30:38.362158 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12731811864
I1002 01:30:38.367275 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12731820056
I1002 01:30:38.373068 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12731828248
I1002 01:30:38.378203 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12731836440
I1002 01:30:38.383390 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12731844632
I1002 01:30:38.388441 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12731852824
I1002 01:30:38.393682 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12731885592
I1002 01:30:38.398727 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12731918360
I1002 01:30:38.403889 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12799027224
I1002 01:30:38.408947 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12866136088
I1002 01:30:38.414086 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12866144280
I1002 01:30:38.419245 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12866152472
I1002 01:30:38.424317 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12933261336
I1002 01:30:38.429496 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13000370200
I1002 01:30:38.434548 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13000378392
I1002 01:30:38.439728 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13000386584
I1002 01:30:38.444853 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13000394776
I1002 01:30:38.449980 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13000402968
I1002 01:30:38.453876 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13000403480
I1002 01:30:38.457686 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13000403992
I1002 01:30:38.462783 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13017181208
I1002 01:30:38.467982 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13033958424
I1002 01:30:38.473058 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13033966616
I1002 01:30:38.478204 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13033974808
I1002 01:30:38.483239 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13050752024
I1002 01:30:38.488977 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13067529240
I1002 01:30:38.494056 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13067537432
I1002 01:30:38.499210 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13067545624
I1002 01:30:38.504276 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13084322840
I1002 01:30:38.509399 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13101100056
I1002 01:30:38.514469 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13101108248
I1002 01:30:38.519659 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13101116440
I1002 01:30:38.524835 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13117893656
I1002 01:30:38.529915 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13134670872
I1002 01:30:38.535050 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13134679064
I1002 01:30:38.540149 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13134687256
I1002 01:30:38.545298 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13134695448
I1002 01:30:38.550404 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13134703640
I1002 01:30:38.555583 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13134711832
I1002 01:30:38.560638 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13134720024
I1002 01:30:38.565783 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13134752792
I1002 01:30:38.570900 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13134785560
I1002 01:30:38.576051 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13201894424
I1002 01:30:38.581125 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13269003288
I1002 01:30:38.586267 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13269011480
I1002 01:30:38.591475 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13269019672
I1002 01:30:38.596537 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13336128536
I1002 01:30:38.602161 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13403237400
I1002 01:30:38.607240 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13403245592
I1002 01:30:38.612410 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13403253784
I1002 01:30:38.617474 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13403261976
I1002 01:30:38.622643 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13403270168
I1002 01:30:38.626586 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13403270680
I1002 01:30:38.630389 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13403271192
I1002 01:30:38.635507 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13420048408
I1002 01:30:38.640719 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13436825624
I1002 01:30:38.645784 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13436833816
I1002 01:30:38.650972 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13436842008
I1002 01:30:38.656070 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13453619224
I1002 01:30:38.661283 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13470396440
I1002 01:30:38.666326 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13470404632
I1002 01:30:38.671523 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13470412824
I1002 01:30:38.676588 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13487190040
I1002 01:30:38.681765 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13503967256
I1002 01:30:38.686842 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13503975448
I1002 01:30:38.692055 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13503983640
I1002 01:30:38.697216 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13520760856
I1002 01:30:38.702388 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13537538072
I1002 01:30:38.707587 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13537546264
I1002 01:30:38.712644 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13537554456
I1002 01:30:38.718389 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13537562648
I1002 01:30:38.723552 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13537570840
I1002 01:30:38.728744 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13537579032
I1002 01:30:38.733819 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13537587224
I1002 01:30:38.738974 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13537619992
I1002 01:30:38.744073 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13537652760
I1002 01:30:38.749233 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13604761624
I1002 01:30:38.754298 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13671870488
I1002 01:30:38.759533 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13671878680
I1002 01:30:38.764690 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13671886872
I1002 01:30:38.769751 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13738995736
I1002 01:30:38.774928 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13806104600
I1002 01:30:38.780082 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13806112792
I1002 01:30:38.785231 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13806120984
I1002 01:30:38.790329 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13806129176
I1002 01:30:38.795504 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13806137368
I1002 01:30:38.799441 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13806137880
I1002 01:30:38.803244 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13806138392
I1002 01:30:38.808355 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13822915608
I1002 01:30:38.813539 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13839692824
I1002 01:30:38.818636 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13839701016
I1002 01:30:38.823829 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13839709208
I1002 01:30:38.828889 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13856486424
I1002 01:30:38.834686 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13873263640
I1002 01:30:38.839797 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13873271832
I1002 01:30:38.844995 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13873280024
I1002 01:30:38.850070 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13890057240
I1002 01:30:38.855245 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13906834456
I1002 01:30:38.860353 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13906842648
I1002 01:30:38.865520 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13906850840
I1002 01:30:38.870691 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13923628056
I1002 01:30:38.875824 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13940405272
I1002 01:30:38.880987 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13940413464
I1002 01:30:38.886075 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13940421656
I1002 01:30:38.891272 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13940429848
I1002 01:30:38.896380 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13940438040
I1002 01:30:38.901566 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13940446232
I1002 01:30:38.906627 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13940454424
I1002 01:30:38.911870 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13940487192
I1002 01:30:38.916962 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13940519960
I1002 01:30:38.922132 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14007628824
I1002 01:30:38.927216 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14074737688
I1002 01:30:38.932377 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14074745880
I1002 01:30:38.937551 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14074754072
I1002 01:30:38.942633 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14141862936
I1002 01:30:38.948385 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14208971800
I1002 01:30:38.953442 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14208979992
I1002 01:30:38.958620 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14208988184
I1002 01:30:38.963726 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14208996376
I1002 01:30:38.968895 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14209004568
I1002 01:30:38.972806 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14209005080
I1002 01:30:38.976629 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14209005592
I1002 01:30:38.981720 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14225782808
I1002 01:30:38.986923 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14242560024
I1002 01:30:38.992018 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14242568216
I1002 01:30:38.997195 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14242576408
I1002 01:30:39.002265 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14259353624
I1002 01:30:39.007489 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14276130840
I1002 01:30:39.012619 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14276139032
I1002 01:30:39.017797 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14276147224
I1002 01:30:39.022882 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14292924440
I1002 01:30:39.028098 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14309701656
I1002 01:30:39.033193 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14309709848
I1002 01:30:39.038423 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14309718040
I1002 01:30:39.043607 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14326495256
I1002 01:30:39.048698 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14343272472
I1002 01:30:39.053860 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14343280664
I1002 01:30:39.058941 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14343288856
I1002 01:30:39.064582 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14343297048
I1002 01:30:39.069643 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14343305240
I1002 01:30:39.074811 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14343313432
I1002 01:30:39.079908 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14343321624
I1002 01:30:39.085091 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14343354392
I1002 01:30:39.090203 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14343387160
I1002 01:30:39.095385 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14410496024
I1002 01:30:39.100482 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14477604888
I1002 01:30:39.105653 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14477613080
I1002 01:30:39.110802 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14477621272
I1002 01:30:39.115911 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14544730136
I1002 01:30:39.121111 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14611839000
I1002 01:30:39.126212 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14611847192
I1002 01:30:39.131522 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14611855384
I1002 01:30:39.136576 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14611863576
I1002 01:30:39.141767 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14611871768
I1002 01:30:39.145681 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14611872280
I1002 01:30:39.149519 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14611872792
I1002 01:30:39.154623 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14628650008
I1002 01:30:39.159817 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14645427224
I1002 01:30:39.164912 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14645435416
I1002 01:30:39.170088 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14645443608
I1002 01:30:39.175224 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14662220824
I1002 01:30:39.180912 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14678998040
I1002 01:30:39.185952 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14679006232
I1002 01:30:39.191141 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14679014424
I1002 01:30:39.196290 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14695791640
I1002 01:30:39.201459 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14712568856
I1002 01:30:39.206540 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14712577048
I1002 01:30:39.211743 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14712585240
I1002 01:30:39.216933 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14729362456
I1002 01:30:39.222101 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14746139672
I1002 01:30:39.227273 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14746147864
I1002 01:30:39.232392 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14746156056
I1002 01:30:39.237560 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14746164248
I1002 01:30:39.242656 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14746172440
I1002 01:30:39.247870 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14746180632
I1002 01:30:39.252954 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14746188824
I1002 01:30:39.258173 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14746221592
I1002 01:30:39.263316 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14746254360
I1002 01:30:39.268518 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14813363224
I1002 01:30:39.273619 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14880472088
I1002 01:30:39.278788 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14880480280
I1002 01:30:39.284026 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14880488472
I1002 01:30:39.289109 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14947597336
I1002 01:30:39.294877 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15014706200
I1002 01:30:39.300104 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15014714392
I1002 01:30:39.305358 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15014722584
I1002 01:30:39.310475 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15014730776
I1002 01:30:39.315721 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15014738968
I1002 01:30:39.319694 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15014739480
I1002 01:30:39.323566 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15014739992
I1002 01:30:39.328760 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15031517208
I1002 01:30:39.333998 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15048294424
I1002 01:30:39.339091 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15048302616
I1002 01:30:39.344379 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15048310808
I1002 01:30:39.349508 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15065088024
I1002 01:30:39.354783 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15081865240
I1002 01:30:39.359962 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15081873432
I1002 01:30:39.365211 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15081881624
I1002 01:30:39.370357 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15098658840
I1002 01:30:39.375631 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15115436056
I1002 01:30:39.380766 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15115444248
I1002 01:30:39.385976 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15115452440
I1002 01:30:39.391196 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15132229656
I1002 01:30:39.396367 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15149006872
I1002 01:30:39.401612 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15149015064
I1002 01:30:39.406805 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15149023256
I1002 01:30:39.412629 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15149031448
I1002 01:30:39.417842 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15149039640
I1002 01:30:39.423094 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15149047832
I1002 01:30:39.428229 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15149056024
I1002 01:30:39.433428 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15149088792
I1002 01:30:39.438581 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15149121560
I1002 01:30:39.443822 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15216230424
I1002 01:30:39.448975 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15283339288
I1002 01:30:39.454204 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15283347480
I1002 01:30:39.459455 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15283355672
I1002 01:30:39.464591 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15350464536
I1002 01:30:39.469807 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15417573400
I1002 01:30:39.474922 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15417581592
I1002 01:30:39.480199 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15417589784
I1002 01:30:39.485370 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15417597976
I1002 01:30:39.490623 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15417606168
I1002 01:30:39.494573 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15417606680
I1002 01:30:39.498438 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15417607192
I1002 01:30:39.503619 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15434384408
I1002 01:30:39.508856 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15451161624
I1002 01:30:39.513939 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15451169816
I1002 01:30:39.519146 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15451178008
I1002 01:30:39.524327 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15467955224
I1002 01:30:39.530158 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15484732440
I1002 01:30:39.535361 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15484740632
I1002 01:30:39.540637 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15484748824
I1002 01:30:39.545793 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15501526040
I1002 01:30:39.551064 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15518303256
I1002 01:30:39.556221 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15518311448
I1002 01:30:39.561454 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15518319640
I1002 01:30:39.566657 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15535096856
I1002 01:30:39.571808 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15551874072
I1002 01:30:39.577007 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15551882264
I1002 01:30:39.582099 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15551890456
I1002 01:30:39.587286 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15551898648
I1002 01:30:39.592487 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15551906840
I1002 01:30:39.597762 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15551915032
I1002 01:30:39.602931 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15551923224
I1002 01:30:39.608199 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15551955992
I1002 01:30:39.613358 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15551988760
I1002 01:30:39.618571 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15619097624
I1002 01:30:39.623703 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15686206488
I1002 01:30:39.628886 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15686214680
I1002 01:30:39.634076 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15686222872
I1002 01:30:39.639182 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15753331736
I1002 01:30:39.644965 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15820440600
I1002 01:30:39.650085 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15820448792
I1002 01:30:39.655347 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15820456984
I1002 01:30:39.660452 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15820465176
I1002 01:30:39.665673 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15820473368
I1002 01:30:39.669618 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15820473880
I1002 01:30:39.673457 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15820474392
I1002 01:30:39.678579 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15837251608
I1002 01:30:39.683797 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15854028824
I1002 01:30:39.688889 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15854037016
I1002 01:30:39.694116 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15854045208
I1002 01:30:39.699208 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15870822424
I1002 01:30:39.704451 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15887599640
I1002 01:30:39.709551 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15887607832
I1002 01:30:39.714730 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15887616024
I1002 01:30:39.719879 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15904393240
I1002 01:30:39.725085 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15921170456
I1002 01:30:39.730190 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15921178648
I1002 01:30:39.735415 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15921186840
I1002 01:30:39.740624 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15937964056
I1002 01:30:39.745769 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15954741272
I1002 01:30:39.750963 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15954749464
I1002 01:30:39.756160 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15954757656
I1002 01:30:39.761889 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15954765848
I1002 01:30:39.767075 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15954774040
I1002 01:30:39.772310 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15954782232
I1002 01:30:39.777431 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15954790424
I1002 01:30:39.782631 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15954823192
I1002 01:30:39.787786 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15954855960
I1002 01:30:39.792981 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16021964824
I1002 01:30:39.798117 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16089073688
I1002 01:30:39.803362 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16089081880
I1002 01:30:39.808557 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16089090072
I1002 01:30:39.813687 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16156198936
I1002 01:30:39.818903 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16223307800
I1002 01:30:39.824091 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16223315992
I1002 01:30:39.829345 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16223324184
I1002 01:30:39.834450 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16223332376
I1002 01:30:39.839680 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16223340568
I1002 01:30:39.843646 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16223341080
I1002 01:30:39.847489 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16223341592
I1002 01:30:39.852586 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16240118808
I1002 01:30:39.857815 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16256896024
I1002 01:30:39.862914 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16256904216
I1002 01:30:39.868145 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16256912408
I1002 01:30:39.873280 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16273689624
I1002 01:30:39.879102 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16290466840
I1002 01:30:39.884249 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16290475032
I1002 01:30:39.889451 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16290483224
I1002 01:30:39.894567 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16307260440
I1002 01:30:39.899813 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16324037656
I1002 01:30:39.904960 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16324045848
I1002 01:30:39.910172 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16324054040
I1002 01:30:39.915424 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16340831256
I1002 01:30:39.920544 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16357608472
I1002 01:30:39.925774 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16357616664
I1002 01:30:39.930869 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16357624856
I1002 01:30:39.936124 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16357633048
I1002 01:30:39.941243 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16357641240
I1002 01:30:39.946445 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16357649432
I1002 01:30:39.951569 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16357657624
I1002 01:30:39.956799 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16357690392
I1002 01:30:39.961953 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16357723160
I1002 01:30:39.967153 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16424832024
I1002 01:30:39.972268 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16491940888
I1002 01:30:39.977489 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16491949080
I1002 01:30:39.982701 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16491957272
I1002 01:30:39.987834 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16559066136
I1002 01:30:39.993538 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16626175000
I1002 01:30:39.998635 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16626183192
I1002 01:30:40.003891 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16626191384
I1002 01:30:40.009017 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16626199576
I1002 01:30:40.014230 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16626207768
I1002 01:30:40.018193 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16626208280
I1002 01:30:40.022042 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16626208792
I1002 01:30:40.027174 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16642986008
I1002 01:30:40.032428 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16659763224
I1002 01:30:40.037529 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16659771416
I1002 01:30:40.042756 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16659779608
I1002 01:30:40.047912 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16676556824
I1002 01:30:40.053118 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16693334040
I1002 01:30:40.058296 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16693342232
I1002 01:30:40.063543 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16693350424
I1002 01:30:40.068662 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16710127640
I1002 01:30:40.073900 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16726904856
I1002 01:30:40.078998 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16726913048
I1002 01:30:40.084279 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16726921240
I1002 01:30:40.089468 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16743698456
I1002 01:30:40.094635 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16760475672
I1002 01:30:40.099856 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16760483864
I1002 01:30:40.104964 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16760492056
I1002 01:30:40.110667 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16760500248
I1002 01:30:40.115815 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16760508440
I1002 01:30:40.121019 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16760516632
I1002 01:30:40.126155 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16760524824
I1002 01:30:40.131472 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16760557592
I1002 01:30:40.136646 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16760590360
I1002 01:30:40.141847 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16827699224
I1002 01:30:40.147044 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16894808088
I1002 01:30:40.152266 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16894816280
I1002 01:30:40.157461 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16894824472
I1002 01:30:40.162609 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16961933336
I1002 01:30:40.167843 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17029042200
I1002 01:30:40.172967 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17029050392
I1002 01:30:40.178206 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17029058584
I1002 01:30:40.183336 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17029066776
I1002 01:30:40.188576 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17029074968
I1002 01:30:40.192537 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17029075480
I1002 01:30:40.196426 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17029075992
I1002 01:30:40.201551 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17045853208
I1002 01:30:40.206781 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17062630424
I1002 01:30:40.211910 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17062638616
I1002 01:30:40.217131 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17062646808
I1002 01:30:40.222249 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17079424024
I1002 01:30:40.228061 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17096201240
I1002 01:30:40.233199 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17096209432
I1002 01:30:40.238426 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17096217624
I1002 01:30:40.243663 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17112994840
I1002 01:30:40.248893 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17129772056
I1002 01:30:40.254018 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17129780248
I1002 01:30:40.259237 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17129788440
I1002 01:30:40.264486 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17146565656
I1002 01:30:40.269700 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17163342872
I1002 01:30:40.274931 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17163351064
I1002 01:30:40.280079 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17163359256
I1002 01:30:40.285321 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17163367448
I1002 01:30:40.290498 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17163375640
I1002 01:30:40.295782 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17163383832
I1002 01:30:40.300909 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17163392024
I1002 01:30:40.306155 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17163424792
I1002 01:30:40.311375 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17163457560
I1002 01:30:40.316636 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17230566424
I1002 01:30:40.321799 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17297675288
I1002 01:30:40.327067 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17297683480
I1002 01:30:40.332381 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17297691672
I1002 01:30:40.337563 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17364800536
I1002 01:30:40.343444 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17431909400
I1002 01:30:40.348657 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17431917592
I1002 01:30:40.353978 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17431925784
I1002 01:30:40.359159 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17431933976
I1002 01:30:40.364470 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17431942168
I1002 01:30:40.368477 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17431942680
I1002 01:30:40.372372 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17431943192
I1002 01:30:40.377626 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17448720408
I1002 01:30:40.382901 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17465497624
I1002 01:30:40.388075 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17465505816
I1002 01:30:40.393321 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17465514008
I1002 01:30:40.398469 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17482291224
I1002 01:30:40.403795 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17499068440
I1002 01:30:40.408944 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17499076632
I1002 01:30:40.414213 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17499084824
I1002 01:30:40.419402 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17515862040
I1002 01:30:40.424673 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17532639256
I1002 01:30:40.429857 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17532647448
I1002 01:30:40.435102 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17532655640
I1002 01:30:40.440413 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17549432856
I1002 01:30:40.445633 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17566210072
I1002 01:30:40.450953 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17566218264
I1002 01:30:40.456137 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17566226456
I1002 01:30:40.461946 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17566234648
I1002 01:30:40.467195 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17566242840
I1002 01:30:40.472499 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17566251032
I1002 01:30:40.477667 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17566259224
I1002 01:30:40.482974 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17566291992
I1002 01:30:40.488199 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17566324760
I1002 01:30:40.493443 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17633433624
I1002 01:30:40.498645 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17700542488
I1002 01:30:40.503941 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17700550680
I1002 01:30:40.509280 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17700558872
I1002 01:30:40.514435 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17767667736
I1002 01:30:40.519731 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17834776600
I1002 01:30:40.524887 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17834784792
I1002 01:30:40.530167 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17834792984
I1002 01:30:40.535391 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17834801176
I1002 01:30:40.540704 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17834809368
I1002 01:30:40.544696 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17834809880
I1002 01:30:40.548596 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17834810392
I1002 01:30:40.553770 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17851587608
I1002 01:30:40.559072 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17868364824
I1002 01:30:40.564259 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17868373016
I1002 01:30:40.569533 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17868381208
I1002 01:30:40.574715 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17885158424
I1002 01:30:40.580693 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17901935640
I1002 01:30:40.585947 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17901943832
I1002 01:30:40.591244 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17901952024
I1002 01:30:40.596461 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17918729240
I1002 01:30:40.601728 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17935506456
I1002 01:30:40.606884 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17935514648
I1002 01:30:40.612180 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17935522840
I1002 01:30:40.617423 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17952300056
I1002 01:30:40.622593 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17969077272
I1002 01:30:40.627895 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17969085464
I1002 01:30:40.633067 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17969093656
I1002 01:30:40.638328 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17969101848
I1002 01:30:40.643606 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17969110040
I1002 01:30:40.648944 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17969118232
I1002 01:30:40.654145 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17969126424
I1002 01:30:40.659438 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17969159192
I1002 01:30:40.664627 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17969191960
I1002 01:30:40.669870 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18036300824
I1002 01:30:40.675018 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18103409688
I1002 01:30:40.680302 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18103417880
I1002 01:30:40.685520 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18103426072
I1002 01:30:40.690694 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18170534936
I1002 01:30:40.696491 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18237643800
I1002 01:30:40.701658 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18237651992
I1002 01:30:40.706946 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18237660184
I1002 01:30:40.712114 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18237668376
I1002 01:30:40.717382 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18237676568
I1002 01:30:40.721390 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18237677080
I1002 01:30:40.725292 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18237677592
I1002 01:30:40.730420 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18254454808
I1002 01:30:40.735694 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18271232024
I1002 01:30:40.740839 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18271240216
I1002 01:30:40.746091 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18271248408
I1002 01:30:40.751251 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18288025624
I1002 01:30:40.756555 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18304802840
I1002 01:30:40.761693 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18304811032
I1002 01:30:40.766927 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18304819224
I1002 01:30:40.772124 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18321596440
I1002 01:30:40.777384 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18338373656
I1002 01:30:40.782516 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18338381848
I1002 01:30:40.787775 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18338390040
I1002 01:30:40.792998 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18355167256
I1002 01:30:40.798168 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18371944472
I1002 01:30:40.803467 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18371952664
I1002 01:30:40.808618 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18371960856
I1002 01:30:40.814367 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18371969048
I1002 01:30:40.819555 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18371977240
I1002 01:30:40.824787 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18371985432
I1002 01:30:40.829940 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18371993624
I1002 01:30:40.835179 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18372026392
I1002 01:30:40.840363 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18372059160
I1002 01:30:40.845592 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18439168024
I1002 01:30:40.850786 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18506276888
I1002 01:30:40.856045 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18506285080
I1002 01:30:40.861287 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18506293272
I1002 01:30:40.866427 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18573402136
I1002 01:30:40.871738 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18640511000
I1002 01:30:40.876862 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18640519192
I1002 01:30:40.882140 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18640527384
I1002 01:30:40.887284 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18640535576
I1002 01:30:40.892548 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18640543768
I1002 01:30:40.896545 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18640544280
I1002 01:30:40.900457 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18640544792
I1002 01:30:40.905697 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18657322008
I1002 01:30:40.910957 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18674099224
I1002 01:30:40.916111 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18674107416
I1002 01:30:40.921355 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18674115608
I1002 01:30:40.926496 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18690892824
I1002 01:30:40.932463 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18707670040
I1002 01:30:40.937611 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18707678232
I1002 01:30:40.942868 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18707686424
I1002 01:30:40.948038 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18724463640
I1002 01:30:40.953293 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18741240856
I1002 01:30:40.958486 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18741249048
I1002 01:30:40.963749 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18741257240
I1002 01:30:40.968996 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18758034456
I1002 01:30:40.974165 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18774811672
I1002 01:30:40.979463 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18774819864
I1002 01:30:40.984623 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18774828056
I1002 01:30:40.989882 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18774836248
I1002 01:30:40.995076 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18774844440
I1002 01:30:41.000355 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18774852632
I1002 01:30:41.005501 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18774860824
I1002 01:30:41.010791 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18774893592
I1002 01:30:41.016010 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18774926360
I1002 01:30:41.021277 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18842035224
I1002 01:30:41.026488 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18909144088
I1002 01:30:41.031761 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18909152280
I1002 01:30:41.037081 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18909160472
I1002 01:30:41.042285 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18976269336
I1002 01:30:41.048108 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19043378200
I1002 01:30:41.053276 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19043386392
I1002 01:30:41.058555 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19043394584
I1002 01:30:41.063824 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19043402776
I1002 01:30:41.069080 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19043410968
I1002 01:30:41.073113 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19043411480
I1002 01:30:41.077026 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19043411992
I1002 01:30:41.082206 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19060189208
I1002 01:30:41.087532 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19076966424
I1002 01:30:41.092701 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19076974616
I1002 01:30:41.097971 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19076982808
I1002 01:30:41.103133 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19093760024
I1002 01:30:41.108447 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19110537240
I1002 01:30:41.113622 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19110545432
I1002 01:30:41.118860 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19110553624
I1002 01:30:41.124061 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19127330840
I1002 01:30:41.129321 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19144108056
I1002 01:30:41.134559 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19144116248
I1002 01:30:41.139929 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19144124440
I1002 01:30:41.145203 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19160901656
I1002 01:30:41.150380 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19177678872
I1002 01:30:41.155662 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19177687064
I1002 01:30:41.160824 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19177695256
I1002 01:30:41.166664 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19177703448
I1002 01:30:41.171910 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19177711640
I1002 01:30:41.177174 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19177719832
I1002 01:30:41.182346 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19177728024
I1002 01:30:41.187644 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19177760792
I1002 01:30:41.192878 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19177793560
I1002 01:30:41.198126 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19244902424
I1002 01:30:41.203334 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19312011288
I1002 01:30:41.208609 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19312019480
I1002 01:30:41.213888 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19312027672
I1002 01:30:41.219069 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19379136536
I1002 01:30:41.224389 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19446245400
I1002 01:30:41.229537 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19446253592
I1002 01:30:41.234825 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19446261784
I1002 01:30:41.240003 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19446269976
I1002 01:30:41.245267 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19446278168
I1002 01:30:41.249288 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19446278680
I1002 01:30:41.253224 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19446279192
I1002 01:30:41.258393 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19463056408
I1002 01:30:41.263722 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19479833624
I1002 01:30:41.268879 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19479841816
I1002 01:30:41.274185 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19479850008
I1002 01:30:41.279392 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19496627224
I1002 01:30:41.285203 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19513404440
I1002 01:30:41.290371 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19513412632
I1002 01:30:41.295673 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19513420824
I1002 01:30:41.300832 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19530198040
I1002 01:30:41.306096 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19546975256
I1002 01:30:41.311233 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19546983448
I1002 01:30:41.316518 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19546991640
I1002 01:30:41.321771 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19563768856
I1002 01:30:41.326961 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19580546072
I1002 01:30:41.332246 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19580554264
I1002 01:30:41.337397 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19580562456
I1002 01:30:41.342669 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19580570648
I1002 01:30:41.347872 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19580578840
I1002 01:30:41.353117 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19580587032
I1002 01:30:41.358270 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19580595224
I1002 01:30:41.363577 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19580627992
I1002 01:30:41.368766 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19580660760
I1002 01:30:41.374047 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19647769624
I1002 01:30:41.379273 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19714878488
I1002 01:30:41.384570 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19714886680
I1002 01:30:41.389821 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19714894872
I1002 01:30:41.395006 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19782003736
I1002 01:30:41.400752 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19849112600
I1002 01:30:41.405949 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19849120792
I1002 01:30:41.411272 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19849128984
I1002 01:30:41.416484 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19849137176
I1002 01:30:41.421743 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19849145368
I1002 01:30:41.425735 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19849145880
I1002 01:30:41.429656 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19849146392
I1002 01:30:41.434897 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19865923608
I1002 01:30:41.440191 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19882700824
I1002 01:30:41.445391 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19882709016
I1002 01:30:41.450630 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19882717208
I1002 01:30:41.455833 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19899494424
I1002 01:30:41.461133 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19916271640
I1002 01:30:41.466326 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19916279832
I1002 01:30:41.471637 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19916288024
I1002 01:30:41.476805 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19933065240
I1002 01:30:41.482075 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19949842456
I1002 01:30:41.487239 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19949850648
I1002 01:30:41.492541 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19949858840
I1002 01:30:41.497816 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19966636056
I1002 01:30:41.502996 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19983413272
I1002 01:30:41.508316 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19983421464
I1002 01:30:41.513479 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19983429656
I1002 01:30:41.519248 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19983437848
I1002 01:30:41.524466 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19983446040
I1002 01:30:41.529742 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19983454232
I1002 01:30:41.534914 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19983462424
I1002 01:30:41.540229 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19983495192
I1002 01:30:41.545416 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19983527960
I1002 01:30:41.550688 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20050636824
I1002 01:30:41.555924 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20117745688
I1002 01:30:41.561196 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20117753880
I1002 01:30:41.566491 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20117762072
I1002 01:30:41.571734 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20184870936
I1002 01:30:41.577026 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20251979800
I1002 01:30:41.582213 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20251987992
I1002 01:30:41.587537 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20251996184
I1002 01:30:41.592700 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20252004376
I1002 01:30:41.597996 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20252012568
I1002 01:30:41.601997 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20252013080
I1002 01:30:41.605928 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20252013592
I1002 01:30:41.611164 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20268790808
I1002 01:30:41.616517 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20285568024
I1002 01:30:41.621685 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20285576216
I1002 01:30:41.626953 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20285584408
I1002 01:30:41.632145 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20302361624
I1002 01:30:41.637967 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20319138840
I1002 01:30:41.643179 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20319147032
I1002 01:30:41.648498 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20319155224
I1002 01:30:41.653684 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20335932440
I1002 01:30:41.658994 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20352709656
I1002 01:30:41.664196 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20352717848
I1002 01:30:41.669477 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20352726040
I1002 01:30:41.674768 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20369503256
I1002 01:30:41.679977 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386280472
I1002 01:30:41.685309 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386288664
I1002 01:30:41.690510 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386296856
I1002 01:30:41.695878 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386305048
I1002 01:30:41.701206 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386313240
I1002 01:30:41.706534 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386321432
I1002 01:30:41.711794 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386329624
I1002 01:30:41.717088 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386337624
I1002 01:30:41.722365 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386345624
I1002 01:30:41.727691 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386353624
I1002 01:30:41.732921 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386361624
I1002 01:30:41.738290 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386369624
I1002 01:30:41.743636 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386377624
I1002 01:30:41.748841 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386385624
I1002 01:30:41.754760 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386393624
I1002 01:30:41.760073 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386401624
I1002 01:30:41.765421 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386409624
I1002 01:30:41.770629 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386417624
I1002 01:30:41.775963 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386425624
I1002 01:30:41.781206 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386433624
I1002 01:30:41.786505 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386441624
I1002 01:30:41.791751 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386449624
I1002 01:30:41.797072 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386457624
I1002 01:30:41.802316 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386465624
I1002 01:30:41.807675 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386473624
I1002 01:30:41.812969 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386481624
I1002 01:30:41.818206 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386489624
I1002 01:30:41.823552 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386497624
I1002 01:30:41.828767 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386505624
I1002 01:30:41.834091 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386513624
I1002 01:30:41.839320 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386521624
I1002 01:30:41.844637 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386529624
I1002 01:30:41.849823 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386537624
I1002 01:30:41.855152 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386545624
I1002 01:30:41.860386 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386553624
I1002 01:30:41.865703 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386561624
I1002 01:30:41.870900 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386569624
I1002 01:30:41.876833 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386577624
I1002 01:30:41.882124 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386585624
I1002 01:30:41.887361 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20402969624
I1002 01:30:41.892708 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20419353624
I1002 01:30:41.897915 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20435737624
I1002 01:30:41.903220 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20452121624
I1002 01:30:41.908448 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20468505624
I1002 01:30:41.913749 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20484889624
I1002 01:30:41.918962 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20501273624
I1002 01:30:41.924302 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20517657624
I1002 01:30:41.929516 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20534041624
I1002 01:30:41.934833 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20550425624
I1002 01:30:41.940172 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20566809624
I1002 01:30:41.945529 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20583193624
I1002 01:30:41.950905 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20599577624
I1002 01:30:41.956265 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20615961624
I1002 01:30:41.961655 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20632345624
I1002 01:30:41.966857 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20648729624
I1002 01:30:41.972212 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20665113624
I1002 01:30:41.977437 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20681497624
I1002 01:30:41.982749 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20697881624
I1002 01:30:41.987982 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20714265624
I1002 01:30:41.993882 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20730649624
I1002 01:30:41.999324 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20747033624
I1002 01:30:42.004577 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20763417624
I1002 01:30:42.009958 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20779801624
I1002 01:30:42.015246 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20796185624
I1002 01:30:42.020639 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20812569624
I1002 01:30:42.025853 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20828953624
I1002 01:30:42.031249 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20845337624
I1002 01:30:42.036481 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20861721624
I1002 01:30:42.041747 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20878105624
I1002 01:30:42.046948 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20894489624
I1002 01:30:42.052261 139650252134208 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20910873624
I1002 01:30:42.799522 139650252134208 cluster.py:515] Place variable total_nan_gradients/var on /job:local/replica:0/task:0/device:CPU:0 20910873632
I1002 01:30:42.801690 139650252134208 py_utils.py:1389] Creating var total_nan_gradients/var:0 shape=() on device /job:local/replica:0/task:0/device:CPU:0
I1002 01:30:43.007746 139650252134208 trainer.py:401] Trainer number of enqueue ops: 0
I1002 01:30:43.007970 139650252134208 trainer.py:410] AttributeError. Expected for single task models.
I1002 01:31:02.926428 139650252134208 trainer.py:1590] Starting runners
I1002 01:31:02.926935 139640791607040 base_runner.py:167] controller started.
I1002 01:31:02.927350 139640783214336 base_runner.py:167] trainer started.
I1002 01:31:02.927491 139650252134208 trainer.py:1609] Waiting for runners to finish...
I1002 01:31:26.728200 139640791607040 checkpointer.py:133] Uninitialized var list: [b'global_step', b'1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var', b'1bwds_wpm_level_lm/total_samples/var', b'beta1_power', b'beta2_power', b'1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var/Adam', b'1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var/Adam_1', b'total_nan_gradients/var'] 
I1002 01:31:26.729679 139640791607040 checkpointer.py:140] Initialize ALL variables: [b'global_step', b'1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var', b'1bwds_wpm_level_lm/total_samples/var', b'beta1_power', b'beta2_power', b'1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var/Adam', b'1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var/Adam_1', b'total_nan_gradients/var']
I1002 01:32:26.920400 139640783214336 base_runner.py:106] step:     0
I1002 01:32:36.637336 139640791607040 checkpointer.py:142] Initialize variables done.
I1002 01:32:42.851758 139640791607040 trainer.py:380] Steps/second: 0.000000, Examples/second: 0.000000
I1002 01:32:42.852478 139640791607040 checkpointer.py:111] Save checkpoint
WARNING:tensorflow:Issue encountered when serializing __model_split_id_stack.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
W1002 01:34:19.529567 139640791607040 meta_graph.py:448] Issue encountered when serializing __model_split_id_stack.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing __batch_norm_update_dict.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'dict' object has no attribute 'name'
W1002 01:34:19.534914 139640791607040 meta_graph.py:448] Issue encountered when serializing __batch_norm_update_dict.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'dict' object has no attribute 'name'
I1002 01:34:21.656945 139640791607040 checkpointer.py:113] Save checkpoint done: /tmp/mnist/log/train/ckpt-00000000
I1002 01:34:21.664879 139640791607040 trainer.py:380] Steps/second: 0.000000, Examples/second: 0.000000
I1002 01:34:31.674447 139640791607040 trainer.py:380] Steps/second: 0.000000, Examples/second: 0.000000
I1002 01:34:41.684734 139640791607040 trainer.py:380] Steps/second: 0.000000, Examples/second: 0.000000
I1002 01:34:51.694519 139640791607040 trainer.py:380] Steps/second: 0.000000, Examples/second: 0.000000
I1002 01:35:01.705054 139640791607040 trainer.py:380] Steps/second: 0.000000, Examples/second: 0.000000
2019-10-02 01:35:06.088992: I ./lingvo/core/ops/input_common.h:71] Create RecordProcessor
2019-10-02 01:35:06.089955: I lingvo/core/ops/input_common.cc:33] Input source weights are empty, fall back to legacy behavior.
2019-10-02 01:35:06.090261: I lingvo/core/ops/record_yielder.cc:324] 0x7ef943186510 Record yielder start
2019-10-02 01:35:06.090296: I ./lingvo/core/ops/input_common.h:76] Create batcher
2019-10-02 01:35:06.090340: I lingvo/core/ops/record_yielder.cc:383] Epoch 1 /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en*
I1002 01:35:11.751210 139640791607040 trainer.py:380] Steps/second: 0.000000, Examples/second: 0.000000
2019-10-02 01:35:27.291321: W tensorflow/core/common_runtime/bfc_allocator.cc:419] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.0KiB (rounded to 8192).  Current allocation summary follows.
2019-10-02 01:35:27.291482: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (256): 	Total Chunks: 242, Chunks in use: 242. 60.5KiB allocated for chunks. 60.5KiB in use in bin. 4.2KiB client-requested in use in bin.
2019-10-02 01:35:27.291496: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (512): 	Total Chunks: 46, Chunks in use: 46. 23.2KiB allocated for chunks. 23.2KiB in use in bin. 23.0KiB client-requested in use in bin.
2019-10-02 01:35:27.291526: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (1024): 	Total Chunks: 1, Chunks in use: 1. 1.2KiB allocated for chunks. 1.2KiB in use in bin. 1.0KiB client-requested in use in bin.
2019-10-02 01:35:27.291535: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (2048): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-10-02 01:35:27.291543: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (4096): 	Total Chunks: 3, Chunks in use: 3. 16.0KiB allocated for chunks. 16.0KiB in use in bin. 12.0KiB client-requested in use in bin.
2019-10-02 01:35:27.291551: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (8192): 	Total Chunks: 451, Chunks in use: 451. 3.53MiB allocated for chunks. 3.53MiB in use in bin. 3.52MiB client-requested in use in bin.
2019-10-02 01:35:27.291559: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (16384): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-10-02 01:35:27.291566: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (32768): 	Total Chunks: 47, Chunks in use: 47. 1.52MiB allocated for chunks. 1.52MiB in use in bin. 1.47MiB client-requested in use in bin.
2019-10-02 01:35:27.291573: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (65536): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-10-02 01:35:27.291581: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (131072): 	Total Chunks: 7, Chunks in use: 7. 896.0KiB allocated for chunks. 896.0KiB in use in bin. 896.0KiB client-requested in use in bin.
2019-10-02 01:35:27.291589: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (262144): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-10-02 01:35:27.291597: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (524288): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-10-02 01:35:27.291604: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (1048576): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-10-02 01:35:27.291611: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (2097152): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-10-02 01:35:27.291617: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (4194304): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-10-02 01:35:27.291625: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (8388608): 	Total Chunks: 35, Chunks in use: 35. 524.00MiB allocated for chunks. 524.00MiB in use in bin. 524.00MiB client-requested in use in bin.
2019-10-02 01:35:27.291634: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (16777216): 	Total Chunks: 192, Chunks in use: 192. 3.00GiB allocated for chunks. 3.00GiB in use in bin. 3.00GiB client-requested in use in bin.
2019-10-02 01:35:27.291641: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (33554432): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-10-02 01:35:27.291648: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (67108864): 	Total Chunks: 99, Chunks in use: 99. 6.25GiB allocated for chunks. 6.25GiB in use in bin. 6.25GiB client-requested in use in bin.
2019-10-02 01:35:27.291655: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (134217728): 	Total Chunks: 1, Chunks in use: 1. 250.00MiB allocated for chunks. 250.00MiB in use in bin. 250.00MiB client-requested in use in bin.
2019-10-02 01:35:27.291671: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (268435456): 	Total Chunks: 4, Chunks in use: 4. 4.66GiB allocated for chunks. 4.66GiB in use in bin. 4.66GiB client-requested in use in bin.
2019-10-02 01:35:27.291682: I tensorflow/core/common_runtime/bfc_allocator.cc:885] Bin for 8.0KiB was 8.0KiB, Chunk State: 
2019-10-02 01:35:27.291689: I tensorflow/core/common_runtime/bfc_allocator.cc:898] Next region of size 15753943296
2019-10-02 01:35:27.291698: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0000000 next 1 of size 1280
2019-10-02 01:35:27.291706: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0000500 next 2 of size 256
2019-10-02 01:35:27.291712: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0000600 next 3 of size 256
2019-10-02 01:35:27.291718: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0000700 next 4 of size 256
2019-10-02 01:35:27.291724: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0000800 next 5 of size 256
2019-10-02 01:35:27.291732: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0000900 next 6 of size 256
2019-10-02 01:35:27.291737: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0000a00 next 7 of size 256
2019-10-02 01:35:27.291743: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0000b00 next 8 of size 256
2019-10-02 01:35:27.291749: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0000c00 next 9 of size 256
2019-10-02 01:35:27.291757: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0000d00 next 10 of size 256
2019-10-02 01:35:27.291763: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0000e00 next 11 of size 256
2019-10-02 01:35:27.291769: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0000f00 next 12 of size 256
2019-10-02 01:35:27.291775: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0001000 next 13 of size 256
2019-10-02 01:35:27.291781: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0001100 next 14 of size 256
2019-10-02 01:35:27.291788: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0001200 next 15 of size 256
2019-10-02 01:35:27.291794: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0001300 next 16 of size 256
2019-10-02 01:35:27.291800: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0001400 next 17 of size 256
2019-10-02 01:35:27.291806: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0001500 next 18 of size 256
2019-10-02 01:35:27.291814: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0001600 next 19 of size 256
2019-10-02 01:35:27.291821: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0001700 next 20 of size 256
2019-10-02 01:35:27.291827: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0001800 next 21 of size 256
2019-10-02 01:35:27.291833: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0001900 next 22 of size 256
2019-10-02 01:35:27.291839: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0001a00 next 23 of size 256
2019-10-02 01:35:27.291845: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0001b00 next 24 of size 256
2019-10-02 01:35:27.291853: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0001c00 next 25 of size 256
2019-10-02 01:35:27.291859: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0001d00 next 26 of size 256
2019-10-02 01:35:27.291865: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0001e00 next 27 of size 256
2019-10-02 01:35:27.291871: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0001f00 next 28 of size 256
2019-10-02 01:35:27.291878: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0002000 next 29 of size 256
2019-10-02 01:35:27.291888: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0002100 next 30 of size 256
2019-10-02 01:35:27.291895: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0002200 next 31 of size 256
2019-10-02 01:35:27.291901: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0002300 next 32 of size 256
2019-10-02 01:35:27.291909: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0002400 next 33 of size 256
2019-10-02 01:35:27.291915: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0002500 next 34 of size 256
2019-10-02 01:35:27.291922: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0002600 next 35 of size 256
2019-10-02 01:35:27.291927: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0002700 next 36 of size 256
2019-10-02 01:35:27.291936: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0002800 next 37 of size 256
2019-10-02 01:35:27.291943: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0002900 next 38 of size 256
2019-10-02 01:35:27.291949: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0002a00 next 39 of size 256
2019-10-02 01:35:27.291955: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0002b00 next 40 of size 256
2019-10-02 01:35:27.291961: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0002c00 next 41 of size 256
2019-10-02 01:35:27.291966: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0002d00 next 42 of size 256
2019-10-02 01:35:27.291972: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0002e00 next 43 of size 256
2019-10-02 01:35:27.291980: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0002f00 next 44 of size 8192
2019-10-02 01:35:27.291986: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0004f00 next 45 of size 67108864
2019-10-02 01:35:27.291993: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb4004f00 next 46 of size 67108864
2019-10-02 01:35:27.291999: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb8004f00 next 47 of size 8192
2019-10-02 01:35:27.292006: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb8006f00 next 48 of size 32768
2019-10-02 01:35:27.292012: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb800ef00 next 49 of size 8192
2019-10-02 01:35:27.292020: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb8010f00 next 50 of size 16777216
2019-10-02 01:35:27.292026: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb9010f00 next 51 of size 8192
2019-10-02 01:35:27.292032: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb9012f00 next 52 of size 8192
2019-10-02 01:35:27.292039: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb9014f00 next 53 of size 32768
2019-10-02 01:35:27.292045: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb901cf00 next 54 of size 8192
2019-10-02 01:35:27.292051: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb901ef00 next 55 of size 8192
2019-10-02 01:35:27.292056: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb9020f00 next 56 of size 32768
2019-10-02 01:35:27.292062: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb9028f00 next 57 of size 8192
2019-10-02 01:35:27.292070: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb902af00 next 58 of size 8192
2019-10-02 01:35:27.292076: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb902cf00 next 59 of size 512
2019-10-02 01:35:27.292082: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb902d100 next 60 of size 8192
2019-10-02 01:35:27.292088: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb902f100 next 61 of size 16777216
2019-10-02 01:35:27.292103: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcba02f100 next 62 of size 8192
2019-10-02 01:35:27.292110: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcba031100 next 63 of size 16777216
2019-10-02 01:35:27.292115: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcbb031100 next 64 of size 8192
2019-10-02 01:35:27.292121: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcbb033100 next 65 of size 8192
2019-10-02 01:35:27.292128: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcbb035100 next 66 of size 16777216
2019-10-02 01:35:27.292134: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcbc035100 next 67 of size 32768
2019-10-02 01:35:27.292140: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcbc03d100 next 68 of size 16777216
2019-10-02 01:35:27.292146: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcbd03d100 next 69 of size 8192
2019-10-02 01:35:27.292152: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcbd03f100 next 70 of size 8192
2019-10-02 01:35:27.292158: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcbd041100 next 71 of size 8192
2019-10-02 01:35:27.292164: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcbd043100 next 72 of size 67108864
2019-10-02 01:35:27.292172: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcc1043100 next 73 of size 8192
2019-10-02 01:35:27.292178: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcc1045100 next 74 of size 67108864
2019-10-02 01:35:27.292184: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcc5045100 next 75 of size 8192
2019-10-02 01:35:27.292190: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcc5047100 next 76 of size 8192
2019-10-02 01:35:27.292197: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcc5049100 next 77 of size 8192
2019-10-02 01:35:27.292203: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcc504b100 next 78 of size 8192
2019-10-02 01:35:27.292210: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcc504d100 next 79 of size 67108864
2019-10-02 01:35:27.292215: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcc904d100 next 80 of size 16777216
2019-10-02 01:35:27.292223: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcca04d100 next 81 of size 8192
2019-10-02 01:35:27.292229: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcca04f100 next 82 of size 16777216
2019-10-02 01:35:27.292235: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efccb04f100 next 83 of size 8192
2019-10-02 01:35:27.292241: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efccb051100 next 84 of size 16777216
2019-10-02 01:35:27.292248: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efccc051100 next 85 of size 8192
2019-10-02 01:35:27.292254: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efccc053100 next 86 of size 67108864
2019-10-02 01:35:27.292260: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd0053100 next 87 of size 8192
2019-10-02 01:35:27.292266: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd0055100 next 88 of size 32768
2019-10-02 01:35:27.292274: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd005d100 next 89 of size 8192
2019-10-02 01:35:27.292279: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd005f100 next 90 of size 8192
2019-10-02 01:35:27.292285: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd0061100 next 91 of size 8192
2019-10-02 01:35:27.292291: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd0063100 next 92 of size 8192
2019-10-02 01:35:27.292301: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd0065100 next 93 of size 8192
2019-10-02 01:35:27.292307: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd0067100 next 94 of size 16777216
2019-10-02 01:35:27.292315: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd1067100 next 95 of size 8192
2019-10-02 01:35:27.292321: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd1069100 next 96 of size 16384000
2019-10-02 01:35:27.292327: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd2009100 next 97 of size 16384000
2019-10-02 01:35:27.292334: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd2fa9100 next 98 of size 8192
2019-10-02 01:35:27.292340: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd2fab100 next 99 of size 512
2019-10-02 01:35:27.292346: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd2fab300 next 100 of size 16777216
2019-10-02 01:35:27.292352: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd3fab300 next 101 of size 16777216
2019-10-02 01:35:27.292358: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd4fab300 next 102 of size 67108864
2019-10-02 01:35:27.292364: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd8fab300 next 103 of size 16777216
2019-10-02 01:35:27.292369: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd9fab300 next 104 of size 16777216
2019-10-02 01:35:27.292377: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcdafab300 next 105 of size 8192
2019-10-02 01:35:27.292382: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcdafad300 next 106 of size 512
2019-10-02 01:35:27.292388: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcdafad500 next 107 of size 16777216
2019-10-02 01:35:27.292394: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcdbfad500 next 108 of size 512
2019-10-02 01:35:27.292402: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcdbfad700 next 109 of size 16777216
2019-10-02 01:35:27.292408: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcdcfad700 next 110 of size 8192
2019-10-02 01:35:27.292414: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcdcfaf700 next 111 of size 8192
2019-10-02 01:35:27.292420: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcdcfb1700 next 112 of size 8192
2019-10-02 01:35:27.292427: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcdcfb3700 next 113 of size 8192
2019-10-02 01:35:27.292433: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcdcfb5700 next 114 of size 8192
2019-10-02 01:35:27.292439: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcdcfb7700 next 115 of size 16777216
2019-10-02 01:35:27.292445: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcddfb7700 next 116 of size 67108864
2019-10-02 01:35:27.292452: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce1fb7700 next 117 of size 512
2019-10-02 01:35:27.292458: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce1fb7900 next 118 of size 8192
2019-10-02 01:35:27.292464: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce1fb9900 next 119 of size 8192
2019-10-02 01:35:27.292469: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce1fbb900 next 120 of size 8192
2019-10-02 01:35:27.292477: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce1fbd900 next 121 of size 16777216
2019-10-02 01:35:27.292483: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce2fbd900 next 122 of size 8192
2019-10-02 01:35:27.292490: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce2fbf900 next 123 of size 8192
2019-10-02 01:35:27.292496: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce2fc1900 next 124 of size 16777216
2019-10-02 01:35:27.292510: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce3fc1900 next 125 of size 512
2019-10-02 01:35:27.292516: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce3fc1b00 next 126 of size 67108864
2019-10-02 01:35:27.292522: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce7fc1b00 next 127 of size 8192
2019-10-02 01:35:27.292528: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce7fc3b00 next 128 of size 8192
2019-10-02 01:35:27.292534: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce7fc5b00 next 129 of size 8192
2019-10-02 01:35:27.292542: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce7fc7b00 next 130 of size 8192
2019-10-02 01:35:27.292548: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce7fc9b00 next 131 of size 16777216
2019-10-02 01:35:27.292553: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce8fc9b00 next 132 of size 8192
2019-10-02 01:35:27.292559: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce8fcbb00 next 133 of size 16777216
2019-10-02 01:35:27.292567: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce9fcbb00 next 134 of size 8192
2019-10-02 01:35:27.292573: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce9fcdb00 next 135 of size 16777216
2019-10-02 01:35:27.292578: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efceafcdb00 next 136 of size 8192
2019-10-02 01:35:27.292584: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efceafcfb00 next 137 of size 8192
2019-10-02 01:35:27.292592: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efceafd1b00 next 138 of size 8192
2019-10-02 01:35:27.292598: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efceafd3b00 next 139 of size 8192
2019-10-02 01:35:27.292604: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efceafd5b00 next 140 of size 8192
2019-10-02 01:35:27.292610: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efceafd7b00 next 141 of size 8192
2019-10-02 01:35:27.292618: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efceafd9b00 next 142 of size 16777216
2019-10-02 01:35:27.292624: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcebfd9b00 next 143 of size 16777216
2019-10-02 01:35:27.292630: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcecfd9b00 next 144 of size 8192
2019-10-02 01:35:27.292635: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcecfdbb00 next 145 of size 8192
2019-10-02 01:35:27.292643: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcecfddb00 next 146 of size 8192
2019-10-02 01:35:27.292649: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcecfdfb00 next 147 of size 32768
2019-10-02 01:35:27.292655: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcecfe7b00 next 148 of size 8192
2019-10-02 01:35:27.292661: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcecfe9b00 next 149 of size 512
2019-10-02 01:35:27.292668: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcecfe9d00 next 150 of size 8192
2019-10-02 01:35:27.292674: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcecfebd00 next 151 of size 8192
2019-10-02 01:35:27.292679: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcecfedd00 next 152 of size 16777216
2019-10-02 01:35:27.292685: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcedfedd00 next 153 of size 8192
2019-10-02 01:35:27.292693: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcedfefd00 next 154 of size 16777216
2019-10-02 01:35:27.292698: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efceefefd00 next 155 of size 8192
2019-10-02 01:35:27.292707: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efceeff1d00 next 156 of size 8192
2019-10-02 01:35:27.292713: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efceeff3d00 next 157 of size 8192
2019-10-02 01:35:27.292719: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efceeff5d00 next 158 of size 67108864
2019-10-02 01:35:27.292725: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcf2ff5d00 next 159 of size 8192
2019-10-02 01:35:27.292731: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcf2ff7d00 next 160 of size 32768
2019-10-02 01:35:27.292737: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcf2fffd00 next 161 of size 16777216
2019-10-02 01:35:27.292743: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcf3fffd00 next 162 of size 16777216
2019-10-02 01:35:27.292749: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcf4fffd00 next 163 of size 67108864
2019-10-02 01:35:27.292757: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcf8fffd00 next 164 of size 8192
2019-10-02 01:35:27.292763: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcf9001d00 next 165 of size 8192
2019-10-02 01:35:27.292769: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcf9003d00 next 166 of size 16384000
2019-10-02 01:35:27.292774: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcf9fa3d00 next 167 of size 8192
2019-10-02 01:35:27.292782: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcf9fa5d00 next 168 of size 67108864
2019-10-02 01:35:27.292787: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcfdfa5d00 next 169 of size 8192
2019-10-02 01:35:27.292793: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcfdfa7d00 next 170 of size 67108864
2019-10-02 01:35:27.292799: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd01fa7d00 next 171 of size 8192
2019-10-02 01:35:27.292807: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd01fa9d00 next 172 of size 67108864
2019-10-02 01:35:27.292813: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd05fa9d00 next 173 of size 16777216
2019-10-02 01:35:27.292819: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd06fa9d00 next 174 of size 512
2019-10-02 01:35:27.292825: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd06fa9f00 next 175 of size 8192
2019-10-02 01:35:27.292833: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd06fabf00 next 176 of size 8192
2019-10-02 01:35:27.292839: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd06fadf00 next 177 of size 67108864
2019-10-02 01:35:27.292845: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd0afadf00 next 178 of size 67108864
2019-10-02 01:35:27.292851: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd0efadf00 next 179 of size 8192
2019-10-02 01:35:27.292857: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd0efaff00 next 180 of size 16777216
2019-10-02 01:35:27.292863: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd0ffaff00 next 181 of size 8192
2019-10-02 01:35:27.292870: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd0ffb1f00 next 182 of size 8192
2019-10-02 01:35:27.292876: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd0ffb3f00 next 183 of size 16384000
2019-10-02 01:35:27.292882: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd10f53f00 next 184 of size 16384000
2019-10-02 01:35:27.292888: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd11ef3f00 next 185 of size 8192
2019-10-02 01:35:27.292894: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd11ef5f00 next 186 of size 8192
2019-10-02 01:35:27.292900: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd11ef7f00 next 187 of size 16777216
2019-10-02 01:35:27.292915: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd12ef7f00 next 188 of size 32768
2019-10-02 01:35:27.292921: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd12efff00 next 189 of size 16777216
2019-10-02 01:35:27.292927: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd13efff00 next 190 of size 8192
2019-10-02 01:35:27.292934: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd13f01f00 next 191 of size 8192
2019-10-02 01:35:27.292940: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd13f03f00 next 192 of size 8192
2019-10-02 01:35:27.292946: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd13f05f00 next 193 of size 16777216
2019-10-02 01:35:27.292951: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd14f05f00 next 194 of size 8192
2019-10-02 01:35:27.292958: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd14f07f00 next 195 of size 8192
2019-10-02 01:35:27.292964: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd14f09f00 next 196 of size 67108864
2019-10-02 01:35:27.292971: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd18f09f00 next 197 of size 16777216
2019-10-02 01:35:27.292977: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd19f09f00 next 198 of size 8192
2019-10-02 01:35:27.292983: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd19f0bf00 next 199 of size 8192
2019-10-02 01:35:27.292989: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd19f0df00 next 200 of size 16777216
2019-10-02 01:35:27.292996: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd1af0df00 next 201 of size 8192
2019-10-02 01:35:27.293002: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd1af0ff00 next 202 of size 8192
2019-10-02 01:35:27.293008: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd1af11f00 next 203 of size 8192
2019-10-02 01:35:27.293014: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd1af13f00 next 204 of size 67108864
2019-10-02 01:35:27.293022: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd1ef13f00 next 205 of size 32768
2019-10-02 01:35:27.293028: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd1ef1bf00 next 206 of size 16777216
2019-10-02 01:35:27.293034: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd1ff1bf00 next 207 of size 8192
2019-10-02 01:35:27.293039: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd1ff1df00 next 208 of size 8192
2019-10-02 01:35:27.293047: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd1ff1ff00 next 209 of size 8192
2019-10-02 01:35:27.293053: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd1ff21f00 next 210 of size 16777216
2019-10-02 01:35:27.293059: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd20f21f00 next 211 of size 32768
2019-10-02 01:35:27.293065: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd20f29f00 next 212 of size 8192
2019-10-02 01:35:27.293071: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd20f2bf00 next 213 of size 8192
2019-10-02 01:35:27.293077: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd20f2df00 next 214 of size 8192
2019-10-02 01:35:27.293082: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd20f2ff00 next 215 of size 8192
2019-10-02 01:35:27.293088: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd20f31f00 next 216 of size 8192
2019-10-02 01:35:27.293094: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd20f33f00 next 217 of size 8192
2019-10-02 01:35:27.293099: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd20f35f00 next 218 of size 16777216
2019-10-02 01:35:27.293111: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd21f35f00 next 219 of size 512
2019-10-02 01:35:27.293117: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd21f36100 next 220 of size 16777216
2019-10-02 01:35:27.293122: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd22f36100 next 221 of size 8192
2019-10-02 01:35:27.293129: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd22f38100 next 222 of size 67108864
2019-10-02 01:35:27.293135: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd26f38100 next 223 of size 16777216
2019-10-02 01:35:27.293140: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd27f38100 next 224 of size 8192
2019-10-02 01:35:27.293146: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd27f3a100 next 225 of size 16384000
2019-10-02 01:35:27.293152: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd28eda100 next 226 of size 512
2019-10-02 01:35:27.293158: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd28eda300 next 227 of size 8192
2019-10-02 01:35:27.293165: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd28edc300 next 228 of size 8192
2019-10-02 01:35:27.293171: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd28ede300 next 229 of size 8192
2019-10-02 01:35:27.293177: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd28ee0300 next 230 of size 8192
2019-10-02 01:35:27.293183: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd28ee2300 next 231 of size 8192
2019-10-02 01:35:27.293191: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd28ee4300 next 232 of size 16777216
2019-10-02 01:35:27.293197: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd29ee4300 next 233 of size 8192
2019-10-02 01:35:27.293203: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd29ee6300 next 234 of size 67108864
2019-10-02 01:35:27.293209: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd2dee6300 next 235 of size 16384000
2019-10-02 01:35:27.293217: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd2ee86300 next 236 of size 16777216
2019-10-02 01:35:27.293222: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd2fe86300 next 237 of size 8192
2019-10-02 01:35:27.293228: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd2fe88300 next 238 of size 8192
2019-10-02 01:35:27.293234: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd2fe8a300 next 239 of size 512
2019-10-02 01:35:27.293241: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd2fe8a500 next 240 of size 16777216
2019-10-02 01:35:27.293247: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd30e8a500 next 241 of size 8192
2019-10-02 01:35:27.293253: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd30e8c500 next 242 of size 8192
2019-10-02 01:35:27.293259: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd30e8e500 next 243 of size 32768
2019-10-02 01:35:27.293267: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd30e96500 next 244 of size 16384000
2019-10-02 01:35:27.293273: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd31e36500 next 245 of size 16384000
2019-10-02 01:35:27.293279: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd32dd6500 next 246 of size 8192
2019-10-02 01:35:27.293284: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd32dd8500 next 247 of size 32768
2019-10-02 01:35:27.293290: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd32de0500 next 248 of size 8192
2019-10-02 01:35:27.293296: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd32de2500 next 249 of size 8192
2019-10-02 01:35:27.293302: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd32de4500 next 250 of size 32768
2019-10-02 01:35:27.293313: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd32dec500 next 251 of size 8192
2019-10-02 01:35:27.293320: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd32dee500 next 252 of size 8192
2019-10-02 01:35:27.293327: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd32df0500 next 253 of size 16777216
2019-10-02 01:35:27.293333: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd33df0500 next 254 of size 16384000
2019-10-02 01:35:27.293339: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd34d90500 next 255 of size 16384000
2019-10-02 01:35:27.293345: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd35d30500 next 256 of size 512
2019-10-02 01:35:27.293351: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd35d30700 next 257 of size 8192
2019-10-02 01:35:27.293357: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd35d32700 next 258 of size 8192
2019-10-02 01:35:27.293363: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd35d34700 next 259 of size 8192
2019-10-02 01:35:27.293369: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd35d36700 next 260 of size 67108864
2019-10-02 01:35:27.293375: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd39d36700 next 261 of size 8192
2019-10-02 01:35:27.293381: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd39d38700 next 262 of size 67108864
2019-10-02 01:35:27.293387: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd3dd38700 next 263 of size 512
2019-10-02 01:35:27.293394: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd3dd38900 next 264 of size 8192
2019-10-02 01:35:27.293400: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd3dd3a900 next 265 of size 8192
2019-10-02 01:35:27.293406: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd3dd3c900 next 266 of size 67108864
2019-10-02 01:35:27.293412: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd41d3c900 next 267 of size 16384000
2019-10-02 01:35:27.293418: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd42cdc900 next 268 of size 8192
2019-10-02 01:35:27.293424: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd42cde900 next 269 of size 8192
2019-10-02 01:35:27.293431: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd42ce0900 next 270 of size 8192
2019-10-02 01:35:27.293437: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd42ce2900 next 271 of size 67108864
2019-10-02 01:35:27.293443: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd46ce2900 next 272 of size 16777216
2019-10-02 01:35:27.293448: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd47ce2900 next 273 of size 16777216
2019-10-02 01:35:27.293456: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd48ce2900 next 274 of size 16777216
2019-10-02 01:35:27.293462: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd49ce2900 next 275 of size 16777216
2019-10-02 01:35:27.293467: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd4ace2900 next 276 of size 8192
2019-10-02 01:35:27.293473: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd4ace4900 next 277 of size 16384000
2019-10-02 01:35:27.293481: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd4bc84900 next 278 of size 16384000
2019-10-02 01:35:27.293486: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd4cc24900 next 279 of size 8192
2019-10-02 01:35:27.293492: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd4cc26900 next 280 of size 8192
2019-10-02 01:35:27.293498: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd4cc28900 next 281 of size 8192
2019-10-02 01:35:27.293508: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd4cc2a900 next 282 of size 8192
2019-10-02 01:35:27.293515: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd4cc2c900 next 283 of size 8192
2019-10-02 01:35:27.293520: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd4cc2e900 next 284 of size 67108864
2019-10-02 01:35:27.293526: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd50c2e900 next 285 of size 8192
2019-10-02 01:35:27.293534: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd50c30900 next 286 of size 16384000
2019-10-02 01:35:27.293540: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd51bd0900 next 287 of size 16384000
2019-10-02 01:35:27.293546: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd52b70900 next 288 of size 8192
2019-10-02 01:35:27.293552: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd52b72900 next 289 of size 8192
2019-10-02 01:35:27.293558: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd52b74900 next 290 of size 67108864
2019-10-02 01:35:27.293564: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd56b74900 next 291 of size 8192
2019-10-02 01:35:27.293570: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd56b76900 next 292 of size 8192
2019-10-02 01:35:27.293576: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd56b78900 next 293 of size 8192
2019-10-02 01:35:27.293581: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd56b7a900 next 294 of size 8192
2019-10-02 01:35:27.293587: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd56b7c900 next 295 of size 16777216
2019-10-02 01:35:27.293596: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd57b7c900 next 296 of size 8192
2019-10-02 01:35:27.293602: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd57b7e900 next 297 of size 32768
2019-10-02 01:35:27.293608: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd57b86900 next 298 of size 16777216
2019-10-02 01:35:27.293614: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd58b86900 next 299 of size 16777216
2019-10-02 01:35:27.293620: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd59b86900 next 300 of size 8192
2019-10-02 01:35:27.293626: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd59b88900 next 301 of size 8192
2019-10-02 01:35:27.293634: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd59b8a900 next 302 of size 16777216
2019-10-02 01:35:27.293639: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd5ab8a900 next 303 of size 512
2019-10-02 01:35:27.293645: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd5ab8ab00 next 304 of size 8192
2019-10-02 01:35:27.293651: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd5ab8cb00 next 305 of size 67108864
2019-10-02 01:35:27.293659: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd5eb8cb00 next 306 of size 8192
2019-10-02 01:35:27.293665: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd5eb8eb00 next 307 of size 16777216
2019-10-02 01:35:27.293671: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd5fb8eb00 next 308 of size 67108864
2019-10-02 01:35:27.293677: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd63b8eb00 next 309 of size 8192
2019-10-02 01:35:27.293684: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd63b90b00 next 310 of size 8192
2019-10-02 01:35:27.293690: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd63b92b00 next 311 of size 8192
2019-10-02 01:35:27.293696: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd63b94b00 next 312 of size 8192
2019-10-02 01:35:27.293711: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd63b96b00 next 313 of size 16777216
2019-10-02 01:35:27.293720: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd64b96b00 next 314 of size 16777216
2019-10-02 01:35:27.293726: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd65b96b00 next 315 of size 67108864
2019-10-02 01:35:27.293732: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd69b96b00 next 316 of size 8192
2019-10-02 01:35:27.293740: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd69b98b00 next 317 of size 8192
2019-10-02 01:35:27.293746: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd69b9ab00 next 318 of size 256
2019-10-02 01:35:27.293752: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd69b9ac00 next 319 of size 8192
2019-10-02 01:35:27.293758: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd69b9cc00 next 320 of size 512
2019-10-02 01:35:27.293765: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd69b9ce00 next 321 of size 67108864
2019-10-02 01:35:27.293771: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd6db9ce00 next 322 of size 32768
2019-10-02 01:35:27.293777: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd6dba4e00 next 323 of size 16777216
2019-10-02 01:35:27.293783: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd6eba4e00 next 324 of size 512
2019-10-02 01:35:27.293792: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd6eba5000 next 325 of size 32768
2019-10-02 01:35:27.293797: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd6ebad000 next 326 of size 8192
2019-10-02 01:35:27.293803: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd6ebaf000 next 327 of size 8192
2019-10-02 01:35:27.293809: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd6ebb1000 next 328 of size 8192
2019-10-02 01:35:27.293815: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd6ebb3000 next 329 of size 8192
2019-10-02 01:35:27.293821: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd6ebb5000 next 330 of size 16777216
2019-10-02 01:35:27.293826: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd6fbb5000 next 331 of size 32768
2019-10-02 01:35:27.293832: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd6fbbd000 next 332 of size 16777216
2019-10-02 01:35:27.293838: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd70bbd000 next 333 of size 8192
2019-10-02 01:35:27.293845: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd70bbf000 next 334 of size 16777216
2019-10-02 01:35:27.293851: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd71bbf000 next 335 of size 8192
2019-10-02 01:35:27.293858: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd71bc1000 next 336 of size 8192
2019-10-02 01:35:27.293864: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd71bc3000 next 337 of size 8192
2019-10-02 01:35:27.293870: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd71bc5000 next 338 of size 16777216
2019-10-02 01:35:27.293876: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd72bc5000 next 339 of size 8192
2019-10-02 01:35:27.293884: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd72bc7000 next 340 of size 16777216
2019-10-02 01:35:27.293890: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd73bc7000 next 341 of size 8192
2019-10-02 01:35:27.293896: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd73bc9000 next 342 of size 8192
2019-10-02 01:35:27.293902: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd73bcb000 next 343 of size 8192
2019-10-02 01:35:27.293908: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd73bcd000 next 344 of size 67108864
2019-10-02 01:35:27.293921: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd77bcd000 next 345 of size 8192
2019-10-02 01:35:27.293928: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd77bcf000 next 346 of size 16777216
2019-10-02 01:35:27.293934: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd78bcf000 next 347 of size 8192
2019-10-02 01:35:27.293940: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd78bd1000 next 348 of size 8192
2019-10-02 01:35:27.293947: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd78bd3000 next 349 of size 8192
2019-10-02 01:35:27.293954: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd78bd5000 next 350 of size 262144000
2019-10-02 01:35:27.293960: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd885d5000 next 351 of size 512
2019-10-02 01:35:27.293966: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd885d5200 next 352 of size 8192
2019-10-02 01:35:27.293971: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd885d7200 next 353 of size 8192
2019-10-02 01:35:27.293977: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd885d9200 next 354 of size 16777216
2019-10-02 01:35:27.293983: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd895d9200 next 355 of size 16777216
2019-10-02 01:35:27.293989: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd8a5d9200 next 356 of size 16777216
2019-10-02 01:35:27.293995: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd8b5d9200 next 357 of size 8192
2019-10-02 01:35:27.294004: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd8b5db200 next 358 of size 16777216
2019-10-02 01:35:27.294010: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd8c5db200 next 359 of size 32768
2019-10-02 01:35:27.294015: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd8c5e3200 next 360 of size 8192
2019-10-02 01:35:27.294021: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd8c5e5200 next 361 of size 8192
2019-10-02 01:35:27.294029: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd8c5e7200 next 362 of size 8192
2019-10-02 01:35:27.294035: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd8c5e9200 next 363 of size 512
2019-10-02 01:35:27.294041: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd8c5e9400 next 364 of size 16777216
2019-10-02 01:35:27.294046: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd8d5e9400 next 365 of size 8192
2019-10-02 01:35:27.294054: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd8d5eb400 next 366 of size 8192
2019-10-02 01:35:27.294060: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd8d5ed400 next 367 of size 16777216
2019-10-02 01:35:27.294066: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd8e5ed400 next 368 of size 16777216
2019-10-02 01:35:27.294072: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd8f5ed400 next 369 of size 67108864
2019-10-02 01:35:27.294080: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd935ed400 next 370 of size 16777216
2019-10-02 01:35:27.294086: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd945ed400 next 371 of size 67108864
2019-10-02 01:35:27.294092: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd985ed400 next 372 of size 8192
2019-10-02 01:35:27.294098: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd985ef400 next 373 of size 67108864
2019-10-02 01:35:27.294103: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd9c5ef400 next 374 of size 16777216
2019-10-02 01:35:27.294110: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd9d5ef400 next 375 of size 8192
2019-10-02 01:35:27.294121: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd9d5f1400 next 376 of size 8192
2019-10-02 01:35:27.294131: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd9d5f3400 next 377 of size 512
2019-10-02 01:35:27.294137: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd9d5f3600 next 378 of size 8192
2019-10-02 01:35:27.294145: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd9d5f5600 next 379 of size 8192
2019-10-02 01:35:27.294151: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd9d5f7600 next 380 of size 8192
2019-10-02 01:35:27.294157: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd9d5f9600 next 381 of size 8192
2019-10-02 01:35:27.294163: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd9d5fb600 next 382 of size 8192
2019-10-02 01:35:27.294170: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd9d5fd600 next 383 of size 16777216
2019-10-02 01:35:27.294176: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd9e5fd600 next 384 of size 32768
2019-10-02 01:35:27.294182: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd9e605600 next 385 of size 67108864
2019-10-02 01:35:27.294188: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda2605600 next 386 of size 8192
2019-10-02 01:35:27.294195: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda2607600 next 387 of size 8192
2019-10-02 01:35:27.294200: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda2609600 next 388 of size 512
2019-10-02 01:35:27.294206: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda2609800 next 389 of size 8192
2019-10-02 01:35:27.294212: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda260b800 next 390 of size 8192
2019-10-02 01:35:27.294220: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda260d800 next 391 of size 512
2019-10-02 01:35:27.294226: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda260da00 next 392 of size 8192
2019-10-02 01:35:27.294232: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda260fa00 next 393 of size 8192
2019-10-02 01:35:27.294237: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda2611a00 next 394 of size 8192
2019-10-02 01:35:27.294245: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda2613a00 next 395 of size 8192
2019-10-02 01:35:27.294250: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda2615a00 next 396 of size 8192
2019-10-02 01:35:27.294256: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda2617a00 next 397 of size 8192
2019-10-02 01:35:27.294262: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda2619a00 next 398 of size 8192
2019-10-02 01:35:27.294270: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda261ba00 next 399 of size 8192
2019-10-02 01:35:27.294276: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda261da00 next 400 of size 8192
2019-10-02 01:35:27.294281: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda261fa00 next 401 of size 8192
2019-10-02 01:35:27.294287: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda2621a00 next 402 of size 67108864
2019-10-02 01:35:27.294295: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda6621a00 next 403 of size 8192
2019-10-02 01:35:27.294301: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda6623a00 next 404 of size 16777216
2019-10-02 01:35:27.294308: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda7623a00 next 405 of size 67108864
2019-10-02 01:35:27.294314: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdab623a00 next 406 of size 8192
2019-10-02 01:35:27.294322: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdab625a00 next 407 of size 8192
2019-10-02 01:35:27.294331: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdab627a00 next 408 of size 8192
2019-10-02 01:35:27.294337: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdab629a00 next 409 of size 67108864
2019-10-02 01:35:27.294345: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdaf629a00 next 410 of size 16777216
2019-10-02 01:35:27.294351: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdb0629a00 next 411 of size 8192
2019-10-02 01:35:27.294357: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdb062ba00 next 412 of size 512
2019-10-02 01:35:27.294363: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdb062bc00 next 413 of size 67108864
2019-10-02 01:35:27.294369: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdb462bc00 next 414 of size 16777216
2019-10-02 01:35:27.294375: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdb562bc00 next 415 of size 8192
2019-10-02 01:35:27.294383: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdb562dc00 next 416 of size 8192
2019-10-02 01:35:27.294389: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdb562fc00 next 417 of size 16777216
2019-10-02 01:35:27.294395: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdb662fc00 next 418 of size 8192
2019-10-02 01:35:27.294401: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdb6631c00 next 419 of size 8192
2019-10-02 01:35:27.294408: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdb6633c00 next 420 of size 32768
2019-10-02 01:35:27.294414: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdb663bc00 next 421 of size 16777216
2019-10-02 01:35:27.294420: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdb763bc00 next 422 of size 16777216
2019-10-02 01:35:27.294426: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdb863bc00 next 423 of size 32768
2019-10-02 01:35:27.294433: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdb8643c00 next 424 of size 16777216
2019-10-02 01:35:27.294439: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdb9643c00 next 425 of size 8192
2019-10-02 01:35:27.294446: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdb9645c00 next 426 of size 16777216
2019-10-02 01:35:27.294452: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdba645c00 next 427 of size 8192
2019-10-02 01:35:27.294458: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdba647c00 next 428 of size 67108864
2019-10-02 01:35:27.294463: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdbe647c00 next 429 of size 8192
2019-10-02 01:35:27.294470: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdbe649c00 next 430 of size 8192
2019-10-02 01:35:27.294476: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdbe64bc00 next 431 of size 16777216
2019-10-02 01:35:27.294483: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdbf64bc00 next 432 of size 8192
2019-10-02 01:35:27.294489: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdbf64dc00 next 433 of size 8192
2019-10-02 01:35:27.294496: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdbf64fc00 next 434 of size 512
2019-10-02 01:35:27.294501: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdbf64fe00 next 435 of size 16777216
2019-10-02 01:35:27.294509: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc064fe00 next 436 of size 8192
2019-10-02 01:35:27.294515: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc0651e00 next 437 of size 16777216
2019-10-02 01:35:27.294521: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc1651e00 next 438 of size 8192
2019-10-02 01:35:27.294531: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc1653e00 next 439 of size 8192
2019-10-02 01:35:27.294537: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc1655e00 next 440 of size 67108864
2019-10-02 01:35:27.294543: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc5655e00 next 441 of size 16777216
2019-10-02 01:35:27.294552: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc6655e00 next 442 of size 8192
2019-10-02 01:35:27.294557: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc6657e00 next 443 of size 16777216
2019-10-02 01:35:27.294563: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc7657e00 next 444 of size 8192
2019-10-02 01:35:27.294569: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc7659e00 next 445 of size 8192
2019-10-02 01:35:27.294577: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc765be00 next 446 of size 8192
2019-10-02 01:35:27.294582: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc765de00 next 447 of size 8192
2019-10-02 01:35:27.294588: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc765fe00 next 448 of size 8192
2019-10-02 01:35:27.294594: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc7661e00 next 449 of size 8192
2019-10-02 01:35:27.294602: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc7663e00 next 450 of size 8192
2019-10-02 01:35:27.294607: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc7665e00 next 451 of size 16777216
2019-10-02 01:35:27.294614: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc8665e00 next 452 of size 512
2019-10-02 01:35:27.294620: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc8666000 next 453 of size 8192
2019-10-02 01:35:27.294627: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc8668000 next 454 of size 8192
2019-10-02 01:35:27.294634: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc866a000 next 455 of size 32768
2019-10-02 01:35:27.294640: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc8672000 next 456 of size 512
2019-10-02 01:35:27.294646: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc8672200 next 457 of size 8192
2019-10-02 01:35:27.294652: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc8674200 next 458 of size 8192
2019-10-02 01:35:27.294658: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc8676200 next 459 of size 16777216
2019-10-02 01:35:27.294666: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc9676200 next 460 of size 16777216
2019-10-02 01:35:27.294671: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdca676200 next 461 of size 8192
2019-10-02 01:35:27.294677: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdca678200 next 462 of size 8192
2019-10-02 01:35:27.294683: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdca67a200 next 463 of size 8192
2019-10-02 01:35:27.294689: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdca67c200 next 464 of size 512
2019-10-02 01:35:27.294695: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdca67c400 next 465 of size 16777216
2019-10-02 01:35:27.294701: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdcb67c400 next 466 of size 32768
2019-10-02 01:35:27.294706: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdcb684400 next 467 of size 8192
2019-10-02 01:35:27.294712: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdcb686400 next 468 of size 8192
2019-10-02 01:35:27.294718: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdcb688400 next 469 of size 16777216
2019-10-02 01:35:27.294740: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdcc688400 next 470 of size 32768
2019-10-02 01:35:27.294747: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdcc690400 next 471 of size 8192
2019-10-02 01:35:27.294754: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdcc692400 next 472 of size 16777216
2019-10-02 01:35:27.294760: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdcd692400 next 473 of size 8192
2019-10-02 01:35:27.294765: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdcd694400 next 474 of size 8192
2019-10-02 01:35:27.294784: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdcd696400 next 475 of size 67108864
2019-10-02 01:35:27.294790: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdd1696400 next 476 of size 8192
2019-10-02 01:35:27.294796: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdd1698400 next 477 of size 67108864
2019-10-02 01:35:27.294802: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdd5698400 next 478 of size 256
2019-10-02 01:35:27.294808: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdd5698500 next 479 of size 8192
2019-10-02 01:35:27.294813: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdd569a500 next 480 of size 512
2019-10-02 01:35:27.294819: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdd569a700 next 481 of size 67108864
2019-10-02 01:35:27.294825: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdd969a700 next 482 of size 256
2019-10-02 01:35:27.294831: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdd969a800 next 514 of size 256
2019-10-02 01:35:27.294837: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdd969a900 next 484 of size 256
2019-10-02 01:35:27.294843: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdd969aa00 next 485 of size 8192
2019-10-02 01:35:27.294850: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdd969ca00 next 486 of size 8192
2019-10-02 01:35:27.294856: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdd969ea00 next 487 of size 16777216
2019-10-02 01:35:27.294863: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdda69ea00 next 515 of size 8192
2019-10-02 01:35:27.294869: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdda6a0a00 next 512 of size 512
2019-10-02 01:35:27.294875: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdda6a0c00 next 513 of size 8192
2019-10-02 01:35:27.294880: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdda6a2c00 next 510 of size 16777216
2019-10-02 01:35:27.294889: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efddb6a2c00 next 511 of size 8192
2019-10-02 01:35:27.294895: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efddb6a4c00 next 500 of size 16777216
2019-10-02 01:35:27.294901: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efddc6a4c00 next 501 of size 16777216
2019-10-02 01:35:27.294906: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efddd6a4c00 next 509 of size 8192
2019-10-02 01:35:27.294914: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efddd6a6c00 next 504 of size 8192
2019-10-02 01:35:27.294920: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efddd6a8c00 next 505 of size 67108864
2019-10-02 01:35:27.294926: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde16a8c00 next 489 of size 32768
2019-10-02 01:35:27.294932: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde16b0c00 next 490 of size 8192
2019-10-02 01:35:27.294940: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde16b2c00 next 488 of size 8192
2019-10-02 01:35:27.294946: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde16b4c00 next 508 of size 16777216
2019-10-02 01:35:27.294955: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde26b4c00 next 503 of size 8192
2019-10-02 01:35:27.294963: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde26b6c00 next 502 of size 8192
2019-10-02 01:35:27.294970: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde26b8c00 next 507 of size 8192
2019-10-02 01:35:27.294976: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde26bac00 next 497 of size 16777216
2019-10-02 01:35:27.294982: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde36bac00 next 498 of size 67108864
2019-10-02 01:35:27.294987: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde76bac00 next 506 of size 8192
2019-10-02 01:35:27.294993: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde76bcc00 next 496 of size 8192
2019-10-02 01:35:27.294999: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde76bec00 next 492 of size 16777216
2019-10-02 01:35:27.295006: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde86bec00 next 493 of size 512
2019-10-02 01:35:27.295012: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde86bee00 next 483 of size 8192
2019-10-02 01:35:27.295018: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde86c0e00 next 495 of size 16777216
2019-10-02 01:35:27.295024: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde96c0e00 next 491 of size 8192
2019-10-02 01:35:27.295030: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde96c2e00 next 499 of size 8192
2019-10-02 01:35:27.295036: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde96c4e00 next 494 of size 32768
2019-10-02 01:35:27.295042: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde96cce00 next 516 of size 16777216
2019-10-02 01:35:27.295048: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdea6cce00 next 517 of size 8192
2019-10-02 01:35:27.295054: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdea6cee00 next 518 of size 8192
2019-10-02 01:35:27.295059: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdea6d0e00 next 519 of size 16777216
2019-10-02 01:35:27.295065: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdeb6d0e00 next 520 of size 67108864
2019-10-02 01:35:27.295072: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdef6d0e00 next 521 of size 8192
2019-10-02 01:35:27.295077: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdef6d2e00 next 522 of size 16777216
2019-10-02 01:35:27.295083: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf06d2e00 next 523 of size 8192
2019-10-02 01:35:27.295098: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf06d4e00 next 524 of size 8192
2019-10-02 01:35:27.295104: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf06d6e00 next 525 of size 8192
2019-10-02 01:35:27.295110: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf06d8e00 next 526 of size 16777216
2019-10-02 01:35:27.295116: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16d8e00 next 527 of size 8192
2019-10-02 01:35:27.295122: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16dae00 next 528 of size 8192
2019-10-02 01:35:27.295128: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16dce00 next 529 of size 8192
2019-10-02 01:35:27.295134: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16dee00 next 530 of size 256
2019-10-02 01:35:27.295140: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16def00 next 545 of size 256
2019-10-02 01:35:27.295146: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16df000 next 547 of size 8192
2019-10-02 01:35:27.295155: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16e1000 next 548 of size 256
2019-10-02 01:35:27.295162: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16e1100 next 549 of size 8192
2019-10-02 01:35:27.295169: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16e3100 next 546 of size 256
2019-10-02 01:35:27.295175: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16e3200 next 550 of size 8192
2019-10-02 01:35:27.295182: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16e5200 next 551 of size 256
2019-10-02 01:35:27.295188: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16e5300 next 552 of size 8192
2019-10-02 01:35:27.295194: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16e7300 next 554 of size 256
2019-10-02 01:35:27.295200: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16e7400 next 586 of size 4096
2019-10-02 01:35:27.295207: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16e8400 next 577 of size 256
2019-10-02 01:35:27.295214: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16e8500 next 590 of size 5376
2019-10-02 01:35:27.295220: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16e9a00 next 559 of size 8192
2019-10-02 01:35:27.295226: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16eba00 next 657 of size 9984
2019-10-02 01:35:27.295235: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16ee100 next 684 of size 6912
2019-10-02 01:35:27.295241: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16efc00 next 685 of size 256
2019-10-02 01:35:27.295247: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16efd00 next 686 of size 256
2019-10-02 01:35:27.295253: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16efe00 next 687 of size 256
2019-10-02 01:35:27.295260: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16eff00 next 688 of size 256
2019-10-02 01:35:27.295266: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f0000 next 683 of size 256
2019-10-02 01:35:27.295272: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f0100 next 690 of size 256
2019-10-02 01:35:27.295279: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f0200 next 665 of size 256
2019-10-02 01:35:27.295285: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f0300 next 605 of size 256
2019-10-02 01:35:27.295291: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f0400 next 697 of size 256
2019-10-02 01:35:27.295338: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f0500 next 936 of size 256
2019-10-02 01:35:27.295352: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f0600 next 630 of size 512
2019-10-02 01:35:27.295358: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f0800 next 1011 of size 256
2019-10-02 01:35:27.295364: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f0900 next 699 of size 512
2019-10-02 01:35:27.295370: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f0b00 next 700 of size 256
2019-10-02 01:35:27.295382: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f0c00 next 701 of size 256
2019-10-02 01:35:27.295388: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f0d00 next 703 of size 256
2019-10-02 01:35:27.295394: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f0e00 next 705 of size 256
2019-10-02 01:35:27.295400: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f0f00 next 706 of size 256
2019-10-02 01:35:27.295412: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f1000 next 707 of size 256
2019-10-02 01:35:27.295422: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f1100 next 712 of size 256
2019-10-02 01:35:27.295428: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f1200 next 637 of size 256
2019-10-02 01:35:27.295441: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f1300 next 714 of size 256
2019-10-02 01:35:27.295449: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f1400 next 531 of size 56064
2019-10-02 01:35:27.295455: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16fef00 next 532 of size 67108864
2019-10-02 01:35:27.295461: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf56fef00 next 533 of size 256
2019-10-02 01:35:27.295467: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf56ff000 next 534 of size 16777216
2019-10-02 01:35:27.295473: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf66ff000 next 535 of size 131072
2019-10-02 01:35:27.295479: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf671f000 next 715 of size 32768
2019-10-02 01:35:27.295485: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6727000 next 718 of size 256
2019-10-02 01:35:27.295491: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6727100 next 677 of size 256
2019-10-02 01:35:27.295497: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6727200 next 720 of size 256
2019-10-02 01:35:27.295503: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6727300 next 721 of size 8192
2019-10-02 01:35:27.295509: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6729300 next 722 of size 8192
2019-10-02 01:35:27.295515: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf672b300 next 724 of size 256
2019-10-02 01:35:27.295521: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf672b400 next 725 of size 256
2019-10-02 01:35:27.295527: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf672b500 next 726 of size 256
2019-10-02 01:35:27.295533: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf672b600 next 727 of size 256
2019-10-02 01:35:27.295539: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf672b700 next 728 of size 8192
2019-10-02 01:35:27.295545: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf672d700 next 729 of size 8192
2019-10-02 01:35:27.295556: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf672f700 next 730 of size 256
2019-10-02 01:35:27.295562: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf672f800 next 731 of size 256
2019-10-02 01:35:27.295568: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf672f900 next 732 of size 256
2019-10-02 01:35:27.295574: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf672fa00 next 733 of size 256
2019-10-02 01:35:27.295586: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf672fb00 next 734 of size 8192
2019-10-02 01:35:27.295592: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6731b00 next 735 of size 8192
2019-10-02 01:35:27.295598: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6733b00 next 736 of size 8192
2019-10-02 01:35:27.295604: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6735b00 next 737 of size 256
2019-10-02 01:35:27.295610: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6735c00 next 738 of size 8192
2019-10-02 01:35:27.295622: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6737c00 next 739 of size 8192
2019-10-02 01:35:27.295628: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6739c00 next 740 of size 256
2019-10-02 01:35:27.295634: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6739d00 next 741 of size 8192
2019-10-02 01:35:27.295652: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf673bd00 next 536 of size 13056
2019-10-02 01:35:27.295659: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf673f000 next 537 of size 8192
2019-10-02 01:35:27.295665: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6741000 next 538 of size 256
2019-10-02 01:35:27.295672: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6741100 next 539 of size 8192
2019-10-02 01:35:27.295678: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6743100 next 540 of size 131072
2019-10-02 01:35:27.295684: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6763100 next 541 of size 256
2019-10-02 01:35:27.295690: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6763200 next 542 of size 67108864
2019-10-02 01:35:27.295696: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdfa763200 next 543 of size 8192
2019-10-02 01:35:27.295702: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdfa765200 next 544 of size 67108864
2019-10-02 01:35:27.295708: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdfe765200 next 553 of size 131072
2019-10-02 01:35:27.295714: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdfe785200 next 582 of size 16777216
2019-10-02 01:35:27.295720: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff785200 next 742 of size 256
2019-10-02 01:35:27.295726: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff785300 next 743 of size 8192
2019-10-02 01:35:27.295732: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff787300 next 744 of size 8192
2019-10-02 01:35:27.295737: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff789300 next 745 of size 256
2019-10-02 01:35:27.295748: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff789400 next 746 of size 8192
2019-10-02 01:35:27.295754: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff78b400 next 747 of size 8192
2019-10-02 01:35:27.295760: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff78d400 next 748 of size 256
2019-10-02 01:35:27.295766: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff78d500 next 749 of size 8192
2019-10-02 01:35:27.295772: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff78f500 next 750 of size 8192
2019-10-02 01:35:27.295778: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff791500 next 751 of size 256
2019-10-02 01:35:27.295783: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff791600 next 752 of size 8192
2019-10-02 01:35:27.295789: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff793600 next 753 of size 256
2019-10-02 01:35:27.295795: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff793700 next 754 of size 256
2019-10-02 01:35:27.295801: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff793800 next 756 of size 256
2019-10-02 01:35:27.295806: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff793900 next 758 of size 8192
2019-10-02 01:35:27.295812: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff795900 next 759 of size 256
2019-10-02 01:35:27.295818: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff795a00 next 760 of size 8192
2019-10-02 01:35:27.295823: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff797a00 next 761 of size 8192
2019-10-02 01:35:27.295829: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff799a00 next 762 of size 256
2019-10-02 01:35:27.295835: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff799b00 next 763 of size 8192
2019-10-02 01:35:27.295841: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff79bb00 next 764 of size 8192
2019-10-02 01:35:27.295856: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff79db00 next 765 of size 256
2019-10-02 01:35:27.295862: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff79dc00 next 766 of size 8192
2019-10-02 01:35:27.295868: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff79fc00 next 769 of size 256
2019-10-02 01:35:27.295874: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff79fd00 next 771 of size 256
2019-10-02 01:35:27.295879: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff79fe00 next 773 of size 256
2019-10-02 01:35:27.295891: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff79ff00 next 776 of size 256
2019-10-02 01:35:27.295897: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff7a0000 next 778 of size 256
2019-10-02 01:35:27.295903: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff7a0100 next 780 of size 256
2019-10-02 01:35:27.295909: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff7a0200 next 782 of size 256
2019-10-02 01:35:27.295920: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff7a0300 next 785 of size 256
2019-10-02 01:35:27.295927: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff7a0400 next 786 of size 8192
2019-10-02 01:35:27.295933: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff7a2400 next 787 of size 512
2019-10-02 01:35:27.295939: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff7a2600 next 581 of size 11264
2019-10-02 01:35:27.295951: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff7a5200 next 580 of size 16777216
2019-10-02 01:35:27.295957: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe007a5200 next 579 of size 67108864
2019-10-02 01:35:27.295963: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe047a5200 next 578 of size 131072
2019-10-02 01:35:27.295969: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe047c5200 next 585 of size 16777216
2019-10-02 01:35:27.295975: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe057c5200 next 584 of size 8388608
2019-10-02 01:35:27.295987: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe05fc5200 next 583 of size 16777216
2019-10-02 01:35:27.295994: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe06fc5200 next 576 of size 16777216
2019-10-02 01:35:27.296000: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe07fc5200 next 702 of size 67108864
2019-10-02 01:35:27.296006: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe0bfc5200 next 704 of size 16777216
2019-10-02 01:35:27.296018: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe0cfc5200 next 708 of size 67108864
2019-10-02 01:35:27.296024: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe10fc5200 next 709 of size 16777216
2019-10-02 01:35:27.296030: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe11fc5200 next 710 of size 67108864
2019-10-02 01:35:27.296035: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe15fc5200 next 711 of size 16777216
2019-10-02 01:35:27.296047: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe16fc5200 next 716 of size 16777216
2019-10-02 01:35:27.296053: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe17fc5200 next 717 of size 67108864
2019-10-02 01:35:27.296059: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe1bfc5200 next 723 of size 67108864
2019-10-02 01:35:27.296065: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe1ffc5200 next 755 of size 16777216
2019-10-02 01:35:27.296076: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe20fc5200 next 757 of size 16777216
2019-10-02 01:35:27.296085: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe21fc5200 next 767 of size 16777216
2019-10-02 01:35:27.296092: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe22fc5200 next 768 of size 16777216
2019-10-02 01:35:27.296098: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe23fc5200 next 770 of size 67108864
2019-10-02 01:35:27.296109: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe27fc5200 next 772 of size 32768
2019-10-02 01:35:27.296116: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe27fcd200 next 774 of size 32768
2019-10-02 01:35:27.296122: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe27fd5200 next 775 of size 16777216
2019-10-02 01:35:27.296128: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe28fd5200 next 777 of size 16777216
2019-10-02 01:35:27.296134: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe29fd5200 next 779 of size 16777216
2019-10-02 01:35:27.296140: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe2afd5200 next 781 of size 67108864
2019-10-02 01:35:27.296146: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe2efd5200 next 783 of size 16777216
2019-10-02 01:35:27.296152: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe2ffd5200 next 784 of size 67108864
2019-10-02 01:35:27.296158: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe33fd5200 next 788 of size 512
2019-10-02 01:35:27.296164: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe33fd5400 next 789 of size 256
2019-10-02 01:35:27.296170: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe33fd5500 next 790 of size 256
2019-10-02 01:35:27.296176: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe33fd5600 next 791 of size 256
2019-10-02 01:35:27.296182: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe33fd5700 next 792 of size 256
2019-10-02 01:35:27.296188: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe33fd5800 next 793 of size 512
2019-10-02 01:35:27.296194: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe33fd5a00 next 794 of size 8192
2019-10-02 01:35:27.296200: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe33fd7a00 next 795 of size 512
2019-10-02 01:35:27.296206: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe33fd7c00 next 796 of size 8192
2019-10-02 01:35:27.296217: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe33fd9c00 next 797 of size 8192
2019-10-02 01:35:27.296223: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe33fdbc00 next 798 of size 8192
2019-10-02 01:35:27.296229: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe33fddc00 next 799 of size 16777216
2019-10-02 01:35:27.296235: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe34fddc00 next 800 of size 256
2019-10-02 01:35:27.296247: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe34fddd00 next 801 of size 16777216
2019-10-02 01:35:27.296254: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe35fddd00 next 802 of size 67108864
2019-10-02 01:35:27.296260: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe39fddd00 next 803 of size 16777216
2019-10-02 01:35:27.296266: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3afddd00 next 804 of size 256
2019-10-02 01:35:27.296277: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3afdde00 next 805 of size 8192
2019-10-02 01:35:27.296284: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3afdfe00 next 806 of size 256
2019-10-02 01:35:27.296290: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3afdff00 next 807 of size 8192
2019-10-02 01:35:27.296295: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3afe1f00 next 808 of size 256
2019-10-02 01:35:27.296312: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3afe2000 next 809 of size 8192
2019-10-02 01:35:27.296320: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3afe4000 next 810 of size 256
2019-10-02 01:35:27.296326: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3afe4100 next 811 of size 8192
2019-10-02 01:35:27.296332: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3afe6100 next 812 of size 8192
2019-10-02 01:35:27.296344: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3afe8100 next 813 of size 256
2019-10-02 01:35:27.296350: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3afe8200 next 814 of size 8192
2019-10-02 01:35:27.296356: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3afea200 next 815 of size 8192
2019-10-02 01:35:27.296362: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3afec200 next 816 of size 256
2019-10-02 01:35:27.296368: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3afec300 next 817 of size 8192
2019-10-02 01:35:27.296374: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3afee300 next 818 of size 32768
2019-10-02 01:35:27.296380: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3aff6300 next 819 of size 256
2019-10-02 01:35:27.296386: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3aff6400 next 820 of size 32768
2019-10-02 01:35:27.296392: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3affe400 next 821 of size 32768
2019-10-02 01:35:27.296398: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3b006400 next 822 of size 256
2019-10-02 01:35:27.296409: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3b006500 next 823 of size 32768
2019-10-02 01:35:27.296416: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3b00e500 next 824 of size 8192
2019-10-02 01:35:27.296422: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3b010500 next 825 of size 256
2019-10-02 01:35:27.296428: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3b010600 next 826 of size 8192
2019-10-02 01:35:27.296439: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3b012600 next 827 of size 16777216
2019-10-02 01:35:27.296445: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3c012600 next 828 of size 256
2019-10-02 01:35:27.296451: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3c012700 next 829 of size 16777216
2019-10-02 01:35:27.296457: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3d012700 next 830 of size 16777216
2019-10-02 01:35:27.296469: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3e012700 next 831 of size 8192
2019-10-02 01:35:27.296475: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3e014700 next 832 of size 256
2019-10-02 01:35:27.296481: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3e014800 next 833 of size 8192
2019-10-02 01:35:27.296504: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3e016800 next 834 of size 8192
2019-10-02 01:35:27.296510: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3e018800 next 835 of size 256
2019-10-02 01:35:27.296516: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3e018900 next 836 of size 8192
2019-10-02 01:35:27.296522: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3e01a900 next 837 of size 16777216
2019-10-02 01:35:27.296544: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3f01a900 next 838 of size 67108864
2019-10-02 01:35:27.296550: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4301a900 next 839 of size 8192
2019-10-02 01:35:27.296559: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4301c900 next 840 of size 256
2019-10-02 01:35:27.296565: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4301ca00 next 841 of size 8192
2019-10-02 01:35:27.296571: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4301ea00 next 842 of size 256
2019-10-02 01:35:27.296577: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4301eb00 next 843 of size 8192
2019-10-02 01:35:27.296590: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe43020b00 next 844 of size 256
2019-10-02 01:35:27.296597: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe43020c00 next 845 of size 8192
2019-10-02 01:35:27.296603: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe43022c00 next 846 of size 8192
2019-10-02 01:35:27.296608: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe43024c00 next 847 of size 256
2019-10-02 01:35:27.296620: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe43024d00 next 848 of size 8192
2019-10-02 01:35:27.296626: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe43026d00 next 849 of size 16777216
2019-10-02 01:35:27.296633: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe44026d00 next 850 of size 16777216
2019-10-02 01:35:27.296639: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe45026d00 next 851 of size 256
2019-10-02 01:35:27.296645: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe45026e00 next 852 of size 256
2019-10-02 01:35:27.296651: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe45026f00 next 853 of size 16777216
2019-10-02 01:35:27.296657: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe46026f00 next 854 of size 16777216
2019-10-02 01:35:27.296663: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe47026f00 next 855 of size 67108864
2019-10-02 01:35:27.296670: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4b026f00 next 856 of size 256
2019-10-02 01:35:27.296676: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4b027000 next 857 of size 256
2019-10-02 01:35:27.296682: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4b027100 next 858 of size 512
2019-10-02 01:35:27.296688: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4b027300 next 859 of size 256
2019-10-02 01:35:27.296694: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4b027400 next 860 of size 512
2019-10-02 01:35:27.296700: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4b027600 next 861 of size 16777216
2019-10-02 01:35:27.296706: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4c027600 next 862 of size 16777216
2019-10-02 01:35:27.296712: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4d027600 next 863 of size 256
2019-10-02 01:35:27.296718: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4d027700 next 864 of size 8192
2019-10-02 01:35:27.296723: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4d029700 next 865 of size 256
2019-10-02 01:35:27.296735: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4d029800 next 866 of size 256
2019-10-02 01:35:27.296741: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4d029900 next 867 of size 8192
2019-10-02 01:35:27.296747: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4d02b900 next 868 of size 256
2019-10-02 01:35:27.296753: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4d02ba00 next 869 of size 8192
2019-10-02 01:35:27.296759: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4d02da00 next 870 of size 256
2019-10-02 01:35:27.296765: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4d02db00 next 871 of size 8192
2019-10-02 01:35:27.296786: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4d02fb00 next 872 of size 8192
2019-10-02 01:35:27.296793: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4d031b00 next 873 of size 256
2019-10-02 01:35:27.296799: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4d031c00 next 874 of size 8192
2019-10-02 01:35:27.296805: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4d033c00 next 875 of size 67108864
2019-10-02 01:35:27.296811: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe51033c00 next 876 of size 16777216
2019-10-02 01:35:27.296817: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe52033c00 next 877 of size 256
2019-10-02 01:35:27.296823: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe52033d00 next 878 of size 67108864
2019-10-02 01:35:27.296829: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe56033d00 next 879 of size 256
2019-10-02 01:35:27.296835: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe56033e00 next 880 of size 8192
2019-10-02 01:35:27.296841: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe56035e00 next 881 of size 256
2019-10-02 01:35:27.296848: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe56035f00 next 882 of size 16777216
2019-10-02 01:35:27.296854: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe57035f00 next 883 of size 8192
2019-10-02 01:35:27.296866: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe57037f00 next 884 of size 256
2019-10-02 01:35:27.296872: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe57038000 next 885 of size 67108864
2019-10-02 01:35:27.296878: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe5b038000 next 886 of size 16777216
2019-10-02 01:35:27.296884: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe5c038000 next 887 of size 16777216
2019-10-02 01:35:27.296896: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe5d038000 next 888 of size 16777216
2019-10-02 01:35:27.296903: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe5e038000 next 889 of size 16777216
2019-10-02 01:35:27.296908: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe5f038000 next 890 of size 67108864
2019-10-02 01:35:27.296914: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe63038000 next 891 of size 256
2019-10-02 01:35:27.296920: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe63038100 next 892 of size 16777216
2019-10-02 01:35:27.296927: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe64038100 next 893 of size 256
2019-10-02 01:35:27.296939: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe64038200 next 894 of size 256
2019-10-02 01:35:27.296945: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe64038300 next 895 of size 8192
2019-10-02 01:35:27.296951: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6403a300 next 896 of size 256
2019-10-02 01:35:27.296957: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6403a400 next 897 of size 8192
2019-10-02 01:35:27.296969: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6403c400 next 898 of size 8192
2019-10-02 01:35:27.296975: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6403e400 next 899 of size 256
2019-10-02 01:35:27.296981: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6403e500 next 900 of size 8192
2019-10-02 01:35:27.296987: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe64040500 next 901 of size 16777216
2019-10-02 01:35:27.296993: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe65040500 next 902 of size 67108864
2019-10-02 01:35:27.297008: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe69040500 next 903 of size 16777216
2019-10-02 01:35:27.297014: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6a040500 next 904 of size 256
2019-10-02 01:35:27.297020: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6a040600 next 905 of size 32768
2019-10-02 01:35:27.297026: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6a048600 next 906 of size 256
2019-10-02 01:35:27.297032: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6a048700 next 907 of size 32768
2019-10-02 01:35:27.297038: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6a050700 next 908 of size 16777216
2019-10-02 01:35:27.297050: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6b050700 next 909 of size 256
2019-10-02 01:35:27.297056: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6b050800 next 910 of size 8192
2019-10-02 01:35:27.297063: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6b052800 next 911 of size 256
2019-10-02 01:35:27.297069: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6b052900 next 912 of size 8192
2019-10-02 01:35:27.297080: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6b054900 next 913 of size 8192
2019-10-02 01:35:27.297086: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6b056900 next 914 of size 256
2019-10-02 01:35:27.297092: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6b056a00 next 915 of size 8192
2019-10-02 01:35:27.297097: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6b058a00 next 916 of size 67108864
2019-10-02 01:35:27.297103: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6f058a00 next 917 of size 16777216
2019-10-02 01:35:27.297116: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe70058a00 next 918 of size 16777216
2019-10-02 01:35:27.297122: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe71058a00 next 919 of size 256
2019-10-02 01:35:27.297128: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe71058b00 next 920 of size 8192
2019-10-02 01:35:27.297134: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7105ab00 next 921 of size 256
2019-10-02 01:35:27.297145: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7105ac00 next 922 of size 8192
2019-10-02 01:35:27.297151: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7105cc00 next 923 of size 8192
2019-10-02 01:35:27.297157: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7105ec00 next 924 of size 256
2019-10-02 01:35:27.297162: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7105ed00 next 925 of size 8192
2019-10-02 01:35:27.297174: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe71060d00 next 926 of size 16777216
2019-10-02 01:35:27.297181: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe72060d00 next 927 of size 256
2019-10-02 01:35:27.297187: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe72060e00 next 928 of size 67108864
2019-10-02 01:35:27.297192: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe76060e00 next 929 of size 67108864
2019-10-02 01:35:27.297204: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7a060e00 next 930 of size 67108864
2019-10-02 01:35:27.297210: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e060e00 next 931 of size 8192
2019-10-02 01:35:27.297217: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e062e00 next 932 of size 256
2019-10-02 01:35:27.297223: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e062f00 next 933 of size 8192
2019-10-02 01:35:27.297234: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e064f00 next 934 of size 256
2019-10-02 01:35:27.297243: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e065000 next 935 of size 256
2019-10-02 01:35:27.297249: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e065100 next 622 of size 8388608
2019-10-02 01:35:27.297255: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e865100 next 1016 of size 256
2019-10-02 01:35:27.297261: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e865200 next 564 of size 512
2019-10-02 01:35:27.297267: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e865400 next 604 of size 256
2019-10-02 01:35:27.297273: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e865500 next 624 of size 256
2019-10-02 01:35:27.297279: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e865600 next 613 of size 512
2019-10-02 01:35:27.297285: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e865800 next 1031 of size 256
2019-10-02 01:35:27.297291: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e865900 next 1033 of size 256
2019-10-02 01:35:27.297297: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e865a00 next 1034 of size 256
2019-10-02 01:35:27.297303: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e865b00 next 619 of size 256
2019-10-02 01:35:27.297314: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e865c00 next 611 of size 256
2019-10-02 01:35:27.297320: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e865d00 next 614 of size 256
2019-10-02 01:35:27.297326: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e865e00 next 680 of size 256
2019-10-02 01:35:27.297331: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e865f00 next 659 of size 256
2019-10-02 01:35:27.297337: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e866000 next 566 of size 256
2019-10-02 01:35:27.297344: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e866100 next 649 of size 256
2019-10-02 01:35:27.297350: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e866200 next 617 of size 768
2019-10-02 01:35:27.297356: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e866500 next 608 of size 256
2019-10-02 01:35:27.297362: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e866600 next 588 of size 256
2019-10-02 01:35:27.297368: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e866700 next 644 of size 256
2019-10-02 01:35:27.297374: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e866800 next 661 of size 256
2019-10-02 01:35:27.297380: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e866900 next 591 of size 256
2019-10-02 01:35:27.297392: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e866a00 next 944 of size 256
2019-10-02 01:35:27.297399: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e866b00 next 945 of size 256
2019-10-02 01:35:27.297406: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e866c00 next 946 of size 256
2019-10-02 01:35:27.297412: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e866d00 next 947 of size 256
2019-10-02 01:35:27.297425: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e866e00 next 948 of size 8192
2019-10-02 01:35:27.297431: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e868e00 next 949 of size 256
2019-10-02 01:35:27.297437: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e868f00 next 950 of size 8192
2019-10-02 01:35:27.297443: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e86af00 next 952 of size 256
2019-10-02 01:35:27.297454: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e86b000 next 953 of size 256
2019-10-02 01:35:27.297463: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e86b100 next 954 of size 256
2019-10-02 01:35:27.297469: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e86b200 next 956 of size 256
2019-10-02 01:35:27.297475: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e86b300 next 961 of size 256
2019-10-02 01:35:27.297481: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e86b400 next 962 of size 256
2019-10-02 01:35:27.297487: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e86b500 next 963 of size 256
2019-10-02 01:35:27.297492: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e86b600 next 964 of size 8192
2019-10-02 01:35:27.297498: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e86d600 next 965 of size 8192
2019-10-02 01:35:27.297504: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e86f600 next 966 of size 256
2019-10-02 01:35:27.297517: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e86f700 next 967 of size 8192
2019-10-02 01:35:27.297523: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e871700 next 968 of size 256
2019-10-02 01:35:27.297528: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e871800 next 969 of size 8192
2019-10-02 01:35:27.297534: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e873800 next 971 of size 256
2019-10-02 01:35:27.297547: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e873900 next 972 of size 256
2019-10-02 01:35:27.297553: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e873a00 next 973 of size 256
2019-10-02 01:35:27.297558: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e873b00 next 974 of size 8192
2019-10-02 01:35:27.297564: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e875b00 next 975 of size 8192
2019-10-02 01:35:27.297571: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e877b00 next 633 of size 61184
2019-10-02 01:35:27.297576: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e886a00 next 937 of size 67108864
2019-10-02 01:35:27.297582: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe82886a00 next 938 of size 256
2019-10-02 01:35:27.297588: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe82886b00 next 939 of size 256
2019-10-02 01:35:27.297594: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe82886c00 next 940 of size 256
2019-10-02 01:35:27.297605: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe82886d00 next 941 of size 256
2019-10-02 01:35:27.297611: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe82886e00 next 942 of size 256
2019-10-02 01:35:27.297617: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe82886f00 next 943 of size 16777216
2019-10-02 01:35:27.297622: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe83886f00 next 951 of size 16777216
2019-10-02 01:35:27.297628: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe84886f00 next 955 of size 67108864
2019-10-02 01:35:27.297634: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe88886f00 next 957 of size 67108864
2019-10-02 01:35:27.297641: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe8c886f00 next 958 of size 16777216
2019-10-02 01:35:27.297647: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe8d886f00 next 959 of size 67108864
2019-10-02 01:35:27.297653: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe91886f00 next 960 of size 16777216
2019-10-02 01:35:27.297659: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe92886f00 next 970 of size 16777216
2019-10-02 01:35:27.297673: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe93886f00 next 976 of size 256
2019-10-02 01:35:27.297680: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe93887000 next 977 of size 32768
2019-10-02 01:35:27.297686: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9388f000 next 978 of size 67108864
2019-10-02 01:35:27.297691: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9788f000 next 979 of size 16777216
2019-10-02 01:35:27.297697: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9888f000 next 980 of size 256
2019-10-02 01:35:27.297703: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9888f100 next 981 of size 16777216
2019-10-02 01:35:27.297709: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9988f100 next 982 of size 256
2019-10-02 01:35:27.297715: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9988f200 next 983 of size 256
2019-10-02 01:35:27.297720: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9988f300 next 984 of size 16777216
2019-10-02 01:35:27.297726: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9a88f300 next 985 of size 67108864
2019-10-02 01:35:27.297731: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9e88f300 next 986 of size 256
2019-10-02 01:35:27.297737: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9e88f400 next 987 of size 256
2019-10-02 01:35:27.297743: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9e88f500 next 988 of size 256
2019-10-02 01:35:27.297749: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9e88f600 next 989 of size 256
2019-10-02 01:35:27.297754: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9e88f700 next 990 of size 256
2019-10-02 01:35:27.297760: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9e88f800 next 991 of size 256
2019-10-02 01:35:27.297766: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9e88f900 next 992 of size 16777216
2019-10-02 01:35:27.297772: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9f88f900 next 993 of size 8192
2019-10-02 01:35:27.297783: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9f891900 next 994 of size 256
2019-10-02 01:35:27.297790: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9f891a00 next 995 of size 8192
2019-10-02 01:35:27.297795: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9f893a00 next 996 of size 8192
2019-10-02 01:35:27.297801: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9f895a00 next 997 of size 256
2019-10-02 01:35:27.297813: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9f895b00 next 998 of size 8192
2019-10-02 01:35:27.297819: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9f897b00 next 999 of size 67108864
2019-10-02 01:35:27.297825: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efea3897b00 next 1000 of size 67108864
2019-10-02 01:35:27.297831: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efea7897b00 next 1001 of size 256
2019-10-02 01:35:27.297836: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efea7897c00 next 1002 of size 256
2019-10-02 01:35:27.297842: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efea7897d00 next 1003 of size 256
2019-10-02 01:35:27.297849: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efea7897e00 next 1004 of size 256
2019-10-02 01:35:27.297855: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efea7897f00 next 1005 of size 67108864
2019-10-02 01:35:27.297861: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efeab897f00 next 1006 of size 256
2019-10-02 01:35:27.297867: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efeab898000 next 1007 of size 256
2019-10-02 01:35:27.297889: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efeab898100 next 601 of size 16777216
2019-10-02 01:35:27.297896: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efeac898100 next 594 of size 8388608
2019-10-02 01:35:27.297902: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efead098100 next 620 of size 268435456
2019-10-02 01:35:27.297908: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd098100 next 629 of size 131072
2019-10-02 01:35:27.297919: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b8100 next 572 of size 256
2019-10-02 01:35:27.297925: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b8200 next 587 of size 256
2019-10-02 01:35:27.297931: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b8300 next 696 of size 256
2019-10-02 01:35:27.297937: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b8400 next 623 of size 256
2019-10-02 01:35:27.297943: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b8500 next 1037 of size 512
2019-10-02 01:35:27.297948: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b8700 next 1041 of size 512
2019-10-02 01:35:27.297954: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b8900 next 1047 of size 256
2019-10-02 01:35:27.297960: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b8a00 next 1059 of size 256
2019-10-02 01:35:27.297965: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b8b00 next 1060 of size 256
2019-10-02 01:35:27.297977: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b8c00 next 1061 of size 256
2019-10-02 01:35:27.297983: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b8d00 next 1076 of size 512
2019-10-02 01:35:27.297989: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b8f00 next 1077 of size 512
2019-10-02 01:35:27.297995: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b9100 next 1079 of size 256
2019-10-02 01:35:27.298001: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b9200 next 1098 of size 256
2019-10-02 01:35:27.298007: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b9300 next 1102 of size 256
2019-10-02 01:35:27.298012: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b9400 next 1106 of size 256
2019-10-02 01:35:27.298018: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b9500 next 1113 of size 512
2019-10-02 01:35:27.298024: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b9700 next 1121 of size 512
2019-10-02 01:35:27.298030: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b9900 next 1122 of size 256
2019-10-02 01:35:27.298036: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b9a00 next 1123 of size 256
2019-10-02 01:35:27.298041: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b9b00 next 1125 of size 256
2019-10-02 01:35:27.298052: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b9c00 next 1126 of size 256
2019-10-02 01:35:27.298058: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b9d00 next 1127 of size 256
2019-10-02 01:35:27.298064: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b9e00 next 1021 of size 256
2019-10-02 01:35:27.298071: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b9f00 next 1022 of size 256
2019-10-02 01:35:27.298083: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0ba000 next 1023 of size 256
2019-10-02 01:35:27.298089: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0ba100 next 1024 of size 256
2019-10-02 01:35:27.298095: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0ba200 next 1025 of size 256
2019-10-02 01:35:27.298109: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0ba300 next 1026 of size 256
2019-10-02 01:35:27.298116: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0ba400 next 596 of size 131072000
2019-10-02 01:35:27.298122: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efec4dba400 next 571 of size 268435456
2019-10-02 01:35:27.298128: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efed4dba400 next 558 of size 131072
2019-10-02 01:35:27.298134: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efed4dda400 next 597 of size 4194304000
2019-10-02 01:35:27.298140: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effcedda400 next 595 of size 67108864
2019-10-02 01:35:27.298146: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effd2dda400 next 638 of size 268435456
2019-10-02 01:35:27.298151: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effe2dda400 next 626 of size 131072
2019-10-02 01:35:27.298157: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effe2dfa400 next 648 of size 16777216
2019-10-02 01:35:27.298163: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effe3dfa400 next 674 of size 16777216
2019-10-02 01:35:27.298175: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effe4dfa400 next 589 of size 8192
2019-10-02 01:35:27.298181: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effe4dfc400 next 615 of size 8192
2019-10-02 01:35:27.298187: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effe4dfe400 next 658 of size 8192
2019-10-02 01:35:27.298193: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effe4e00400 next 698 of size 8192
2019-10-02 01:35:27.298199: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effe4e02400 next 676 of size 67108864
2019-10-02 01:35:27.298205: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effe8e02400 next 646 of size 8192
2019-10-02 01:35:27.298211: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effe8e04400 next 692 of size 16777216
2019-10-02 01:35:27.298216: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effe9e04400 next 682 of size 16384000
2019-10-02 01:35:27.298222: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effeada4400 next 681 of size 32768
2019-10-02 01:35:27.298233: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effeadac400 next 675 of size 8192
2019-10-02 01:35:27.298239: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effeadae400 next 612 of size 16384000
2019-10-02 01:35:27.298245: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effebd4e400 next 568 of size 16384000
2019-10-02 01:35:27.298251: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effeccee400 next 573 of size 16384000
2019-10-02 01:35:27.298256: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effedc8e400 next 625 of size 16384000
2019-10-02 01:35:27.298262: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effeec2e400 next 565 of size 8192
2019-10-02 01:35:27.298268: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effeec30400 next 670 of size 16384000
2019-10-02 01:35:27.298274: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effefbd0400 next 560 of size 67108864
2019-10-02 01:35:27.298287: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efff3bd0400 next 640 of size 16384000
2019-10-02 01:35:27.298295: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efff4b70400 next 561 of size 16384000
2019-10-02 01:35:27.298300: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efff5b10400 next 651 of size 16384000
2019-10-02 01:35:27.298306: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efff6ab0400 next 656 of size 16384000
2019-10-02 01:35:27.298320: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efff7a50400 next 645 of size 16384000
2019-10-02 01:35:27.298326: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efff89f0400 next 641 of size 8192
2019-10-02 01:35:27.298332: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efff89f2400 next 654 of size 8192
2019-10-02 01:35:27.298338: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efff89f4400 next 556 of size 16384000
2019-10-02 01:35:27.298349: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efff9994400 next 574 of size 16384000
2019-10-02 01:35:27.298355: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffa934400 next 570 of size 8192
2019-10-02 01:35:27.298361: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffa936400 next 562 of size 8192
2019-10-02 01:35:27.298367: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffa938400 next 1008 of size 8192
2019-10-02 01:35:27.298390: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffa93a400 next 666 of size 8192
2019-10-02 01:35:27.298396: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffa93c400 next 567 of size 8192
2019-10-02 01:35:27.298401: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffa93e400 next 557 of size 16384000
2019-10-02 01:35:27.298407: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffb8de400 next 691 of size 16384000
2019-10-02 01:35:27.298413: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffc87e400 next 634 of size 8192
2019-10-02 01:35:27.298419: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffc880400 next 631 of size 8192
2019-10-02 01:35:27.298425: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffc882400 next 672 of size 8192
2019-10-02 01:35:27.298430: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffc884400 next 679 of size 8192
2019-10-02 01:35:27.298443: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffc886400 next 669 of size 16384000
2019-10-02 01:35:27.298449: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffd826400 next 628 of size 8192
2019-10-02 01:35:27.298455: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffd828400 next 610 of size 8192
2019-10-02 01:35:27.298460: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffd82a400 next 653 of size 8192
2019-10-02 01:35:27.298466: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffd82c400 next 650 of size 8192
2019-10-02 01:35:27.298472: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffd82e400 next 713 of size 8192
2019-10-02 01:35:27.298478: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffd830400 next 632 of size 8192
2019-10-02 01:35:27.298484: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffd832400 next 575 of size 16777216
2019-10-02 01:35:27.298495: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffe832400 next 664 of size 8192
2019-10-02 01:35:27.298501: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffe834400 next 618 of size 8192
2019-10-02 01:35:27.298507: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffe836400 next 621 of size 8192
2019-10-02 01:35:27.298512: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffe838400 next 555 of size 16777216
2019-10-02 01:35:27.298518: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effff838400 next 602 of size 8192
2019-10-02 01:35:27.298525: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effff83a400 next 606 of size 32768
2019-10-02 01:35:27.298531: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effff842400 next 600 of size 8192
2019-10-02 01:35:27.298547: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effff844400 next 609 of size 8192
2019-10-02 01:35:27.298554: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effff846400 next 593 of size 8192
2019-10-02 01:35:27.298559: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effff848400 next 592 of size 8192
2019-10-02 01:35:27.298565: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effff84a400 next 652 of size 8192
2019-10-02 01:35:27.298571: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effff84c400 next 1012 of size 16777216
2019-10-02 01:35:27.298576: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f000084c400 next 647 of size 8192
2019-10-02 01:35:27.298582: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f000084e400 next 660 of size 8192
2019-10-02 01:35:27.298588: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0000850400 next 603 of size 8192
2019-10-02 01:35:27.298594: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0000852400 next 673 of size 8192
2019-10-02 01:35:27.298607: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0000854400 next 1020 of size 8192
2019-10-02 01:35:27.298614: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0000856400 next 1015 of size 8192
2019-10-02 01:35:27.298619: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0000858400 next 643 of size 16777216
2019-10-02 01:35:27.298625: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0001858400 next 1009 of size 67108864
2019-10-02 01:35:27.298630: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0005858400 next 1010 of size 16777216
2019-10-02 01:35:27.298636: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0006858400 next 616 of size 16777216
2019-10-02 01:35:27.298642: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0007858400 next 663 of size 16777216
2019-10-02 01:35:27.298648: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0008858400 next 1013 of size 8192
2019-10-02 01:35:27.298660: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f000885a400 next 1014 of size 8192
2019-10-02 01:35:27.298666: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f000885c400 next 671 of size 16777216
2019-10-02 01:35:27.298672: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f000985c400 next 607 of size 67108864
2019-10-02 01:35:27.298677: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f000d85c400 next 635 of size 16777216
2019-10-02 01:35:27.298688: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f000e85c400 next 662 of size 16777216
2019-10-02 01:35:27.298695: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f000f85c400 next 636 of size 8192
2019-10-02 01:35:27.298700: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f000f85e400 next 668 of size 8192
2019-10-02 01:35:27.298706: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f000f860400 next 642 of size 8192
2019-10-02 01:35:27.298718: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f000f862400 next 667 of size 32768
2019-10-02 01:35:27.298724: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f000f86a400 next 569 of size 8192
2019-10-02 01:35:27.298730: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f000f86c400 next 639 of size 67108864
2019-10-02 01:35:27.298735: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f001386c400 next 1017 of size 8192
2019-10-02 01:35:27.298741: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f001386e400 next 1018 of size 8192
2019-10-02 01:35:27.298755: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0013870400 next 627 of size 8192
2019-10-02 01:35:27.298771: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0013872400 next 598 of size 8192
2019-10-02 01:35:27.298778: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0013874400 next 655 of size 8192
2019-10-02 01:35:27.298783: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0013876400 next 678 of size 8192
2019-10-02 01:35:27.298789: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0013878400 next 599 of size 8192
2019-10-02 01:35:27.298795: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f001387a400 next 689 of size 8192
2019-10-02 01:35:27.298801: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f001387c400 next 693 of size 8192
2019-10-02 01:35:27.298807: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f001387e400 next 563 of size 32768
2019-10-02 01:35:27.298812: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0013886400 next 1019 of size 16777216
2019-10-02 01:35:27.298818: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0014886400 next 719 of size 16777216
2019-10-02 01:35:27.298824: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0015886400 next 695 of size 8192
2019-10-02 01:35:27.298830: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0015888400 next 694 of size 16777216
2019-10-02 01:35:27.298835: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0016888400 next 1027 of size 16777216
2019-10-02 01:35:27.298842: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0017888400 next 1028 of size 16777216
2019-10-02 01:35:27.298848: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0018888400 next 1029 of size 16777216
2019-10-02 01:35:27.298859: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0019888400 next 1030 of size 67108864
2019-10-02 01:35:27.298865: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f001d888400 next 1032 of size 8192
2019-10-02 01:35:27.298871: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f001d88a400 next 1035 of size 67108864
2019-10-02 01:35:27.298877: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f002188a400 next 1036 of size 16777216
2019-10-02 01:35:27.298883: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f002288a400 next 1038 of size 8192
2019-10-02 01:35:27.298889: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f002288c400 next 1039 of size 67108864
2019-10-02 01:35:27.298895: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f002688c400 next 1040 of size 8192
2019-10-02 01:35:27.298900: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f002688e400 next 1042 of size 8192
2019-10-02 01:35:27.298906: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0026890400 next 1043 of size 16777216
2019-10-02 01:35:27.298911: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0027890400 next 1044 of size 8192
2019-10-02 01:35:27.298917: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0027892400 next 1045 of size 16777216
2019-10-02 01:35:27.298923: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0028892400 next 1046 of size 8192
2019-10-02 01:35:27.298929: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0028894400 next 1048 of size 16777216
2019-10-02 01:35:27.298934: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0029894400 next 1049 of size 8192
2019-10-02 01:35:27.298940: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0029896400 next 1050 of size 16777216
2019-10-02 01:35:27.298946: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f002a896400 next 1051 of size 8192
2019-10-02 01:35:27.298957: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f002a898400 next 1052 of size 8192
2019-10-02 01:35:27.298964: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f002a89a400 next 1053 of size 8192
2019-10-02 01:35:27.298973: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f002a89c400 next 1054 of size 67108864
2019-10-02 01:35:27.298984: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f002e89c400 next 1055 of size 16777216
2019-10-02 01:35:27.298991: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f002f89c400 next 1056 of size 8192
2019-10-02 01:35:27.298997: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f002f89e400 next 1057 of size 32768
2019-10-02 01:35:27.299002: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f002f8a6400 next 1058 of size 8192
2019-10-02 01:35:27.299008: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f002f8a8400 next 1062 of size 16777216
2019-10-02 01:35:27.299014: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00308a8400 next 1063 of size 8192
2019-10-02 01:35:27.299020: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00308aa400 next 1064 of size 32768
2019-10-02 01:35:27.299025: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00308b2400 next 1065 of size 8192
2019-10-02 01:35:27.299031: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00308b4400 next 1066 of size 67108864
2019-10-02 01:35:27.299043: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00348b4400 next 1067 of size 8192
2019-10-02 01:35:27.299049: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00348b6400 next 1068 of size 8192
2019-10-02 01:35:27.299055: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00348b8400 next 1069 of size 8192
2019-10-02 01:35:27.299061: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00348ba400 next 1070 of size 8192
2019-10-02 01:35:27.299073: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00348bc400 next 1071 of size 32768
2019-10-02 01:35:27.299079: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00348c4400 next 1072 of size 8192
2019-10-02 01:35:27.299085: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00348c6400 next 1073 of size 8192
2019-10-02 01:35:27.299090: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00348c8400 next 1074 of size 67108864
2019-10-02 01:35:27.299096: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00388c8400 next 1075 of size 8192
2019-10-02 01:35:27.299102: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00388ca400 next 1078 of size 8192
2019-10-02 01:35:27.299108: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00388cc400 next 1080 of size 16777216
2019-10-02 01:35:27.299114: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00398cc400 next 1081 of size 8192
2019-10-02 01:35:27.299119: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00398ce400 next 1082 of size 16777216
2019-10-02 01:35:27.299125: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f003a8ce400 next 1083 of size 8192
2019-10-02 01:35:27.299131: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f003a8d0400 next 1084 of size 8192
2019-10-02 01:35:27.299136: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f003a8d2400 next 1085 of size 8192
2019-10-02 01:35:27.299148: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f003a8d4400 next 1086 of size 8192
2019-10-02 01:35:27.299154: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f003a8d6400 next 1087 of size 16777216
2019-10-02 01:35:27.299160: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f003b8d6400 next 1088 of size 67108864
2019-10-02 01:35:27.299166: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f003f8d6400 next 1089 of size 67108864
2019-10-02 01:35:27.299177: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00438d6400 next 1090 of size 8192
2019-10-02 01:35:27.299193: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00438d8400 next 1091 of size 8192
2019-10-02 01:35:27.299199: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00438da400 next 1092 of size 8192
2019-10-02 01:35:27.299205: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00438dc400 next 1093 of size 16777216
2019-10-02 01:35:27.299210: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00448dc400 next 1094 of size 8192
2019-10-02 01:35:27.299216: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00448de400 next 1095 of size 16777216
2019-10-02 01:35:27.299222: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00458de400 next 1096 of size 8192
2019-10-02 01:35:27.299228: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00458e0400 next 1097 of size 8192
2019-10-02 01:35:27.299234: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00458e2400 next 1099 of size 67108864
2019-10-02 01:35:27.299240: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00498e2400 next 1100 of size 16777216
2019-10-02 01:35:27.299245: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f004a8e2400 next 1101 of size 8192
2019-10-02 01:35:27.299251: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f004a8e4400 next 1103 of size 16777216
2019-10-02 01:35:27.299258: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f004b8e4400 next 1104 of size 32768
2019-10-02 01:35:27.299263: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f004b8ec400 next 1105 of size 8192
2019-10-02 01:35:27.299269: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f004b8ee400 next 1107 of size 16777216
2019-10-02 01:35:27.299275: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f004c8ee400 next 1108 of size 32768
2019-10-02 01:35:27.299280: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f004c8f6400 next 1109 of size 16777216
2019-10-02 01:35:27.299286: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f004d8f6400 next 1110 of size 8192
2019-10-02 01:35:27.299308: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f004d8f8400 next 1111 of size 8192
2019-10-02 01:35:27.299315: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f004d8fa400 next 1112 of size 8192
2019-10-02 01:35:27.299321: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f004d8fc400 next 1114 of size 8192
2019-10-02 01:35:27.299327: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f004d8fe400 next 1115 of size 67108864
2019-10-02 01:35:27.299332: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00518fe400 next 1116 of size 8192
2019-10-02 01:35:27.299338: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0051900400 next 1117 of size 8192
2019-10-02 01:35:27.299344: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0051902400 next 1118 of size 16777216
2019-10-02 01:35:27.299350: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0052902400 next 1119 of size 8192
2019-10-02 01:35:27.299355: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0052904400 next 1120 of size 8192
2019-10-02 01:35:27.299361: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0052906400 next 1124 of size 67108864
2019-10-02 01:35:27.299367: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0056906400 next 18446744073709551615 of size 74560768
2019-10-02 01:35:27.299374: I tensorflow/core/common_runtime/bfc_allocator.cc:914]      Summary of in-use Chunks by size: 
2019-10-02 01:35:27.299382: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 242 Chunks of size 256 totalling 60.5KiB
2019-10-02 01:35:27.299389: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 45 Chunks of size 512 totalling 22.5KiB
2019-10-02 01:35:27.299401: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 768 totalling 768B
2019-10-02 01:35:27.299409: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 1280 totalling 1.2KiB
2019-10-02 01:35:27.299415: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 4096 totalling 4.0KiB
2019-10-02 01:35:27.299421: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 5376 totalling 5.2KiB
2019-10-02 01:35:27.299427: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 6912 totalling 6.8KiB
2019-10-02 01:35:27.299433: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 448 Chunks of size 8192 totalling 3.50MiB
2019-10-02 01:35:27.299440: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 9984 totalling 9.8KiB
2019-10-02 01:35:27.299446: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 11264 totalling 11.0KiB
2019-10-02 01:35:27.299452: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 13056 totalling 12.8KiB
2019-10-02 01:35:27.299458: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 45 Chunks of size 32768 totalling 1.41MiB
2019-10-02 01:35:27.299465: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 56064 totalling 54.8KiB
2019-10-02 01:35:27.299471: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 61184 totalling 59.8KiB
2019-10-02 01:35:27.299478: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 7 Chunks of size 131072 totalling 896.0KiB
2019-10-02 01:35:27.299484: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 3 Chunks of size 8388608 totalling 24.00MiB
2019-10-02 01:35:27.299490: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 32 Chunks of size 16384000 totalling 500.00MiB
2019-10-02 01:35:27.299496: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 192 Chunks of size 16777216 totalling 3.00GiB
2019-10-02 01:35:27.299502: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 97 Chunks of size 67108864 totalling 6.06GiB
2019-10-02 01:35:27.299508: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 74560768 totalling 71.11MiB
2019-10-02 01:35:27.299515: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 131072000 totalling 125.00MiB
2019-10-02 01:35:27.299521: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 262144000 totalling 250.00MiB
2019-10-02 01:35:27.299535: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 3 Chunks of size 268435456 totalling 768.00MiB
2019-10-02 01:35:27.299542: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 4194304000 totalling 3.91GiB
2019-10-02 01:35:27.299548: I tensorflow/core/common_runtime/bfc_allocator.cc:921] Sum Total of in-use chunks: 14.67GiB
2019-10-02 01:35:27.299554: I tensorflow/core/common_runtime/bfc_allocator.cc:923] total_region_allocated_bytes_: 15753943296 memory_limit_: 15753943450 available bytes: 154 curr_region_allocation_bytes_: 31507887104
2019-10-02 01:35:27.299566: I tensorflow/core/common_runtime/bfc_allocator.cc:929] Stats: 
Limit:                 15753943450
InUse:                 15753943296
MaxInUse:              15753943296
NumAllocs:                    2276
MaxAllocSize:           4194304000

2019-10-02 01:35:27.299619: W tensorflow/core/common_runtime/bfc_allocator.cc:424] ****************************************************************************************************
2019-10-02 01:35:27.299682: W tensorflow/core/common_runtime/bfc_allocator.cc:419] Allocator (GPU_0_bfc) ran out of memory trying to allocate 16.00MiB (rounded to 16777216).  Current allocation summary follows.
2019-10-02 01:35:27.299742: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (256): 	Total Chunks: 242, Chunks in use: 242. 60.5KiB allocated for chunks. 60.5KiB in use in bin. 4.2KiB client-requested in use in bin.
2019-10-02 01:35:27.299768: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (512): 	Total Chunks: 46, Chunks in use: 46. 23.2KiB allocated for chunks. 23.2KiB in use in bin. 23.0KiB client-requested in use in bin.
2019-10-02 01:35:27.299800: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (1024): 	Total Chunks: 1, Chunks in use: 1. 1.2KiB allocated for chunks. 1.2KiB in use in bin. 1.0KiB client-requested in use in bin.
2019-10-02 01:35:27.299814: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (2048): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-10-02 01:35:27.299830: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (4096): 	Total Chunks: 3, Chunks in use: 3. 16.0KiB allocated for chunks. 16.0KiB in use in bin. 12.0KiB client-requested in use in bin.
2019-10-02 01:35:27.299842: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (8192): 	Total Chunks: 451, Chunks in use: 451. 3.53MiB allocated for chunks. 3.53MiB in use in bin. 3.52MiB client-requested in use in bin.
2019-10-02 01:35:27.299856: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (16384): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-10-02 01:35:27.299867: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (32768): 	Total Chunks: 47, Chunks in use: 47. 1.52MiB allocated for chunks. 1.52MiB in use in bin. 1.47MiB client-requested in use in bin.
2019-10-02 01:35:27.299880: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (65536): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-10-02 01:35:27.299903: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (131072): 	Total Chunks: 7, Chunks in use: 7. 896.0KiB allocated for chunks. 896.0KiB in use in bin. 896.0KiB client-requested in use in bin.
2019-10-02 01:35:27.299916: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (262144): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-10-02 01:35:27.299927: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (524288): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-10-02 01:35:27.299939: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (1048576): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-10-02 01:35:27.299950: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (2097152): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-10-02 01:35:27.299966: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (4194304): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-10-02 01:35:27.299980: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (8388608): 	Total Chunks: 35, Chunks in use: 35. 524.00MiB allocated for chunks. 524.00MiB in use in bin. 524.00MiB client-requested in use in bin.
2019-10-02 01:35:27.299992: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (16777216): 	Total Chunks: 192, Chunks in use: 192. 3.00GiB allocated for chunks. 3.00GiB in use in bin. 3.00GiB client-requested in use in bin.
2019-10-02 01:35:27.300005: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (33554432): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-10-02 01:35:27.300016: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (67108864): 	Total Chunks: 99, Chunks in use: 99. 6.25GiB allocated for chunks. 6.25GiB in use in bin. 6.25GiB client-requested in use in bin.
2019-10-02 01:35:27.300029: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (134217728): 	Total Chunks: 1, Chunks in use: 1. 250.00MiB allocated for chunks. 250.00MiB in use in bin. 250.00MiB client-requested in use in bin.
2019-10-02 01:35:27.300059: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (268435456): 	Total Chunks: 4, Chunks in use: 4. 4.66GiB allocated for chunks. 4.66GiB in use in bin. 4.66GiB client-requested in use in bin.
2019-10-02 01:35:27.300072: I tensorflow/core/common_runtime/bfc_allocator.cc:885] Bin for 16.00MiB was 16.00MiB, Chunk State: 
2019-10-02 01:35:27.300084: I tensorflow/core/common_runtime/bfc_allocator.cc:898] Next region of size 15753943296
2019-10-02 01:35:27.300095: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0000000 next 1 of size 1280
2019-10-02 01:35:27.300105: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0000500 next 2 of size 256
2019-10-02 01:35:27.300116: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0000600 next 3 of size 256
2019-10-02 01:35:27.300126: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0000700 next 4 of size 256
2019-10-02 01:35:27.300136: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0000800 next 5 of size 256
2019-10-02 01:35:27.300147: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0000900 next 6 of size 256
2019-10-02 01:35:27.300156: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0000a00 next 7 of size 256
2019-10-02 01:35:27.300165: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0000b00 next 8 of size 256
2019-10-02 01:35:27.300176: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0000c00 next 9 of size 256
2019-10-02 01:35:27.300186: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0000d00 next 10 of size 256
2019-10-02 01:35:27.300196: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0000e00 next 11 of size 256
2019-10-02 01:35:27.300205: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0000f00 next 12 of size 256
2019-10-02 01:35:27.300214: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0001000 next 13 of size 256
2019-10-02 01:35:27.300226: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0001100 next 14 of size 256
2019-10-02 01:35:27.300235: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0001200 next 15 of size 256
2019-10-02 01:35:27.300245: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0001300 next 16 of size 256
2019-10-02 01:35:27.300257: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0001400 next 17 of size 256
2019-10-02 01:35:27.300266: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0001500 next 18 of size 256
2019-10-02 01:35:27.300275: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0001600 next 19 of size 256
2019-10-02 01:35:27.300287: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0001700 next 20 of size 256
2019-10-02 01:35:27.300298: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0001800 next 21 of size 256
2019-10-02 01:35:27.300308: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0001900 next 22 of size 256
2019-10-02 01:35:27.300317: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0001a00 next 23 of size 256
2019-10-02 01:35:27.300328: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0001b00 next 24 of size 256
2019-10-02 01:35:27.300337: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0001c00 next 25 of size 256
2019-10-02 01:35:27.300347: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0001d00 next 26 of size 256
2019-10-02 01:35:27.300358: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0001e00 next 27 of size 256
2019-10-02 01:35:27.300368: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0001f00 next 28 of size 256
2019-10-02 01:35:27.300377: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0002000 next 29 of size 256
2019-10-02 01:35:27.300398: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0002100 next 30 of size 256
2019-10-02 01:35:27.300410: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0002200 next 31 of size 256
2019-10-02 01:35:27.300419: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0002300 next 32 of size 256
2019-10-02 01:35:27.300431: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0002400 next 33 of size 256
2019-10-02 01:35:27.300440: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0002500 next 34 of size 256
2019-10-02 01:35:27.300449: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0002600 next 35 of size 256
2019-10-02 01:35:27.300461: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0002700 next 36 of size 256
2019-10-02 01:35:27.300470: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0002800 next 37 of size 256
2019-10-02 01:35:27.300480: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0002900 next 38 of size 256
2019-10-02 01:35:27.300492: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0002a00 next 39 of size 256
2019-10-02 01:35:27.300501: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0002b00 next 40 of size 256
2019-10-02 01:35:27.300510: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0002c00 next 41 of size 256
2019-10-02 01:35:27.300522: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0002d00 next 42 of size 256
2019-10-02 01:35:27.300532: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0002e00 next 43 of size 256
2019-10-02 01:35:27.300541: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0002f00 next 44 of size 8192
2019-10-02 01:35:27.300552: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0004f00 next 45 of size 67108864
2019-10-02 01:35:27.300562: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb4004f00 next 46 of size 67108864
2019-10-02 01:35:27.300574: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb8004f00 next 47 of size 8192
2019-10-02 01:35:27.300583: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb8006f00 next 48 of size 32768
2019-10-02 01:35:27.300593: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb800ef00 next 49 of size 8192
2019-10-02 01:35:27.300606: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb8010f00 next 50 of size 16777216
2019-10-02 01:35:27.300616: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb9010f00 next 51 of size 8192
2019-10-02 01:35:27.300625: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb9012f00 next 52 of size 8192
2019-10-02 01:35:27.300634: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb9014f00 next 53 of size 32768
2019-10-02 01:35:27.300643: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb901cf00 next 54 of size 8192
2019-10-02 01:35:27.300653: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb901ef00 next 55 of size 8192
2019-10-02 01:35:27.300672: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb9020f00 next 56 of size 32768
2019-10-02 01:35:27.300682: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb9028f00 next 57 of size 8192
2019-10-02 01:35:27.300692: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb902af00 next 58 of size 8192
2019-10-02 01:35:27.300709: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb902cf00 next 59 of size 512
2019-10-02 01:35:27.300720: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb902d100 next 60 of size 8192
2019-10-02 01:35:27.300730: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb902f100 next 61 of size 16777216
2019-10-02 01:35:27.300751: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcba02f100 next 62 of size 8192
2019-10-02 01:35:27.300763: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcba031100 next 63 of size 16777216
2019-10-02 01:35:27.300772: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcbb031100 next 64 of size 8192
2019-10-02 01:35:27.300784: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcbb033100 next 65 of size 8192
2019-10-02 01:35:27.300794: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcbb035100 next 66 of size 16777216
2019-10-02 01:35:27.300804: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcbc035100 next 67 of size 32768
2019-10-02 01:35:27.300813: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcbc03d100 next 68 of size 16777216
2019-10-02 01:35:27.300822: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcbd03d100 next 69 of size 8192
2019-10-02 01:35:27.300835: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcbd03f100 next 70 of size 8192
2019-10-02 01:35:27.300843: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcbd041100 next 71 of size 8192
2019-10-02 01:35:27.300853: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcbd043100 next 72 of size 67108864
2019-10-02 01:35:27.300870: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcc1043100 next 73 of size 8192
2019-10-02 01:35:27.300880: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcc1045100 next 74 of size 67108864
2019-10-02 01:35:27.300890: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcc5045100 next 75 of size 8192
2019-10-02 01:35:27.300902: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcc5047100 next 76 of size 8192
2019-10-02 01:35:27.300911: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcc5049100 next 77 of size 8192
2019-10-02 01:35:27.300920: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcc504b100 next 78 of size 8192
2019-10-02 01:35:27.300930: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcc504d100 next 79 of size 67108864
2019-10-02 01:35:27.300943: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcc904d100 next 80 of size 16777216
2019-10-02 01:35:27.300954: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcca04d100 next 81 of size 8192
2019-10-02 01:35:27.300964: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcca04f100 next 82 of size 16777216
2019-10-02 01:35:27.300975: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efccb04f100 next 83 of size 8192
2019-10-02 01:35:27.300984: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efccb051100 next 84 of size 16777216
2019-10-02 01:35:27.300994: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efccc051100 next 85 of size 8192
2019-10-02 01:35:27.301006: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efccc053100 next 86 of size 67108864
2019-10-02 01:35:27.301016: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd0053100 next 87 of size 8192
2019-10-02 01:35:27.301027: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd0055100 next 88 of size 32768
2019-10-02 01:35:27.301038: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd005d100 next 89 of size 8192
2019-10-02 01:35:27.301047: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd005f100 next 90 of size 8192
2019-10-02 01:35:27.301056: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd0061100 next 91 of size 8192
2019-10-02 01:35:27.301068: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd0063100 next 92 of size 8192
2019-10-02 01:35:27.301084: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd0065100 next 93 of size 8192
2019-10-02 01:35:27.301095: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd0067100 next 94 of size 16777216
2019-10-02 01:35:27.301104: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd1067100 next 95 of size 8192
2019-10-02 01:35:27.301114: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd1069100 next 96 of size 16384000
2019-10-02 01:35:27.301126: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd2009100 next 97 of size 16384000
2019-10-02 01:35:27.301136: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd2fa9100 next 98 of size 8192
2019-10-02 01:35:27.301145: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd2fab100 next 99 of size 512
2019-10-02 01:35:27.301156: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd2fab300 next 100 of size 16777216
2019-10-02 01:35:27.301166: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd3fab300 next 101 of size 16777216
2019-10-02 01:35:27.301175: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd4fab300 next 102 of size 67108864
2019-10-02 01:35:27.301187: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd8fab300 next 103 of size 16777216
2019-10-02 01:35:27.301196: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd9fab300 next 104 of size 16777216
2019-10-02 01:35:27.301206: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcdafab300 next 105 of size 8192
2019-10-02 01:35:27.301216: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcdafad300 next 106 of size 512
2019-10-02 01:35:27.301225: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcdafad500 next 107 of size 16777216
2019-10-02 01:35:27.301237: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcdbfad500 next 108 of size 512
2019-10-02 01:35:27.301248: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcdbfad700 next 109 of size 16777216
2019-10-02 01:35:27.301257: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcdcfad700 next 110 of size 8192
2019-10-02 01:35:27.301266: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcdcfaf700 next 111 of size 8192
2019-10-02 01:35:27.301278: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcdcfb1700 next 112 of size 8192
2019-10-02 01:35:27.301287: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcdcfb3700 next 113 of size 8192
2019-10-02 01:35:27.301298: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcdcfb5700 next 114 of size 8192
2019-10-02 01:35:27.301307: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcdcfb7700 next 115 of size 16777216
2019-10-02 01:35:27.301316: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcddfb7700 next 116 of size 67108864
2019-10-02 01:35:27.301327: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce1fb7700 next 117 of size 512
2019-10-02 01:35:27.301337: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce1fb7900 next 118 of size 8192
2019-10-02 01:35:27.301348: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce1fb9900 next 119 of size 8192
2019-10-02 01:35:27.301358: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce1fbb900 next 120 of size 8192
2019-10-02 01:35:27.301370: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce1fbd900 next 121 of size 16777216
2019-10-02 01:35:27.301379: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce2fbd900 next 122 of size 8192
2019-10-02 01:35:27.301389: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce2fbf900 next 123 of size 8192
2019-10-02 01:35:27.301400: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce2fc1900 next 124 of size 16777216
2019-10-02 01:35:27.301414: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce3fc1900 next 125 of size 512
2019-10-02 01:35:27.301426: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce3fc1b00 next 126 of size 67108864
2019-10-02 01:35:27.301436: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce7fc1b00 next 127 of size 8192
2019-10-02 01:35:27.301445: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce7fc3b00 next 128 of size 8192
2019-10-02 01:35:27.301458: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce7fc5b00 next 129 of size 8192
2019-10-02 01:35:27.301467: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce7fc7b00 next 130 of size 8192
2019-10-02 01:35:27.301476: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce7fc9b00 next 131 of size 16777216
2019-10-02 01:35:27.301488: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce8fc9b00 next 132 of size 8192
2019-10-02 01:35:27.301497: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce8fcbb00 next 133 of size 16777216
2019-10-02 01:35:27.301506: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce9fcbb00 next 134 of size 8192
2019-10-02 01:35:27.301519: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce9fcdb00 next 135 of size 16777216
2019-10-02 01:35:27.301529: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efceafcdb00 next 136 of size 8192
2019-10-02 01:35:27.301538: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efceafcfb00 next 137 of size 8192
2019-10-02 01:35:27.301550: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efceafd1b00 next 138 of size 8192
2019-10-02 01:35:27.301560: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efceafd3b00 next 139 of size 8192
2019-10-02 01:35:27.301569: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efceafd5b00 next 140 of size 8192
2019-10-02 01:35:27.301578: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efceafd7b00 next 141 of size 8192
2019-10-02 01:35:27.301588: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efceafd9b00 next 142 of size 16777216
2019-10-02 01:35:27.301606: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcebfd9b00 next 143 of size 16777216
2019-10-02 01:35:27.301616: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcecfd9b00 next 144 of size 8192
2019-10-02 01:35:27.301626: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcecfdbb00 next 145 of size 8192
2019-10-02 01:35:27.301637: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcecfddb00 next 146 of size 8192
2019-10-02 01:35:27.301647: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcecfdfb00 next 147 of size 32768
2019-10-02 01:35:27.301656: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcecfe7b00 next 148 of size 8192
2019-10-02 01:35:27.301665: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcecfe9b00 next 149 of size 512
2019-10-02 01:35:27.301674: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcecfe9d00 next 150 of size 8192
2019-10-02 01:35:27.301686: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcecfebd00 next 151 of size 8192
2019-10-02 01:35:27.301697: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcecfedd00 next 152 of size 16777216
2019-10-02 01:35:27.301706: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcedfedd00 next 153 of size 8192
2019-10-02 01:35:27.301723: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcedfefd00 next 154 of size 16777216
2019-10-02 01:35:27.301734: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efceefefd00 next 155 of size 8192
2019-10-02 01:35:27.301752: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efceeff1d00 next 156 of size 8192
2019-10-02 01:35:27.301763: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efceeff3d00 next 157 of size 8192
2019-10-02 01:35:27.301780: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efceeff5d00 next 158 of size 67108864
2019-10-02 01:35:27.301790: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcf2ff5d00 next 159 of size 8192
2019-10-02 01:35:27.301800: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcf2ff7d00 next 160 of size 32768
2019-10-02 01:35:27.301816: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcf2fffd00 next 161 of size 16777216
2019-10-02 01:35:27.301827: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcf3fffd00 next 162 of size 16777216
2019-10-02 01:35:27.301836: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcf4fffd00 next 163 of size 67108864
2019-10-02 01:35:27.301852: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcf8fffd00 next 164 of size 8192
2019-10-02 01:35:27.301863: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcf9001d00 next 165 of size 8192
2019-10-02 01:35:27.301873: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcf9003d00 next 166 of size 16384000
2019-10-02 01:35:27.301889: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcf9fa3d00 next 167 of size 8192
2019-10-02 01:35:27.301900: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcf9fa5d00 next 168 of size 67108864
2019-10-02 01:35:27.301909: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcfdfa5d00 next 169 of size 8192
2019-10-02 01:35:27.301925: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcfdfa7d00 next 170 of size 67108864
2019-10-02 01:35:27.301936: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd01fa7d00 next 171 of size 8192
2019-10-02 01:35:27.301945: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd01fa9d00 next 172 of size 67108864
2019-10-02 01:35:27.301961: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd05fa9d00 next 173 of size 16777216
2019-10-02 01:35:27.301972: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd06fa9d00 next 174 of size 512
2019-10-02 01:35:27.301981: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd06fa9f00 next 175 of size 8192
2019-10-02 01:35:27.301997: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd06fabf00 next 176 of size 8192
2019-10-02 01:35:27.302008: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd06fadf00 next 177 of size 67108864
2019-10-02 01:35:27.302017: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd0afadf00 next 178 of size 67108864
2019-10-02 01:35:27.302035: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd0efadf00 next 179 of size 8192
2019-10-02 01:35:27.302046: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd0efaff00 next 180 of size 16777216
2019-10-02 01:35:27.302058: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd0ffaff00 next 181 of size 8192
2019-10-02 01:35:27.302068: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd0ffb1f00 next 182 of size 8192
2019-10-02 01:35:27.302080: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd0ffb3f00 next 183 of size 16384000
2019-10-02 01:35:27.302091: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd10f53f00 next 184 of size 16384000
2019-10-02 01:35:27.302103: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd11ef3f00 next 185 of size 8192
2019-10-02 01:35:27.302113: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd11ef5f00 next 186 of size 8192
2019-10-02 01:35:27.302124: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd11ef7f00 next 187 of size 16777216
2019-10-02 01:35:27.302139: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd12ef7f00 next 188 of size 32768
2019-10-02 01:35:27.302156: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd12efff00 next 189 of size 16777216
2019-10-02 01:35:27.302166: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd13efff00 next 190 of size 8192
2019-10-02 01:35:27.302176: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd13f01f00 next 191 of size 8192
2019-10-02 01:35:27.302193: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd13f03f00 next 192 of size 8192
2019-10-02 01:35:27.302203: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd13f05f00 next 193 of size 16777216
2019-10-02 01:35:27.302212: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd14f05f00 next 194 of size 8192
2019-10-02 01:35:27.302229: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd14f07f00 next 195 of size 8192
2019-10-02 01:35:27.302239: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd14f09f00 next 196 of size 67108864
2019-10-02 01:35:27.302249: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd18f09f00 next 197 of size 16777216
2019-10-02 01:35:27.302265: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd19f09f00 next 198 of size 8192
2019-10-02 01:35:27.302275: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd19f0bf00 next 199 of size 8192
2019-10-02 01:35:27.302285: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd19f0df00 next 200 of size 16777216
2019-10-02 01:35:27.302301: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd1af0df00 next 201 of size 8192
2019-10-02 01:35:27.302312: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd1af0ff00 next 202 of size 8192
2019-10-02 01:35:27.302321: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd1af11f00 next 203 of size 8192
2019-10-02 01:35:27.302337: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd1af13f00 next 204 of size 67108864
2019-10-02 01:35:27.302349: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd1ef13f00 next 205 of size 32768
2019-10-02 01:35:27.302358: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd1ef1bf00 next 206 of size 16777216
2019-10-02 01:35:27.302374: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd1ff1bf00 next 207 of size 8192
2019-10-02 01:35:27.302385: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd1ff1df00 next 208 of size 8192
2019-10-02 01:35:27.302394: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd1ff1ff00 next 209 of size 8192
2019-10-02 01:35:27.302412: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd1ff21f00 next 210 of size 16777216
2019-10-02 01:35:27.302422: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd20f21f00 next 211 of size 32768
2019-10-02 01:35:27.302432: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd20f29f00 next 212 of size 8192
2019-10-02 01:35:27.302448: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd20f2bf00 next 213 of size 8192
2019-10-02 01:35:27.302459: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd20f2df00 next 214 of size 8192
2019-10-02 01:35:27.302468: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd20f2ff00 next 215 of size 8192
2019-10-02 01:35:27.302487: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd20f31f00 next 216 of size 8192
2019-10-02 01:35:27.302497: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd20f33f00 next 217 of size 8192
2019-10-02 01:35:27.302507: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd20f35f00 next 218 of size 16777216
2019-10-02 01:35:27.302527: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd21f35f00 next 219 of size 512
2019-10-02 01:35:27.302539: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd21f36100 next 220 of size 16777216
2019-10-02 01:35:27.302548: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd22f36100 next 221 of size 8192
2019-10-02 01:35:27.302565: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd22f38100 next 222 of size 67108864
2019-10-02 01:35:27.302576: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd26f38100 next 223 of size 16777216
2019-10-02 01:35:27.302586: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd27f38100 next 224 of size 8192
2019-10-02 01:35:27.302602: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd27f3a100 next 225 of size 16384000
2019-10-02 01:35:27.302613: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd28eda100 next 226 of size 512
2019-10-02 01:35:27.302622: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd28eda300 next 227 of size 8192
2019-10-02 01:35:27.302639: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd28edc300 next 228 of size 8192
2019-10-02 01:35:27.302650: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd28ede300 next 229 of size 8192
2019-10-02 01:35:27.302659: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd28ee0300 next 230 of size 8192
2019-10-02 01:35:27.302675: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd28ee2300 next 231 of size 8192
2019-10-02 01:35:27.302686: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd28ee4300 next 232 of size 16777216
2019-10-02 01:35:27.302695: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd29ee4300 next 233 of size 8192
2019-10-02 01:35:27.302712: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd29ee6300 next 234 of size 67108864
2019-10-02 01:35:27.302722: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd2dee6300 next 235 of size 16384000
2019-10-02 01:35:27.302732: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd2ee86300 next 236 of size 16777216
2019-10-02 01:35:27.302749: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd2fe86300 next 237 of size 8192
2019-10-02 01:35:27.302760: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd2fe88300 next 238 of size 8192
2019-10-02 01:35:27.302769: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd2fe8a300 next 239 of size 512
2019-10-02 01:35:27.302786: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd2fe8a500 next 240 of size 16777216
2019-10-02 01:35:27.302797: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd30e8a500 next 241 of size 8192
2019-10-02 01:35:27.302806: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd30e8c500 next 242 of size 8192
2019-10-02 01:35:27.302823: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd30e8e500 next 243 of size 32768
2019-10-02 01:35:27.302833: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd30e96500 next 244 of size 16384000
2019-10-02 01:35:27.302843: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd31e36500 next 245 of size 16384000
2019-10-02 01:35:27.302860: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd32dd6500 next 246 of size 8192
2019-10-02 01:35:27.302871: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd32dd8500 next 247 of size 32768
2019-10-02 01:35:27.302885: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd32de0500 next 248 of size 8192
2019-10-02 01:35:27.302895: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd32de2500 next 249 of size 8192
2019-10-02 01:35:27.302917: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd32de4500 next 250 of size 32768
2019-10-02 01:35:27.302929: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd32dec500 next 251 of size 8192
2019-10-02 01:35:27.302941: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd32dee500 next 252 of size 8192
2019-10-02 01:35:27.302951: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd32df0500 next 253 of size 16777216
2019-10-02 01:35:27.302964: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd33df0500 next 254 of size 16384000
2019-10-02 01:35:27.302973: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd34d90500 next 255 of size 16384000
2019-10-02 01:35:27.302985: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd35d30500 next 256 of size 512
2019-10-02 01:35:27.302995: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd35d30700 next 257 of size 8192
2019-10-02 01:35:27.303007: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd35d32700 next 258 of size 8192
2019-10-02 01:35:27.303017: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd35d34700 next 259 of size 8192
2019-10-02 01:35:27.303033: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd35d36700 next 260 of size 67108864
2019-10-02 01:35:27.303043: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd39d36700 next 261 of size 8192
2019-10-02 01:35:27.303053: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd39d38700 next 262 of size 67108864
2019-10-02 01:35:27.303070: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd3dd38700 next 263 of size 512
2019-10-02 01:35:27.303080: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd3dd38900 next 264 of size 8192
2019-10-02 01:35:27.303090: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd3dd3a900 next 265 of size 8192
2019-10-02 01:35:27.303107: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd3dd3c900 next 266 of size 67108864
2019-10-02 01:35:27.303118: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd41d3c900 next 267 of size 16384000
2019-10-02 01:35:27.303130: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd42cdc900 next 268 of size 8192
2019-10-02 01:35:27.303139: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd42cde900 next 269 of size 8192
2019-10-02 01:35:27.303152: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd42ce0900 next 270 of size 8192
2019-10-02 01:35:27.303162: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd42ce2900 next 271 of size 67108864
2019-10-02 01:35:27.303173: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd46ce2900 next 272 of size 16777216
2019-10-02 01:35:27.303184: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd47ce2900 next 273 of size 16777216
2019-10-02 01:35:27.303195: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd48ce2900 next 274 of size 16777216
2019-10-02 01:35:27.303205: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd49ce2900 next 275 of size 16777216
2019-10-02 01:35:27.303215: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd4ace2900 next 276 of size 8192
2019-10-02 01:35:27.303231: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd4ace4900 next 277 of size 16384000
2019-10-02 01:35:27.303241: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd4bc84900 next 278 of size 16384000
2019-10-02 01:35:27.303252: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd4cc24900 next 279 of size 8192
2019-10-02 01:35:27.303271: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd4cc26900 next 280 of size 8192
2019-10-02 01:35:27.303282: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd4cc28900 next 281 of size 8192
2019-10-02 01:35:27.303336: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd4cc2a900 next 282 of size 8192
2019-10-02 01:35:27.303357: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd4cc2c900 next 283 of size 8192
2019-10-02 01:35:27.303367: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd4cc2e900 next 284 of size 67108864
2019-10-02 01:35:27.303384: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd50c2e900 next 285 of size 8192
2019-10-02 01:35:27.303395: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd50c30900 next 286 of size 16384000
2019-10-02 01:35:27.303414: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd51bd0900 next 287 of size 16384000
2019-10-02 01:35:27.303425: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd52b70900 next 288 of size 8192
2019-10-02 01:35:27.303443: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd52b72900 next 289 of size 8192
2019-10-02 01:35:27.303454: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd52b74900 next 290 of size 67108864
2019-10-02 01:35:27.303463: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd56b74900 next 291 of size 8192
2019-10-02 01:35:27.303480: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd56b76900 next 292 of size 8192
2019-10-02 01:35:27.303490: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd56b78900 next 293 of size 8192
2019-10-02 01:35:27.303508: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd56b7a900 next 294 of size 8192
2019-10-02 01:35:27.303519: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd56b7c900 next 295 of size 16777216
2019-10-02 01:35:27.303537: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd57b7c900 next 296 of size 8192
2019-10-02 01:35:27.303548: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd57b7e900 next 297 of size 32768
2019-10-02 01:35:27.303565: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd57b86900 next 298 of size 16777216
2019-10-02 01:35:27.303576: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd58b86900 next 299 of size 16777216
2019-10-02 01:35:27.303593: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd59b86900 next 300 of size 8192
2019-10-02 01:35:27.303604: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd59b88900 next 301 of size 8192
2019-10-02 01:35:27.303622: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd59b8a900 next 302 of size 16777216
2019-10-02 01:35:27.303633: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd5ab8a900 next 303 of size 512
2019-10-02 01:35:27.303651: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd5ab8ab00 next 304 of size 8192
2019-10-02 01:35:27.303662: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd5ab8cb00 next 305 of size 67108864
2019-10-02 01:35:27.303679: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd5eb8cb00 next 306 of size 8192
2019-10-02 01:35:27.303690: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd5eb8eb00 next 307 of size 16777216
2019-10-02 01:35:27.303707: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd5fb8eb00 next 308 of size 67108864
2019-10-02 01:35:27.303718: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd63b8eb00 next 309 of size 8192
2019-10-02 01:35:27.303727: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd63b90b00 next 310 of size 8192
2019-10-02 01:35:27.303744: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd63b92b00 next 311 of size 8192
2019-10-02 01:35:27.303756: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd63b94b00 next 312 of size 8192
2019-10-02 01:35:27.303779: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd63b96b00 next 313 of size 16777216
2019-10-02 01:35:27.303790: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd64b96b00 next 314 of size 16777216
2019-10-02 01:35:27.303807: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd65b96b00 next 315 of size 67108864
2019-10-02 01:35:27.303818: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd69b96b00 next 316 of size 8192
2019-10-02 01:35:27.303835: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd69b98b00 next 317 of size 8192
2019-10-02 01:35:27.303846: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd69b9ab00 next 318 of size 256
2019-10-02 01:35:27.303863: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd69b9ac00 next 319 of size 8192
2019-10-02 01:35:27.303874: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd69b9cc00 next 320 of size 512
2019-10-02 01:35:27.303892: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd69b9ce00 next 321 of size 67108864
2019-10-02 01:35:27.303903: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd6db9ce00 next 322 of size 32768
2019-10-02 01:35:27.303921: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd6dba4e00 next 323 of size 16777216
2019-10-02 01:35:27.303932: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd6eba4e00 next 324 of size 512
2019-10-02 01:35:27.303949: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd6eba5000 next 325 of size 32768
2019-10-02 01:35:27.303960: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd6ebad000 next 326 of size 8192
2019-10-02 01:35:27.303977: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd6ebaf000 next 327 of size 8192
2019-10-02 01:35:27.303988: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd6ebb1000 next 328 of size 8192
2019-10-02 01:35:27.303998: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd6ebb3000 next 329 of size 8192
2019-10-02 01:35:27.304008: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd6ebb5000 next 330 of size 16777216
2019-10-02 01:35:27.304026: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd6fbb5000 next 331 of size 32768
2019-10-02 01:35:27.304036: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd6fbbd000 next 332 of size 16777216
2019-10-02 01:35:27.304046: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd70bbd000 next 333 of size 8192
2019-10-02 01:35:27.304063: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd70bbf000 next 334 of size 16777216
2019-10-02 01:35:27.304073: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd71bbf000 next 335 of size 8192
2019-10-02 01:35:27.304090: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd71bc1000 next 336 of size 8192
2019-10-02 01:35:27.304101: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd71bc3000 next 337 of size 8192
2019-10-02 01:35:27.304119: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd71bc5000 next 338 of size 16777216
2019-10-02 01:35:27.304129: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd72bc5000 next 339 of size 8192
2019-10-02 01:35:27.304139: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd72bc7000 next 340 of size 16777216
2019-10-02 01:35:27.304156: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd73bc7000 next 341 of size 8192
2019-10-02 01:35:27.304166: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd73bc9000 next 342 of size 8192
2019-10-02 01:35:27.304176: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd73bcb000 next 343 of size 8192
2019-10-02 01:35:27.304194: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd73bcd000 next 344 of size 67108864
2019-10-02 01:35:27.304219: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd77bcd000 next 345 of size 8192
2019-10-02 01:35:27.304230: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd77bcf000 next 346 of size 16777216
2019-10-02 01:35:27.304247: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd78bcf000 next 347 of size 8192
2019-10-02 01:35:27.304258: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd78bd1000 next 348 of size 8192
2019-10-02 01:35:27.304275: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd78bd3000 next 349 of size 8192
2019-10-02 01:35:27.304287: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd78bd5000 next 350 of size 262144000
2019-10-02 01:35:27.304299: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd885d5000 next 351 of size 512
2019-10-02 01:35:27.304309: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd885d5200 next 352 of size 8192
2019-10-02 01:35:27.304319: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd885d7200 next 353 of size 8192
2019-10-02 01:35:27.304338: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd885d9200 next 354 of size 16777216
2019-10-02 01:35:27.304349: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd895d9200 next 355 of size 16777216
2019-10-02 01:35:27.304358: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd8a5d9200 next 356 of size 16777216
2019-10-02 01:35:27.304367: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd8b5d9200 next 357 of size 8192
2019-10-02 01:35:27.304376: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd8b5db200 next 358 of size 16777216
2019-10-02 01:35:27.304386: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd8c5db200 next 359 of size 32768
2019-10-02 01:35:27.304402: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd8c5e3200 next 360 of size 8192
2019-10-02 01:35:27.304413: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd8c5e5200 next 361 of size 8192
2019-10-02 01:35:27.304423: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd8c5e7200 next 362 of size 8192
2019-10-02 01:35:27.304440: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd8c5e9200 next 363 of size 512
2019-10-02 01:35:27.304450: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd8c5e9400 next 364 of size 16777216
2019-10-02 01:35:27.304459: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd8d5e9400 next 365 of size 8192
2019-10-02 01:35:27.304486: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd8d5eb400 next 366 of size 8192
2019-10-02 01:35:27.304497: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd8d5ed400 next 367 of size 16777216
2019-10-02 01:35:27.304506: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd8e5ed400 next 368 of size 16777216
2019-10-02 01:35:27.304515: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd8f5ed400 next 369 of size 67108864
2019-10-02 01:35:27.304524: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd935ed400 next 370 of size 16777216
2019-10-02 01:35:27.304533: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd945ed400 next 371 of size 67108864
2019-10-02 01:35:27.304545: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd985ed400 next 372 of size 8192
2019-10-02 01:35:27.304555: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd985ef400 next 373 of size 67108864
2019-10-02 01:35:27.304567: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd9c5ef400 next 374 of size 16777216
2019-10-02 01:35:27.304576: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd9d5ef400 next 375 of size 8192
2019-10-02 01:35:27.304599: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd9d5f1400 next 376 of size 8192
2019-10-02 01:35:27.304609: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd9d5f3400 next 377 of size 512
2019-10-02 01:35:27.304620: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd9d5f3600 next 378 of size 8192
2019-10-02 01:35:27.304637: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd9d5f5600 next 379 of size 8192
2019-10-02 01:35:27.304647: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd9d5f7600 next 380 of size 8192
2019-10-02 01:35:27.304657: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd9d5f9600 next 381 of size 8192
2019-10-02 01:35:27.304674: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd9d5fb600 next 382 of size 8192
2019-10-02 01:35:27.304685: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd9d5fd600 next 383 of size 16777216
2019-10-02 01:35:27.304695: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd9e5fd600 next 384 of size 32768
2019-10-02 01:35:27.304711: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd9e605600 next 385 of size 67108864
2019-10-02 01:35:27.304720: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda2605600 next 386 of size 8192
2019-10-02 01:35:27.304729: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda2607600 next 387 of size 8192
2019-10-02 01:35:27.304746: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda2609600 next 388 of size 512
2019-10-02 01:35:27.304757: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda2609800 next 389 of size 8192
2019-10-02 01:35:27.304766: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda260b800 next 390 of size 8192
2019-10-02 01:35:27.304781: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda260d800 next 391 of size 512
2019-10-02 01:35:27.304791: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda260da00 next 392 of size 8192
2019-10-02 01:35:27.304802: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda260fa00 next 393 of size 8192
2019-10-02 01:35:27.304818: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda2611a00 next 394 of size 8192
2019-10-02 01:35:27.304828: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda2613a00 next 395 of size 8192
2019-10-02 01:35:27.304838: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda2615a00 next 396 of size 8192
2019-10-02 01:35:27.304854: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda2617a00 next 397 of size 8192
2019-10-02 01:35:27.304864: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda2619a00 next 398 of size 8192
2019-10-02 01:35:27.304874: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda261ba00 next 399 of size 8192
2019-10-02 01:35:27.304890: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda261da00 next 400 of size 8192
2019-10-02 01:35:27.304900: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda261fa00 next 401 of size 8192
2019-10-02 01:35:27.304910: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda2621a00 next 402 of size 67108864
2019-10-02 01:35:27.304926: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda6621a00 next 403 of size 8192
2019-10-02 01:35:27.304937: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda6623a00 next 404 of size 16777216
2019-10-02 01:35:27.304946: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda7623a00 next 405 of size 67108864
2019-10-02 01:35:27.304963: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdab623a00 next 406 of size 8192
2019-10-02 01:35:27.304981: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdab625a00 next 407 of size 8192
2019-10-02 01:35:27.304993: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdab627a00 next 408 of size 8192
2019-10-02 01:35:27.305011: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdab629a00 next 409 of size 67108864
2019-10-02 01:35:27.305021: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdaf629a00 next 410 of size 16777216
2019-10-02 01:35:27.305030: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdb0629a00 next 411 of size 8192
2019-10-02 01:35:27.305047: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdb062ba00 next 412 of size 512
2019-10-02 01:35:27.305058: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdb062bc00 next 413 of size 67108864
2019-10-02 01:35:27.305067: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdb462bc00 next 414 of size 16777216
2019-10-02 01:35:27.305084: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdb562bc00 next 415 of size 8192
2019-10-02 01:35:27.305094: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdb562dc00 next 416 of size 8192
2019-10-02 01:35:27.305104: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdb562fc00 next 417 of size 16777216
2019-10-02 01:35:27.305121: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdb662fc00 next 418 of size 8192
2019-10-02 01:35:27.305132: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdb6631c00 next 419 of size 8192
2019-10-02 01:35:27.305141: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdb6633c00 next 420 of size 32768
2019-10-02 01:35:27.305158: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdb663bc00 next 421 of size 16777216
2019-10-02 01:35:27.305168: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdb763bc00 next 422 of size 16777216
2019-10-02 01:35:27.305177: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdb863bc00 next 423 of size 32768
2019-10-02 01:35:27.305194: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdb8643c00 next 424 of size 16777216
2019-10-02 01:35:27.305204: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdb9643c00 next 425 of size 8192
2019-10-02 01:35:27.305214: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdb9645c00 next 426 of size 16777216
2019-10-02 01:35:27.305231: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdba645c00 next 427 of size 8192
2019-10-02 01:35:27.305241: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdba647c00 next 428 of size 67108864
2019-10-02 01:35:27.305251: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdbe647c00 next 429 of size 8192
2019-10-02 01:35:27.305267: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdbe649c00 next 430 of size 8192
2019-10-02 01:35:27.305278: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdbe64bc00 next 431 of size 16777216
2019-10-02 01:35:27.305287: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdbf64bc00 next 432 of size 8192
2019-10-02 01:35:27.305304: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdbf64dc00 next 433 of size 8192
2019-10-02 01:35:27.305314: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdbf64fc00 next 434 of size 512
2019-10-02 01:35:27.305324: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdbf64fe00 next 435 of size 16777216
2019-10-02 01:35:27.305340: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc064fe00 next 436 of size 8192
2019-10-02 01:35:27.305351: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc0651e00 next 437 of size 16777216
2019-10-02 01:35:27.305361: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc1651e00 next 438 of size 8192
2019-10-02 01:35:27.305382: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc1653e00 next 439 of size 8192
2019-10-02 01:35:27.305395: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc1655e00 next 440 of size 67108864
2019-10-02 01:35:27.305407: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc5655e00 next 441 of size 16777216
2019-10-02 01:35:27.305417: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc6655e00 next 442 of size 8192
2019-10-02 01:35:27.305428: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc6657e00 next 443 of size 16777216
2019-10-02 01:35:27.305438: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc7657e00 next 444 of size 8192
2019-10-02 01:35:27.305448: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc7659e00 next 445 of size 8192
2019-10-02 01:35:27.305464: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc765be00 next 446 of size 8192
2019-10-02 01:35:27.305476: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc765de00 next 447 of size 8192
2019-10-02 01:35:27.305485: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc765fe00 next 448 of size 8192
2019-10-02 01:35:27.305501: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc7661e00 next 449 of size 8192
2019-10-02 01:35:27.305512: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc7663e00 next 450 of size 8192
2019-10-02 01:35:27.305521: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc7665e00 next 451 of size 16777216
2019-10-02 01:35:27.305537: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc8665e00 next 452 of size 512
2019-10-02 01:35:27.305548: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc8666000 next 453 of size 8192
2019-10-02 01:35:27.305557: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc8668000 next 454 of size 8192
2019-10-02 01:35:27.305573: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc866a000 next 455 of size 32768
2019-10-02 01:35:27.305584: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc8672000 next 456 of size 512
2019-10-02 01:35:27.305593: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc8672200 next 457 of size 8192
2019-10-02 01:35:27.305610: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc8674200 next 458 of size 8192
2019-10-02 01:35:27.305620: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc8676200 next 459 of size 16777216
2019-10-02 01:35:27.305630: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc9676200 next 460 of size 16777216
2019-10-02 01:35:27.305647: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdca676200 next 461 of size 8192
2019-10-02 01:35:27.305658: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdca678200 next 462 of size 8192
2019-10-02 01:35:27.305667: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdca67a200 next 463 of size 8192
2019-10-02 01:35:27.305683: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdca67c200 next 464 of size 512
2019-10-02 01:35:27.305694: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdca67c400 next 465 of size 16777216
2019-10-02 01:35:27.305704: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdcb67c400 next 466 of size 32768
2019-10-02 01:35:27.305720: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdcb684400 next 467 of size 8192
2019-10-02 01:35:27.305731: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdcb686400 next 468 of size 8192
2019-10-02 01:35:27.305740: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdcb688400 next 469 of size 16777216
2019-10-02 01:35:27.305761: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdcc688400 next 470 of size 32768
2019-10-02 01:35:27.305772: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdcc690400 next 471 of size 8192
2019-10-02 01:35:27.305782: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdcc692400 next 472 of size 16777216
2019-10-02 01:35:27.305799: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdcd692400 next 473 of size 8192
2019-10-02 01:35:27.305810: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdcd694400 next 474 of size 8192
2019-10-02 01:35:27.305819: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdcd696400 next 475 of size 67108864
2019-10-02 01:35:27.305835: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdd1696400 next 476 of size 8192
2019-10-02 01:35:27.305846: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdd1698400 next 477 of size 67108864
2019-10-02 01:35:27.305859: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdd5698400 next 478 of size 256
2019-10-02 01:35:27.305869: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdd5698500 next 479 of size 8192
2019-10-02 01:35:27.305879: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdd569a500 next 480 of size 512
2019-10-02 01:35:27.305896: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdd569a700 next 481 of size 67108864
2019-10-02 01:35:27.305906: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdd969a700 next 482 of size 256
2019-10-02 01:35:27.305915: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdd969a800 next 514 of size 256
2019-10-02 01:35:27.305932: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdd969a900 next 484 of size 256
2019-10-02 01:35:27.305943: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdd969aa00 next 485 of size 8192
2019-10-02 01:35:27.305952: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdd969ca00 next 486 of size 8192
2019-10-02 01:35:27.305968: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdd969ea00 next 487 of size 16777216
2019-10-02 01:35:27.305979: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdda69ea00 next 515 of size 8192
2019-10-02 01:35:27.305988: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdda6a0a00 next 512 of size 512
2019-10-02 01:35:27.306005: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdda6a0c00 next 513 of size 8192
2019-10-02 01:35:27.306016: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdda6a2c00 next 510 of size 16777216
2019-10-02 01:35:27.306025: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efddb6a2c00 next 511 of size 8192
2019-10-02 01:35:27.306042: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efddb6a4c00 next 500 of size 16777216
2019-10-02 01:35:27.306052: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efddc6a4c00 next 501 of size 16777216
2019-10-02 01:35:27.306062: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efddd6a4c00 next 509 of size 8192
2019-10-02 01:35:27.306079: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efddd6a6c00 next 504 of size 8192
2019-10-02 01:35:27.306090: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efddd6a8c00 next 505 of size 67108864
2019-10-02 01:35:27.306099: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde16a8c00 next 489 of size 32768
2019-10-02 01:35:27.306116: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde16b0c00 next 490 of size 8192
2019-10-02 01:35:27.306126: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde16b2c00 next 488 of size 8192
2019-10-02 01:35:27.306135: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde16b4c00 next 508 of size 16777216
2019-10-02 01:35:27.306157: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde26b4c00 next 503 of size 8192
2019-10-02 01:35:27.306168: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde26b6c00 next 502 of size 8192
2019-10-02 01:35:27.306187: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde26b8c00 next 507 of size 8192
2019-10-02 01:35:27.306198: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde26bac00 next 497 of size 16777216
2019-10-02 01:35:27.306207: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde36bac00 next 498 of size 67108864
2019-10-02 01:35:27.306224: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde76bac00 next 506 of size 8192
2019-10-02 01:35:27.306235: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde76bcc00 next 496 of size 8192
2019-10-02 01:35:27.306244: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde76bec00 next 492 of size 16777216
2019-10-02 01:35:27.306261: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde86bec00 next 493 of size 512
2019-10-02 01:35:27.306272: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde86bee00 next 483 of size 8192
2019-10-02 01:35:27.306281: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde86c0e00 next 495 of size 16777216
2019-10-02 01:35:27.306298: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde96c0e00 next 491 of size 8192
2019-10-02 01:35:27.306308: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde96c2e00 next 499 of size 8192
2019-10-02 01:35:27.306317: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde96c4e00 next 494 of size 32768
2019-10-02 01:35:27.306334: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde96cce00 next 516 of size 16777216
2019-10-02 01:35:27.306345: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdea6cce00 next 517 of size 8192
2019-10-02 01:35:27.306354: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdea6cee00 next 518 of size 8192
2019-10-02 01:35:27.306364: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdea6d0e00 next 519 of size 16777216
2019-10-02 01:35:27.306374: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdeb6d0e00 next 520 of size 67108864
2019-10-02 01:35:27.306386: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdef6d0e00 next 521 of size 8192
2019-10-02 01:35:27.306395: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdef6d2e00 next 522 of size 16777216
2019-10-02 01:35:27.306405: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf06d2e00 next 523 of size 8192
2019-10-02 01:35:27.306415: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf06d4e00 next 524 of size 8192
2019-10-02 01:35:27.306427: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf06d6e00 next 525 of size 8192
2019-10-02 01:35:27.306437: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf06d8e00 next 526 of size 16777216
2019-10-02 01:35:27.306447: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16d8e00 next 527 of size 8192
2019-10-02 01:35:27.306458: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16dae00 next 528 of size 8192
2019-10-02 01:35:27.306467: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16dce00 next 529 of size 8192
2019-10-02 01:35:27.306477: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16dee00 next 530 of size 256
2019-10-02 01:35:27.306489: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16def00 next 545 of size 256
2019-10-02 01:35:27.306498: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16df000 next 547 of size 8192
2019-10-02 01:35:27.306516: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16e1000 next 548 of size 256
2019-10-02 01:35:27.306526: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16e1100 next 549 of size 8192
2019-10-02 01:35:27.306535: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16e3100 next 546 of size 256
2019-10-02 01:35:27.306554: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16e3200 next 550 of size 8192
2019-10-02 01:35:27.306564: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16e5200 next 551 of size 256
2019-10-02 01:35:27.306573: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16e5300 next 552 of size 8192
2019-10-02 01:35:27.306585: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16e7300 next 554 of size 256
2019-10-02 01:35:27.306596: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16e7400 next 586 of size 4096
2019-10-02 01:35:27.306605: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16e8400 next 577 of size 256
2019-10-02 01:35:27.306617: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16e8500 next 590 of size 5376
2019-10-02 01:35:27.306628: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16e9a00 next 559 of size 8192
2019-10-02 01:35:27.306639: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16eba00 next 657 of size 9984
2019-10-02 01:35:27.306650: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16ee100 next 684 of size 6912
2019-10-02 01:35:27.306661: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16efc00 next 685 of size 256
2019-10-02 01:35:27.306670: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16efd00 next 686 of size 256
2019-10-02 01:35:27.306679: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16efe00 next 687 of size 256
2019-10-02 01:35:27.306688: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16eff00 next 688 of size 256
2019-10-02 01:35:27.306701: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f0000 next 683 of size 256
2019-10-02 01:35:27.306710: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f0100 next 690 of size 256
2019-10-02 01:35:27.306719: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f0200 next 665 of size 256
2019-10-02 01:35:27.306731: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f0300 next 605 of size 256
2019-10-02 01:35:27.306741: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f0400 next 697 of size 256
2019-10-02 01:35:27.306750: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f0500 next 936 of size 256
2019-10-02 01:35:27.306759: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f0600 next 630 of size 512
2019-10-02 01:35:27.306768: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f0800 next 1011 of size 256
2019-10-02 01:35:27.306780: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f0900 next 699 of size 512
2019-10-02 01:35:27.306790: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f0b00 next 700 of size 256
2019-10-02 01:35:27.306799: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f0c00 next 701 of size 256
2019-10-02 01:35:27.306808: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f0d00 next 703 of size 256
2019-10-02 01:35:27.306818: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f0e00 next 705 of size 256
2019-10-02 01:35:27.306830: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f0f00 next 706 of size 256
2019-10-02 01:35:27.306841: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f1000 next 707 of size 256
2019-10-02 01:35:27.306857: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f1100 next 712 of size 256
2019-10-02 01:35:27.306866: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f1200 next 637 of size 256
2019-10-02 01:35:27.306876: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f1300 next 714 of size 256
2019-10-02 01:35:27.306889: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f1400 next 531 of size 56064
2019-10-02 01:35:27.306899: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16fef00 next 532 of size 67108864
2019-10-02 01:35:27.306909: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf56fef00 next 533 of size 256
2019-10-02 01:35:27.306920: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf56ff000 next 534 of size 16777216
2019-10-02 01:35:27.306930: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf66ff000 next 535 of size 131072
2019-10-02 01:35:27.306940: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf671f000 next 715 of size 32768
2019-10-02 01:35:27.306952: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6727000 next 718 of size 256
2019-10-02 01:35:27.306962: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6727100 next 677 of size 256
2019-10-02 01:35:27.306971: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6727200 next 720 of size 256
2019-10-02 01:35:27.306983: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6727300 next 721 of size 8192
2019-10-02 01:35:27.306993: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6729300 next 722 of size 8192
2019-10-02 01:35:27.307002: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf672b300 next 724 of size 256
2019-10-02 01:35:27.307013: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf672b400 next 725 of size 256
2019-10-02 01:35:27.307023: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf672b500 next 726 of size 256
2019-10-02 01:35:27.307033: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf672b600 next 727 of size 256
2019-10-02 01:35:27.307042: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf672b700 next 728 of size 8192
2019-10-02 01:35:27.307051: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf672d700 next 729 of size 8192
2019-10-02 01:35:27.307061: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf672f700 next 730 of size 256
2019-10-02 01:35:27.307070: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf672f800 next 731 of size 256
2019-10-02 01:35:27.307082: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf672f900 next 732 of size 256
2019-10-02 01:35:27.307092: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf672fa00 next 733 of size 256
2019-10-02 01:35:27.307102: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf672fb00 next 734 of size 8192
2019-10-02 01:35:27.307111: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6731b00 next 735 of size 8192
2019-10-02 01:35:27.307121: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6733b00 next 736 of size 8192
2019-10-02 01:35:27.307130: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6735b00 next 737 of size 256
2019-10-02 01:35:27.307140: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6735c00 next 738 of size 8192
2019-10-02 01:35:27.307153: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6737c00 next 739 of size 8192
2019-10-02 01:35:27.307162: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6739c00 next 740 of size 256
2019-10-02 01:35:27.307171: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6739d00 next 741 of size 8192
2019-10-02 01:35:27.307195: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf673bd00 next 536 of size 13056
2019-10-02 01:35:27.307207: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf673f000 next 537 of size 8192
2019-10-02 01:35:27.307216: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6741000 next 538 of size 256
2019-10-02 01:35:27.307234: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6741100 next 539 of size 8192
2019-10-02 01:35:27.307245: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6743100 next 540 of size 131072
2019-10-02 01:35:27.307255: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6763100 next 541 of size 256
2019-10-02 01:35:27.307265: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6763200 next 542 of size 67108864
2019-10-02 01:35:27.307275: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdfa763200 next 543 of size 8192
2019-10-02 01:35:27.307287: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdfa765200 next 544 of size 67108864
2019-10-02 01:35:27.307329: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdfe765200 next 553 of size 131072
2019-10-02 01:35:27.307340: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdfe785200 next 582 of size 16777216
2019-10-02 01:35:27.307349: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff785200 next 742 of size 256
2019-10-02 01:35:27.307359: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff785300 next 743 of size 8192
2019-10-02 01:35:27.307368: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff787300 next 744 of size 8192
2019-10-02 01:35:27.307380: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff789300 next 745 of size 256
2019-10-02 01:35:27.307389: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff789400 next 746 of size 8192
2019-10-02 01:35:27.307399: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff78b400 next 747 of size 8192
2019-10-02 01:35:27.307411: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff78d400 next 748 of size 256
2019-10-02 01:35:27.307420: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff78d500 next 749 of size 8192
2019-10-02 01:35:27.307430: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff78f500 next 750 of size 8192
2019-10-02 01:35:27.307442: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff791500 next 751 of size 256
2019-10-02 01:35:27.307451: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff791600 next 752 of size 8192
2019-10-02 01:35:27.307461: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff793600 next 753 of size 256
2019-10-02 01:35:27.307478: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff793700 next 754 of size 256
2019-10-02 01:35:27.307488: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff793800 next 756 of size 256
2019-10-02 01:35:27.307499: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff793900 next 758 of size 8192
2019-10-02 01:35:27.307508: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff795900 next 759 of size 256
2019-10-02 01:35:27.307517: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff795a00 next 760 of size 8192
2019-10-02 01:35:27.307530: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff797a00 next 761 of size 8192
2019-10-02 01:35:27.307540: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff799a00 next 762 of size 256
2019-10-02 01:35:27.307549: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff799b00 next 763 of size 8192
2019-10-02 01:35:27.307561: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff79bb00 next 764 of size 8192
2019-10-02 01:35:27.307576: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff79db00 next 765 of size 256
2019-10-02 01:35:27.307594: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff79dc00 next 766 of size 8192
2019-10-02 01:35:27.307604: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff79fc00 next 769 of size 256
2019-10-02 01:35:27.307616: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff79fd00 next 771 of size 256
2019-10-02 01:35:27.307628: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff79fe00 next 773 of size 256
2019-10-02 01:35:27.307638: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff79ff00 next 776 of size 256
2019-10-02 01:35:27.307648: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff7a0000 next 778 of size 256
2019-10-02 01:35:27.307659: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff7a0100 next 780 of size 256
2019-10-02 01:35:27.307670: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff7a0200 next 782 of size 256
2019-10-02 01:35:27.307679: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff7a0300 next 785 of size 256
2019-10-02 01:35:27.307688: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff7a0400 next 786 of size 8192
2019-10-02 01:35:27.307698: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff7a2400 next 787 of size 512
2019-10-02 01:35:27.307710: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff7a2600 next 581 of size 11264
2019-10-02 01:35:27.307720: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff7a5200 next 580 of size 16777216
2019-10-02 01:35:27.307729: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe007a5200 next 579 of size 67108864
2019-10-02 01:35:27.307739: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe047a5200 next 578 of size 131072
2019-10-02 01:35:27.307748: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe047c5200 next 585 of size 16777216
2019-10-02 01:35:27.307760: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe057c5200 next 584 of size 8388608
2019-10-02 01:35:27.307770: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe05fc5200 next 583 of size 16777216
2019-10-02 01:35:27.307779: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe06fc5200 next 576 of size 16777216
2019-10-02 01:35:27.307791: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe07fc5200 next 702 of size 67108864
2019-10-02 01:35:27.307800: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe0bfc5200 next 704 of size 16777216
2019-10-02 01:35:27.307810: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe0cfc5200 next 708 of size 67108864
2019-10-02 01:35:27.307829: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe10fc5200 next 709 of size 16777216
2019-10-02 01:35:27.307838: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe11fc5200 next 710 of size 67108864
2019-10-02 01:35:27.307848: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe15fc5200 next 711 of size 16777216
2019-10-02 01:35:27.307865: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe16fc5200 next 716 of size 16777216
2019-10-02 01:35:27.307875: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe17fc5200 next 717 of size 67108864
2019-10-02 01:35:27.307885: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe1bfc5200 next 723 of size 67108864
2019-10-02 01:35:27.307895: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe1ffc5200 next 755 of size 16777216
2019-10-02 01:35:27.307904: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe20fc5200 next 757 of size 16777216
2019-10-02 01:35:27.307926: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe21fc5200 next 767 of size 16777216
2019-10-02 01:35:27.307936: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe22fc5200 next 768 of size 16777216
2019-10-02 01:35:27.307946: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe23fc5200 next 770 of size 67108864
2019-10-02 01:35:27.307955: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe27fc5200 next 772 of size 32768
2019-10-02 01:35:27.307966: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe27fcd200 next 774 of size 32768
2019-10-02 01:35:27.307978: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe27fd5200 next 775 of size 16777216
2019-10-02 01:35:27.307987: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe28fd5200 next 777 of size 16777216
2019-10-02 01:35:27.307997: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe29fd5200 next 779 of size 16777216
2019-10-02 01:35:27.308007: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe2afd5200 next 781 of size 67108864
2019-10-02 01:35:27.308018: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe2efd5200 next 783 of size 16777216
2019-10-02 01:35:27.308028: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe2ffd5200 next 784 of size 67108864
2019-10-02 01:35:27.308037: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe33fd5200 next 788 of size 512
2019-10-02 01:35:27.308050: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe33fd5400 next 789 of size 256
2019-10-02 01:35:27.308059: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe33fd5500 next 790 of size 256
2019-10-02 01:35:27.308069: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe33fd5600 next 791 of size 256
2019-10-02 01:35:27.308078: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe33fd5700 next 792 of size 256
2019-10-02 01:35:27.308090: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe33fd5800 next 793 of size 512
2019-10-02 01:35:27.308100: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe33fd5a00 next 794 of size 8192
2019-10-02 01:35:27.308109: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe33fd7a00 next 795 of size 512
2019-10-02 01:35:27.308118: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe33fd7c00 next 796 of size 8192
2019-10-02 01:35:27.308131: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe33fd9c00 next 797 of size 8192
2019-10-02 01:35:27.308141: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe33fdbc00 next 798 of size 8192
2019-10-02 01:35:27.308150: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe33fddc00 next 799 of size 16777216
2019-10-02 01:35:27.308167: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe34fddc00 next 800 of size 256
2019-10-02 01:35:27.308177: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe34fddd00 next 801 of size 16777216
2019-10-02 01:35:27.308187: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe35fddd00 next 802 of size 67108864
2019-10-02 01:35:27.308204: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe39fddd00 next 803 of size 16777216
2019-10-02 01:35:27.308214: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3afddd00 next 804 of size 256
2019-10-02 01:35:27.308224: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3afdde00 next 805 of size 8192
2019-10-02 01:35:27.308240: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3afdfe00 next 806 of size 256
2019-10-02 01:35:27.308250: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3afdff00 next 807 of size 8192
2019-10-02 01:35:27.308267: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3afe1f00 next 808 of size 256
2019-10-02 01:35:27.308281: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3afe2000 next 809 of size 8192
2019-10-02 01:35:27.308298: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3afe4000 next 810 of size 256
2019-10-02 01:35:27.308309: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3afe4100 next 811 of size 8192
2019-10-02 01:35:27.308318: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3afe6100 next 812 of size 8192
2019-10-02 01:35:27.308336: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3afe8100 next 813 of size 256
2019-10-02 01:35:27.308346: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3afe8200 next 814 of size 8192
2019-10-02 01:35:27.308356: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3afea200 next 815 of size 8192
2019-10-02 01:35:27.308372: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3afec200 next 816 of size 256
2019-10-02 01:35:27.308383: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3afec300 next 817 of size 8192
2019-10-02 01:35:27.308393: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3afee300 next 818 of size 32768
2019-10-02 01:35:27.308409: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3aff6300 next 819 of size 256
2019-10-02 01:35:27.308420: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3aff6400 next 820 of size 32768
2019-10-02 01:35:27.308429: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3affe400 next 821 of size 32768
2019-10-02 01:35:27.308446: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3b006400 next 822 of size 256
2019-10-02 01:35:27.308456: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3b006500 next 823 of size 32768
2019-10-02 01:35:27.308465: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3b00e500 next 824 of size 8192
2019-10-02 01:35:27.308482: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3b010500 next 825 of size 256
2019-10-02 01:35:27.308493: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3b010600 next 826 of size 8192
2019-10-02 01:35:27.308502: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3b012600 next 827 of size 16777216
2019-10-02 01:35:27.308519: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3c012600 next 828 of size 256
2019-10-02 01:35:27.308530: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3c012700 next 829 of size 16777216
2019-10-02 01:35:27.308539: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3d012700 next 830 of size 16777216
2019-10-02 01:35:27.308556: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3e012700 next 831 of size 8192
2019-10-02 01:35:27.308567: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3e014700 next 832 of size 256
2019-10-02 01:35:27.308576: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3e014800 next 833 of size 8192
2019-10-02 01:35:27.308593: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3e016800 next 834 of size 8192
2019-10-02 01:35:27.308604: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3e018800 next 835 of size 256
2019-10-02 01:35:27.308614: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3e018900 next 836 of size 8192
2019-10-02 01:35:27.308630: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3e01a900 next 837 of size 16777216
2019-10-02 01:35:27.308640: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3f01a900 next 838 of size 67108864
2019-10-02 01:35:27.308650: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4301a900 next 839 of size 8192
2019-10-02 01:35:27.308672: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4301c900 next 840 of size 256
2019-10-02 01:35:27.308686: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4301ca00 next 841 of size 8192
2019-10-02 01:35:27.308695: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4301ea00 next 842 of size 256
2019-10-02 01:35:27.308705: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4301eb00 next 843 of size 8192
2019-10-02 01:35:27.308717: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe43020b00 next 844 of size 256
2019-10-02 01:35:27.308729: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe43020c00 next 845 of size 8192
2019-10-02 01:35:27.308738: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe43022c00 next 846 of size 8192
2019-10-02 01:35:27.308750: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe43024c00 next 847 of size 256
2019-10-02 01:35:27.308761: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe43024d00 next 848 of size 8192
2019-10-02 01:35:27.308770: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe43026d00 next 849 of size 16777216
2019-10-02 01:35:27.308780: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe44026d00 next 850 of size 16777216
2019-10-02 01:35:27.308790: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe45026d00 next 851 of size 256
2019-10-02 01:35:27.308803: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe45026e00 next 852 of size 256
2019-10-02 01:35:27.308813: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe45026f00 next 853 of size 16777216
2019-10-02 01:35:27.308823: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe46026f00 next 854 of size 16777216
2019-10-02 01:35:27.308833: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe47026f00 next 855 of size 67108864
2019-10-02 01:35:27.308843: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4b026f00 next 856 of size 256
2019-10-02 01:35:27.308855: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4b027000 next 857 of size 256
2019-10-02 01:35:27.308864: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4b027100 next 858 of size 512
2019-10-02 01:35:27.308873: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4b027300 next 859 of size 256
2019-10-02 01:35:27.308886: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4b027400 next 860 of size 512
2019-10-02 01:35:27.308896: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4b027600 next 861 of size 16777216
2019-10-02 01:35:27.308906: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4c027600 next 862 of size 16777216
2019-10-02 01:35:27.308918: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4d027600 next 863 of size 256
2019-10-02 01:35:27.308928: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4d027700 next 864 of size 8192
2019-10-02 01:35:27.308937: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4d029700 next 865 of size 256
2019-10-02 01:35:27.308949: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4d029800 next 866 of size 256
2019-10-02 01:35:27.308958: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4d029900 next 867 of size 8192
2019-10-02 01:35:27.308968: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4d02b900 next 868 of size 256
2019-10-02 01:35:27.308981: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4d02ba00 next 869 of size 8192
2019-10-02 01:35:27.308990: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4d02da00 next 870 of size 256
2019-10-02 01:35:27.309000: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4d02db00 next 871 of size 8192
2019-10-02 01:35:27.309030: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4d02fb00 next 872 of size 8192
2019-10-02 01:35:27.309041: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4d031b00 next 873 of size 256
2019-10-02 01:35:27.309050: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4d031c00 next 874 of size 8192
2019-10-02 01:35:27.309064: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4d033c00 next 875 of size 67108864
2019-10-02 01:35:27.309074: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe51033c00 next 876 of size 16777216
2019-10-02 01:35:27.309085: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe52033c00 next 877 of size 256
2019-10-02 01:35:27.309097: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe52033d00 next 878 of size 67108864
2019-10-02 01:35:27.309107: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe56033d00 next 879 of size 256
2019-10-02 01:35:27.309117: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe56033e00 next 880 of size 8192
2019-10-02 01:35:27.309129: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe56035e00 next 881 of size 256
2019-10-02 01:35:27.309139: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe56035f00 next 882 of size 16777216
2019-10-02 01:35:27.309148: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe57035f00 next 883 of size 8192
2019-10-02 01:35:27.309157: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe57037f00 next 884 of size 256
2019-10-02 01:35:27.309167: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe57038000 next 885 of size 67108864
2019-10-02 01:35:27.309180: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe5b038000 next 886 of size 16777216
2019-10-02 01:35:27.309189: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe5c038000 next 887 of size 16777216
2019-10-02 01:35:27.309204: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe5d038000 next 888 of size 16777216
2019-10-02 01:35:27.309222: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe5e038000 next 889 of size 16777216
2019-10-02 01:35:27.309232: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe5f038000 next 890 of size 67108864
2019-10-02 01:35:27.309240: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe63038000 next 891 of size 256
2019-10-02 01:35:27.309252: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe63038100 next 892 of size 16777216
2019-10-02 01:35:27.309261: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe64038100 next 893 of size 256
2019-10-02 01:35:27.309271: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe64038200 next 894 of size 256
2019-10-02 01:35:27.309282: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe64038300 next 895 of size 8192
2019-10-02 01:35:27.309291: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6403a300 next 896 of size 256
2019-10-02 01:35:27.309302: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6403a400 next 897 of size 8192
2019-10-02 01:35:27.309318: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6403c400 next 898 of size 8192
2019-10-02 01:35:27.309327: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6403e400 next 899 of size 256
2019-10-02 01:35:27.309336: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6403e500 next 900 of size 8192
2019-10-02 01:35:27.309345: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe64040500 next 901 of size 16777216
2019-10-02 01:35:27.309354: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe65040500 next 902 of size 67108864
2019-10-02 01:35:27.309369: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe69040500 next 903 of size 16777216
2019-10-02 01:35:27.309379: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6a040500 next 904 of size 256
2019-10-02 01:35:27.309385: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6a040600 next 905 of size 32768
2019-10-02 01:35:27.309390: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6a048600 next 906 of size 256
2019-10-02 01:35:27.309395: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6a048700 next 907 of size 32768
2019-10-02 01:35:27.309401: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6a050700 next 908 of size 16777216
2019-10-02 01:35:27.309408: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6b050700 next 909 of size 256
2019-10-02 01:35:27.309413: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6b050800 next 910 of size 8192
2019-10-02 01:35:27.309418: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6b052800 next 911 of size 256
2019-10-02 01:35:27.309424: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6b052900 next 912 of size 8192
2019-10-02 01:35:27.309429: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6b054900 next 913 of size 8192
2019-10-02 01:35:27.309434: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6b056900 next 914 of size 256
2019-10-02 01:35:27.309440: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6b056a00 next 915 of size 8192
2019-10-02 01:35:27.309445: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6b058a00 next 916 of size 67108864
2019-10-02 01:35:27.309450: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6f058a00 next 917 of size 16777216
2019-10-02 01:35:27.309456: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe70058a00 next 918 of size 16777216
2019-10-02 01:35:27.309461: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe71058a00 next 919 of size 256
2019-10-02 01:35:27.309466: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe71058b00 next 920 of size 8192
2019-10-02 01:35:27.309472: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7105ab00 next 921 of size 256
2019-10-02 01:35:27.309477: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7105ac00 next 922 of size 8192
2019-10-02 01:35:27.309482: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7105cc00 next 923 of size 8192
2019-10-02 01:35:27.309488: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7105ec00 next 924 of size 256
2019-10-02 01:35:27.309493: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7105ed00 next 925 of size 8192
2019-10-02 01:35:27.309498: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe71060d00 next 926 of size 16777216
2019-10-02 01:35:27.309504: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe72060d00 next 927 of size 256
2019-10-02 01:35:27.309509: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe72060e00 next 928 of size 67108864
2019-10-02 01:35:27.309514: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe76060e00 next 929 of size 67108864
2019-10-02 01:35:27.309520: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7a060e00 next 930 of size 67108864
2019-10-02 01:35:27.309525: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e060e00 next 931 of size 8192
2019-10-02 01:35:27.309530: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e062e00 next 932 of size 256
2019-10-02 01:35:27.309536: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e062f00 next 933 of size 8192
2019-10-02 01:35:27.309541: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e064f00 next 934 of size 256
2019-10-02 01:35:27.309549: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e065000 next 935 of size 256
2019-10-02 01:35:27.309555: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e065100 next 622 of size 8388608
2019-10-02 01:35:27.309560: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e865100 next 1016 of size 256
2019-10-02 01:35:27.309566: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e865200 next 564 of size 512
2019-10-02 01:35:27.309571: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e865400 next 604 of size 256
2019-10-02 01:35:27.309577: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e865500 next 624 of size 256
2019-10-02 01:35:27.309583: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e865600 next 613 of size 512
2019-10-02 01:35:27.309589: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e865800 next 1031 of size 256
2019-10-02 01:35:27.309594: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e865900 next 1033 of size 256
2019-10-02 01:35:27.309599: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e865a00 next 1034 of size 256
2019-10-02 01:35:27.309604: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e865b00 next 619 of size 256
2019-10-02 01:35:27.309610: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e865c00 next 611 of size 256
2019-10-02 01:35:27.309615: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e865d00 next 614 of size 256
2019-10-02 01:35:27.309620: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e865e00 next 680 of size 256
2019-10-02 01:35:27.309625: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e865f00 next 659 of size 256
2019-10-02 01:35:27.309631: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e866000 next 566 of size 256
2019-10-02 01:35:27.309636: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e866100 next 649 of size 256
2019-10-02 01:35:27.309642: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e866200 next 617 of size 768
2019-10-02 01:35:27.309647: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e866500 next 608 of size 256
2019-10-02 01:35:27.309653: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e866600 next 588 of size 256
2019-10-02 01:35:27.309658: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e866700 next 644 of size 256
2019-10-02 01:35:27.309663: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e866800 next 661 of size 256
2019-10-02 01:35:27.309668: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e866900 next 591 of size 256
2019-10-02 01:35:27.309674: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e866a00 next 944 of size 256
2019-10-02 01:35:27.309679: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e866b00 next 945 of size 256
2019-10-02 01:35:27.309684: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e866c00 next 946 of size 256
2019-10-02 01:35:27.309690: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e866d00 next 947 of size 256
2019-10-02 01:35:27.309695: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e866e00 next 948 of size 8192
2019-10-02 01:35:27.309700: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e868e00 next 949 of size 256
2019-10-02 01:35:27.309705: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e868f00 next 950 of size 8192
2019-10-02 01:35:27.309711: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e86af00 next 952 of size 256
2019-10-02 01:35:27.309716: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e86b000 next 953 of size 256
2019-10-02 01:35:27.309724: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e86b100 next 954 of size 256
2019-10-02 01:35:27.309730: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e86b200 next 956 of size 256
2019-10-02 01:35:27.309735: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e86b300 next 961 of size 256
2019-10-02 01:35:27.309741: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e86b400 next 962 of size 256
2019-10-02 01:35:27.309746: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e86b500 next 963 of size 256
2019-10-02 01:35:27.309752: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e86b600 next 964 of size 8192
2019-10-02 01:35:27.309758: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e86d600 next 965 of size 8192
2019-10-02 01:35:27.309764: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e86f600 next 966 of size 256
2019-10-02 01:35:27.309769: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e86f700 next 967 of size 8192
2019-10-02 01:35:27.309774: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e871700 next 968 of size 256
2019-10-02 01:35:27.309780: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e871800 next 969 of size 8192
2019-10-02 01:35:27.309785: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e873800 next 971 of size 256
2019-10-02 01:35:27.309790: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e873900 next 972 of size 256
2019-10-02 01:35:27.309796: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e873a00 next 973 of size 256
2019-10-02 01:35:27.309801: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e873b00 next 974 of size 8192
2019-10-02 01:35:27.309806: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e875b00 next 975 of size 8192
2019-10-02 01:35:27.309812: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e877b00 next 633 of size 61184
2019-10-02 01:35:27.309818: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e886a00 next 937 of size 67108864
2019-10-02 01:35:27.309823: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe82886a00 next 938 of size 256
2019-10-02 01:35:27.309828: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe82886b00 next 939 of size 256
2019-10-02 01:35:27.309834: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe82886c00 next 940 of size 256
2019-10-02 01:35:27.309839: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe82886d00 next 941 of size 256
2019-10-02 01:35:27.309844: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe82886e00 next 942 of size 256
2019-10-02 01:35:27.309850: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe82886f00 next 943 of size 16777216
2019-10-02 01:35:27.309855: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe83886f00 next 951 of size 16777216
2019-10-02 01:35:27.309861: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe84886f00 next 955 of size 67108864
2019-10-02 01:35:27.309866: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe88886f00 next 957 of size 67108864
2019-10-02 01:35:27.309871: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe8c886f00 next 958 of size 16777216
2019-10-02 01:35:27.309877: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe8d886f00 next 959 of size 67108864
2019-10-02 01:35:27.309882: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe91886f00 next 960 of size 16777216
2019-10-02 01:35:27.309887: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe92886f00 next 970 of size 16777216
2019-10-02 01:35:27.309895: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe93886f00 next 976 of size 256
2019-10-02 01:35:27.309901: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe93887000 next 977 of size 32768
2019-10-02 01:35:27.309907: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9388f000 next 978 of size 67108864
2019-10-02 01:35:27.309912: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9788f000 next 979 of size 16777216
2019-10-02 01:35:27.309917: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9888f000 next 980 of size 256
2019-10-02 01:35:27.309923: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9888f100 next 981 of size 16777216
2019-10-02 01:35:27.309928: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9988f100 next 982 of size 256
2019-10-02 01:35:27.309934: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9988f200 next 983 of size 256
2019-10-02 01:35:27.309940: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9988f300 next 984 of size 16777216
2019-10-02 01:35:27.309945: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9a88f300 next 985 of size 67108864
2019-10-02 01:35:27.309951: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9e88f300 next 986 of size 256
2019-10-02 01:35:27.309956: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9e88f400 next 987 of size 256
2019-10-02 01:35:27.309961: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9e88f500 next 988 of size 256
2019-10-02 01:35:27.309967: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9e88f600 next 989 of size 256
2019-10-02 01:35:27.309972: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9e88f700 next 990 of size 256
2019-10-02 01:35:27.309977: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9e88f800 next 991 of size 256
2019-10-02 01:35:27.309983: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9e88f900 next 992 of size 16777216
2019-10-02 01:35:27.309988: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9f88f900 next 993 of size 8192
2019-10-02 01:35:27.309994: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9f891900 next 994 of size 256
2019-10-02 01:35:27.309999: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9f891a00 next 995 of size 8192
2019-10-02 01:35:27.310008: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9f893a00 next 996 of size 8192
2019-10-02 01:35:27.310013: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9f895a00 next 997 of size 256
2019-10-02 01:35:27.310019: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9f895b00 next 998 of size 8192
2019-10-02 01:35:27.310024: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9f897b00 next 999 of size 67108864
2019-10-02 01:35:27.310029: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efea3897b00 next 1000 of size 67108864
2019-10-02 01:35:27.310035: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efea7897b00 next 1001 of size 256
2019-10-02 01:35:27.310040: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efea7897c00 next 1002 of size 256
2019-10-02 01:35:27.310045: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efea7897d00 next 1003 of size 256
2019-10-02 01:35:27.310051: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efea7897e00 next 1004 of size 256
2019-10-02 01:35:27.310056: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efea7897f00 next 1005 of size 67108864
2019-10-02 01:35:27.310061: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efeab897f00 next 1006 of size 256
2019-10-02 01:35:27.310066: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efeab898000 next 1007 of size 256
2019-10-02 01:35:27.310075: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efeab898100 next 601 of size 16777216
2019-10-02 01:35:27.310080: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efeac898100 next 594 of size 8388608
2019-10-02 01:35:27.310086: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efead098100 next 620 of size 268435456
2019-10-02 01:35:27.310091: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd098100 next 629 of size 131072
2019-10-02 01:35:27.310099: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b8100 next 572 of size 256
2019-10-02 01:35:27.310105: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b8200 next 587 of size 256
2019-10-02 01:35:27.310110: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b8300 next 696 of size 256
2019-10-02 01:35:27.310117: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b8400 next 623 of size 256
2019-10-02 01:35:27.310122: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b8500 next 1037 of size 512
2019-10-02 01:35:27.310127: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b8700 next 1041 of size 512
2019-10-02 01:35:27.310133: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b8900 next 1047 of size 256
2019-10-02 01:35:27.310138: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b8a00 next 1059 of size 256
2019-10-02 01:35:27.310143: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b8b00 next 1060 of size 256
2019-10-02 01:35:27.310149: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b8c00 next 1061 of size 256
2019-10-02 01:35:27.310154: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b8d00 next 1076 of size 512
2019-10-02 01:35:27.310159: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b8f00 next 1077 of size 512
2019-10-02 01:35:27.310165: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b9100 next 1079 of size 256
2019-10-02 01:35:27.310170: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b9200 next 1098 of size 256
2019-10-02 01:35:27.310175: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b9300 next 1102 of size 256
2019-10-02 01:35:27.310180: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b9400 next 1106 of size 256
2019-10-02 01:35:27.310186: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b9500 next 1113 of size 512
2019-10-02 01:35:27.310191: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b9700 next 1121 of size 512
2019-10-02 01:35:27.310196: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b9900 next 1122 of size 256
2019-10-02 01:35:27.310202: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b9a00 next 1123 of size 256
2019-10-02 01:35:27.310207: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b9b00 next 1125 of size 256
2019-10-02 01:35:27.310213: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b9c00 next 1126 of size 256
2019-10-02 01:35:27.310218: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b9d00 next 1127 of size 256
2019-10-02 01:35:27.310223: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b9e00 next 1021 of size 256
2019-10-02 01:35:27.310228: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b9f00 next 1022 of size 256
2019-10-02 01:35:27.310234: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0ba000 next 1023 of size 256
2019-10-02 01:35:27.310239: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0ba100 next 1024 of size 256
2019-10-02 01:35:27.310247: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0ba200 next 1025 of size 256
2019-10-02 01:35:27.310253: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0ba300 next 1026 of size 256
2019-10-02 01:35:27.310259: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0ba400 next 596 of size 131072000
2019-10-02 01:35:27.310264: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efec4dba400 next 571 of size 268435456
2019-10-02 01:35:27.310269: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efed4dba400 next 558 of size 131072
2019-10-02 01:35:27.310275: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efed4dda400 next 597 of size 4194304000
2019-10-02 01:35:27.310280: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effcedda400 next 595 of size 67108864
2019-10-02 01:35:27.310286: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effd2dda400 next 638 of size 268435456
2019-10-02 01:35:27.310293: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effe2dda400 next 626 of size 131072
2019-10-02 01:35:27.310298: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effe2dfa400 next 648 of size 16777216
2019-10-02 01:35:27.310304: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effe3dfa400 next 674 of size 16777216
2019-10-02 01:35:27.310309: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effe4dfa400 next 589 of size 8192
2019-10-02 01:35:27.310314: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effe4dfc400 next 615 of size 8192
2019-10-02 01:35:27.310320: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effe4dfe400 next 658 of size 8192
2019-10-02 01:35:27.310325: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effe4e00400 next 698 of size 8192
2019-10-02 01:35:27.310331: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effe4e02400 next 676 of size 67108864
2019-10-02 01:35:27.310336: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effe8e02400 next 646 of size 8192
2019-10-02 01:35:27.310342: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effe8e04400 next 692 of size 16777216
2019-10-02 01:35:27.310348: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effe9e04400 next 682 of size 16384000
2019-10-02 01:35:27.310353: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effeada4400 next 681 of size 32768
2019-10-02 01:35:27.310358: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effeadac400 next 675 of size 8192
2019-10-02 01:35:27.310364: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effeadae400 next 612 of size 16384000
2019-10-02 01:35:27.310369: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effebd4e400 next 568 of size 16384000
2019-10-02 01:35:27.310374: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effeccee400 next 573 of size 16384000
2019-10-02 01:35:27.310380: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effedc8e400 next 625 of size 16384000
2019-10-02 01:35:27.310385: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effeec2e400 next 565 of size 8192
2019-10-02 01:35:27.310390: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effeec30400 next 670 of size 16384000
2019-10-02 01:35:27.310396: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effefbd0400 next 560 of size 67108864
2019-10-02 01:35:27.310401: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efff3bd0400 next 640 of size 16384000
2019-10-02 01:35:27.310406: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efff4b70400 next 561 of size 16384000
2019-10-02 01:35:27.310412: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efff5b10400 next 651 of size 16384000
2019-10-02 01:35:27.310422: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efff6ab0400 next 656 of size 16384000
2019-10-02 01:35:27.310427: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efff7a50400 next 645 of size 16384000
2019-10-02 01:35:27.310433: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efff89f0400 next 641 of size 8192
2019-10-02 01:35:27.310438: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efff89f2400 next 654 of size 8192
2019-10-02 01:35:27.310444: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efff89f4400 next 556 of size 16384000
2019-10-02 01:35:27.310449: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efff9994400 next 574 of size 16384000
2019-10-02 01:35:27.310454: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffa934400 next 570 of size 8192
2019-10-02 01:35:27.310463: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffa936400 next 562 of size 8192
2019-10-02 01:35:27.310474: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffa938400 next 1008 of size 8192
2019-10-02 01:35:27.310485: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffa93a400 next 666 of size 8192
2019-10-02 01:35:27.310493: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffa93c400 next 567 of size 8192
2019-10-02 01:35:27.310503: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffa93e400 next 557 of size 16384000
2019-10-02 01:35:27.310512: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffb8de400 next 691 of size 16384000
2019-10-02 01:35:27.310519: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffc87e400 next 634 of size 8192
2019-10-02 01:35:27.310525: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffc880400 next 631 of size 8192
2019-10-02 01:35:27.310530: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffc882400 next 672 of size 8192
2019-10-02 01:35:27.310536: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffc884400 next 679 of size 8192
2019-10-02 01:35:27.310541: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffc886400 next 669 of size 16384000
2019-10-02 01:35:27.310547: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffd826400 next 628 of size 8192
2019-10-02 01:35:27.310552: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffd828400 next 610 of size 8192
2019-10-02 01:35:27.310557: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffd82a400 next 653 of size 8192
2019-10-02 01:35:27.310563: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffd82c400 next 650 of size 8192
2019-10-02 01:35:27.310568: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffd82e400 next 713 of size 8192
2019-10-02 01:35:27.310574: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffd830400 next 632 of size 8192
2019-10-02 01:35:27.310579: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffd832400 next 575 of size 16777216
2019-10-02 01:35:27.310585: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffe832400 next 664 of size 8192
2019-10-02 01:35:27.310590: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffe834400 next 618 of size 8192
2019-10-02 01:35:27.310595: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffe836400 next 621 of size 8192
2019-10-02 01:35:27.310601: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffe838400 next 555 of size 16777216
2019-10-02 01:35:27.310606: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effff838400 next 602 of size 8192
2019-10-02 01:35:27.310612: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effff83a400 next 606 of size 32768
2019-10-02 01:35:27.310617: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effff842400 next 600 of size 8192
2019-10-02 01:35:27.310626: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effff844400 next 609 of size 8192
2019-10-02 01:35:27.310632: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effff846400 next 593 of size 8192
2019-10-02 01:35:27.310637: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effff848400 next 592 of size 8192
2019-10-02 01:35:27.310643: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effff84a400 next 652 of size 8192
2019-10-02 01:35:27.310648: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effff84c400 next 1012 of size 16777216
2019-10-02 01:35:27.310653: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f000084c400 next 647 of size 8192
2019-10-02 01:35:27.310659: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f000084e400 next 660 of size 8192
2019-10-02 01:35:27.310664: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0000850400 next 603 of size 8192
2019-10-02 01:35:27.310670: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0000852400 next 673 of size 8192
2019-10-02 01:35:27.310676: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0000854400 next 1020 of size 8192
2019-10-02 01:35:27.310682: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0000856400 next 1015 of size 8192
2019-10-02 01:35:27.310687: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0000858400 next 643 of size 16777216
2019-10-02 01:35:27.310693: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0001858400 next 1009 of size 67108864
2019-10-02 01:35:27.310698: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0005858400 next 1010 of size 16777216
2019-10-02 01:35:27.310703: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0006858400 next 616 of size 16777216
2019-10-02 01:35:27.310709: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0007858400 next 663 of size 16777216
2019-10-02 01:35:27.310714: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0008858400 next 1013 of size 8192
2019-10-02 01:35:27.310720: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f000885a400 next 1014 of size 8192
2019-10-02 01:35:27.310725: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f000885c400 next 671 of size 16777216
2019-10-02 01:35:27.310730: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f000985c400 next 607 of size 67108864
2019-10-02 01:35:27.310735: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f000d85c400 next 635 of size 16777216
2019-10-02 01:35:27.310741: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f000e85c400 next 662 of size 16777216
2019-10-02 01:35:27.310746: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f000f85c400 next 636 of size 8192
2019-10-02 01:35:27.310752: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f000f85e400 next 668 of size 8192
2019-10-02 01:35:27.310757: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f000f860400 next 642 of size 8192
2019-10-02 01:35:27.310762: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f000f862400 next 667 of size 32768
2019-10-02 01:35:27.310768: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f000f86a400 next 569 of size 8192
2019-10-02 01:35:27.310773: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f000f86c400 next 639 of size 67108864
2019-10-02 01:35:27.310778: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f001386c400 next 1017 of size 8192
2019-10-02 01:35:27.310784: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f001386e400 next 1018 of size 8192
2019-10-02 01:35:27.310789: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0013870400 next 627 of size 8192
2019-10-02 01:35:27.310804: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0013872400 next 598 of size 8192
2019-10-02 01:35:27.310815: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0013874400 next 655 of size 8192
2019-10-02 01:35:27.310824: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0013876400 next 678 of size 8192
2019-10-02 01:35:27.310833: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0013878400 next 599 of size 8192
2019-10-02 01:35:27.310841: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f001387a400 next 689 of size 8192
2019-10-02 01:35:27.310849: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f001387c400 next 693 of size 8192
2019-10-02 01:35:27.310858: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f001387e400 next 563 of size 32768
2019-10-02 01:35:27.310866: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0013886400 next 1019 of size 16777216
2019-10-02 01:35:27.310875: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0014886400 next 719 of size 16777216
2019-10-02 01:35:27.310883: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0015886400 next 695 of size 8192
2019-10-02 01:35:27.310894: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0015888400 next 694 of size 16777216
2019-10-02 01:35:27.310902: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0016888400 next 1027 of size 16777216
2019-10-02 01:35:27.310910: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0017888400 next 1028 of size 16777216
2019-10-02 01:35:27.310919: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0018888400 next 1029 of size 16777216
2019-10-02 01:35:27.310927: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0019888400 next 1030 of size 67108864
2019-10-02 01:35:27.310936: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f001d888400 next 1032 of size 8192
2019-10-02 01:35:27.310945: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f001d88a400 next 1035 of size 67108864
2019-10-02 01:35:27.310953: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f002188a400 next 1036 of size 16777216
2019-10-02 01:35:27.310962: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f002288a400 next 1038 of size 8192
2019-10-02 01:35:27.310970: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f002288c400 next 1039 of size 67108864
2019-10-02 01:35:27.310979: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f002688c400 next 1040 of size 8192
2019-10-02 01:35:27.310988: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f002688e400 next 1042 of size 8192
2019-10-02 01:35:27.310997: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0026890400 next 1043 of size 16777216
2019-10-02 01:35:27.311005: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0027890400 next 1044 of size 8192
2019-10-02 01:35:27.311014: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0027892400 next 1045 of size 16777216
2019-10-02 01:35:27.311023: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0028892400 next 1046 of size 8192
2019-10-02 01:35:27.311031: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0028894400 next 1048 of size 16777216
2019-10-02 01:35:27.311040: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0029894400 next 1049 of size 8192
2019-10-02 01:35:27.311048: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0029896400 next 1050 of size 16777216
2019-10-02 01:35:27.311057: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f002a896400 next 1051 of size 8192
2019-10-02 01:35:27.311066: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f002a898400 next 1052 of size 8192
2019-10-02 01:35:27.311079: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f002a89a400 next 1053 of size 8192
2019-10-02 01:35:27.311088: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f002a89c400 next 1054 of size 67108864
2019-10-02 01:35:27.311096: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f002e89c400 next 1055 of size 16777216
2019-10-02 01:35:27.311105: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f002f89c400 next 1056 of size 8192
2019-10-02 01:35:27.311114: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f002f89e400 next 1057 of size 32768
2019-10-02 01:35:27.311123: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f002f8a6400 next 1058 of size 8192
2019-10-02 01:35:27.311133: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f002f8a8400 next 1062 of size 16777216
2019-10-02 01:35:27.311142: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00308a8400 next 1063 of size 8192
2019-10-02 01:35:27.311151: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00308aa400 next 1064 of size 32768
2019-10-02 01:35:27.311159: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00308b2400 next 1065 of size 8192
2019-10-02 01:35:27.311168: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00308b4400 next 1066 of size 67108864
2019-10-02 01:35:27.311179: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00348b4400 next 1067 of size 8192
2019-10-02 01:35:27.311187: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00348b6400 next 1068 of size 8192
2019-10-02 01:35:27.311196: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00348b8400 next 1069 of size 8192
2019-10-02 01:35:27.311205: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00348ba400 next 1070 of size 8192
2019-10-02 01:35:27.311213: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00348bc400 next 1071 of size 32768
2019-10-02 01:35:27.311222: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00348c4400 next 1072 of size 8192
2019-10-02 01:35:27.311231: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00348c6400 next 1073 of size 8192
2019-10-02 01:35:27.311240: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00348c8400 next 1074 of size 67108864
2019-10-02 01:35:27.311248: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00388c8400 next 1075 of size 8192
2019-10-02 01:35:27.311257: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00388ca400 next 1078 of size 8192
2019-10-02 01:35:27.311266: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00388cc400 next 1080 of size 16777216
2019-10-02 01:35:27.311274: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00398cc400 next 1081 of size 8192
2019-10-02 01:35:27.311283: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00398ce400 next 1082 of size 16777216
2019-10-02 01:35:27.311292: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f003a8ce400 next 1083 of size 8192
2019-10-02 01:35:27.311319: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f003a8d0400 next 1084 of size 8192
2019-10-02 01:35:27.311329: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f003a8d2400 next 1085 of size 8192
2019-10-02 01:35:27.311338: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f003a8d4400 next 1086 of size 8192
2019-10-02 01:35:27.311346: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f003a8d6400 next 1087 of size 16777216
2019-10-02 01:35:27.311355: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f003b8d6400 next 1088 of size 67108864
2019-10-02 01:35:27.311364: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f003f8d6400 next 1089 of size 67108864
2019-10-02 01:35:27.311373: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00438d6400 next 1090 of size 8192
2019-10-02 01:35:27.311386: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00438d8400 next 1091 of size 8192
2019-10-02 01:35:27.311395: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00438da400 next 1092 of size 8192
2019-10-02 01:35:27.311404: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00438dc400 next 1093 of size 16777216
2019-10-02 01:35:27.311413: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00448dc400 next 1094 of size 8192
2019-10-02 01:35:27.311421: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00448de400 next 1095 of size 16777216
2019-10-02 01:35:27.311430: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00458de400 next 1096 of size 8192
2019-10-02 01:35:27.311440: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00458e0400 next 1097 of size 8192
2019-10-02 01:35:27.311449: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00458e2400 next 1099 of size 67108864
2019-10-02 01:35:27.311459: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00498e2400 next 1100 of size 16777216
2019-10-02 01:35:27.311468: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f004a8e2400 next 1101 of size 8192
2019-10-02 01:35:27.311476: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f004a8e4400 next 1103 of size 16777216
2019-10-02 01:35:27.311487: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f004b8e4400 next 1104 of size 32768
2019-10-02 01:35:27.311495: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f004b8ec400 next 1105 of size 8192
2019-10-02 01:35:27.311504: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f004b8ee400 next 1107 of size 16777216
2019-10-02 01:35:27.311513: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f004c8ee400 next 1108 of size 32768
2019-10-02 01:35:27.311522: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f004c8f6400 next 1109 of size 16777216
2019-10-02 01:35:27.311530: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f004d8f6400 next 1110 of size 8192
2019-10-02 01:35:27.311539: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f004d8f8400 next 1111 of size 8192
2019-10-02 01:35:27.311547: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f004d8fa400 next 1112 of size 8192
2019-10-02 01:35:27.311556: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f004d8fc400 next 1114 of size 8192
2019-10-02 01:35:27.311565: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f004d8fe400 next 1115 of size 67108864
2019-10-02 01:35:27.311574: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00518fe400 next 1116 of size 8192
2019-10-02 01:35:27.311582: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0051900400 next 1117 of size 8192
2019-10-02 01:35:27.311591: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0051902400 next 1118 of size 16777216
2019-10-02 01:35:27.311600: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0052902400 next 1119 of size 8192
2019-10-02 01:35:27.311608: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0052904400 next 1120 of size 8192
2019-10-02 01:35:27.311617: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0052906400 next 1124 of size 67108864
2019-10-02 01:35:27.311626: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0056906400 next 18446744073709551615 of size 74560768
2019-10-02 01:35:27.311634: I tensorflow/core/common_runtime/bfc_allocator.cc:914]      Summary of in-use Chunks by size: 
2019-10-02 01:35:27.311651: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 242 Chunks of size 256 totalling 60.5KiB
2019-10-02 01:35:27.311661: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 45 Chunks of size 512 totalling 22.5KiB
2019-10-02 01:35:27.311675: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 768 totalling 768B
2019-10-02 01:35:27.311685: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 1280 totalling 1.2KiB
2019-10-02 01:35:27.311694: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 4096 totalling 4.0KiB
2019-10-02 01:35:27.311703: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 5376 totalling 5.2KiB
2019-10-02 01:35:27.311712: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 6912 totalling 6.8KiB
2019-10-02 01:35:27.311722: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 448 Chunks of size 8192 totalling 3.50MiB
2019-10-02 01:35:27.311733: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 9984 totalling 9.8KiB
2019-10-02 01:35:27.311744: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 11264 totalling 11.0KiB
2019-10-02 01:35:27.311754: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 13056 totalling 12.8KiB
2019-10-02 01:35:27.311763: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 45 Chunks of size 32768 totalling 1.41MiB
2019-10-02 01:35:27.311771: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 56064 totalling 54.8KiB
2019-10-02 01:35:27.311780: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 61184 totalling 59.8KiB
2019-10-02 01:35:27.311791: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 7 Chunks of size 131072 totalling 896.0KiB
2019-10-02 01:35:27.311799: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 3 Chunks of size 8388608 totalling 24.00MiB
2019-10-02 01:35:27.311810: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 32 Chunks of size 16384000 totalling 500.00MiB
2019-10-02 01:35:27.311820: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 192 Chunks of size 16777216 totalling 3.00GiB
2019-10-02 01:35:27.311829: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 97 Chunks of size 67108864 totalling 6.06GiB
2019-10-02 01:35:27.311838: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 74560768 totalling 71.11MiB
2019-10-02 01:35:27.311848: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 131072000 totalling 125.00MiB
2019-10-02 01:35:27.311857: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 262144000 totalling 250.00MiB
2019-10-02 01:35:27.311867: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 3 Chunks of size 268435456 totalling 768.00MiB
2019-10-02 01:35:27.311876: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 4194304000 totalling 3.91GiB
2019-10-02 01:35:27.311885: I tensorflow/core/common_runtime/bfc_allocator.cc:921] Sum Total of in-use chunks: 14.67GiB
2019-10-02 01:35:27.311895: I tensorflow/core/common_runtime/bfc_allocator.cc:923] total_region_allocated_bytes_: 15753943296 memory_limit_: 15753943450 available bytes: 154 curr_region_allocation_bytes_: 31507887104
2019-10-02 01:35:27.311908: I tensorflow/core/common_runtime/bfc_allocator.cc:929] Stats: 
Limit:                 15753943450
InUse:                 15753943296
MaxInUse:              15753943296
NumAllocs:                    2276
MaxAllocSize:           4194304000

2019-10-02 01:35:27.311981: W tensorflow/core/common_runtime/bfc_allocator.cc:424] ****************************************************************************************************
2019-10-02 01:35:27.312035: W tensorflow/core/common_runtime/bfc_allocator.cc:419] Allocator (GPU_0_bfc) ran out of memory trying to allocate 64.00MiB (rounded to 67108864).  Current allocation summary follows.
2019-10-02 01:35:27.312100: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (256): 	Total Chunks: 242, Chunks in use: 242. 60.5KiB allocated for chunks. 60.5KiB in use in bin. 4.2KiB client-requested in use in bin.
2019-10-02 01:35:27.312132: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (512): 	Total Chunks: 46, Chunks in use: 46. 23.2KiB allocated for chunks. 23.2KiB in use in bin. 23.0KiB client-requested in use in bin.
2019-10-02 01:35:27.312147: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (1024): 	Total Chunks: 1, Chunks in use: 1. 1.2KiB allocated for chunks. 1.2KiB in use in bin. 1.0KiB client-requested in use in bin.
2019-10-02 01:35:27.312155: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (2048): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-10-02 01:35:27.312165: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (4096): 	Total Chunks: 3, Chunks in use: 3. 16.0KiB allocated for chunks. 16.0KiB in use in bin. 12.0KiB client-requested in use in bin.
2019-10-02 01:35:27.312172: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (8192): 	Total Chunks: 451, Chunks in use: 451. 3.53MiB allocated for chunks. 3.53MiB in use in bin. 3.52MiB client-requested in use in bin.
2019-10-02 01:35:27.312179: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (16384): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-10-02 01:35:27.312186: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (32768): 	Total Chunks: 47, Chunks in use: 47. 1.52MiB allocated for chunks. 1.52MiB in use in bin. 1.47MiB client-requested in use in bin.
2019-10-02 01:35:27.312192: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (65536): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-10-02 01:35:27.312200: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (131072): 	Total Chunks: 7, Chunks in use: 7. 896.0KiB allocated for chunks. 896.0KiB in use in bin. 896.0KiB client-requested in use in bin.
2019-10-02 01:35:27.312206: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (262144): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-10-02 01:35:27.312212: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (524288): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-10-02 01:35:27.312219: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (1048576): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-10-02 01:35:27.312225: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (2097152): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-10-02 01:35:27.312231: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (4194304): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-10-02 01:35:27.312238: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (8388608): 	Total Chunks: 35, Chunks in use: 35. 524.00MiB allocated for chunks. 524.00MiB in use in bin. 524.00MiB client-requested in use in bin.
2019-10-02 01:35:27.312250: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (16777216): 	Total Chunks: 192, Chunks in use: 192. 3.00GiB allocated for chunks. 3.00GiB in use in bin. 3.00GiB client-requested in use in bin.
2019-10-02 01:35:27.312261: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (33554432): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-10-02 01:35:27.312273: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (67108864): 	Total Chunks: 99, Chunks in use: 99. 6.25GiB allocated for chunks. 6.25GiB in use in bin. 6.25GiB client-requested in use in bin.
2019-10-02 01:35:27.312282: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (134217728): 	Total Chunks: 1, Chunks in use: 1. 250.00MiB allocated for chunks. 250.00MiB in use in bin. 250.00MiB client-requested in use in bin.
2019-10-02 01:35:27.312293: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (268435456): 	Total Chunks: 4, Chunks in use: 4. 4.66GiB allocated for chunks. 4.66GiB in use in bin. 4.66GiB client-requested in use in bin.
2019-10-02 01:35:27.312300: I tensorflow/core/common_runtime/bfc_allocator.cc:885] Bin for 64.00MiB was 64.00MiB, Chunk State: 
2019-10-02 01:35:27.312307: I tensorflow/core/common_runtime/bfc_allocator.cc:898] Next region of size 15753943296
2019-10-02 01:35:27.312315: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0000000 next 1 of size 1280
2019-10-02 01:35:27.312321: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0000500 next 2 of size 256
2019-10-02 01:35:27.312326: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0000600 next 3 of size 256
2019-10-02 01:35:27.312332: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0000700 next 4 of size 256
2019-10-02 01:35:27.312337: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0000800 next 5 of size 256
2019-10-02 01:35:27.312343: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0000900 next 6 of size 256
2019-10-02 01:35:27.312348: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0000a00 next 7 of size 256
2019-10-02 01:35:27.312354: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0000b00 next 8 of size 256
2019-10-02 01:35:27.312359: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0000c00 next 9 of size 256
2019-10-02 01:35:27.312365: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0000d00 next 10 of size 256
2019-10-02 01:35:27.312370: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0000e00 next 11 of size 256
2019-10-02 01:35:27.312376: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0000f00 next 12 of size 256
2019-10-02 01:35:27.312381: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0001000 next 13 of size 256
2019-10-02 01:35:27.312389: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0001100 next 14 of size 256
2019-10-02 01:35:27.312399: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0001200 next 15 of size 256
2019-10-02 01:35:27.312409: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0001300 next 16 of size 256
2019-10-02 01:35:27.312418: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0001400 next 17 of size 256
2019-10-02 01:35:27.312426: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0001500 next 18 of size 256
2019-10-02 01:35:27.312431: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0001600 next 19 of size 256
2019-10-02 01:35:27.312437: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0001700 next 20 of size 256
2019-10-02 01:35:27.312442: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0001800 next 21 of size 256
2019-10-02 01:35:27.312448: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0001900 next 22 of size 256
2019-10-02 01:35:27.312453: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0001a00 next 23 of size 256
2019-10-02 01:35:27.312459: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0001b00 next 24 of size 256
2019-10-02 01:35:27.312464: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0001c00 next 25 of size 256
2019-10-02 01:35:27.312469: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0001d00 next 26 of size 256
2019-10-02 01:35:27.312475: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0001e00 next 27 of size 256
2019-10-02 01:35:27.312480: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0001f00 next 28 of size 256
2019-10-02 01:35:27.312490: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0002000 next 29 of size 256
2019-10-02 01:35:27.312496: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0002100 next 30 of size 256
2019-10-02 01:35:27.312501: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0002200 next 31 of size 256
2019-10-02 01:35:27.312507: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0002300 next 32 of size 256
2019-10-02 01:35:27.312513: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0002400 next 33 of size 256
2019-10-02 01:35:27.312518: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0002500 next 34 of size 256
2019-10-02 01:35:27.312523: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0002600 next 35 of size 256
2019-10-02 01:35:27.312529: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0002700 next 36 of size 256
2019-10-02 01:35:27.312534: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0002800 next 37 of size 256
2019-10-02 01:35:27.312539: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0002900 next 38 of size 256
2019-10-02 01:35:27.312545: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0002a00 next 39 of size 256
2019-10-02 01:35:27.312550: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0002b00 next 40 of size 256
2019-10-02 01:35:27.312555: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0002c00 next 41 of size 256
2019-10-02 01:35:27.312561: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0002d00 next 42 of size 256
2019-10-02 01:35:27.312566: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0002e00 next 43 of size 256
2019-10-02 01:35:27.312572: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0002f00 next 44 of size 8192
2019-10-02 01:35:27.312582: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0004f00 next 45 of size 67108864
2019-10-02 01:35:27.312595: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb4004f00 next 46 of size 67108864
2019-10-02 01:35:27.312603: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb8004f00 next 47 of size 8192
2019-10-02 01:35:27.312610: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb8006f00 next 48 of size 32768
2019-10-02 01:35:27.312616: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb800ef00 next 49 of size 8192
2019-10-02 01:35:27.312622: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb8010f00 next 50 of size 16777216
2019-10-02 01:35:27.312627: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb9010f00 next 51 of size 8192
2019-10-02 01:35:27.312633: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb9012f00 next 52 of size 8192
2019-10-02 01:35:27.312638: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb9014f00 next 53 of size 32768
2019-10-02 01:35:27.312644: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb901cf00 next 54 of size 8192
2019-10-02 01:35:27.312649: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb901ef00 next 55 of size 8192
2019-10-02 01:35:27.312654: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb9020f00 next 56 of size 32768
2019-10-02 01:35:27.312660: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb9028f00 next 57 of size 8192
2019-10-02 01:35:27.312665: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb902af00 next 58 of size 8192
2019-10-02 01:35:27.312671: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb902cf00 next 59 of size 512
2019-10-02 01:35:27.312676: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb902d100 next 60 of size 8192
2019-10-02 01:35:27.312682: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb902f100 next 61 of size 16777216
2019-10-02 01:35:27.312690: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcba02f100 next 62 of size 8192
2019-10-02 01:35:27.312696: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcba031100 next 63 of size 16777216
2019-10-02 01:35:27.312702: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcbb031100 next 64 of size 8192
2019-10-02 01:35:27.312707: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcbb033100 next 65 of size 8192
2019-10-02 01:35:27.312713: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcbb035100 next 66 of size 16777216
2019-10-02 01:35:27.312718: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcbc035100 next 67 of size 32768
2019-10-02 01:35:27.312724: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcbc03d100 next 68 of size 16777216
2019-10-02 01:35:27.312729: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcbd03d100 next 69 of size 8192
2019-10-02 01:35:27.312734: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcbd03f100 next 70 of size 8192
2019-10-02 01:35:27.312740: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcbd041100 next 71 of size 8192
2019-10-02 01:35:27.312745: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcbd043100 next 72 of size 67108864
2019-10-02 01:35:27.312751: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcc1043100 next 73 of size 8192
2019-10-02 01:35:27.312756: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcc1045100 next 74 of size 67108864
2019-10-02 01:35:27.312762: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcc5045100 next 75 of size 8192
2019-10-02 01:35:27.312767: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcc5047100 next 76 of size 8192
2019-10-02 01:35:27.312773: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcc5049100 next 77 of size 8192
2019-10-02 01:35:27.312779: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcc504b100 next 78 of size 8192
2019-10-02 01:35:27.312785: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcc504d100 next 79 of size 67108864
2019-10-02 01:35:27.312791: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcc904d100 next 80 of size 16777216
2019-10-02 01:35:27.312796: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcca04d100 next 81 of size 8192
2019-10-02 01:35:27.312802: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcca04f100 next 82 of size 16777216
2019-10-02 01:35:27.312807: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efccb04f100 next 83 of size 8192
2019-10-02 01:35:27.312812: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efccb051100 next 84 of size 16777216
2019-10-02 01:35:27.312818: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efccc051100 next 85 of size 8192
2019-10-02 01:35:27.312823: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efccc053100 next 86 of size 67108864
2019-10-02 01:35:27.312829: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd0053100 next 87 of size 8192
2019-10-02 01:35:27.312834: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd0055100 next 88 of size 32768
2019-10-02 01:35:27.312839: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd005d100 next 89 of size 8192
2019-10-02 01:35:27.312845: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd005f100 next 90 of size 8192
2019-10-02 01:35:27.312850: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd0061100 next 91 of size 8192
2019-10-02 01:35:27.312855: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd0063100 next 92 of size 8192
2019-10-02 01:35:27.312864: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd0065100 next 93 of size 8192
2019-10-02 01:35:27.312870: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd0067100 next 94 of size 16777216
2019-10-02 01:35:27.312876: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd1067100 next 95 of size 8192
2019-10-02 01:35:27.312881: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd1069100 next 96 of size 16384000
2019-10-02 01:35:27.312887: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd2009100 next 97 of size 16384000
2019-10-02 01:35:27.312892: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd2fa9100 next 98 of size 8192
2019-10-02 01:35:27.312898: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd2fab100 next 99 of size 512
2019-10-02 01:35:27.312903: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd2fab300 next 100 of size 16777216
2019-10-02 01:35:27.312909: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd3fab300 next 101 of size 16777216
2019-10-02 01:35:27.312914: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd4fab300 next 102 of size 67108864
2019-10-02 01:35:27.312920: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd8fab300 next 103 of size 16777216
2019-10-02 01:35:27.312925: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd9fab300 next 104 of size 16777216
2019-10-02 01:35:27.312931: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcdafab300 next 105 of size 8192
2019-10-02 01:35:27.312936: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcdafad300 next 106 of size 512
2019-10-02 01:35:27.312942: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcdafad500 next 107 of size 16777216
2019-10-02 01:35:27.312947: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcdbfad500 next 108 of size 512
2019-10-02 01:35:27.312953: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcdbfad700 next 109 of size 16777216
2019-10-02 01:35:27.312959: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcdcfad700 next 110 of size 8192
2019-10-02 01:35:27.312965: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcdcfaf700 next 111 of size 8192
2019-10-02 01:35:27.312970: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcdcfb1700 next 112 of size 8192
2019-10-02 01:35:27.312976: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcdcfb3700 next 113 of size 8192
2019-10-02 01:35:27.312981: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcdcfb5700 next 114 of size 8192
2019-10-02 01:35:27.312987: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcdcfb7700 next 115 of size 16777216
2019-10-02 01:35:27.312992: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcddfb7700 next 116 of size 67108864
2019-10-02 01:35:27.312998: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce1fb7700 next 117 of size 512
2019-10-02 01:35:27.313003: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce1fb7900 next 118 of size 8192
2019-10-02 01:35:27.313008: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce1fb9900 next 119 of size 8192
2019-10-02 01:35:27.313014: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce1fbb900 next 120 of size 8192
2019-10-02 01:35:27.313019: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce1fbd900 next 121 of size 16777216
2019-10-02 01:35:27.313025: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce2fbd900 next 122 of size 8192
2019-10-02 01:35:27.313030: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce2fbf900 next 123 of size 8192
2019-10-02 01:35:27.313035: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce2fc1900 next 124 of size 16777216
2019-10-02 01:35:27.313044: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce3fc1900 next 125 of size 512
2019-10-02 01:35:27.313050: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce3fc1b00 next 126 of size 67108864
2019-10-02 01:35:27.313056: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce7fc1b00 next 127 of size 8192
2019-10-02 01:35:27.313061: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce7fc3b00 next 128 of size 8192
2019-10-02 01:35:27.313067: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce7fc5b00 next 129 of size 8192
2019-10-02 01:35:27.313072: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce7fc7b00 next 130 of size 8192
2019-10-02 01:35:27.313077: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce7fc9b00 next 131 of size 16777216
2019-10-02 01:35:27.313083: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce8fc9b00 next 132 of size 8192
2019-10-02 01:35:27.313088: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce8fcbb00 next 133 of size 16777216
2019-10-02 01:35:27.313093: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce9fcbb00 next 134 of size 8192
2019-10-02 01:35:27.313099: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce9fcdb00 next 135 of size 16777216
2019-10-02 01:35:27.313104: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efceafcdb00 next 136 of size 8192
2019-10-02 01:35:27.313109: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efceafcfb00 next 137 of size 8192
2019-10-02 01:35:27.313115: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efceafd1b00 next 138 of size 8192
2019-10-02 01:35:27.313120: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efceafd3b00 next 139 of size 8192
2019-10-02 01:35:27.313126: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efceafd5b00 next 140 of size 8192
2019-10-02 01:35:27.313131: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efceafd7b00 next 141 of size 8192
2019-10-02 01:35:27.313138: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efceafd9b00 next 142 of size 16777216
2019-10-02 01:35:27.313144: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcebfd9b00 next 143 of size 16777216
2019-10-02 01:35:27.313149: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcecfd9b00 next 144 of size 8192
2019-10-02 01:35:27.313155: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcecfdbb00 next 145 of size 8192
2019-10-02 01:35:27.313160: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcecfddb00 next 146 of size 8192
2019-10-02 01:35:27.313166: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcecfdfb00 next 147 of size 32768
2019-10-02 01:35:27.313171: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcecfe7b00 next 148 of size 8192
2019-10-02 01:35:27.313176: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcecfe9b00 next 149 of size 512
2019-10-02 01:35:27.313182: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcecfe9d00 next 150 of size 8192
2019-10-02 01:35:27.313187: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcecfebd00 next 151 of size 8192
2019-10-02 01:35:27.313193: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcecfedd00 next 152 of size 16777216
2019-10-02 01:35:27.313198: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcedfedd00 next 153 of size 8192
2019-10-02 01:35:27.313203: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcedfefd00 next 154 of size 16777216
2019-10-02 01:35:27.313209: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efceefefd00 next 155 of size 8192
2019-10-02 01:35:27.313217: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efceeff1d00 next 156 of size 8192
2019-10-02 01:35:27.313223: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efceeff3d00 next 157 of size 8192
2019-10-02 01:35:27.313228: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efceeff5d00 next 158 of size 67108864
2019-10-02 01:35:27.313234: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcf2ff5d00 next 159 of size 8192
2019-10-02 01:35:27.313239: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcf2ff7d00 next 160 of size 32768
2019-10-02 01:35:27.313245: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcf2fffd00 next 161 of size 16777216
2019-10-02 01:35:27.313251: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcf3fffd00 next 162 of size 16777216
2019-10-02 01:35:27.313256: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcf4fffd00 next 163 of size 67108864
2019-10-02 01:35:27.313262: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcf8fffd00 next 164 of size 8192
2019-10-02 01:35:27.313267: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcf9001d00 next 165 of size 8192
2019-10-02 01:35:27.313273: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcf9003d00 next 166 of size 16384000
2019-10-02 01:35:27.313278: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcf9fa3d00 next 167 of size 8192
2019-10-02 01:35:27.313284: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcf9fa5d00 next 168 of size 67108864
2019-10-02 01:35:27.313289: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcfdfa5d00 next 169 of size 8192
2019-10-02 01:35:27.313295: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcfdfa7d00 next 170 of size 67108864
2019-10-02 01:35:27.313300: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd01fa7d00 next 171 of size 8192
2019-10-02 01:35:27.313305: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd01fa9d00 next 172 of size 67108864
2019-10-02 01:35:27.313311: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd05fa9d00 next 173 of size 16777216
2019-10-02 01:35:27.313318: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd06fa9d00 next 174 of size 512
2019-10-02 01:35:27.313323: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd06fa9f00 next 175 of size 8192
2019-10-02 01:35:27.313329: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd06fabf00 next 176 of size 8192
2019-10-02 01:35:27.313334: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd06fadf00 next 177 of size 67108864
2019-10-02 01:35:27.313340: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd0afadf00 next 178 of size 67108864
2019-10-02 01:35:27.313345: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd0efadf00 next 179 of size 8192
2019-10-02 01:35:27.313351: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd0efaff00 next 180 of size 16777216
2019-10-02 01:35:27.313357: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd0ffaff00 next 181 of size 8192
2019-10-02 01:35:27.313362: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd0ffb1f00 next 182 of size 8192
2019-10-02 01:35:27.313368: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd0ffb3f00 next 183 of size 16384000
2019-10-02 01:35:27.313373: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd10f53f00 next 184 of size 16384000
2019-10-02 01:35:27.313379: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd11ef3f00 next 185 of size 8192
2019-10-02 01:35:27.313384: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd11ef5f00 next 186 of size 8192
2019-10-02 01:35:27.313392: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd11ef7f00 next 187 of size 16777216
2019-10-02 01:35:27.313398: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd12ef7f00 next 188 of size 32768
2019-10-02 01:35:27.313404: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd12efff00 next 189 of size 16777216
2019-10-02 01:35:27.313409: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd13efff00 next 190 of size 8192
2019-10-02 01:35:27.313414: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd13f01f00 next 191 of size 8192
2019-10-02 01:35:27.313420: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd13f03f00 next 192 of size 8192
2019-10-02 01:35:27.313425: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd13f05f00 next 193 of size 16777216
2019-10-02 01:35:27.313431: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd14f05f00 next 194 of size 8192
2019-10-02 01:35:27.313436: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd14f07f00 next 195 of size 8192
2019-10-02 01:35:27.313442: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd14f09f00 next 196 of size 67108864
2019-10-02 01:35:27.313447: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd18f09f00 next 197 of size 16777216
2019-10-02 01:35:27.313452: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd19f09f00 next 198 of size 8192
2019-10-02 01:35:27.313458: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd19f0bf00 next 199 of size 8192
2019-10-02 01:35:27.313463: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd19f0df00 next 200 of size 16777216
2019-10-02 01:35:27.313468: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd1af0df00 next 201 of size 8192
2019-10-02 01:35:27.313474: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd1af0ff00 next 202 of size 8192
2019-10-02 01:35:27.313479: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd1af11f00 next 203 of size 8192
2019-10-02 01:35:27.313484: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd1af13f00 next 204 of size 67108864
2019-10-02 01:35:27.313490: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd1ef13f00 next 205 of size 32768
2019-10-02 01:35:27.313497: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd1ef1bf00 next 206 of size 16777216
2019-10-02 01:35:27.313502: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd1ff1bf00 next 207 of size 8192
2019-10-02 01:35:27.313508: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd1ff1df00 next 208 of size 8192
2019-10-02 01:35:27.313513: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd1ff1ff00 next 209 of size 8192
2019-10-02 01:35:27.313518: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd1ff21f00 next 210 of size 16777216
2019-10-02 01:35:27.313524: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd20f21f00 next 211 of size 32768
2019-10-02 01:35:27.313529: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd20f29f00 next 212 of size 8192
2019-10-02 01:35:27.313535: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd20f2bf00 next 213 of size 8192
2019-10-02 01:35:27.313540: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd20f2df00 next 214 of size 8192
2019-10-02 01:35:27.313546: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd20f2ff00 next 215 of size 8192
2019-10-02 01:35:27.313551: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd20f31f00 next 216 of size 8192
2019-10-02 01:35:27.313556: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd20f33f00 next 217 of size 8192
2019-10-02 01:35:27.313562: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd20f35f00 next 218 of size 16777216
2019-10-02 01:35:27.313570: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd21f35f00 next 219 of size 512
2019-10-02 01:35:27.313576: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd21f36100 next 220 of size 16777216
2019-10-02 01:35:27.313581: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd22f36100 next 221 of size 8192
2019-10-02 01:35:27.313587: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd22f38100 next 222 of size 67108864
2019-10-02 01:35:27.313592: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd26f38100 next 223 of size 16777216
2019-10-02 01:35:27.313597: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd27f38100 next 224 of size 8192
2019-10-02 01:35:27.313603: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd27f3a100 next 225 of size 16384000
2019-10-02 01:35:27.313608: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd28eda100 next 226 of size 512
2019-10-02 01:35:27.313614: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd28eda300 next 227 of size 8192
2019-10-02 01:35:27.313619: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd28edc300 next 228 of size 8192
2019-10-02 01:35:27.313625: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd28ede300 next 229 of size 8192
2019-10-02 01:35:27.313630: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd28ee0300 next 230 of size 8192
2019-10-02 01:35:27.313636: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd28ee2300 next 231 of size 8192
2019-10-02 01:35:27.313641: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd28ee4300 next 232 of size 16777216
2019-10-02 01:35:27.313646: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd29ee4300 next 233 of size 8192
2019-10-02 01:35:27.313652: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd29ee6300 next 234 of size 67108864
2019-10-02 01:35:27.313657: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd2dee6300 next 235 of size 16384000
2019-10-02 01:35:27.313663: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd2ee86300 next 236 of size 16777216
2019-10-02 01:35:27.313668: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd2fe86300 next 237 of size 8192
2019-10-02 01:35:27.313675: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd2fe88300 next 238 of size 8192
2019-10-02 01:35:27.313681: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd2fe8a300 next 239 of size 512
2019-10-02 01:35:27.313686: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd2fe8a500 next 240 of size 16777216
2019-10-02 01:35:27.313691: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd30e8a500 next 241 of size 8192
2019-10-02 01:35:27.313697: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd30e8c500 next 242 of size 8192
2019-10-02 01:35:27.313702: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd30e8e500 next 243 of size 32768
2019-10-02 01:35:27.313708: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd30e96500 next 244 of size 16384000
2019-10-02 01:35:27.313713: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd31e36500 next 245 of size 16384000
2019-10-02 01:35:27.313719: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd32dd6500 next 246 of size 8192
2019-10-02 01:35:27.313724: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd32dd8500 next 247 of size 32768
2019-10-02 01:35:27.313729: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd32de0500 next 248 of size 8192
2019-10-02 01:35:27.313735: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd32de2500 next 249 of size 8192
2019-10-02 01:35:27.313743: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd32de4500 next 250 of size 32768
2019-10-02 01:35:27.313749: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd32dec500 next 251 of size 8192
2019-10-02 01:35:27.313755: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd32dee500 next 252 of size 8192
2019-10-02 01:35:27.313760: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd32df0500 next 253 of size 16777216
2019-10-02 01:35:27.313766: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd33df0500 next 254 of size 16384000
2019-10-02 01:35:27.313771: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd34d90500 next 255 of size 16384000
2019-10-02 01:35:27.313777: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd35d30500 next 256 of size 512
2019-10-02 01:35:27.313782: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd35d30700 next 257 of size 8192
2019-10-02 01:35:27.313788: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd35d32700 next 258 of size 8192
2019-10-02 01:35:27.313793: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd35d34700 next 259 of size 8192
2019-10-02 01:35:27.313799: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd35d36700 next 260 of size 67108864
2019-10-02 01:35:27.313804: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd39d36700 next 261 of size 8192
2019-10-02 01:35:27.313810: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd39d38700 next 262 of size 67108864
2019-10-02 01:35:27.313815: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd3dd38700 next 263 of size 512
2019-10-02 01:35:27.313820: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd3dd38900 next 264 of size 8192
2019-10-02 01:35:27.313826: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd3dd3a900 next 265 of size 8192
2019-10-02 01:35:27.313831: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd3dd3c900 next 266 of size 67108864
2019-10-02 01:35:27.313837: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd41d3c900 next 267 of size 16384000
2019-10-02 01:35:27.313842: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd42cdc900 next 268 of size 8192
2019-10-02 01:35:27.313847: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd42cde900 next 269 of size 8192
2019-10-02 01:35:27.313854: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd42ce0900 next 270 of size 8192
2019-10-02 01:35:27.313860: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd42ce2900 next 271 of size 67108864
2019-10-02 01:35:27.313866: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd46ce2900 next 272 of size 16777216
2019-10-02 01:35:27.313871: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd47ce2900 next 273 of size 16777216
2019-10-02 01:35:27.313877: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd48ce2900 next 274 of size 16777216
2019-10-02 01:35:27.313882: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd49ce2900 next 275 of size 16777216
2019-10-02 01:35:27.313887: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd4ace2900 next 276 of size 8192
2019-10-02 01:35:27.313893: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd4ace4900 next 277 of size 16384000
2019-10-02 01:35:27.313898: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd4bc84900 next 278 of size 16384000
2019-10-02 01:35:27.313904: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd4cc24900 next 279 of size 8192
2019-10-02 01:35:27.313909: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd4cc26900 next 280 of size 8192
2019-10-02 01:35:27.313915: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd4cc28900 next 281 of size 8192
2019-10-02 01:35:27.313923: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd4cc2a900 next 282 of size 8192
2019-10-02 01:35:27.313929: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd4cc2c900 next 283 of size 8192
2019-10-02 01:35:27.313934: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd4cc2e900 next 284 of size 67108864
2019-10-02 01:35:27.313940: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd50c2e900 next 285 of size 8192
2019-10-02 01:35:27.313945: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd50c30900 next 286 of size 16384000
2019-10-02 01:35:27.313951: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd51bd0900 next 287 of size 16384000
2019-10-02 01:35:27.313956: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd52b70900 next 288 of size 8192
2019-10-02 01:35:27.313962: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd52b72900 next 289 of size 8192
2019-10-02 01:35:27.313972: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd52b74900 next 290 of size 67108864
2019-10-02 01:35:27.313982: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd56b74900 next 291 of size 8192
2019-10-02 01:35:27.313991: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd56b76900 next 292 of size 8192
2019-10-02 01:35:27.314001: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd56b78900 next 293 of size 8192
2019-10-02 01:35:27.314011: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd56b7a900 next 294 of size 8192
2019-10-02 01:35:27.314020: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd56b7c900 next 295 of size 16777216
2019-10-02 01:35:27.314030: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd57b7c900 next 296 of size 8192
2019-10-02 01:35:27.314039: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd57b7e900 next 297 of size 32768
2019-10-02 01:35:27.314049: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd57b86900 next 298 of size 16777216
2019-10-02 01:35:27.314058: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd58b86900 next 299 of size 16777216
2019-10-02 01:35:27.314073: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd59b86900 next 300 of size 8192
2019-10-02 01:35:27.314082: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd59b88900 next 301 of size 8192
2019-10-02 01:35:27.314093: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd59b8a900 next 302 of size 16777216
2019-10-02 01:35:27.314102: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd5ab8a900 next 303 of size 512
2019-10-02 01:35:27.314112: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd5ab8ab00 next 304 of size 8192
2019-10-02 01:35:27.314121: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd5ab8cb00 next 305 of size 67108864
2019-10-02 01:35:27.314129: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd5eb8cb00 next 306 of size 8192
2019-10-02 01:35:27.314139: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd5eb8eb00 next 307 of size 16777216
2019-10-02 01:35:27.314148: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd5fb8eb00 next 308 of size 67108864
2019-10-02 01:35:27.314156: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd63b8eb00 next 309 of size 8192
2019-10-02 01:35:27.314162: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd63b90b00 next 310 of size 8192
2019-10-02 01:35:27.314173: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd63b92b00 next 311 of size 8192
2019-10-02 01:35:27.314182: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd63b94b00 next 312 of size 8192
2019-10-02 01:35:27.314196: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd63b96b00 next 313 of size 16777216
2019-10-02 01:35:27.314206: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd64b96b00 next 314 of size 16777216
2019-10-02 01:35:27.314216: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd65b96b00 next 315 of size 67108864
2019-10-02 01:35:27.314226: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd69b96b00 next 316 of size 8192
2019-10-02 01:35:27.314235: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd69b98b00 next 317 of size 8192
2019-10-02 01:35:27.314243: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd69b9ab00 next 318 of size 256
2019-10-02 01:35:27.314252: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd69b9ac00 next 319 of size 8192
2019-10-02 01:35:27.314262: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd69b9cc00 next 320 of size 512
2019-10-02 01:35:27.314271: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd69b9ce00 next 321 of size 67108864
2019-10-02 01:35:27.314280: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd6db9ce00 next 322 of size 32768
2019-10-02 01:35:27.314290: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd6dba4e00 next 323 of size 16777216
2019-10-02 01:35:27.314299: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd6eba4e00 next 324 of size 512
2019-10-02 01:35:27.314308: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd6eba5000 next 325 of size 32768
2019-10-02 01:35:27.314317: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd6ebad000 next 326 of size 8192
2019-10-02 01:35:27.314326: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd6ebaf000 next 327 of size 8192
2019-10-02 01:35:27.314335: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd6ebb1000 next 328 of size 8192
2019-10-02 01:35:27.314341: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd6ebb3000 next 329 of size 8192
2019-10-02 01:35:27.314347: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd6ebb5000 next 330 of size 16777216
2019-10-02 01:35:27.314356: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd6fbb5000 next 331 of size 32768
2019-10-02 01:35:27.314366: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd6fbbd000 next 332 of size 16777216
2019-10-02 01:35:27.314376: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd70bbd000 next 333 of size 8192
2019-10-02 01:35:27.314387: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd70bbf000 next 334 of size 16777216
2019-10-02 01:35:27.314396: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd71bbf000 next 335 of size 8192
2019-10-02 01:35:27.314406: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd71bc1000 next 336 of size 8192
2019-10-02 01:35:27.314415: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd71bc3000 next 337 of size 8192
2019-10-02 01:35:27.314424: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd71bc5000 next 338 of size 16777216
2019-10-02 01:35:27.314434: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd72bc5000 next 339 of size 8192
2019-10-02 01:35:27.314443: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd72bc7000 next 340 of size 16777216
2019-10-02 01:35:27.314451: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd73bc7000 next 341 of size 8192
2019-10-02 01:35:27.314461: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd73bc9000 next 342 of size 8192
2019-10-02 01:35:27.314470: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd73bcb000 next 343 of size 8192
2019-10-02 01:35:27.314485: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd73bcd000 next 344 of size 67108864
2019-10-02 01:35:27.314495: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd77bcd000 next 345 of size 8192
2019-10-02 01:35:27.314504: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd77bcf000 next 346 of size 16777216
2019-10-02 01:35:27.314514: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd78bcf000 next 347 of size 8192
2019-10-02 01:35:27.314523: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd78bd1000 next 348 of size 8192
2019-10-02 01:35:27.314532: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd78bd3000 next 349 of size 8192
2019-10-02 01:35:27.314543: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd78bd5000 next 350 of size 262144000
2019-10-02 01:35:27.314552: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd885d5000 next 351 of size 512
2019-10-02 01:35:27.314562: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd885d5200 next 352 of size 8192
2019-10-02 01:35:27.314572: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd885d7200 next 353 of size 8192
2019-10-02 01:35:27.314578: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd885d9200 next 354 of size 16777216
2019-10-02 01:35:27.314584: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd895d9200 next 355 of size 16777216
2019-10-02 01:35:27.314590: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd8a5d9200 next 356 of size 16777216
2019-10-02 01:35:27.314596: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd8b5d9200 next 357 of size 8192
2019-10-02 01:35:27.314601: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd8b5db200 next 358 of size 16777216
2019-10-02 01:35:27.314607: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd8c5db200 next 359 of size 32768
2019-10-02 01:35:27.314612: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd8c5e3200 next 360 of size 8192
2019-10-02 01:35:27.314618: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd8c5e5200 next 361 of size 8192
2019-10-02 01:35:27.314623: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd8c5e7200 next 362 of size 8192
2019-10-02 01:35:27.314629: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd8c5e9200 next 363 of size 512
2019-10-02 01:35:27.314634: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd8c5e9400 next 364 of size 16777216
2019-10-02 01:35:27.314640: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd8d5e9400 next 365 of size 8192
2019-10-02 01:35:27.314647: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd8d5eb400 next 366 of size 8192
2019-10-02 01:35:27.314652: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd8d5ed400 next 367 of size 16777216
2019-10-02 01:35:27.314658: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd8e5ed400 next 368 of size 16777216
2019-10-02 01:35:27.314664: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd8f5ed400 next 369 of size 67108864
2019-10-02 01:35:27.314669: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd935ed400 next 370 of size 16777216
2019-10-02 01:35:27.314674: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd945ed400 next 371 of size 67108864
2019-10-02 01:35:27.314680: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd985ed400 next 372 of size 8192
2019-10-02 01:35:27.314685: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd985ef400 next 373 of size 67108864
2019-10-02 01:35:27.314691: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd9c5ef400 next 374 of size 16777216
2019-10-02 01:35:27.314696: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd9d5ef400 next 375 of size 8192
2019-10-02 01:35:27.314705: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd9d5f1400 next 376 of size 8192
2019-10-02 01:35:27.314711: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd9d5f3400 next 377 of size 512
2019-10-02 01:35:27.314717: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd9d5f3600 next 378 of size 8192
2019-10-02 01:35:27.314722: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd9d5f5600 next 379 of size 8192
2019-10-02 01:35:27.314727: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd9d5f7600 next 380 of size 8192
2019-10-02 01:35:27.314733: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd9d5f9600 next 381 of size 8192
2019-10-02 01:35:27.314738: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd9d5fb600 next 382 of size 8192
2019-10-02 01:35:27.314744: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd9d5fd600 next 383 of size 16777216
2019-10-02 01:35:27.314749: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd9e5fd600 next 384 of size 32768
2019-10-02 01:35:27.314755: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd9e605600 next 385 of size 67108864
2019-10-02 01:35:27.314760: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda2605600 next 386 of size 8192
2019-10-02 01:35:27.314765: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda2607600 next 387 of size 8192
2019-10-02 01:35:27.314771: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda2609600 next 388 of size 512
2019-10-02 01:35:27.314776: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda2609800 next 389 of size 8192
2019-10-02 01:35:27.314781: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda260b800 next 390 of size 8192
2019-10-02 01:35:27.314787: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda260d800 next 391 of size 512
2019-10-02 01:35:27.314792: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda260da00 next 392 of size 8192
2019-10-02 01:35:27.314798: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda260fa00 next 393 of size 8192
2019-10-02 01:35:27.314803: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda2611a00 next 394 of size 8192
2019-10-02 01:35:27.314809: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda2613a00 next 395 of size 8192
2019-10-02 01:35:27.314814: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda2615a00 next 396 of size 8192
2019-10-02 01:35:27.314820: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda2617a00 next 397 of size 8192
2019-10-02 01:35:27.314831: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda2619a00 next 398 of size 8192
2019-10-02 01:35:27.314841: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda261ba00 next 399 of size 8192
2019-10-02 01:35:27.314851: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda261da00 next 400 of size 8192
2019-10-02 01:35:27.314858: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda261fa00 next 401 of size 8192
2019-10-02 01:35:27.314869: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda2621a00 next 402 of size 67108864
2019-10-02 01:35:27.314879: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda6621a00 next 403 of size 8192
2019-10-02 01:35:27.314888: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda6623a00 next 404 of size 16777216
2019-10-02 01:35:27.314897: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda7623a00 next 405 of size 67108864
2019-10-02 01:35:27.314906: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdab623a00 next 406 of size 8192
2019-10-02 01:35:27.314920: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdab625a00 next 407 of size 8192
2019-10-02 01:35:27.314926: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdab627a00 next 408 of size 8192
2019-10-02 01:35:27.314932: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdab629a00 next 409 of size 67108864
2019-10-02 01:35:27.314937: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdaf629a00 next 410 of size 16777216
2019-10-02 01:35:27.314943: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdb0629a00 next 411 of size 8192
2019-10-02 01:35:27.314948: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdb062ba00 next 412 of size 512
2019-10-02 01:35:27.314954: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdb062bc00 next 413 of size 67108864
2019-10-02 01:35:27.314959: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdb462bc00 next 414 of size 16777216
2019-10-02 01:35:27.314964: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdb562bc00 next 415 of size 8192
2019-10-02 01:35:27.314970: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdb562dc00 next 416 of size 8192
2019-10-02 01:35:27.314975: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdb562fc00 next 417 of size 16777216
2019-10-02 01:35:27.314980: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdb662fc00 next 418 of size 8192
2019-10-02 01:35:27.314986: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdb6631c00 next 419 of size 8192
2019-10-02 01:35:27.314991: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdb6633c00 next 420 of size 32768
2019-10-02 01:35:27.314996: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdb663bc00 next 421 of size 16777216
2019-10-02 01:35:27.315002: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdb763bc00 next 422 of size 16777216
2019-10-02 01:35:27.315007: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdb863bc00 next 423 of size 32768
2019-10-02 01:35:27.315013: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdb8643c00 next 424 of size 16777216
2019-10-02 01:35:27.315018: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdb9643c00 next 425 of size 8192
2019-10-02 01:35:27.315023: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdb9645c00 next 426 of size 16777216
2019-10-02 01:35:27.315029: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdba645c00 next 427 of size 8192
2019-10-02 01:35:27.315034: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdba647c00 next 428 of size 67108864
2019-10-02 01:35:27.315040: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdbe647c00 next 429 of size 8192
2019-10-02 01:35:27.315046: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdbe649c00 next 430 of size 8192
2019-10-02 01:35:27.315052: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdbe64bc00 next 431 of size 16777216
2019-10-02 01:35:27.315057: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdbf64bc00 next 432 of size 8192
2019-10-02 01:35:27.315063: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdbf64dc00 next 433 of size 8192
2019-10-02 01:35:27.315068: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdbf64fc00 next 434 of size 512
2019-10-02 01:35:27.315074: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdbf64fe00 next 435 of size 16777216
2019-10-02 01:35:27.315079: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc064fe00 next 436 of size 8192
2019-10-02 01:35:27.315085: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc0651e00 next 437 of size 16777216
2019-10-02 01:35:27.315090: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc1651e00 next 438 of size 8192
2019-10-02 01:35:27.315098: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc1653e00 next 439 of size 8192
2019-10-02 01:35:27.315104: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc1655e00 next 440 of size 67108864
2019-10-02 01:35:27.315110: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc5655e00 next 441 of size 16777216
2019-10-02 01:35:27.315115: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc6655e00 next 442 of size 8192
2019-10-02 01:35:27.315121: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc6657e00 next 443 of size 16777216
2019-10-02 01:35:27.315126: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc7657e00 next 444 of size 8192
2019-10-02 01:35:27.315132: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc7659e00 next 445 of size 8192
2019-10-02 01:35:27.315142: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc765be00 next 446 of size 8192
2019-10-02 01:35:27.315152: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc765de00 next 447 of size 8192
2019-10-02 01:35:27.315161: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc765fe00 next 448 of size 8192
2019-10-02 01:35:27.315171: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc7661e00 next 449 of size 8192
2019-10-02 01:35:27.315181: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc7663e00 next 450 of size 8192
2019-10-02 01:35:27.315191: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc7665e00 next 451 of size 16777216
2019-10-02 01:35:27.315200: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc8665e00 next 452 of size 512
2019-10-02 01:35:27.315209: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc8666000 next 453 of size 8192
2019-10-02 01:35:27.315218: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc8668000 next 454 of size 8192
2019-10-02 01:35:27.315224: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc866a000 next 455 of size 32768
2019-10-02 01:35:27.315230: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc8672000 next 456 of size 512
2019-10-02 01:35:27.315235: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc8672200 next 457 of size 8192
2019-10-02 01:35:27.315241: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc8674200 next 458 of size 8192
2019-10-02 01:35:27.315249: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc8676200 next 459 of size 16777216
2019-10-02 01:35:27.315254: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc9676200 next 460 of size 16777216
2019-10-02 01:35:27.315260: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdca676200 next 461 of size 8192
2019-10-02 01:35:27.315268: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdca678200 next 462 of size 8192
2019-10-02 01:35:27.315273: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdca67a200 next 463 of size 8192
2019-10-02 01:35:27.315279: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdca67c200 next 464 of size 512
2019-10-02 01:35:27.315284: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdca67c400 next 465 of size 16777216
2019-10-02 01:35:27.315290: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdcb67c400 next 466 of size 32768
2019-10-02 01:35:27.315331: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdcb684400 next 467 of size 8192
2019-10-02 01:35:27.315345: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdcb686400 next 468 of size 8192
2019-10-02 01:35:27.315352: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdcb688400 next 469 of size 16777216
2019-10-02 01:35:27.315362: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdcc688400 next 470 of size 32768
2019-10-02 01:35:27.315369: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdcc690400 next 471 of size 8192
2019-10-02 01:35:27.315374: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdcc692400 next 472 of size 16777216
2019-10-02 01:35:27.315380: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdcd692400 next 473 of size 8192
2019-10-02 01:35:27.315386: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdcd694400 next 474 of size 8192
2019-10-02 01:35:27.315391: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdcd696400 next 475 of size 67108864
2019-10-02 01:35:27.315396: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdd1696400 next 476 of size 8192
2019-10-02 01:35:27.315402: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdd1698400 next 477 of size 67108864
2019-10-02 01:35:27.315408: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdd5698400 next 478 of size 256
2019-10-02 01:35:27.315413: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdd5698500 next 479 of size 8192
2019-10-02 01:35:27.315419: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdd569a500 next 480 of size 512
2019-10-02 01:35:27.315425: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdd569a700 next 481 of size 67108864
2019-10-02 01:35:27.315430: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdd969a700 next 482 of size 256
2019-10-02 01:35:27.315436: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdd969a800 next 514 of size 256
2019-10-02 01:35:27.315441: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdd969a900 next 484 of size 256
2019-10-02 01:35:27.315447: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdd969aa00 next 485 of size 8192
2019-10-02 01:35:27.315452: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdd969ca00 next 486 of size 8192
2019-10-02 01:35:27.315457: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdd969ea00 next 487 of size 16777216
2019-10-02 01:35:27.315463: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdda69ea00 next 515 of size 8192
2019-10-02 01:35:27.315468: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdda6a0a00 next 512 of size 512
2019-10-02 01:35:27.315474: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdda6a0c00 next 513 of size 8192
2019-10-02 01:35:27.315480: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdda6a2c00 next 510 of size 16777216
2019-10-02 01:35:27.315490: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efddb6a2c00 next 511 of size 8192
2019-10-02 01:35:27.315501: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efddb6a4c00 next 500 of size 16777216
2019-10-02 01:35:27.315508: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efddc6a4c00 next 501 of size 16777216
2019-10-02 01:35:27.315514: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efddd6a4c00 next 509 of size 8192
2019-10-02 01:35:27.315519: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efddd6a6c00 next 504 of size 8192
2019-10-02 01:35:27.315525: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efddd6a8c00 next 505 of size 67108864
2019-10-02 01:35:27.315531: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde16a8c00 next 489 of size 32768
2019-10-02 01:35:27.315536: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde16b0c00 next 490 of size 8192
2019-10-02 01:35:27.315542: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde16b2c00 next 488 of size 8192
2019-10-02 01:35:27.315548: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde16b4c00 next 508 of size 16777216
2019-10-02 01:35:27.315557: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde26b4c00 next 503 of size 8192
2019-10-02 01:35:27.315563: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde26b6c00 next 502 of size 8192
2019-10-02 01:35:27.315569: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde26b8c00 next 507 of size 8192
2019-10-02 01:35:27.315575: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde26bac00 next 497 of size 16777216
2019-10-02 01:35:27.315580: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde36bac00 next 498 of size 67108864
2019-10-02 01:35:27.315586: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde76bac00 next 506 of size 8192
2019-10-02 01:35:27.315591: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde76bcc00 next 496 of size 8192
2019-10-02 01:35:27.315597: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde76bec00 next 492 of size 16777216
2019-10-02 01:35:27.315603: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde86bec00 next 493 of size 512
2019-10-02 01:35:27.315608: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde86bee00 next 483 of size 8192
2019-10-02 01:35:27.315614: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde86c0e00 next 495 of size 16777216
2019-10-02 01:35:27.315619: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde96c0e00 next 491 of size 8192
2019-10-02 01:35:27.315625: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde96c2e00 next 499 of size 8192
2019-10-02 01:35:27.315630: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde96c4e00 next 494 of size 32768
2019-10-02 01:35:27.315636: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde96cce00 next 516 of size 16777216
2019-10-02 01:35:27.315641: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdea6cce00 next 517 of size 8192
2019-10-02 01:35:27.315647: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdea6cee00 next 518 of size 8192
2019-10-02 01:35:27.315652: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdea6d0e00 next 519 of size 16777216
2019-10-02 01:35:27.315658: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdeb6d0e00 next 520 of size 67108864
2019-10-02 01:35:27.315664: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdef6d0e00 next 521 of size 8192
2019-10-02 01:35:27.315669: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdef6d2e00 next 522 of size 16777216
2019-10-02 01:35:27.315674: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf06d2e00 next 523 of size 8192
2019-10-02 01:35:27.315680: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf06d4e00 next 524 of size 8192
2019-10-02 01:35:27.315686: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf06d6e00 next 525 of size 8192
2019-10-02 01:35:27.315692: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf06d8e00 next 526 of size 16777216
2019-10-02 01:35:27.315697: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16d8e00 next 527 of size 8192
2019-10-02 01:35:27.315703: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16dae00 next 528 of size 8192
2019-10-02 01:35:27.315708: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16dce00 next 529 of size 8192
2019-10-02 01:35:27.315714: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16dee00 next 530 of size 256
2019-10-02 01:35:27.315719: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16def00 next 545 of size 256
2019-10-02 01:35:27.315725: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16df000 next 547 of size 8192
2019-10-02 01:35:27.315733: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16e1000 next 548 of size 256
2019-10-02 01:35:27.315739: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16e1100 next 549 of size 8192
2019-10-02 01:35:27.315745: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16e3100 next 546 of size 256
2019-10-02 01:35:27.315750: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16e3200 next 550 of size 8192
2019-10-02 01:35:27.315756: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16e5200 next 551 of size 256
2019-10-02 01:35:27.315761: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16e5300 next 552 of size 8192
2019-10-02 01:35:27.315767: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16e7300 next 554 of size 256
2019-10-02 01:35:27.315774: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16e7400 next 586 of size 4096
2019-10-02 01:35:27.315780: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16e8400 next 577 of size 256
2019-10-02 01:35:27.315786: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16e8500 next 590 of size 5376
2019-10-02 01:35:27.315792: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16e9a00 next 559 of size 8192
2019-10-02 01:35:27.315798: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16eba00 next 657 of size 9984
2019-10-02 01:35:27.315803: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16ee100 next 684 of size 6912
2019-10-02 01:35:27.315809: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16efc00 next 685 of size 256
2019-10-02 01:35:27.315814: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16efd00 next 686 of size 256
2019-10-02 01:35:27.315820: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16efe00 next 687 of size 256
2019-10-02 01:35:27.315825: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16eff00 next 688 of size 256
2019-10-02 01:35:27.315831: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f0000 next 683 of size 256
2019-10-02 01:35:27.315836: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f0100 next 690 of size 256
2019-10-02 01:35:27.315842: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f0200 next 665 of size 256
2019-10-02 01:35:27.315847: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f0300 next 605 of size 256
2019-10-02 01:35:27.315853: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f0400 next 697 of size 256
2019-10-02 01:35:27.315858: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f0500 next 936 of size 256
2019-10-02 01:35:27.315863: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f0600 next 630 of size 512
2019-10-02 01:35:27.315869: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f0800 next 1011 of size 256
2019-10-02 01:35:27.315875: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f0900 next 699 of size 512
2019-10-02 01:35:27.315881: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f0b00 next 700 of size 256
2019-10-02 01:35:27.315886: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f0c00 next 701 of size 256
2019-10-02 01:35:27.315905: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f0d00 next 703 of size 256
2019-10-02 01:35:27.315914: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f0e00 next 705 of size 256
2019-10-02 01:35:27.315923: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f0f00 next 706 of size 256
2019-10-02 01:35:27.315929: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f1000 next 707 of size 256
2019-10-02 01:35:27.315938: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f1100 next 712 of size 256
2019-10-02 01:35:27.315944: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f1200 next 637 of size 256
2019-10-02 01:35:27.315950: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f1300 next 714 of size 256
2019-10-02 01:35:27.315956: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f1400 next 531 of size 56064
2019-10-02 01:35:27.315962: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16fef00 next 532 of size 67108864
2019-10-02 01:35:27.315967: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf56fef00 next 533 of size 256
2019-10-02 01:35:27.315973: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf56ff000 next 534 of size 16777216
2019-10-02 01:35:27.315979: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf66ff000 next 535 of size 131072
2019-10-02 01:35:27.315985: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf671f000 next 715 of size 32768
2019-10-02 01:35:27.315991: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6727000 next 718 of size 256
2019-10-02 01:35:27.315996: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6727100 next 677 of size 256
2019-10-02 01:35:27.316002: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6727200 next 720 of size 256
2019-10-02 01:35:27.316008: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6727300 next 721 of size 8192
2019-10-02 01:35:27.316014: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6729300 next 722 of size 8192
2019-10-02 01:35:27.316019: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf672b300 next 724 of size 256
2019-10-02 01:35:27.316025: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf672b400 next 725 of size 256
2019-10-02 01:35:27.316031: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf672b500 next 726 of size 256
2019-10-02 01:35:27.316036: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf672b600 next 727 of size 256
2019-10-02 01:35:27.316042: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf672b700 next 728 of size 8192
2019-10-02 01:35:27.316047: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf672d700 next 729 of size 8192
2019-10-02 01:35:27.316053: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf672f700 next 730 of size 256
2019-10-02 01:35:27.316058: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf672f800 next 731 of size 256
2019-10-02 01:35:27.316064: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf672f900 next 732 of size 256
2019-10-02 01:35:27.316070: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf672fa00 next 733 of size 256
2019-10-02 01:35:27.316076: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf672fb00 next 734 of size 8192
2019-10-02 01:35:27.316081: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6731b00 next 735 of size 8192
2019-10-02 01:35:27.316087: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6733b00 next 736 of size 8192
2019-10-02 01:35:27.316092: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6735b00 next 737 of size 256
2019-10-02 01:35:27.316099: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6735c00 next 738 of size 8192
2019-10-02 01:35:27.316104: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6737c00 next 739 of size 8192
2019-10-02 01:35:27.316110: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6739c00 next 740 of size 256
2019-10-02 01:35:27.316116: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6739d00 next 741 of size 8192
2019-10-02 01:35:27.316124: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf673bd00 next 536 of size 13056
2019-10-02 01:35:27.316130: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf673f000 next 537 of size 8192
2019-10-02 01:35:27.316136: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6741000 next 538 of size 256
2019-10-02 01:35:27.316142: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6741100 next 539 of size 8192
2019-10-02 01:35:27.316148: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6743100 next 540 of size 131072
2019-10-02 01:35:27.316153: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6763100 next 541 of size 256
2019-10-02 01:35:27.316159: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6763200 next 542 of size 67108864
2019-10-02 01:35:27.316164: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdfa763200 next 543 of size 8192
2019-10-02 01:35:27.316170: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdfa765200 next 544 of size 67108864
2019-10-02 01:35:27.316176: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdfe765200 next 553 of size 131072
2019-10-02 01:35:27.316181: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdfe785200 next 582 of size 16777216
2019-10-02 01:35:27.316187: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff785200 next 742 of size 256
2019-10-02 01:35:27.316192: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff785300 next 743 of size 8192
2019-10-02 01:35:27.316198: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff787300 next 744 of size 8192
2019-10-02 01:35:27.316203: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff789300 next 745 of size 256
2019-10-02 01:35:27.316209: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff789400 next 746 of size 8192
2019-10-02 01:35:27.316215: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff78b400 next 747 of size 8192
2019-10-02 01:35:27.316221: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff78d400 next 748 of size 256
2019-10-02 01:35:27.316232: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff78d500 next 749 of size 8192
2019-10-02 01:35:27.316242: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff78f500 next 750 of size 8192
2019-10-02 01:35:27.316249: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff791500 next 751 of size 256
2019-10-02 01:35:27.316254: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff791600 next 752 of size 8192
2019-10-02 01:35:27.316260: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff793600 next 753 of size 256
2019-10-02 01:35:27.316266: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff793700 next 754 of size 256
2019-10-02 01:35:27.316271: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff793800 next 756 of size 256
2019-10-02 01:35:27.316277: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff793900 next 758 of size 8192
2019-10-02 01:35:27.316282: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff795900 next 759 of size 256
2019-10-02 01:35:27.316288: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff795a00 next 760 of size 8192
2019-10-02 01:35:27.316293: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff797a00 next 761 of size 8192
2019-10-02 01:35:27.316299: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff799a00 next 762 of size 256
2019-10-02 01:35:27.316305: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff799b00 next 763 of size 8192
2019-10-02 01:35:27.316310: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff79bb00 next 764 of size 8192
2019-10-02 01:35:27.316319: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff79db00 next 765 of size 256
2019-10-02 01:35:27.316326: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff79dc00 next 766 of size 8192
2019-10-02 01:35:27.316331: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff79fc00 next 769 of size 256
2019-10-02 01:35:27.316337: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff79fd00 next 771 of size 256
2019-10-02 01:35:27.316343: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff79fe00 next 773 of size 256
2019-10-02 01:35:27.316348: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff79ff00 next 776 of size 256
2019-10-02 01:35:27.316354: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff7a0000 next 778 of size 256
2019-10-02 01:35:27.316359: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff7a0100 next 780 of size 256
2019-10-02 01:35:27.316365: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff7a0200 next 782 of size 256
2019-10-02 01:35:27.316370: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff7a0300 next 785 of size 256
2019-10-02 01:35:27.316376: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff7a0400 next 786 of size 8192
2019-10-02 01:35:27.316382: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff7a2400 next 787 of size 512
2019-10-02 01:35:27.316388: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff7a2600 next 581 of size 11264
2019-10-02 01:35:27.316394: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff7a5200 next 580 of size 16777216
2019-10-02 01:35:27.316399: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe007a5200 next 579 of size 67108864
2019-10-02 01:35:27.316405: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe047a5200 next 578 of size 131072
2019-10-02 01:35:27.316411: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe047c5200 next 585 of size 16777216
2019-10-02 01:35:27.316417: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe057c5200 next 584 of size 8388608
2019-10-02 01:35:27.316423: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe05fc5200 next 583 of size 16777216
2019-10-02 01:35:27.316428: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe06fc5200 next 576 of size 16777216
2019-10-02 01:35:27.316434: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe07fc5200 next 702 of size 67108864
2019-10-02 01:35:27.316439: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe0bfc5200 next 704 of size 16777216
2019-10-02 01:35:27.316445: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe0cfc5200 next 708 of size 67108864
2019-10-02 01:35:27.316450: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe10fc5200 next 709 of size 16777216
2019-10-02 01:35:27.316456: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe11fc5200 next 710 of size 67108864
2019-10-02 01:35:27.316461: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe15fc5200 next 711 of size 16777216
2019-10-02 01:35:27.316467: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe16fc5200 next 716 of size 16777216
2019-10-02 01:35:27.316473: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe17fc5200 next 717 of size 67108864
2019-10-02 01:35:27.316478: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe1bfc5200 next 723 of size 67108864
2019-10-02 01:35:27.316484: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe1ffc5200 next 755 of size 16777216
2019-10-02 01:35:27.316489: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe20fc5200 next 757 of size 16777216
2019-10-02 01:35:27.316498: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe21fc5200 next 767 of size 16777216
2019-10-02 01:35:27.316503: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe22fc5200 next 768 of size 16777216
2019-10-02 01:35:27.316509: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe23fc5200 next 770 of size 67108864
2019-10-02 01:35:27.316514: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe27fc5200 next 772 of size 32768
2019-10-02 01:35:27.316520: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe27fcd200 next 774 of size 32768
2019-10-02 01:35:27.316525: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe27fd5200 next 775 of size 16777216
2019-10-02 01:35:27.316531: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe28fd5200 next 777 of size 16777216
2019-10-02 01:35:27.316537: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe29fd5200 next 779 of size 16777216
2019-10-02 01:35:27.316547: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe2afd5200 next 781 of size 67108864
2019-10-02 01:35:27.316557: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe2efd5200 next 783 of size 16777216
2019-10-02 01:35:27.316565: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe2ffd5200 next 784 of size 67108864
2019-10-02 01:35:27.316571: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe33fd5200 next 788 of size 512
2019-10-02 01:35:27.316577: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe33fd5400 next 789 of size 256
2019-10-02 01:35:27.316583: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe33fd5500 next 790 of size 256
2019-10-02 01:35:27.316588: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe33fd5600 next 791 of size 256
2019-10-02 01:35:27.316594: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe33fd5700 next 792 of size 256
2019-10-02 01:35:27.316600: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe33fd5800 next 793 of size 512
2019-10-02 01:35:27.316605: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe33fd5a00 next 794 of size 8192
2019-10-02 01:35:27.316611: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe33fd7a00 next 795 of size 512
2019-10-02 01:35:27.316617: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe33fd7c00 next 796 of size 8192
2019-10-02 01:35:27.316622: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe33fd9c00 next 797 of size 8192
2019-10-02 01:35:27.316628: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe33fdbc00 next 798 of size 8192
2019-10-02 01:35:27.316633: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe33fddc00 next 799 of size 16777216
2019-10-02 01:35:27.316639: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe34fddc00 next 800 of size 256
2019-10-02 01:35:27.316644: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe34fddd00 next 801 of size 16777216
2019-10-02 01:35:27.316650: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe35fddd00 next 802 of size 67108864
2019-10-02 01:35:27.316655: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe39fddd00 next 803 of size 16777216
2019-10-02 01:35:27.316661: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3afddd00 next 804 of size 256
2019-10-02 01:35:27.316667: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3afdde00 next 805 of size 8192
2019-10-02 01:35:27.316673: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3afdfe00 next 806 of size 256
2019-10-02 01:35:27.316678: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3afdff00 next 807 of size 8192
2019-10-02 01:35:27.316690: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3afe1f00 next 808 of size 256
2019-10-02 01:35:27.316696: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3afe2000 next 809 of size 8192
2019-10-02 01:35:27.316702: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3afe4000 next 810 of size 256
2019-10-02 01:35:27.316707: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3afe4100 next 811 of size 8192
2019-10-02 01:35:27.316713: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3afe6100 next 812 of size 8192
2019-10-02 01:35:27.316719: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3afe8100 next 813 of size 256
2019-10-02 01:35:27.316724: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3afe8200 next 814 of size 8192
2019-10-02 01:35:27.316730: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3afea200 next 815 of size 8192
2019-10-02 01:35:27.316735: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3afec200 next 816 of size 256
2019-10-02 01:35:27.316741: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3afec300 next 817 of size 8192
2019-10-02 01:35:27.316747: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3afee300 next 818 of size 32768
2019-10-02 01:35:27.316752: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3aff6300 next 819 of size 256
2019-10-02 01:35:27.316758: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3aff6400 next 820 of size 32768
2019-10-02 01:35:27.316763: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3affe400 next 821 of size 32768
2019-10-02 01:35:27.316769: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3b006400 next 822 of size 256
2019-10-02 01:35:27.316775: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3b006500 next 823 of size 32768
2019-10-02 01:35:27.316780: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3b00e500 next 824 of size 8192
2019-10-02 01:35:27.316786: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3b010500 next 825 of size 256
2019-10-02 01:35:27.316791: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3b010600 next 826 of size 8192
2019-10-02 01:35:27.316797: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3b012600 next 827 of size 16777216
2019-10-02 01:35:27.316802: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3c012600 next 828 of size 256
2019-10-02 01:35:27.316808: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3c012700 next 829 of size 16777216
2019-10-02 01:35:27.316813: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3d012700 next 830 of size 16777216
2019-10-02 01:35:27.316819: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3e012700 next 831 of size 8192
2019-10-02 01:35:27.316824: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3e014700 next 832 of size 256
2019-10-02 01:35:27.316830: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3e014800 next 833 of size 8192
2019-10-02 01:35:27.316835: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3e016800 next 834 of size 8192
2019-10-02 01:35:27.316841: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3e018800 next 835 of size 256
2019-10-02 01:35:27.316846: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3e018900 next 836 of size 8192
2019-10-02 01:35:27.316852: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3e01a900 next 837 of size 16777216
2019-10-02 01:35:27.316857: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3f01a900 next 838 of size 67108864
2019-10-02 01:35:27.316863: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4301a900 next 839 of size 8192
2019-10-02 01:35:27.316872: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4301c900 next 840 of size 256
2019-10-02 01:35:27.316878: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4301ca00 next 841 of size 8192
2019-10-02 01:35:27.316883: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4301ea00 next 842 of size 256
2019-10-02 01:35:27.316889: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4301eb00 next 843 of size 8192
2019-10-02 01:35:27.316894: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe43020b00 next 844 of size 256
2019-10-02 01:35:27.316900: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe43020c00 next 845 of size 8192
2019-10-02 01:35:27.316906: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe43022c00 next 846 of size 8192
2019-10-02 01:35:27.316911: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe43024c00 next 847 of size 256
2019-10-02 01:35:27.316917: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe43024d00 next 848 of size 8192
2019-10-02 01:35:27.316923: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe43026d00 next 849 of size 16777216
2019-10-02 01:35:27.316929: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe44026d00 next 850 of size 16777216
2019-10-02 01:35:27.316934: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe45026d00 next 851 of size 256
2019-10-02 01:35:27.316940: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe45026e00 next 852 of size 256
2019-10-02 01:35:27.316946: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe45026f00 next 853 of size 16777216
2019-10-02 01:35:27.316951: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe46026f00 next 854 of size 16777216
2019-10-02 01:35:27.316957: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe47026f00 next 855 of size 67108864
2019-10-02 01:35:27.316962: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4b026f00 next 856 of size 256
2019-10-02 01:35:27.316968: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4b027000 next 857 of size 256
2019-10-02 01:35:27.316974: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4b027100 next 858 of size 512
2019-10-02 01:35:27.316980: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4b027300 next 859 of size 256
2019-10-02 01:35:27.316986: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4b027400 next 860 of size 512
2019-10-02 01:35:27.316991: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4b027600 next 861 of size 16777216
2019-10-02 01:35:27.316997: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4c027600 next 862 of size 16777216
2019-10-02 01:35:27.317002: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4d027600 next 863 of size 256
2019-10-02 01:35:27.317007: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4d027700 next 864 of size 8192
2019-10-02 01:35:27.317013: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4d029700 next 865 of size 256
2019-10-02 01:35:27.317018: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4d029800 next 866 of size 256
2019-10-02 01:35:27.317024: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4d029900 next 867 of size 8192
2019-10-02 01:35:27.317029: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4d02b900 next 868 of size 256
2019-10-02 01:35:27.317035: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4d02ba00 next 869 of size 8192
2019-10-02 01:35:27.317040: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4d02da00 next 870 of size 256
2019-10-02 01:35:27.317046: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4d02db00 next 871 of size 8192
2019-10-02 01:35:27.317054: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4d02fb00 next 872 of size 8192
2019-10-02 01:35:27.317060: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4d031b00 next 873 of size 256
2019-10-02 01:35:27.317065: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4d031c00 next 874 of size 8192
2019-10-02 01:35:27.317071: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4d033c00 next 875 of size 67108864
2019-10-02 01:35:27.317076: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe51033c00 next 876 of size 16777216
2019-10-02 01:35:27.317082: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe52033c00 next 877 of size 256
2019-10-02 01:35:27.317087: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe52033d00 next 878 of size 67108864
2019-10-02 01:35:27.317093: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe56033d00 next 879 of size 256
2019-10-02 01:35:27.317099: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe56033e00 next 880 of size 8192
2019-10-02 01:35:27.317105: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe56035e00 next 881 of size 256
2019-10-02 01:35:27.317111: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe56035f00 next 882 of size 16777216
2019-10-02 01:35:27.317116: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe57035f00 next 883 of size 8192
2019-10-02 01:35:27.317122: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe57037f00 next 884 of size 256
2019-10-02 01:35:27.317128: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe57038000 next 885 of size 67108864
2019-10-02 01:35:27.317133: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe5b038000 next 886 of size 16777216
2019-10-02 01:35:27.317139: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe5c038000 next 887 of size 16777216
2019-10-02 01:35:27.317144: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe5d038000 next 888 of size 16777216
2019-10-02 01:35:27.317150: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe5e038000 next 889 of size 16777216
2019-10-02 01:35:27.317156: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe5f038000 next 890 of size 67108864
2019-10-02 01:35:27.317162: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe63038000 next 891 of size 256
2019-10-02 01:35:27.317168: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe63038100 next 892 of size 16777216
2019-10-02 01:35:27.317174: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe64038100 next 893 of size 256
2019-10-02 01:35:27.317180: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe64038200 next 894 of size 256
2019-10-02 01:35:27.317185: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe64038300 next 895 of size 8192
2019-10-02 01:35:27.317191: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6403a300 next 896 of size 256
2019-10-02 01:35:27.317196: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6403a400 next 897 of size 8192
2019-10-02 01:35:27.317201: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6403c400 next 898 of size 8192
2019-10-02 01:35:27.317207: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6403e400 next 899 of size 256
2019-10-02 01:35:27.317212: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6403e500 next 900 of size 8192
2019-10-02 01:35:27.317218: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe64040500 next 901 of size 16777216
2019-10-02 01:35:27.317223: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe65040500 next 902 of size 67108864
2019-10-02 01:35:27.317232: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe69040500 next 903 of size 16777216
2019-10-02 01:35:27.317238: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6a040500 next 904 of size 256
2019-10-02 01:35:27.317243: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6a040600 next 905 of size 32768
2019-10-02 01:35:27.317249: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6a048600 next 906 of size 256
2019-10-02 01:35:27.317255: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6a048700 next 907 of size 32768
2019-10-02 01:35:27.317260: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6a050700 next 908 of size 16777216
2019-10-02 01:35:27.317266: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6b050700 next 909 of size 256
2019-10-02 01:35:27.317271: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6b050800 next 910 of size 8192
2019-10-02 01:35:27.317277: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6b052800 next 911 of size 256
2019-10-02 01:35:27.317282: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6b052900 next 912 of size 8192
2019-10-02 01:35:27.317288: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6b054900 next 913 of size 8192
2019-10-02 01:35:27.317294: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6b056900 next 914 of size 256
2019-10-02 01:35:27.317300: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6b056a00 next 915 of size 8192
2019-10-02 01:35:27.317305: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6b058a00 next 916 of size 67108864
2019-10-02 01:35:27.317310: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6f058a00 next 917 of size 16777216
2019-10-02 01:35:27.317316: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe70058a00 next 918 of size 16777216
2019-10-02 01:35:27.317322: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe71058a00 next 919 of size 256
2019-10-02 01:35:27.317327: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe71058b00 next 920 of size 8192
2019-10-02 01:35:27.317333: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7105ab00 next 921 of size 256
2019-10-02 01:35:27.317338: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7105ac00 next 922 of size 8192
2019-10-02 01:35:27.317344: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7105cc00 next 923 of size 8192
2019-10-02 01:35:27.317349: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7105ec00 next 924 of size 256
2019-10-02 01:35:27.317354: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7105ed00 next 925 of size 8192
2019-10-02 01:35:27.317360: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe71060d00 next 926 of size 16777216
2019-10-02 01:35:27.317365: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe72060d00 next 927 of size 256
2019-10-02 01:35:27.317371: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe72060e00 next 928 of size 67108864
2019-10-02 01:35:27.317376: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe76060e00 next 929 of size 67108864
2019-10-02 01:35:27.317382: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7a060e00 next 930 of size 67108864
2019-10-02 01:35:27.317387: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e060e00 next 931 of size 8192
2019-10-02 01:35:27.317392: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e062e00 next 932 of size 256
2019-10-02 01:35:27.317398: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e062f00 next 933 of size 8192
2019-10-02 01:35:27.317403: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e064f00 next 934 of size 256
2019-10-02 01:35:27.317411: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e065000 next 935 of size 256
2019-10-02 01:35:27.317418: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e065100 next 622 of size 8388608
2019-10-02 01:35:27.317423: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e865100 next 1016 of size 256
2019-10-02 01:35:27.317429: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e865200 next 564 of size 512
2019-10-02 01:35:27.317435: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e865400 next 604 of size 256
2019-10-02 01:35:27.317441: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e865500 next 624 of size 256
2019-10-02 01:35:27.317446: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e865600 next 613 of size 512
2019-10-02 01:35:27.317452: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e865800 next 1031 of size 256
2019-10-02 01:35:27.317458: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e865900 next 1033 of size 256
2019-10-02 01:35:27.317464: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e865a00 next 1034 of size 256
2019-10-02 01:35:27.317469: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e865b00 next 619 of size 256
2019-10-02 01:35:27.317475: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e865c00 next 611 of size 256
2019-10-02 01:35:27.317480: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e865d00 next 614 of size 256
2019-10-02 01:35:27.317486: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e865e00 next 680 of size 256
2019-10-02 01:35:27.317492: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e865f00 next 659 of size 256
2019-10-02 01:35:27.317498: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e866000 next 566 of size 256
2019-10-02 01:35:27.317504: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e866100 next 649 of size 256
2019-10-02 01:35:27.317510: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e866200 next 617 of size 768
2019-10-02 01:35:27.317516: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e866500 next 608 of size 256
2019-10-02 01:35:27.317522: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e866600 next 588 of size 256
2019-10-02 01:35:27.317528: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e866700 next 644 of size 256
2019-10-02 01:35:27.317533: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e866800 next 661 of size 256
2019-10-02 01:35:27.317539: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e866900 next 591 of size 256
2019-10-02 01:35:27.317545: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e866a00 next 944 of size 256
2019-10-02 01:35:27.317550: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e866b00 next 945 of size 256
2019-10-02 01:35:27.317556: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e866c00 next 946 of size 256
2019-10-02 01:35:27.317561: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e866d00 next 947 of size 256
2019-10-02 01:35:27.317567: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e866e00 next 948 of size 8192
2019-10-02 01:35:27.317572: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e868e00 next 949 of size 256
2019-10-02 01:35:27.317578: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e868f00 next 950 of size 8192
2019-10-02 01:35:27.317584: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e86af00 next 952 of size 256
2019-10-02 01:35:27.317594: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e86b000 next 953 of size 256
2019-10-02 01:35:27.317600: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e86b100 next 954 of size 256
2019-10-02 01:35:27.317606: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e86b200 next 956 of size 256
2019-10-02 01:35:27.317611: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e86b300 next 961 of size 256
2019-10-02 01:35:27.317617: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e86b400 next 962 of size 256
2019-10-02 01:35:27.317623: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e86b500 next 963 of size 256
2019-10-02 01:35:27.317629: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e86b600 next 964 of size 8192
2019-10-02 01:35:27.317635: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e86d600 next 965 of size 8192
2019-10-02 01:35:27.317641: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e86f600 next 966 of size 256
2019-10-02 01:35:27.317646: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e86f700 next 967 of size 8192
2019-10-02 01:35:27.317652: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e871700 next 968 of size 256
2019-10-02 01:35:27.317658: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e871800 next 969 of size 8192
2019-10-02 01:35:27.317664: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e873800 next 971 of size 256
2019-10-02 01:35:27.317669: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e873900 next 972 of size 256
2019-10-02 01:35:27.317675: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e873a00 next 973 of size 256
2019-10-02 01:35:27.317680: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e873b00 next 974 of size 8192
2019-10-02 01:35:27.317686: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e875b00 next 975 of size 8192
2019-10-02 01:35:27.317692: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e877b00 next 633 of size 61184
2019-10-02 01:35:27.317698: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e886a00 next 937 of size 67108864
2019-10-02 01:35:27.317704: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe82886a00 next 938 of size 256
2019-10-02 01:35:27.317710: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe82886b00 next 939 of size 256
2019-10-02 01:35:27.317715: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe82886c00 next 940 of size 256
2019-10-02 01:35:27.317721: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe82886d00 next 941 of size 256
2019-10-02 01:35:27.317726: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe82886e00 next 942 of size 256
2019-10-02 01:35:27.317732: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe82886f00 next 943 of size 16777216
2019-10-02 01:35:27.317738: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe83886f00 next 951 of size 16777216
2019-10-02 01:35:27.317743: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe84886f00 next 955 of size 67108864
2019-10-02 01:35:27.317749: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe88886f00 next 957 of size 67108864
2019-10-02 01:35:27.317754: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe8c886f00 next 958 of size 16777216
2019-10-02 01:35:27.317760: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe8d886f00 next 959 of size 67108864
2019-10-02 01:35:27.317765: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe91886f00 next 960 of size 16777216
2019-10-02 01:35:27.317771: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe92886f00 next 970 of size 16777216
2019-10-02 01:35:27.317780: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe93886f00 next 976 of size 256
2019-10-02 01:35:27.317786: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe93887000 next 977 of size 32768
2019-10-02 01:35:27.317791: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9388f000 next 978 of size 67108864
2019-10-02 01:35:27.317797: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9788f000 next 979 of size 16777216
2019-10-02 01:35:27.317802: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9888f000 next 980 of size 256
2019-10-02 01:35:27.317808: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9888f100 next 981 of size 16777216
2019-10-02 01:35:27.317813: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9988f100 next 982 of size 256
2019-10-02 01:35:27.317819: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9988f200 next 983 of size 256
2019-10-02 01:35:27.317825: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9988f300 next 984 of size 16777216
2019-10-02 01:35:27.317831: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9a88f300 next 985 of size 67108864
2019-10-02 01:35:27.317837: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9e88f300 next 986 of size 256
2019-10-02 01:35:27.317843: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9e88f400 next 987 of size 256
2019-10-02 01:35:27.317849: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9e88f500 next 988 of size 256
2019-10-02 01:35:27.317854: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9e88f600 next 989 of size 256
2019-10-02 01:35:27.317860: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9e88f700 next 990 of size 256
2019-10-02 01:35:27.317866: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9e88f800 next 991 of size 256
2019-10-02 01:35:27.317872: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9e88f900 next 992 of size 16777216
2019-10-02 01:35:27.317877: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9f88f900 next 993 of size 8192
2019-10-02 01:35:27.317883: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9f891900 next 994 of size 256
2019-10-02 01:35:27.317889: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9f891a00 next 995 of size 8192
2019-10-02 01:35:27.317894: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9f893a00 next 996 of size 8192
2019-10-02 01:35:27.317900: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9f895a00 next 997 of size 256
2019-10-02 01:35:27.317905: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9f895b00 next 998 of size 8192
2019-10-02 01:35:27.317910: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9f897b00 next 999 of size 67108864
2019-10-02 01:35:27.317916: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efea3897b00 next 1000 of size 67108864
2019-10-02 01:35:27.317922: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efea7897b00 next 1001 of size 256
2019-10-02 01:35:27.317928: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efea7897c00 next 1002 of size 256
2019-10-02 01:35:27.317933: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efea7897d00 next 1003 of size 256
2019-10-02 01:35:27.317939: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efea7897e00 next 1004 of size 256
2019-10-02 01:35:27.317945: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efea7897f00 next 1005 of size 67108864
2019-10-02 01:35:27.317950: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efeab897f00 next 1006 of size 256
2019-10-02 01:35:27.317956: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efeab898000 next 1007 of size 256
2019-10-02 01:35:27.317966: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efeab898100 next 601 of size 16777216
2019-10-02 01:35:27.317972: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efeac898100 next 594 of size 8388608
2019-10-02 01:35:27.317979: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efead098100 next 620 of size 268435456
2019-10-02 01:35:27.317984: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd098100 next 629 of size 131072
2019-10-02 01:35:27.317990: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b8100 next 572 of size 256
2019-10-02 01:35:27.317996: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b8200 next 587 of size 256
2019-10-02 01:35:27.318001: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b8300 next 696 of size 256
2019-10-02 01:35:27.318007: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b8400 next 623 of size 256
2019-10-02 01:35:27.318013: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b8500 next 1037 of size 512
2019-10-02 01:35:27.318027: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b8700 next 1041 of size 512
2019-10-02 01:35:27.318033: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b8900 next 1047 of size 256
2019-10-02 01:35:27.318039: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b8a00 next 1059 of size 256
2019-10-02 01:35:27.318044: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b8b00 next 1060 of size 256
2019-10-02 01:35:27.318050: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b8c00 next 1061 of size 256
2019-10-02 01:35:27.318056: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b8d00 next 1076 of size 512
2019-10-02 01:35:27.318061: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b8f00 next 1077 of size 512
2019-10-02 01:35:27.318067: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b9100 next 1079 of size 256
2019-10-02 01:35:27.318072: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b9200 next 1098 of size 256
2019-10-02 01:35:27.318078: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b9300 next 1102 of size 256
2019-10-02 01:35:27.318084: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b9400 next 1106 of size 256
2019-10-02 01:35:27.318089: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b9500 next 1113 of size 512
2019-10-02 01:35:27.318095: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b9700 next 1121 of size 512
2019-10-02 01:35:27.318101: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b9900 next 1122 of size 256
2019-10-02 01:35:27.318106: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b9a00 next 1123 of size 256
2019-10-02 01:35:27.318112: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b9b00 next 1125 of size 256
2019-10-02 01:35:27.318117: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b9c00 next 1126 of size 256
2019-10-02 01:35:27.318123: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b9d00 next 1127 of size 256
2019-10-02 01:35:27.318128: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b9e00 next 1021 of size 256
2019-10-02 01:35:27.318134: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b9f00 next 1022 of size 256
2019-10-02 01:35:27.318140: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0ba000 next 1023 of size 256
2019-10-02 01:35:27.318146: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0ba100 next 1024 of size 256
2019-10-02 01:35:27.318156: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0ba200 next 1025 of size 256
2019-10-02 01:35:27.318162: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0ba300 next 1026 of size 256
2019-10-02 01:35:27.318168: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0ba400 next 596 of size 131072000
2019-10-02 01:35:27.318174: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efec4dba400 next 571 of size 268435456
2019-10-02 01:35:27.318180: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efed4dba400 next 558 of size 131072
2019-10-02 01:35:27.318186: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efed4dda400 next 597 of size 4194304000
2019-10-02 01:35:27.318192: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effcedda400 next 595 of size 67108864
2019-10-02 01:35:27.318198: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effd2dda400 next 638 of size 268435456
2019-10-02 01:35:27.318205: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effe2dda400 next 626 of size 131072
2019-10-02 01:35:27.318211: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effe2dfa400 next 648 of size 16777216
2019-10-02 01:35:27.318216: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effe3dfa400 next 674 of size 16777216
2019-10-02 01:35:27.318222: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effe4dfa400 next 589 of size 8192
2019-10-02 01:35:27.318228: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effe4dfc400 next 615 of size 8192
2019-10-02 01:35:27.318233: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effe4dfe400 next 658 of size 8192
2019-10-02 01:35:27.318239: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effe4e00400 next 698 of size 8192
2019-10-02 01:35:27.318245: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effe4e02400 next 676 of size 67108864
2019-10-02 01:35:27.318251: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effe8e02400 next 646 of size 8192
2019-10-02 01:35:27.318256: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effe8e04400 next 692 of size 16777216
2019-10-02 01:35:27.318263: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effe9e04400 next 682 of size 16384000
2019-10-02 01:35:27.318269: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effeada4400 next 681 of size 32768
2019-10-02 01:35:27.318275: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effeadac400 next 675 of size 8192
2019-10-02 01:35:27.318285: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effeadae400 next 612 of size 16384000
2019-10-02 01:35:27.318291: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effebd4e400 next 568 of size 16384000
2019-10-02 01:35:27.318296: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effeccee400 next 573 of size 16384000
2019-10-02 01:35:27.318303: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effedc8e400 next 625 of size 16384000
2019-10-02 01:35:27.318308: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effeec2e400 next 565 of size 8192
2019-10-02 01:35:27.318314: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effeec30400 next 670 of size 16384000
2019-10-02 01:35:27.318320: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effefbd0400 next 560 of size 67108864
2019-10-02 01:35:27.318325: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efff3bd0400 next 640 of size 16384000
2019-10-02 01:35:27.318332: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efff4b70400 next 561 of size 16384000
2019-10-02 01:35:27.318337: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efff5b10400 next 651 of size 16384000
2019-10-02 01:35:27.318346: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efff6ab0400 next 656 of size 16384000
2019-10-02 01:35:27.318352: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efff7a50400 next 645 of size 16384000
2019-10-02 01:35:27.318358: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efff89f0400 next 641 of size 8192
2019-10-02 01:35:27.318364: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efff89f2400 next 654 of size 8192
2019-10-02 01:35:27.318370: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efff89f4400 next 556 of size 16384000
2019-10-02 01:35:27.318376: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efff9994400 next 574 of size 16384000
2019-10-02 01:35:27.318382: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffa934400 next 570 of size 8192
2019-10-02 01:35:27.318387: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffa936400 next 562 of size 8192
2019-10-02 01:35:27.318393: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffa938400 next 1008 of size 8192
2019-10-02 01:35:27.318399: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffa93a400 next 666 of size 8192
2019-10-02 01:35:27.318405: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffa93c400 next 567 of size 8192
2019-10-02 01:35:27.318410: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffa93e400 next 557 of size 16384000
2019-10-02 01:35:27.318417: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffb8de400 next 691 of size 16384000
2019-10-02 01:35:27.318422: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffc87e400 next 634 of size 8192
2019-10-02 01:35:27.318429: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffc880400 next 631 of size 8192
2019-10-02 01:35:27.318434: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffc882400 next 672 of size 8192
2019-10-02 01:35:27.318440: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffc884400 next 679 of size 8192
2019-10-02 01:35:27.318446: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffc886400 next 669 of size 16384000
2019-10-02 01:35:27.318452: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffd826400 next 628 of size 8192
2019-10-02 01:35:27.318458: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffd828400 next 610 of size 8192
2019-10-02 01:35:27.318464: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffd82a400 next 653 of size 8192
2019-10-02 01:35:27.318470: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffd82c400 next 650 of size 8192
2019-10-02 01:35:27.318475: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffd82e400 next 713 of size 8192
2019-10-02 01:35:27.318481: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffd830400 next 632 of size 8192
2019-10-02 01:35:27.318486: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffd832400 next 575 of size 16777216
2019-10-02 01:35:27.318492: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffe832400 next 664 of size 8192
2019-10-02 01:35:27.318498: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffe834400 next 618 of size 8192
2019-10-02 01:35:27.318503: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffe836400 next 621 of size 8192
2019-10-02 01:35:27.318509: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffe838400 next 555 of size 16777216
2019-10-02 01:35:27.318514: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effff838400 next 602 of size 8192
2019-10-02 01:35:27.318520: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effff83a400 next 606 of size 32768
2019-10-02 01:35:27.318525: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effff842400 next 600 of size 8192
2019-10-02 01:35:27.318534: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effff844400 next 609 of size 8192
2019-10-02 01:35:27.318540: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effff846400 next 593 of size 8192
2019-10-02 01:35:27.318546: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effff848400 next 592 of size 8192
2019-10-02 01:35:27.318551: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effff84a400 next 652 of size 8192
2019-10-02 01:35:27.318557: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effff84c400 next 1012 of size 16777216
2019-10-02 01:35:27.318562: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f000084c400 next 647 of size 8192
2019-10-02 01:35:27.318568: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f000084e400 next 660 of size 8192
2019-10-02 01:35:27.318573: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0000850400 next 603 of size 8192
2019-10-02 01:35:27.318579: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0000852400 next 673 of size 8192
2019-10-02 01:35:27.318585: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0000854400 next 1020 of size 8192
2019-10-02 01:35:27.318591: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0000856400 next 1015 of size 8192
2019-10-02 01:35:27.318596: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0000858400 next 643 of size 16777216
2019-10-02 01:35:27.318602: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0001858400 next 1009 of size 67108864
2019-10-02 01:35:27.318608: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0005858400 next 1010 of size 16777216
2019-10-02 01:35:27.318613: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0006858400 next 616 of size 16777216
2019-10-02 01:35:27.318619: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0007858400 next 663 of size 16777216
2019-10-02 01:35:27.318624: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0008858400 next 1013 of size 8192
2019-10-02 01:35:27.318630: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f000885a400 next 1014 of size 8192
2019-10-02 01:35:27.318635: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f000885c400 next 671 of size 16777216
2019-10-02 01:35:27.318641: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f000985c400 next 607 of size 67108864
2019-10-02 01:35:27.318646: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f000d85c400 next 635 of size 16777216
2019-10-02 01:35:27.318652: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f000e85c400 next 662 of size 16777216
2019-10-02 01:35:27.318657: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f000f85c400 next 636 of size 8192
2019-10-02 01:35:27.318663: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f000f85e400 next 668 of size 8192
2019-10-02 01:35:27.318668: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f000f860400 next 642 of size 8192
2019-10-02 01:35:27.318674: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f000f862400 next 667 of size 32768
2019-10-02 01:35:27.318679: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f000f86a400 next 569 of size 8192
2019-10-02 01:35:27.318685: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f000f86c400 next 639 of size 67108864
2019-10-02 01:35:27.318691: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f001386c400 next 1017 of size 8192
2019-10-02 01:35:27.318696: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f001386e400 next 1018 of size 8192
2019-10-02 01:35:27.318702: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0013870400 next 627 of size 8192
2019-10-02 01:35:27.318711: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0013872400 next 598 of size 8192
2019-10-02 01:35:27.318717: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0013874400 next 655 of size 8192
2019-10-02 01:35:27.318722: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0013876400 next 678 of size 8192
2019-10-02 01:35:27.318728: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0013878400 next 599 of size 8192
2019-10-02 01:35:27.318734: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f001387a400 next 689 of size 8192
2019-10-02 01:35:27.318739: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f001387c400 next 693 of size 8192
2019-10-02 01:35:27.318745: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f001387e400 next 563 of size 32768
2019-10-02 01:35:27.318751: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0013886400 next 1019 of size 16777216
2019-10-02 01:35:27.318757: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0014886400 next 719 of size 16777216
2019-10-02 01:35:27.318763: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0015886400 next 695 of size 8192
2019-10-02 01:35:27.318769: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0015888400 next 694 of size 16777216
2019-10-02 01:35:27.318774: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0016888400 next 1027 of size 16777216
2019-10-02 01:35:27.318780: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0017888400 next 1028 of size 16777216
2019-10-02 01:35:27.318785: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0018888400 next 1029 of size 16777216
2019-10-02 01:35:27.318791: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0019888400 next 1030 of size 67108864
2019-10-02 01:35:27.318797: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f001d888400 next 1032 of size 8192
2019-10-02 01:35:27.318803: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f001d88a400 next 1035 of size 67108864
2019-10-02 01:35:27.318809: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f002188a400 next 1036 of size 16777216
2019-10-02 01:35:27.318815: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f002288a400 next 1038 of size 8192
2019-10-02 01:35:27.318821: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f002288c400 next 1039 of size 67108864
2019-10-02 01:35:27.318826: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f002688c400 next 1040 of size 8192
2019-10-02 01:35:27.318832: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f002688e400 next 1042 of size 8192
2019-10-02 01:35:27.318839: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0026890400 next 1043 of size 16777216
2019-10-02 01:35:27.318844: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0027890400 next 1044 of size 8192
2019-10-02 01:35:27.318850: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0027892400 next 1045 of size 16777216
2019-10-02 01:35:27.318855: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0028892400 next 1046 of size 8192
2019-10-02 01:35:27.318861: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0028894400 next 1048 of size 16777216
2019-10-02 01:35:27.318867: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0029894400 next 1049 of size 8192
2019-10-02 01:35:27.318873: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0029896400 next 1050 of size 16777216
2019-10-02 01:35:27.318878: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f002a896400 next 1051 of size 8192
2019-10-02 01:35:27.318884: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f002a898400 next 1052 of size 8192
2019-10-02 01:35:27.318892: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f002a89a400 next 1053 of size 8192
2019-10-02 01:35:27.318898: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f002a89c400 next 1054 of size 67108864
2019-10-02 01:35:27.318904: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f002e89c400 next 1055 of size 16777216
2019-10-02 01:35:27.318910: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f002f89c400 next 1056 of size 8192
2019-10-02 01:35:27.318916: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f002f89e400 next 1057 of size 32768
2019-10-02 01:35:27.318921: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f002f8a6400 next 1058 of size 8192
2019-10-02 01:35:27.318926: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f002f8a8400 next 1062 of size 16777216
2019-10-02 01:35:27.318932: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00308a8400 next 1063 of size 8192
2019-10-02 01:35:27.318938: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00308aa400 next 1064 of size 32768
2019-10-02 01:35:27.318944: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00308b2400 next 1065 of size 8192
2019-10-02 01:35:27.318950: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00308b4400 next 1066 of size 67108864
2019-10-02 01:35:27.318955: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00348b4400 next 1067 of size 8192
2019-10-02 01:35:27.318961: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00348b6400 next 1068 of size 8192
2019-10-02 01:35:27.318966: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00348b8400 next 1069 of size 8192
2019-10-02 01:35:27.318972: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00348ba400 next 1070 of size 8192
2019-10-02 01:35:27.318977: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00348bc400 next 1071 of size 32768
2019-10-02 01:35:27.318983: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00348c4400 next 1072 of size 8192
2019-10-02 01:35:27.318989: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00348c6400 next 1073 of size 8192
2019-10-02 01:35:27.318994: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00348c8400 next 1074 of size 67108864
2019-10-02 01:35:27.319000: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00388c8400 next 1075 of size 8192
2019-10-02 01:35:27.319006: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00388ca400 next 1078 of size 8192
2019-10-02 01:35:27.319012: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00388cc400 next 1080 of size 16777216
2019-10-02 01:35:27.319017: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00398cc400 next 1081 of size 8192
2019-10-02 01:35:27.319023: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00398ce400 next 1082 of size 16777216
2019-10-02 01:35:27.319028: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f003a8ce400 next 1083 of size 8192
2019-10-02 01:35:27.319034: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f003a8d0400 next 1084 of size 8192
2019-10-02 01:35:27.319040: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f003a8d2400 next 1085 of size 8192
2019-10-02 01:35:27.319046: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f003a8d4400 next 1086 of size 8192
2019-10-02 01:35:27.319051: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f003a8d6400 next 1087 of size 16777216
2019-10-02 01:35:27.319056: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f003b8d6400 next 1088 of size 67108864
2019-10-02 01:35:27.319062: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f003f8d6400 next 1089 of size 67108864
2019-10-02 01:35:27.319071: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00438d6400 next 1090 of size 8192
2019-10-02 01:35:27.319077: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00438d8400 next 1091 of size 8192
2019-10-02 01:35:27.319082: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00438da400 next 1092 of size 8192
2019-10-02 01:35:27.319089: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00438dc400 next 1093 of size 16777216
2019-10-02 01:35:27.319095: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00448dc400 next 1094 of size 8192
2019-10-02 01:35:27.319100: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00448de400 next 1095 of size 16777216
2019-10-02 01:35:27.319106: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00458de400 next 1096 of size 8192
2019-10-02 01:35:27.319112: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00458e0400 next 1097 of size 8192
2019-10-02 01:35:27.319117: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00458e2400 next 1099 of size 67108864
2019-10-02 01:35:27.319123: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00498e2400 next 1100 of size 16777216
2019-10-02 01:35:27.319129: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f004a8e2400 next 1101 of size 8192
2019-10-02 01:35:27.319134: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f004a8e4400 next 1103 of size 16777216
2019-10-02 01:35:27.319140: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f004b8e4400 next 1104 of size 32768
2019-10-02 01:35:27.319145: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f004b8ec400 next 1105 of size 8192
2019-10-02 01:35:27.319151: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f004b8ee400 next 1107 of size 16777216
2019-10-02 01:35:27.319156: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f004c8ee400 next 1108 of size 32768
2019-10-02 01:35:27.319162: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f004c8f6400 next 1109 of size 16777216
2019-10-02 01:35:27.319168: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f004d8f6400 next 1110 of size 8192
2019-10-02 01:35:27.319173: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f004d8f8400 next 1111 of size 8192
2019-10-02 01:35:27.319179: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f004d8fa400 next 1112 of size 8192
2019-10-02 01:35:27.319184: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f004d8fc400 next 1114 of size 8192
2019-10-02 01:35:27.319191: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f004d8fe400 next 1115 of size 67108864
2019-10-02 01:35:27.319196: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00518fe400 next 1116 of size 8192
2019-10-02 01:35:27.319202: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0051900400 next 1117 of size 8192
2019-10-02 01:35:27.319207: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0051902400 next 1118 of size 16777216
2019-10-02 01:35:27.319213: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0052902400 next 1119 of size 8192
2019-10-02 01:35:27.319218: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0052904400 next 1120 of size 8192
2019-10-02 01:35:27.319224: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0052906400 next 1124 of size 67108864
2019-10-02 01:35:27.319230: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0056906400 next 18446744073709551615 of size 74560768
2019-10-02 01:35:27.319237: I tensorflow/core/common_runtime/bfc_allocator.cc:914]      Summary of in-use Chunks by size: 
2019-10-02 01:35:27.319253: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 242 Chunks of size 256 totalling 60.5KiB
2019-10-02 01:35:27.319260: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 45 Chunks of size 512 totalling 22.5KiB
2019-10-02 01:35:27.319270: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 768 totalling 768B
2019-10-02 01:35:27.319277: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 1280 totalling 1.2KiB
2019-10-02 01:35:27.319283: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 4096 totalling 4.0KiB
2019-10-02 01:35:27.319289: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 5376 totalling 5.2KiB
2019-10-02 01:35:27.319295: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 6912 totalling 6.8KiB
2019-10-02 01:35:27.319337: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 448 Chunks of size 8192 totalling 3.50MiB
2019-10-02 01:35:27.319347: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 9984 totalling 9.8KiB
2019-10-02 01:35:27.319357: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 11264 totalling 11.0KiB
2019-10-02 01:35:27.319367: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 13056 totalling 12.8KiB
2019-10-02 01:35:27.319399: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 45 Chunks of size 32768 totalling 1.41MiB
2019-10-02 01:35:27.319410: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 56064 totalling 54.8KiB
2019-10-02 01:35:27.319420: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 61184 totalling 59.8KiB
2019-10-02 01:35:27.319431: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 7 Chunks of size 131072 totalling 896.0KiB
2019-10-02 01:35:27.319440: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 3 Chunks of size 8388608 totalling 24.00MiB
2019-10-02 01:35:27.319450: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 32 Chunks of size 16384000 totalling 500.00MiB
2019-10-02 01:35:27.319461: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 192 Chunks of size 16777216 totalling 3.00GiB
2019-10-02 01:35:27.319474: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 97 Chunks of size 67108864 totalling 6.06GiB
2019-10-02 01:35:27.319488: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 74560768 totalling 71.11MiB
2019-10-02 01:35:27.319499: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 131072000 totalling 125.00MiB
2019-10-02 01:35:27.319511: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 262144000 totalling 250.00MiB
2019-10-02 01:35:27.319519: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 3 Chunks of size 268435456 totalling 768.00MiB
2019-10-02 01:35:27.319526: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 4194304000 totalling 3.91GiB
2019-10-02 01:35:27.319532: I tensorflow/core/common_runtime/bfc_allocator.cc:921] Sum Total of in-use chunks: 14.67GiB
2019-10-02 01:35:27.319538: I tensorflow/core/common_runtime/bfc_allocator.cc:923] total_region_allocated_bytes_: 15753943296 memory_limit_: 15753943450 available bytes: 154 curr_region_allocation_bytes_: 31507887104
2019-10-02 01:35:27.319551: I tensorflow/core/common_runtime/bfc_allocator.cc:929] Stats: 
Limit:                 15753943450
InUse:                 15753943296
MaxInUse:              15753943296
NumAllocs:                    2276
MaxAllocSize:           4194304000

2019-10-02 01:35:27.319617: W tensorflow/core/common_runtime/bfc_allocator.cc:424] ****************************************************************************************************
2019-10-02 01:35:27.325805: W tensorflow/core/common_runtime/bfc_allocator.cc:419] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4B (rounded to 256).  Current allocation summary follows.
2019-10-02 01:35:27.325991: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (256): 	Total Chunks: 242, Chunks in use: 242. 60.5KiB allocated for chunks. 60.5KiB in use in bin. 4.2KiB client-requested in use in bin.
2019-10-02 01:35:27.326021: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (512): 	Total Chunks: 46, Chunks in use: 46. 23.2KiB allocated for chunks. 23.2KiB in use in bin. 23.0KiB client-requested in use in bin.
2019-10-02 01:35:27.326065: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (1024): 	Total Chunks: 1, Chunks in use: 1. 1.2KiB allocated for chunks. 1.2KiB in use in bin. 1.0KiB client-requested in use in bin.
2019-10-02 01:35:27.326088: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (2048): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-10-02 01:35:27.326109: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (4096): 	Total Chunks: 3, Chunks in use: 3. 16.0KiB allocated for chunks. 16.0KiB in use in bin. 12.0KiB client-requested in use in bin.
2019-10-02 01:35:27.326131: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (8192): 	Total Chunks: 451, Chunks in use: 451. 3.53MiB allocated for chunks. 3.53MiB in use in bin. 3.52MiB client-requested in use in bin.
2019-10-02 01:35:27.326152: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (16384): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-10-02 01:35:27.326174: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (32768): 	Total Chunks: 47, Chunks in use: 47. 1.52MiB allocated for chunks. 1.52MiB in use in bin. 1.47MiB client-requested in use in bin.
2019-10-02 01:35:27.326195: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (65536): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-10-02 01:35:27.326219: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (131072): 	Total Chunks: 7, Chunks in use: 7. 896.0KiB allocated for chunks. 896.0KiB in use in bin. 896.0KiB client-requested in use in bin.
2019-10-02 01:35:27.326241: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (262144): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-10-02 01:35:27.326260: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (524288): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-10-02 01:35:27.326279: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (1048576): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-10-02 01:35:27.326297: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (2097152): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-10-02 01:35:27.326317: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (4194304): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-10-02 01:35:27.326340: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (8388608): 	Total Chunks: 35, Chunks in use: 35. 524.00MiB allocated for chunks. 524.00MiB in use in bin. 524.00MiB client-requested in use in bin.
2019-10-02 01:35:27.326361: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (16777216): 	Total Chunks: 192, Chunks in use: 192. 3.00GiB allocated for chunks. 3.00GiB in use in bin. 3.00GiB client-requested in use in bin.
2019-10-02 01:35:27.326380: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (33554432): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-10-02 01:35:27.326400: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (67108864): 	Total Chunks: 99, Chunks in use: 99. 6.25GiB allocated for chunks. 6.25GiB in use in bin. 6.25GiB client-requested in use in bin.
2019-10-02 01:35:27.326422: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (134217728): 	Total Chunks: 1, Chunks in use: 1. 250.00MiB allocated for chunks. 250.00MiB in use in bin. 250.00MiB client-requested in use in bin.
2019-10-02 01:35:27.326454: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (268435456): 	Total Chunks: 4, Chunks in use: 4. 4.66GiB allocated for chunks. 4.66GiB in use in bin. 4.66GiB client-requested in use in bin.
2019-10-02 01:35:27.326475: I tensorflow/core/common_runtime/bfc_allocator.cc:885] Bin for 256B was 256B, Chunk State: 
2019-10-02 01:35:27.326493: I tensorflow/core/common_runtime/bfc_allocator.cc:898] Next region of size 15753943296
2019-10-02 01:35:27.326516: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0000000 next 1 of size 1280
2019-10-02 01:35:27.326535: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0000500 next 2 of size 256
2019-10-02 01:35:27.326553: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0000600 next 3 of size 256
2019-10-02 01:35:27.326569: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0000700 next 4 of size 256
2019-10-02 01:35:27.326586: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0000800 next 5 of size 256
2019-10-02 01:35:27.326603: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0000900 next 6 of size 256
2019-10-02 01:35:27.326619: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0000a00 next 7 of size 256
2019-10-02 01:35:27.326636: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0000b00 next 8 of size 256
2019-10-02 01:35:27.326653: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0000c00 next 9 of size 256
2019-10-02 01:35:27.326669: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0000d00 next 10 of size 256
2019-10-02 01:35:27.326685: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0000e00 next 11 of size 256
2019-10-02 01:35:27.326702: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0000f00 next 12 of size 256
2019-10-02 01:35:27.326718: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0001000 next 13 of size 256
2019-10-02 01:35:27.326734: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0001100 next 14 of size 256
2019-10-02 01:35:27.326750: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0001200 next 15 of size 256
2019-10-02 01:35:27.326767: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0001300 next 16 of size 256
2019-10-02 01:35:27.326785: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0001400 next 17 of size 256
2019-10-02 01:35:27.326802: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0001500 next 18 of size 256
2019-10-02 01:35:27.326818: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0001600 next 19 of size 256
2019-10-02 01:35:27.326836: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0001700 next 20 of size 256
2019-10-02 01:35:27.326854: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0001800 next 21 of size 256
2019-10-02 01:35:27.326871: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0001900 next 22 of size 256
2019-10-02 01:35:27.326888: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0001a00 next 23 of size 256
2019-10-02 01:35:27.326902: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0001b00 next 24 of size 256
2019-10-02 01:35:27.326919: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0001c00 next 25 of size 256
2019-10-02 01:35:27.326937: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0001d00 next 26 of size 256
2019-10-02 01:35:27.326954: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0001e00 next 27 of size 256
2019-10-02 01:35:27.326971: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0001f00 next 28 of size 256
2019-10-02 01:35:27.327001: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0002000 next 29 of size 256
2019-10-02 01:35:27.327022: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0002100 next 30 of size 256
2019-10-02 01:35:27.327040: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0002200 next 31 of size 256
2019-10-02 01:35:27.327057: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0002300 next 32 of size 256
2019-10-02 01:35:27.327073: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0002400 next 33 of size 256
2019-10-02 01:35:27.327089: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0002500 next 34 of size 256
2019-10-02 01:35:27.327106: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0002600 next 35 of size 256
2019-10-02 01:35:27.327124: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0002700 next 36 of size 256
2019-10-02 01:35:27.327141: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0002800 next 37 of size 256
2019-10-02 01:35:27.327158: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0002900 next 38 of size 256
2019-10-02 01:35:27.327176: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0002a00 next 39 of size 256
2019-10-02 01:35:27.327191: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0002b00 next 40 of size 256
2019-10-02 01:35:27.327207: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0002c00 next 41 of size 256
2019-10-02 01:35:27.327222: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0002d00 next 42 of size 256
2019-10-02 01:35:27.327240: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0002e00 next 43 of size 256
2019-10-02 01:35:27.327258: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0002f00 next 44 of size 8192
2019-10-02 01:35:27.327277: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0004f00 next 45 of size 67108864
2019-10-02 01:35:27.327329: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb4004f00 next 46 of size 67108864
2019-10-02 01:35:27.327353: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb8004f00 next 47 of size 8192
2019-10-02 01:35:27.327373: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb8006f00 next 48 of size 32768
2019-10-02 01:35:27.327392: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb800ef00 next 49 of size 8192
2019-10-02 01:35:27.327408: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb8010f00 next 50 of size 16777216
2019-10-02 01:35:27.327426: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb9010f00 next 51 of size 8192
2019-10-02 01:35:27.327444: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb9012f00 next 52 of size 8192
2019-10-02 01:35:27.327460: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb9014f00 next 53 of size 32768
2019-10-02 01:35:27.327477: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb901cf00 next 54 of size 8192
2019-10-02 01:35:27.327493: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb901ef00 next 55 of size 8192
2019-10-02 01:35:27.327510: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb9020f00 next 56 of size 32768
2019-10-02 01:35:27.327527: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb9028f00 next 57 of size 8192
2019-10-02 01:35:27.327545: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb902af00 next 58 of size 8192
2019-10-02 01:35:27.327563: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb902cf00 next 59 of size 512
2019-10-02 01:35:27.327581: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb902d100 next 60 of size 8192
2019-10-02 01:35:27.327599: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb902f100 next 61 of size 16777216
2019-10-02 01:35:27.327625: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcba02f100 next 62 of size 8192
2019-10-02 01:35:27.327644: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcba031100 next 63 of size 16777216
2019-10-02 01:35:27.327661: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcbb031100 next 64 of size 8192
2019-10-02 01:35:27.327679: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcbb033100 next 65 of size 8192
2019-10-02 01:35:27.327696: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcbb035100 next 66 of size 16777216
2019-10-02 01:35:27.327712: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcbc035100 next 67 of size 32768
2019-10-02 01:35:27.327729: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcbc03d100 next 68 of size 16777216
2019-10-02 01:35:27.327746: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcbd03d100 next 69 of size 8192
2019-10-02 01:35:27.327763: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcbd03f100 next 70 of size 8192
2019-10-02 01:35:27.327779: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcbd041100 next 71 of size 8192
2019-10-02 01:35:27.327796: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcbd043100 next 72 of size 67108864
2019-10-02 01:35:27.327812: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcc1043100 next 73 of size 8192
2019-10-02 01:35:27.327829: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcc1045100 next 74 of size 67108864
2019-10-02 01:35:27.327857: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcc5045100 next 75 of size 8192
2019-10-02 01:35:27.327873: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcc5047100 next 76 of size 8192
2019-10-02 01:35:27.327889: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcc5049100 next 77 of size 8192
2019-10-02 01:35:27.327906: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcc504b100 next 78 of size 8192
2019-10-02 01:35:27.327926: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcc504d100 next 79 of size 67108864
2019-10-02 01:35:27.327942: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcc904d100 next 80 of size 16777216
2019-10-02 01:35:27.327960: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcca04d100 next 81 of size 8192
2019-10-02 01:35:27.327977: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcca04f100 next 82 of size 16777216
2019-10-02 01:35:27.327996: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efccb04f100 next 83 of size 8192
2019-10-02 01:35:27.328014: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efccb051100 next 84 of size 16777216
2019-10-02 01:35:27.328031: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efccc051100 next 85 of size 8192
2019-10-02 01:35:27.328047: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efccc053100 next 86 of size 67108864
2019-10-02 01:35:27.328063: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd0053100 next 87 of size 8192
2019-10-02 01:35:27.328079: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd0055100 next 88 of size 32768
2019-10-02 01:35:27.328096: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd005d100 next 89 of size 8192
2019-10-02 01:35:27.328112: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd005f100 next 90 of size 8192
2019-10-02 01:35:27.328128: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd0061100 next 91 of size 8192
2019-10-02 01:35:27.328144: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd0063100 next 92 of size 8192
2019-10-02 01:35:27.328170: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd0065100 next 93 of size 8192
2019-10-02 01:35:27.328190: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd0067100 next 94 of size 16777216
2019-10-02 01:35:27.328207: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd1067100 next 95 of size 8192
2019-10-02 01:35:27.328227: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd1069100 next 96 of size 16384000
2019-10-02 01:35:27.328245: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd2009100 next 97 of size 16384000
2019-10-02 01:35:27.328262: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd2fa9100 next 98 of size 8192
2019-10-02 01:35:27.328279: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd2fab100 next 99 of size 512
2019-10-02 01:35:27.328296: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd2fab300 next 100 of size 16777216
2019-10-02 01:35:27.328313: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd3fab300 next 101 of size 16777216
2019-10-02 01:35:27.328335: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd4fab300 next 102 of size 67108864
2019-10-02 01:35:27.328353: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd8fab300 next 103 of size 16777216
2019-10-02 01:35:27.328370: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd9fab300 next 104 of size 16777216
2019-10-02 01:35:27.328387: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcdafab300 next 105 of size 8192
2019-10-02 01:35:27.328403: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcdafad300 next 106 of size 512
2019-10-02 01:35:27.328418: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcdafad500 next 107 of size 16777216
2019-10-02 01:35:27.328436: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcdbfad500 next 108 of size 512
2019-10-02 01:35:27.328454: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcdbfad700 next 109 of size 16777216
2019-10-02 01:35:27.328472: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcdcfad700 next 110 of size 8192
2019-10-02 01:35:27.328488: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcdcfaf700 next 111 of size 8192
2019-10-02 01:35:27.328505: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcdcfb1700 next 112 of size 8192
2019-10-02 01:35:27.328521: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcdcfb3700 next 113 of size 8192
2019-10-02 01:35:27.328537: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcdcfb5700 next 114 of size 8192
2019-10-02 01:35:27.328553: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcdcfb7700 next 115 of size 16777216
2019-10-02 01:35:27.328569: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcddfb7700 next 116 of size 67108864
2019-10-02 01:35:27.328586: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce1fb7700 next 117 of size 512
2019-10-02 01:35:27.328603: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce1fb7900 next 118 of size 8192
2019-10-02 01:35:27.328619: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce1fb9900 next 119 of size 8192
2019-10-02 01:35:27.328636: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce1fbb900 next 120 of size 8192
2019-10-02 01:35:27.328653: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce1fbd900 next 121 of size 16777216
2019-10-02 01:35:27.328670: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce2fbd900 next 122 of size 8192
2019-10-02 01:35:27.328688: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce2fbf900 next 123 of size 8192
2019-10-02 01:35:27.328705: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce2fc1900 next 124 of size 16777216
2019-10-02 01:35:27.328730: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce3fc1900 next 125 of size 512
2019-10-02 01:35:27.328749: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce3fc1b00 next 126 of size 67108864
2019-10-02 01:35:27.328766: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce7fc1b00 next 127 of size 8192
2019-10-02 01:35:27.328783: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce7fc3b00 next 128 of size 8192
2019-10-02 01:35:27.328798: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce7fc5b00 next 129 of size 8192
2019-10-02 01:35:27.328817: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce7fc7b00 next 130 of size 8192
2019-10-02 01:35:27.328833: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce7fc9b00 next 131 of size 16777216
2019-10-02 01:35:27.328850: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce8fc9b00 next 132 of size 8192
2019-10-02 01:35:27.328866: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce8fcbb00 next 133 of size 16777216
2019-10-02 01:35:27.328883: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce9fcbb00 next 134 of size 8192
2019-10-02 01:35:27.328899: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce9fcdb00 next 135 of size 16777216
2019-10-02 01:35:27.328916: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efceafcdb00 next 136 of size 8192
2019-10-02 01:35:27.328933: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efceafcfb00 next 137 of size 8192
2019-10-02 01:35:27.328948: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efceafd1b00 next 138 of size 8192
2019-10-02 01:35:27.328964: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efceafd3b00 next 139 of size 8192
2019-10-02 01:35:27.328980: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efceafd5b00 next 140 of size 8192
2019-10-02 01:35:27.328997: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efceafd7b00 next 141 of size 8192
2019-10-02 01:35:27.329015: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efceafd9b00 next 142 of size 16777216
2019-10-02 01:35:27.329031: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcebfd9b00 next 143 of size 16777216
2019-10-02 01:35:27.329050: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcecfd9b00 next 144 of size 8192
2019-10-02 01:35:27.329068: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcecfdbb00 next 145 of size 8192
2019-10-02 01:35:27.329086: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcecfddb00 next 146 of size 8192
2019-10-02 01:35:27.329102: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcecfdfb00 next 147 of size 32768
2019-10-02 01:35:27.329117: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcecfe7b00 next 148 of size 8192
2019-10-02 01:35:27.329133: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcecfe9b00 next 149 of size 512
2019-10-02 01:35:27.329149: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcecfe9d00 next 150 of size 8192
2019-10-02 01:35:27.329165: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcecfebd00 next 151 of size 8192
2019-10-02 01:35:27.329181: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcecfedd00 next 152 of size 16777216
2019-10-02 01:35:27.329198: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcedfedd00 next 153 of size 8192
2019-10-02 01:35:27.329214: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcedfefd00 next 154 of size 16777216
2019-10-02 01:35:27.329231: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efceefefd00 next 155 of size 8192
2019-10-02 01:35:27.329257: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efceeff1d00 next 156 of size 8192
2019-10-02 01:35:27.329277: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efceeff3d00 next 157 of size 8192
2019-10-02 01:35:27.329295: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efceeff5d00 next 158 of size 67108864
2019-10-02 01:35:27.329311: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcf2ff5d00 next 159 of size 8192
2019-10-02 01:35:27.329328: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcf2ff7d00 next 160 of size 32768
2019-10-02 01:35:27.329344: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcf2fffd00 next 161 of size 16777216
2019-10-02 01:35:27.329363: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcf3fffd00 next 162 of size 16777216
2019-10-02 01:35:27.329381: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcf4fffd00 next 163 of size 67108864
2019-10-02 01:35:27.329398: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcf8fffd00 next 164 of size 8192
2019-10-02 01:35:27.329415: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcf9001d00 next 165 of size 8192
2019-10-02 01:35:27.329433: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcf9003d00 next 166 of size 16384000
2019-10-02 01:35:27.329450: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcf9fa3d00 next 167 of size 8192
2019-10-02 01:35:27.329468: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcf9fa5d00 next 168 of size 67108864
2019-10-02 01:35:27.329485: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcfdfa5d00 next 169 of size 8192
2019-10-02 01:35:27.329501: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcfdfa7d00 next 170 of size 67108864
2019-10-02 01:35:27.329519: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd01fa7d00 next 171 of size 8192
2019-10-02 01:35:27.329536: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd01fa9d00 next 172 of size 67108864
2019-10-02 01:35:27.329554: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd05fa9d00 next 173 of size 16777216
2019-10-02 01:35:27.329572: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd06fa9d00 next 174 of size 512
2019-10-02 01:35:27.329587: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd06fa9f00 next 175 of size 8192
2019-10-02 01:35:27.329603: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd06fabf00 next 176 of size 8192
2019-10-02 01:35:27.329620: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd06fadf00 next 177 of size 67108864
2019-10-02 01:35:27.329636: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd0afadf00 next 178 of size 67108864
2019-10-02 01:35:27.329653: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd0efadf00 next 179 of size 8192
2019-10-02 01:35:27.329670: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd0efaff00 next 180 of size 16777216
2019-10-02 01:35:27.329688: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd0ffaff00 next 181 of size 8192
2019-10-02 01:35:27.329704: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd0ffb1f00 next 182 of size 8192
2019-10-02 01:35:27.329721: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd0ffb3f00 next 183 of size 16384000
2019-10-02 01:35:27.329738: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd10f53f00 next 184 of size 16384000
2019-10-02 01:35:27.329754: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd11ef3f00 next 185 of size 8192
2019-10-02 01:35:27.329770: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd11ef5f00 next 186 of size 8192
2019-10-02 01:35:27.329793: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd11ef7f00 next 187 of size 16777216
2019-10-02 01:35:27.329811: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd12ef7f00 next 188 of size 32768
2019-10-02 01:35:27.329827: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd12efff00 next 189 of size 16777216
2019-10-02 01:35:27.329845: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd13efff00 next 190 of size 8192
2019-10-02 01:35:27.329861: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd13f01f00 next 191 of size 8192
2019-10-02 01:35:27.329876: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd13f03f00 next 192 of size 8192
2019-10-02 01:35:27.329893: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd13f05f00 next 193 of size 16777216
2019-10-02 01:35:27.329911: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd14f05f00 next 194 of size 8192
2019-10-02 01:35:27.329927: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd14f07f00 next 195 of size 8192
2019-10-02 01:35:27.329944: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd14f09f00 next 196 of size 67108864
2019-10-02 01:35:27.329962: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd18f09f00 next 197 of size 16777216
2019-10-02 01:35:27.329979: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd19f09f00 next 198 of size 8192
2019-10-02 01:35:27.329996: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd19f0bf00 next 199 of size 8192
2019-10-02 01:35:27.330018: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd19f0df00 next 200 of size 16777216
2019-10-02 01:35:27.330034: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd1af0df00 next 201 of size 8192
2019-10-02 01:35:27.330051: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd1af0ff00 next 202 of size 8192
2019-10-02 01:35:27.330069: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd1af11f00 next 203 of size 8192
2019-10-02 01:35:27.330086: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd1af13f00 next 204 of size 67108864
2019-10-02 01:35:27.330103: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd1ef13f00 next 205 of size 32768
2019-10-02 01:35:27.330119: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd1ef1bf00 next 206 of size 16777216
2019-10-02 01:35:27.330137: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd1ff1bf00 next 207 of size 8192
2019-10-02 01:35:27.330154: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd1ff1df00 next 208 of size 8192
2019-10-02 01:35:27.330171: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd1ff1ff00 next 209 of size 8192
2019-10-02 01:35:27.330202: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd1ff21f00 next 210 of size 16777216
2019-10-02 01:35:27.330219: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd20f21f00 next 211 of size 32768
2019-10-02 01:35:27.330241: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd20f29f00 next 212 of size 8192
2019-10-02 01:35:27.330257: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd20f2bf00 next 213 of size 8192
2019-10-02 01:35:27.330275: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd20f2df00 next 214 of size 8192
2019-10-02 01:35:27.330293: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd20f2ff00 next 215 of size 8192
2019-10-02 01:35:27.330309: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd20f31f00 next 216 of size 8192
2019-10-02 01:35:27.330326: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd20f33f00 next 217 of size 8192
2019-10-02 01:35:27.330342: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd20f35f00 next 218 of size 16777216
2019-10-02 01:35:27.330366: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd21f35f00 next 219 of size 512
2019-10-02 01:35:27.330385: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd21f36100 next 220 of size 16777216
2019-10-02 01:35:27.330402: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd22f36100 next 221 of size 8192
2019-10-02 01:35:27.330419: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd22f38100 next 222 of size 67108864
2019-10-02 01:35:27.330438: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd26f38100 next 223 of size 16777216
2019-10-02 01:35:27.330453: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd27f38100 next 224 of size 8192
2019-10-02 01:35:27.330470: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd27f3a100 next 225 of size 16384000
2019-10-02 01:35:27.330486: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd28eda100 next 226 of size 512
2019-10-02 01:35:27.330502: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd28eda300 next 227 of size 8192
2019-10-02 01:35:27.330519: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd28edc300 next 228 of size 8192
2019-10-02 01:35:27.330536: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd28ede300 next 229 of size 8192
2019-10-02 01:35:27.330552: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd28ee0300 next 230 of size 8192
2019-10-02 01:35:27.330569: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd28ee2300 next 231 of size 8192
2019-10-02 01:35:27.330589: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd28ee4300 next 232 of size 16777216
2019-10-02 01:35:27.330606: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd29ee4300 next 233 of size 8192
2019-10-02 01:35:27.330621: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd29ee6300 next 234 of size 67108864
2019-10-02 01:35:27.330638: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd2dee6300 next 235 of size 16384000
2019-10-02 01:35:27.330654: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd2ee86300 next 236 of size 16777216
2019-10-02 01:35:27.330671: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd2fe86300 next 237 of size 8192
2019-10-02 01:35:27.330688: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd2fe88300 next 238 of size 8192
2019-10-02 01:35:27.330704: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd2fe8a300 next 239 of size 512
2019-10-02 01:35:27.330720: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd2fe8a500 next 240 of size 16777216
2019-10-02 01:35:27.330736: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd30e8a500 next 241 of size 8192
2019-10-02 01:35:27.330751: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd30e8c500 next 242 of size 8192
2019-10-02 01:35:27.330768: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd30e8e500 next 243 of size 32768
2019-10-02 01:35:27.330785: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd30e96500 next 244 of size 16384000
2019-10-02 01:35:27.330802: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd31e36500 next 245 of size 16384000
2019-10-02 01:35:27.330818: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd32dd6500 next 246 of size 8192
2019-10-02 01:35:27.330835: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd32dd8500 next 247 of size 32768
2019-10-02 01:35:27.330851: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd32de0500 next 248 of size 8192
2019-10-02 01:35:27.330867: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd32de2500 next 249 of size 8192
2019-10-02 01:35:27.330896: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd32de4500 next 250 of size 32768
2019-10-02 01:35:27.330914: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd32dec500 next 251 of size 8192
2019-10-02 01:35:27.330931: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd32dee500 next 252 of size 8192
2019-10-02 01:35:27.330948: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd32df0500 next 253 of size 16777216
2019-10-02 01:35:27.330964: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd33df0500 next 254 of size 16384000
2019-10-02 01:35:27.330982: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd34d90500 next 255 of size 16384000
2019-10-02 01:35:27.330999: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd35d30500 next 256 of size 512
2019-10-02 01:35:27.331015: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd35d30700 next 257 of size 8192
2019-10-02 01:35:27.331032: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd35d32700 next 258 of size 8192
2019-10-02 01:35:27.331047: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd35d34700 next 259 of size 8192
2019-10-02 01:35:27.331065: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd35d36700 next 260 of size 67108864
2019-10-02 01:35:27.331081: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd39d36700 next 261 of size 8192
2019-10-02 01:35:27.331097: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd39d38700 next 262 of size 67108864
2019-10-02 01:35:27.331112: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd3dd38700 next 263 of size 512
2019-10-02 01:35:27.331131: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd3dd38900 next 264 of size 8192
2019-10-02 01:35:27.331148: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd3dd3a900 next 265 of size 8192
2019-10-02 01:35:27.331164: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd3dd3c900 next 266 of size 67108864
2019-10-02 01:35:27.331182: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd41d3c900 next 267 of size 16384000
2019-10-02 01:35:27.331200: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd42cdc900 next 268 of size 8192
2019-10-02 01:35:27.331218: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd42cde900 next 269 of size 8192
2019-10-02 01:35:27.331235: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd42ce0900 next 270 of size 8192
2019-10-02 01:35:27.331250: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd42ce2900 next 271 of size 67108864
2019-10-02 01:35:27.331269: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd46ce2900 next 272 of size 16777216
2019-10-02 01:35:27.331287: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd47ce2900 next 273 of size 16777216
2019-10-02 01:35:27.331339: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd48ce2900 next 274 of size 16777216
2019-10-02 01:35:27.331359: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd49ce2900 next 275 of size 16777216
2019-10-02 01:35:27.331373: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd4ace2900 next 276 of size 8192
2019-10-02 01:35:27.331390: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd4ace4900 next 277 of size 16384000
2019-10-02 01:35:27.331406: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd4bc84900 next 278 of size 16384000
2019-10-02 01:35:27.331422: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd4cc24900 next 279 of size 8192
2019-10-02 01:35:27.331440: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd4cc26900 next 280 of size 8192
2019-10-02 01:35:27.331456: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd4cc28900 next 281 of size 8192
2019-10-02 01:35:27.331480: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd4cc2a900 next 282 of size 8192
2019-10-02 01:35:27.331498: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd4cc2c900 next 283 of size 8192
2019-10-02 01:35:27.331516: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd4cc2e900 next 284 of size 67108864
2019-10-02 01:35:27.331531: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd50c2e900 next 285 of size 8192
2019-10-02 01:35:27.331549: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd50c30900 next 286 of size 16384000
2019-10-02 01:35:27.331565: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd51bd0900 next 287 of size 16384000
2019-10-02 01:35:27.331580: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd52b70900 next 288 of size 8192
2019-10-02 01:35:27.331597: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd52b72900 next 289 of size 8192
2019-10-02 01:35:27.331615: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd52b74900 next 290 of size 67108864
2019-10-02 01:35:27.331632: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd56b74900 next 291 of size 8192
2019-10-02 01:35:27.331648: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd56b76900 next 292 of size 8192
2019-10-02 01:35:27.331665: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd56b78900 next 293 of size 8192
2019-10-02 01:35:27.331682: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd56b7a900 next 294 of size 8192
2019-10-02 01:35:27.331698: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd56b7c900 next 295 of size 16777216
2019-10-02 01:35:27.331716: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd57b7c900 next 296 of size 8192
2019-10-02 01:35:27.331734: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd57b7e900 next 297 of size 32768
2019-10-02 01:35:27.331750: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd57b86900 next 298 of size 16777216
2019-10-02 01:35:27.331766: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd58b86900 next 299 of size 16777216
2019-10-02 01:35:27.331782: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd59b86900 next 300 of size 8192
2019-10-02 01:35:27.331800: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd59b88900 next 301 of size 8192
2019-10-02 01:35:27.331816: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd59b8a900 next 302 of size 16777216
2019-10-02 01:35:27.331832: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd5ab8a900 next 303 of size 512
2019-10-02 01:35:27.331849: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd5ab8ab00 next 304 of size 8192
2019-10-02 01:35:27.331867: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd5ab8cb00 next 305 of size 67108864
2019-10-02 01:35:27.331884: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd5eb8cb00 next 306 of size 8192
2019-10-02 01:35:27.331900: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd5eb8eb00 next 307 of size 16777216
2019-10-02 01:35:27.331918: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd5fb8eb00 next 308 of size 67108864
2019-10-02 01:35:27.331935: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd63b8eb00 next 309 of size 8192
2019-10-02 01:35:27.331952: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd63b90b00 next 310 of size 8192
2019-10-02 01:35:27.331968: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd63b92b00 next 311 of size 8192
2019-10-02 01:35:27.331985: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd63b94b00 next 312 of size 8192
2019-10-02 01:35:27.332008: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd63b96b00 next 313 of size 16777216
2019-10-02 01:35:27.332025: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd64b96b00 next 314 of size 16777216
2019-10-02 01:35:27.332042: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd65b96b00 next 315 of size 67108864
2019-10-02 01:35:27.332057: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd69b96b00 next 316 of size 8192
2019-10-02 01:35:27.332074: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd69b98b00 next 317 of size 8192
2019-10-02 01:35:27.332089: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd69b9ab00 next 318 of size 256
2019-10-02 01:35:27.332107: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd69b9ac00 next 319 of size 8192
2019-10-02 01:35:27.332125: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd69b9cc00 next 320 of size 512
2019-10-02 01:35:27.332141: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd69b9ce00 next 321 of size 67108864
2019-10-02 01:35:27.332159: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd6db9ce00 next 322 of size 32768
2019-10-02 01:35:27.332175: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd6dba4e00 next 323 of size 16777216
2019-10-02 01:35:27.332192: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd6eba4e00 next 324 of size 512
2019-10-02 01:35:27.332208: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd6eba5000 next 325 of size 32768
2019-10-02 01:35:27.332225: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd6ebad000 next 326 of size 8192
2019-10-02 01:35:27.332242: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd6ebaf000 next 327 of size 8192
2019-10-02 01:35:27.332259: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd6ebb1000 next 328 of size 8192
2019-10-02 01:35:27.332276: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd6ebb3000 next 329 of size 8192
2019-10-02 01:35:27.332293: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd6ebb5000 next 330 of size 16777216
2019-10-02 01:35:27.332311: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd6fbb5000 next 331 of size 32768
2019-10-02 01:35:27.332327: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd6fbbd000 next 332 of size 16777216
2019-10-02 01:35:27.332343: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd70bbd000 next 333 of size 8192
2019-10-02 01:35:27.332366: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd70bbf000 next 334 of size 16777216
2019-10-02 01:35:27.332390: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd71bbf000 next 335 of size 8192
2019-10-02 01:35:27.332408: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd71bc1000 next 336 of size 8192
2019-10-02 01:35:27.332424: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd71bc3000 next 337 of size 8192
2019-10-02 01:35:27.332440: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd71bc5000 next 338 of size 16777216
2019-10-02 01:35:27.332458: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd72bc5000 next 339 of size 8192
2019-10-02 01:35:27.332476: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd72bc7000 next 340 of size 16777216
2019-10-02 01:35:27.332492: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd73bc7000 next 341 of size 8192
2019-10-02 01:35:27.332508: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd73bc9000 next 342 of size 8192
2019-10-02 01:35:27.332525: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd73bcb000 next 343 of size 8192
2019-10-02 01:35:27.332550: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd73bcd000 next 344 of size 67108864
2019-10-02 01:35:27.332568: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd77bcd000 next 345 of size 8192
2019-10-02 01:35:27.332585: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd77bcf000 next 346 of size 16777216
2019-10-02 01:35:27.332603: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd78bcf000 next 347 of size 8192
2019-10-02 01:35:27.332619: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd78bd1000 next 348 of size 8192
2019-10-02 01:35:27.332635: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd78bd3000 next 349 of size 8192
2019-10-02 01:35:27.332653: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd78bd5000 next 350 of size 262144000
2019-10-02 01:35:27.332669: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd885d5000 next 351 of size 512
2019-10-02 01:35:27.332686: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd885d5200 next 352 of size 8192
2019-10-02 01:35:27.332702: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd885d7200 next 353 of size 8192
2019-10-02 01:35:27.332718: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd885d9200 next 354 of size 16777216
2019-10-02 01:35:27.332734: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd895d9200 next 355 of size 16777216
2019-10-02 01:35:27.332750: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd8a5d9200 next 356 of size 16777216
2019-10-02 01:35:27.332766: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd8b5d9200 next 357 of size 8192
2019-10-02 01:35:27.332783: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd8b5db200 next 358 of size 16777216
2019-10-02 01:35:27.332800: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd8c5db200 next 359 of size 32768
2019-10-02 01:35:27.332817: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd8c5e3200 next 360 of size 8192
2019-10-02 01:35:27.332833: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd8c5e5200 next 361 of size 8192
2019-10-02 01:35:27.332849: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd8c5e7200 next 362 of size 8192
2019-10-02 01:35:27.332866: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd8c5e9200 next 363 of size 512
2019-10-02 01:35:27.332883: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd8c5e9400 next 364 of size 16777216
2019-10-02 01:35:27.332899: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd8d5e9400 next 365 of size 8192
2019-10-02 01:35:27.332915: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd8d5eb400 next 366 of size 8192
2019-10-02 01:35:27.332932: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd8d5ed400 next 367 of size 16777216
2019-10-02 01:35:27.332948: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd8e5ed400 next 368 of size 16777216
2019-10-02 01:35:27.332965: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd8f5ed400 next 369 of size 67108864
2019-10-02 01:35:27.332981: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd935ed400 next 370 of size 16777216
2019-10-02 01:35:27.332997: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd945ed400 next 371 of size 67108864
2019-10-02 01:35:27.333012: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd985ed400 next 372 of size 8192
2019-10-02 01:35:27.333029: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd985ef400 next 373 of size 67108864
2019-10-02 01:35:27.333045: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd9c5ef400 next 374 of size 16777216
2019-10-02 01:35:27.333062: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd9d5ef400 next 375 of size 8192
2019-10-02 01:35:27.333086: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd9d5f1400 next 376 of size 8192
2019-10-02 01:35:27.333104: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd9d5f3400 next 377 of size 512
2019-10-02 01:35:27.333119: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd9d5f3600 next 378 of size 8192
2019-10-02 01:35:27.333135: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd9d5f5600 next 379 of size 8192
2019-10-02 01:35:27.333154: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd9d5f7600 next 380 of size 8192
2019-10-02 01:35:27.333170: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd9d5f9600 next 381 of size 8192
2019-10-02 01:35:27.333185: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd9d5fb600 next 382 of size 8192
2019-10-02 01:35:27.333202: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd9d5fd600 next 383 of size 16777216
2019-10-02 01:35:27.333218: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd9e5fd600 next 384 of size 32768
2019-10-02 01:35:27.333234: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd9e605600 next 385 of size 67108864
2019-10-02 01:35:27.333250: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda2605600 next 386 of size 8192
2019-10-02 01:35:27.333266: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda2607600 next 387 of size 8192
2019-10-02 01:35:27.333282: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda2609600 next 388 of size 512
2019-10-02 01:35:27.333298: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda2609800 next 389 of size 8192
2019-10-02 01:35:27.333313: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda260b800 next 390 of size 8192
2019-10-02 01:35:27.333329: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda260d800 next 391 of size 512
2019-10-02 01:35:27.333346: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda260da00 next 392 of size 8192
2019-10-02 01:35:27.333363: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda260fa00 next 393 of size 8192
2019-10-02 01:35:27.333379: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda2611a00 next 394 of size 8192
2019-10-02 01:35:27.333396: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda2613a00 next 395 of size 8192
2019-10-02 01:35:27.333412: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda2615a00 next 396 of size 8192
2019-10-02 01:35:27.333428: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda2617a00 next 397 of size 8192
2019-10-02 01:35:27.333444: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda2619a00 next 398 of size 8192
2019-10-02 01:35:27.333459: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda261ba00 next 399 of size 8192
2019-10-02 01:35:27.333475: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda261da00 next 400 of size 8192
2019-10-02 01:35:27.333491: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda261fa00 next 401 of size 8192
2019-10-02 01:35:27.333506: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda2621a00 next 402 of size 67108864
2019-10-02 01:35:27.333523: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda6621a00 next 403 of size 8192
2019-10-02 01:35:27.333540: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda6623a00 next 404 of size 16777216
2019-10-02 01:35:27.333557: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda7623a00 next 405 of size 67108864
2019-10-02 01:35:27.333572: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdab623a00 next 406 of size 8192
2019-10-02 01:35:27.333595: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdab625a00 next 407 of size 8192
2019-10-02 01:35:27.333612: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdab627a00 next 408 of size 8192
2019-10-02 01:35:27.333628: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdab629a00 next 409 of size 67108864
2019-10-02 01:35:27.333644: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdaf629a00 next 410 of size 16777216
2019-10-02 01:35:27.333660: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdb0629a00 next 411 of size 8192
2019-10-02 01:35:27.333676: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdb062ba00 next 412 of size 512
2019-10-02 01:35:27.333694: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdb062bc00 next 413 of size 67108864
2019-10-02 01:35:27.333711: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdb462bc00 next 414 of size 16777216
2019-10-02 01:35:27.333728: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdb562bc00 next 415 of size 8192
2019-10-02 01:35:27.333744: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdb562dc00 next 416 of size 8192
2019-10-02 01:35:27.333761: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdb562fc00 next 417 of size 16777216
2019-10-02 01:35:27.333778: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdb662fc00 next 418 of size 8192
2019-10-02 01:35:27.333795: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdb6631c00 next 419 of size 8192
2019-10-02 01:35:27.333813: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdb6633c00 next 420 of size 32768
2019-10-02 01:35:27.333829: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdb663bc00 next 421 of size 16777216
2019-10-02 01:35:27.333845: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdb763bc00 next 422 of size 16777216
2019-10-02 01:35:27.333860: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdb863bc00 next 423 of size 32768
2019-10-02 01:35:27.333877: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdb8643c00 next 424 of size 16777216
2019-10-02 01:35:27.333893: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdb9643c00 next 425 of size 8192
2019-10-02 01:35:27.333909: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdb9645c00 next 426 of size 16777216
2019-10-02 01:35:27.333925: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdba645c00 next 427 of size 8192
2019-10-02 01:35:27.333943: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdba647c00 next 428 of size 67108864
2019-10-02 01:35:27.333959: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdbe647c00 next 429 of size 8192
2019-10-02 01:35:27.333975: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdbe649c00 next 430 of size 8192
2019-10-02 01:35:27.333992: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdbe64bc00 next 431 of size 16777216
2019-10-02 01:35:27.334009: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdbf64bc00 next 432 of size 8192
2019-10-02 01:35:27.334024: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdbf64dc00 next 433 of size 8192
2019-10-02 01:35:27.334040: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdbf64fc00 next 434 of size 512
2019-10-02 01:35:27.334056: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdbf64fe00 next 435 of size 16777216
2019-10-02 01:35:27.334071: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc064fe00 next 436 of size 8192
2019-10-02 01:35:27.334089: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc0651e00 next 437 of size 16777216
2019-10-02 01:35:27.334105: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc1651e00 next 438 of size 8192
2019-10-02 01:35:27.334129: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc1653e00 next 439 of size 8192
2019-10-02 01:35:27.334148: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc1655e00 next 440 of size 67108864
2019-10-02 01:35:27.334165: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc5655e00 next 441 of size 16777216
2019-10-02 01:35:27.334181: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc6655e00 next 442 of size 8192
2019-10-02 01:35:27.334199: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc6657e00 next 443 of size 16777216
2019-10-02 01:35:27.334215: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc7657e00 next 444 of size 8192
2019-10-02 01:35:27.334231: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc7659e00 next 445 of size 8192
2019-10-02 01:35:27.334247: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc765be00 next 446 of size 8192
2019-10-02 01:35:27.334265: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc765de00 next 447 of size 8192
2019-10-02 01:35:27.334280: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc765fe00 next 448 of size 8192
2019-10-02 01:35:27.334297: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc7661e00 next 449 of size 8192
2019-10-02 01:35:27.334314: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc7663e00 next 450 of size 8192
2019-10-02 01:35:27.334330: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc7665e00 next 451 of size 16777216
2019-10-02 01:35:27.334346: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc8665e00 next 452 of size 512
2019-10-02 01:35:27.334362: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc8666000 next 453 of size 8192
2019-10-02 01:35:27.334379: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc8668000 next 454 of size 8192
2019-10-02 01:35:27.334396: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc866a000 next 455 of size 32768
2019-10-02 01:35:27.334414: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc8672000 next 456 of size 512
2019-10-02 01:35:27.334432: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc8672200 next 457 of size 8192
2019-10-02 01:35:27.334449: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc8674200 next 458 of size 8192
2019-10-02 01:35:27.334467: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc8676200 next 459 of size 16777216
2019-10-02 01:35:27.334483: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc9676200 next 460 of size 16777216
2019-10-02 01:35:27.334500: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdca676200 next 461 of size 8192
2019-10-02 01:35:27.334517: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdca678200 next 462 of size 8192
2019-10-02 01:35:27.334534: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdca67a200 next 463 of size 8192
2019-10-02 01:35:27.334552: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdca67c200 next 464 of size 512
2019-10-02 01:35:27.334568: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdca67c400 next 465 of size 16777216
2019-10-02 01:35:27.334585: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdcb67c400 next 466 of size 32768
2019-10-02 01:35:27.334601: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdcb684400 next 467 of size 8192
2019-10-02 01:35:27.334618: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdcb686400 next 468 of size 8192
2019-10-02 01:35:27.334635: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdcb688400 next 469 of size 16777216
2019-10-02 01:35:27.334666: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdcc688400 next 470 of size 32768
2019-10-02 01:35:27.334685: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdcc690400 next 471 of size 8192
2019-10-02 01:35:27.334702: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdcc692400 next 472 of size 16777216
2019-10-02 01:35:27.334720: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdcd692400 next 473 of size 8192
2019-10-02 01:35:27.334738: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdcd694400 next 474 of size 8192
2019-10-02 01:35:27.334755: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdcd696400 next 475 of size 67108864
2019-10-02 01:35:27.334772: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdd1696400 next 476 of size 8192
2019-10-02 01:35:27.334790: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdd1698400 next 477 of size 67108864
2019-10-02 01:35:27.334807: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdd5698400 next 478 of size 256
2019-10-02 01:35:27.334824: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdd5698500 next 479 of size 8192
2019-10-02 01:35:27.334841: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdd569a500 next 480 of size 512
2019-10-02 01:35:27.334859: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdd569a700 next 481 of size 67108864
2019-10-02 01:35:27.334875: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdd969a700 next 482 of size 256
2019-10-02 01:35:27.334892: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdd969a800 next 514 of size 256
2019-10-02 01:35:27.334910: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdd969a900 next 484 of size 256
2019-10-02 01:35:27.334928: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdd969aa00 next 485 of size 8192
2019-10-02 01:35:27.334944: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdd969ca00 next 486 of size 8192
2019-10-02 01:35:27.334960: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdd969ea00 next 487 of size 16777216
2019-10-02 01:35:27.334976: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdda69ea00 next 515 of size 8192
2019-10-02 01:35:27.334993: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdda6a0a00 next 512 of size 512
2019-10-02 01:35:27.335010: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdda6a0c00 next 513 of size 8192
2019-10-02 01:35:27.335027: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdda6a2c00 next 510 of size 16777216
2019-10-02 01:35:27.335044: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efddb6a2c00 next 511 of size 8192
2019-10-02 01:35:27.335061: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efddb6a4c00 next 500 of size 16777216
2019-10-02 01:35:27.335078: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efddc6a4c00 next 501 of size 16777216
2019-10-02 01:35:27.335094: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efddd6a4c00 next 509 of size 8192
2019-10-02 01:35:27.335109: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efddd6a6c00 next 504 of size 8192
2019-10-02 01:35:27.335126: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efddd6a8c00 next 505 of size 67108864
2019-10-02 01:35:27.335144: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde16a8c00 next 489 of size 32768
2019-10-02 01:35:27.335160: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde16b0c00 next 490 of size 8192
2019-10-02 01:35:27.335176: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde16b2c00 next 488 of size 8192
2019-10-02 01:35:27.335193: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde16b4c00 next 508 of size 16777216
2019-10-02 01:35:27.335222: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde26b4c00 next 503 of size 8192
2019-10-02 01:35:27.335241: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde26b6c00 next 502 of size 8192
2019-10-02 01:35:27.335259: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde26b8c00 next 507 of size 8192
2019-10-02 01:35:27.335276: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde26bac00 next 497 of size 16777216
2019-10-02 01:35:27.335294: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde36bac00 next 498 of size 67108864
2019-10-02 01:35:27.335351: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde76bac00 next 506 of size 8192
2019-10-02 01:35:27.335369: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde76bcc00 next 496 of size 8192
2019-10-02 01:35:27.335386: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde76bec00 next 492 of size 16777216
2019-10-02 01:35:27.335403: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde86bec00 next 493 of size 512
2019-10-02 01:35:27.335419: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde86bee00 next 483 of size 8192
2019-10-02 01:35:27.335437: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde86c0e00 next 495 of size 16777216
2019-10-02 01:35:27.335454: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde96c0e00 next 491 of size 8192
2019-10-02 01:35:27.335470: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde96c2e00 next 499 of size 8192
2019-10-02 01:35:27.335487: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde96c4e00 next 494 of size 32768
2019-10-02 01:35:27.335504: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde96cce00 next 516 of size 16777216
2019-10-02 01:35:27.335523: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdea6cce00 next 517 of size 8192
2019-10-02 01:35:27.335541: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdea6cee00 next 518 of size 8192
2019-10-02 01:35:27.335558: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdea6d0e00 next 519 of size 16777216
2019-10-02 01:35:27.335575: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdeb6d0e00 next 520 of size 67108864
2019-10-02 01:35:27.335591: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdef6d0e00 next 521 of size 8192
2019-10-02 01:35:27.335607: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdef6d2e00 next 522 of size 16777216
2019-10-02 01:35:27.335624: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf06d2e00 next 523 of size 8192
2019-10-02 01:35:27.335640: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf06d4e00 next 524 of size 8192
2019-10-02 01:35:27.335657: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf06d6e00 next 525 of size 8192
2019-10-02 01:35:27.335673: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf06d8e00 next 526 of size 16777216
2019-10-02 01:35:27.335690: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16d8e00 next 527 of size 8192
2019-10-02 01:35:27.335706: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16dae00 next 528 of size 8192
2019-10-02 01:35:27.335721: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16dce00 next 529 of size 8192
2019-10-02 01:35:27.335738: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16dee00 next 530 of size 256
2019-10-02 01:35:27.335755: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16def00 next 545 of size 256
2019-10-02 01:35:27.335772: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16df000 next 547 of size 8192
2019-10-02 01:35:27.335798: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16e1000 next 548 of size 256
2019-10-02 01:35:27.335819: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16e1100 next 549 of size 8192
2019-10-02 01:35:27.335837: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16e3100 next 546 of size 256
2019-10-02 01:35:27.335853: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16e3200 next 550 of size 8192
2019-10-02 01:35:27.335870: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16e5200 next 551 of size 256
2019-10-02 01:35:27.335883: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16e5300 next 552 of size 8192
2019-10-02 01:35:27.335898: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16e7300 next 554 of size 256
2019-10-02 01:35:27.335916: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16e7400 next 586 of size 4096
2019-10-02 01:35:27.335934: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16e8400 next 577 of size 256
2019-10-02 01:35:27.335951: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16e8500 next 590 of size 5376
2019-10-02 01:35:27.335969: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16e9a00 next 559 of size 8192
2019-10-02 01:35:27.335989: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16eba00 next 657 of size 9984
2019-10-02 01:35:27.336008: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16ee100 next 684 of size 6912
2019-10-02 01:35:27.336026: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16efc00 next 685 of size 256
2019-10-02 01:35:27.336044: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16efd00 next 686 of size 256
2019-10-02 01:35:27.336061: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16efe00 next 687 of size 256
2019-10-02 01:35:27.336077: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16eff00 next 688 of size 256
2019-10-02 01:35:27.336094: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f0000 next 683 of size 256
2019-10-02 01:35:27.336110: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f0100 next 690 of size 256
2019-10-02 01:35:27.336127: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f0200 next 665 of size 256
2019-10-02 01:35:27.336143: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f0300 next 605 of size 256
2019-10-02 01:35:27.336161: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f0400 next 697 of size 256
2019-10-02 01:35:27.336177: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f0500 next 936 of size 256
2019-10-02 01:35:27.336193: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f0600 next 630 of size 512
2019-10-02 01:35:27.336210: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f0800 next 1011 of size 256
2019-10-02 01:35:27.336227: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f0900 next 699 of size 512
2019-10-02 01:35:27.336243: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f0b00 next 700 of size 256
2019-10-02 01:35:27.336260: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f0c00 next 701 of size 256
2019-10-02 01:35:27.336276: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f0d00 next 703 of size 256
2019-10-02 01:35:27.336291: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f0e00 next 705 of size 256
2019-10-02 01:35:27.336308: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f0f00 next 706 of size 256
2019-10-02 01:35:27.336324: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f1000 next 707 of size 256
2019-10-02 01:35:27.336348: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f1100 next 712 of size 256
2019-10-02 01:35:27.336369: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f1200 next 637 of size 256
2019-10-02 01:35:27.336386: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f1300 next 714 of size 256
2019-10-02 01:35:27.336404: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f1400 next 531 of size 56064
2019-10-02 01:35:27.336421: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16fef00 next 532 of size 67108864
2019-10-02 01:35:27.336439: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf56fef00 next 533 of size 256
2019-10-02 01:35:27.336456: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf56ff000 next 534 of size 16777216
2019-10-02 01:35:27.336473: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf66ff000 next 535 of size 131072
2019-10-02 01:35:27.336489: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf671f000 next 715 of size 32768
2019-10-02 01:35:27.336504: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6727000 next 718 of size 256
2019-10-02 01:35:27.336521: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6727100 next 677 of size 256
2019-10-02 01:35:27.336538: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6727200 next 720 of size 256
2019-10-02 01:35:27.336555: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6727300 next 721 of size 8192
2019-10-02 01:35:27.336572: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6729300 next 722 of size 8192
2019-10-02 01:35:27.336589: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf672b300 next 724 of size 256
2019-10-02 01:35:27.336605: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf672b400 next 725 of size 256
2019-10-02 01:35:27.336622: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf672b500 next 726 of size 256
2019-10-02 01:35:27.336638: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf672b600 next 727 of size 256
2019-10-02 01:35:27.336655: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf672b700 next 728 of size 8192
2019-10-02 01:35:27.336672: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf672d700 next 729 of size 8192
2019-10-02 01:35:27.336690: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf672f700 next 730 of size 256
2019-10-02 01:35:27.336706: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf672f800 next 731 of size 256
2019-10-02 01:35:27.336722: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf672f900 next 732 of size 256
2019-10-02 01:35:27.336739: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf672fa00 next 733 of size 256
2019-10-02 01:35:27.336756: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf672fb00 next 734 of size 8192
2019-10-02 01:35:27.336774: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6731b00 next 735 of size 8192
2019-10-02 01:35:27.336789: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6733b00 next 736 of size 8192
2019-10-02 01:35:27.336806: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6735b00 next 737 of size 256
2019-10-02 01:35:27.336822: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6735c00 next 738 of size 8192
2019-10-02 01:35:27.336839: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6737c00 next 739 of size 8192
2019-10-02 01:35:27.336855: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6739c00 next 740 of size 256
2019-10-02 01:35:27.336872: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6739d00 next 741 of size 8192
2019-10-02 01:35:27.336897: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf673bd00 next 536 of size 13056
2019-10-02 01:35:27.336917: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf673f000 next 537 of size 8192
2019-10-02 01:35:27.336934: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6741000 next 538 of size 256
2019-10-02 01:35:27.336951: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6741100 next 539 of size 8192
2019-10-02 01:35:27.336969: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6743100 next 540 of size 131072
2019-10-02 01:35:27.336986: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6763100 next 541 of size 256
2019-10-02 01:35:27.337004: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6763200 next 542 of size 67108864
2019-10-02 01:35:27.337021: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdfa763200 next 543 of size 8192
2019-10-02 01:35:27.337038: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdfa765200 next 544 of size 67108864
2019-10-02 01:35:27.337053: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdfe765200 next 553 of size 131072
2019-10-02 01:35:27.337069: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdfe785200 next 582 of size 16777216
2019-10-02 01:35:27.337085: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff785200 next 742 of size 256
2019-10-02 01:35:27.337102: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff785300 next 743 of size 8192
2019-10-02 01:35:27.337118: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff787300 next 744 of size 8192
2019-10-02 01:35:27.337136: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff789300 next 745 of size 256
2019-10-02 01:35:27.337153: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff789400 next 746 of size 8192
2019-10-02 01:35:27.337169: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff78b400 next 747 of size 8192
2019-10-02 01:35:27.337185: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff78d400 next 748 of size 256
2019-10-02 01:35:27.337202: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff78d500 next 749 of size 8192
2019-10-02 01:35:27.337218: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff78f500 next 750 of size 8192
2019-10-02 01:35:27.337235: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff791500 next 751 of size 256
2019-10-02 01:35:27.337252: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff791600 next 752 of size 8192
2019-10-02 01:35:27.337269: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff793600 next 753 of size 256
2019-10-02 01:35:27.337287: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff793700 next 754 of size 256
2019-10-02 01:35:27.337305: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff793800 next 756 of size 256
2019-10-02 01:35:27.337321: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff793900 next 758 of size 8192
2019-10-02 01:35:27.337336: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff795900 next 759 of size 256
2019-10-02 01:35:27.337352: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff795a00 next 760 of size 8192
2019-10-02 01:35:27.337369: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff797a00 next 761 of size 8192
2019-10-02 01:35:27.337385: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff799a00 next 762 of size 256
2019-10-02 01:35:27.337401: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff799b00 next 763 of size 8192
2019-10-02 01:35:27.337417: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff79bb00 next 764 of size 8192
2019-10-02 01:35:27.337440: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff79db00 next 765 of size 256
2019-10-02 01:35:27.337461: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff79dc00 next 766 of size 8192
2019-10-02 01:35:27.337479: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff79fc00 next 769 of size 256
2019-10-02 01:35:27.337496: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff79fd00 next 771 of size 256
2019-10-02 01:35:27.337513: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff79fe00 next 773 of size 256
2019-10-02 01:35:27.337530: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff79ff00 next 776 of size 256
2019-10-02 01:35:27.337548: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff7a0000 next 778 of size 256
2019-10-02 01:35:27.337564: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff7a0100 next 780 of size 256
2019-10-02 01:35:27.337579: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff7a0200 next 782 of size 256
2019-10-02 01:35:27.337595: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff7a0300 next 785 of size 256
2019-10-02 01:35:27.337611: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff7a0400 next 786 of size 8192
2019-10-02 01:35:27.337628: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff7a2400 next 787 of size 512
2019-10-02 01:35:27.337646: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff7a2600 next 581 of size 11264
2019-10-02 01:35:27.337663: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff7a5200 next 580 of size 16777216
2019-10-02 01:35:27.337680: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe007a5200 next 579 of size 67108864
2019-10-02 01:35:27.337697: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe047a5200 next 578 of size 131072
2019-10-02 01:35:27.337715: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe047c5200 next 585 of size 16777216
2019-10-02 01:35:27.337732: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe057c5200 next 584 of size 8388608
2019-10-02 01:35:27.337748: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe05fc5200 next 583 of size 16777216
2019-10-02 01:35:27.337766: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe06fc5200 next 576 of size 16777216
2019-10-02 01:35:27.337782: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe07fc5200 next 702 of size 67108864
2019-10-02 01:35:27.337799: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe0bfc5200 next 704 of size 16777216
2019-10-02 01:35:27.337815: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe0cfc5200 next 708 of size 67108864
2019-10-02 01:35:27.337831: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe10fc5200 next 709 of size 16777216
2019-10-02 01:35:27.337847: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe11fc5200 next 710 of size 67108864
2019-10-02 01:35:27.337864: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe15fc5200 next 711 of size 16777216
2019-10-02 01:35:27.337880: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe16fc5200 next 716 of size 16777216
2019-10-02 01:35:27.337897: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe17fc5200 next 717 of size 67108864
2019-10-02 01:35:27.337914: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe1bfc5200 next 723 of size 67108864
2019-10-02 01:35:27.337931: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe1ffc5200 next 755 of size 16777216
2019-10-02 01:35:27.337948: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe20fc5200 next 757 of size 16777216
2019-10-02 01:35:27.337973: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe21fc5200 next 767 of size 16777216
2019-10-02 01:35:27.337990: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe22fc5200 next 768 of size 16777216
2019-10-02 01:35:27.338010: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe23fc5200 next 770 of size 67108864
2019-10-02 01:35:27.338024: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe27fc5200 next 772 of size 32768
2019-10-02 01:35:27.338039: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe27fcd200 next 774 of size 32768
2019-10-02 01:35:27.338055: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe27fd5200 next 775 of size 16777216
2019-10-02 01:35:27.338070: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe28fd5200 next 777 of size 16777216
2019-10-02 01:35:27.338086: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe29fd5200 next 779 of size 16777216
2019-10-02 01:35:27.338168: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe2afd5200 next 781 of size 67108864
2019-10-02 01:35:27.338194: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe2efd5200 next 783 of size 16777216
2019-10-02 01:35:27.338211: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe2ffd5200 next 784 of size 67108864
2019-10-02 01:35:27.338226: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe33fd5200 next 788 of size 512
2019-10-02 01:35:27.338241: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe33fd5400 next 789 of size 256
2019-10-02 01:35:27.338256: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe33fd5500 next 790 of size 256
2019-10-02 01:35:27.338272: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe33fd5600 next 791 of size 256
2019-10-02 01:35:27.338287: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe33fd5700 next 792 of size 256
2019-10-02 01:35:27.338302: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe33fd5800 next 793 of size 512
2019-10-02 01:35:27.338317: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe33fd5a00 next 794 of size 8192
2019-10-02 01:35:27.338331: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe33fd7a00 next 795 of size 512
2019-10-02 01:35:27.338346: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe33fd7c00 next 796 of size 8192
2019-10-02 01:35:27.338362: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe33fd9c00 next 797 of size 8192
2019-10-02 01:35:27.338377: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe33fdbc00 next 798 of size 8192
2019-10-02 01:35:27.338396: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe33fddc00 next 799 of size 16777216
2019-10-02 01:35:27.338410: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe34fddc00 next 800 of size 256
2019-10-02 01:35:27.338425: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe34fddd00 next 801 of size 16777216
2019-10-02 01:35:27.338441: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe35fddd00 next 802 of size 67108864
2019-10-02 01:35:27.338456: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe39fddd00 next 803 of size 16777216
2019-10-02 01:35:27.338471: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3afddd00 next 804 of size 256
2019-10-02 01:35:27.338487: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3afdde00 next 805 of size 8192
2019-10-02 01:35:27.338503: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3afdfe00 next 806 of size 256
2019-10-02 01:35:27.338519: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3afdff00 next 807 of size 8192
2019-10-02 01:35:27.338547: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3afe1f00 next 808 of size 256
2019-10-02 01:35:27.338566: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3afe2000 next 809 of size 8192
2019-10-02 01:35:27.338581: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3afe4000 next 810 of size 256
2019-10-02 01:35:27.338600: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3afe4100 next 811 of size 8192
2019-10-02 01:35:27.338616: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3afe6100 next 812 of size 8192
2019-10-02 01:35:27.338631: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3afe8100 next 813 of size 256
2019-10-02 01:35:27.338646: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3afe8200 next 814 of size 8192
2019-10-02 01:35:27.338660: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3afea200 next 815 of size 8192
2019-10-02 01:35:27.338676: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3afec200 next 816 of size 256
2019-10-02 01:35:27.338691: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3afec300 next 817 of size 8192
2019-10-02 01:35:27.338707: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3afee300 next 818 of size 32768
2019-10-02 01:35:27.338721: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3aff6300 next 819 of size 256
2019-10-02 01:35:27.338736: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3aff6400 next 820 of size 32768
2019-10-02 01:35:27.338750: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3affe400 next 821 of size 32768
2019-10-02 01:35:27.338765: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3b006400 next 822 of size 256
2019-10-02 01:35:27.338780: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3b006500 next 823 of size 32768
2019-10-02 01:35:27.338795: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3b00e500 next 824 of size 8192
2019-10-02 01:35:27.338810: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3b010500 next 825 of size 256
2019-10-02 01:35:27.338824: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3b010600 next 826 of size 8192
2019-10-02 01:35:27.338839: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3b012600 next 827 of size 16777216
2019-10-02 01:35:27.338853: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3c012600 next 828 of size 256
2019-10-02 01:35:27.338869: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3c012700 next 829 of size 16777216
2019-10-02 01:35:27.338886: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3d012700 next 830 of size 16777216
2019-10-02 01:35:27.338902: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3e012700 next 831 of size 8192
2019-10-02 01:35:27.338918: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3e014700 next 832 of size 256
2019-10-02 01:35:27.338934: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3e014800 next 833 of size 8192
2019-10-02 01:35:27.338950: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3e016800 next 834 of size 8192
2019-10-02 01:35:27.338968: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3e018800 next 835 of size 256
2019-10-02 01:35:27.338984: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3e018900 next 836 of size 8192
2019-10-02 01:35:27.339000: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3e01a900 next 837 of size 16777216
2019-10-02 01:35:27.339016: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3f01a900 next 838 of size 67108864
2019-10-02 01:35:27.339033: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4301a900 next 839 of size 8192
2019-10-02 01:35:27.339058: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4301c900 next 840 of size 256
2019-10-02 01:35:27.339077: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4301ca00 next 841 of size 8192
2019-10-02 01:35:27.339093: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4301ea00 next 842 of size 256
2019-10-02 01:35:27.339111: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4301eb00 next 843 of size 8192
2019-10-02 01:35:27.339127: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe43020b00 next 844 of size 256
2019-10-02 01:35:27.339144: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe43020c00 next 845 of size 8192
2019-10-02 01:35:27.339160: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe43022c00 next 846 of size 8192
2019-10-02 01:35:27.339177: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe43024c00 next 847 of size 256
2019-10-02 01:35:27.339193: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe43024d00 next 848 of size 8192
2019-10-02 01:35:27.339210: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe43026d00 next 849 of size 16777216
2019-10-02 01:35:27.339227: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe44026d00 next 850 of size 16777216
2019-10-02 01:35:27.339242: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe45026d00 next 851 of size 256
2019-10-02 01:35:27.339258: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe45026e00 next 852 of size 256
2019-10-02 01:35:27.339274: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe45026f00 next 853 of size 16777216
2019-10-02 01:35:27.339290: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe46026f00 next 854 of size 16777216
2019-10-02 01:35:27.339329: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe47026f00 next 855 of size 67108864
2019-10-02 01:35:27.339348: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4b026f00 next 856 of size 256
2019-10-02 01:35:27.339365: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4b027000 next 857 of size 256
2019-10-02 01:35:27.339381: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4b027100 next 858 of size 512
2019-10-02 01:35:27.339396: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4b027300 next 859 of size 256
2019-10-02 01:35:27.339411: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4b027400 next 860 of size 512
2019-10-02 01:35:27.339426: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4b027600 next 861 of size 16777216
2019-10-02 01:35:27.339441: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4c027600 next 862 of size 16777216
2019-10-02 01:35:27.339457: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4d027600 next 863 of size 256
2019-10-02 01:35:27.339472: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4d027700 next 864 of size 8192
2019-10-02 01:35:27.339489: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4d029700 next 865 of size 256
2019-10-02 01:35:27.339504: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4d029800 next 866 of size 256
2019-10-02 01:35:27.339520: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4d029900 next 867 of size 8192
2019-10-02 01:35:27.339532: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4d02b900 next 868 of size 256
2019-10-02 01:35:27.339541: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4d02ba00 next 869 of size 8192
2019-10-02 01:35:27.339550: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4d02da00 next 870 of size 256
2019-10-02 01:35:27.339559: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4d02db00 next 871 of size 8192
2019-10-02 01:35:27.339574: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4d02fb00 next 872 of size 8192
2019-10-02 01:35:27.339583: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4d031b00 next 873 of size 256
2019-10-02 01:35:27.339592: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4d031c00 next 874 of size 8192
2019-10-02 01:35:27.339605: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4d033c00 next 875 of size 67108864
2019-10-02 01:35:27.339611: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe51033c00 next 876 of size 16777216
2019-10-02 01:35:27.339617: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe52033c00 next 877 of size 256
2019-10-02 01:35:27.339623: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe52033d00 next 878 of size 67108864
2019-10-02 01:35:27.339629: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe56033d00 next 879 of size 256
2019-10-02 01:35:27.339635: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe56033e00 next 880 of size 8192
2019-10-02 01:35:27.339641: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe56035e00 next 881 of size 256
2019-10-02 01:35:27.339647: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe56035f00 next 882 of size 16777216
2019-10-02 01:35:27.339653: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe57035f00 next 883 of size 8192
2019-10-02 01:35:27.339659: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe57037f00 next 884 of size 256
2019-10-02 01:35:27.339665: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe57038000 next 885 of size 67108864
2019-10-02 01:35:27.339671: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe5b038000 next 886 of size 16777216
2019-10-02 01:35:27.339677: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe5c038000 next 887 of size 16777216
2019-10-02 01:35:27.339682: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe5d038000 next 888 of size 16777216
2019-10-02 01:35:27.339689: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe5e038000 next 889 of size 16777216
2019-10-02 01:35:27.339694: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe5f038000 next 890 of size 67108864
2019-10-02 01:35:27.339700: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe63038000 next 891 of size 256
2019-10-02 01:35:27.339706: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe63038100 next 892 of size 16777216
2019-10-02 01:35:27.339712: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe64038100 next 893 of size 256
2019-10-02 01:35:27.339718: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe64038200 next 894 of size 256
2019-10-02 01:35:27.339724: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe64038300 next 895 of size 8192
2019-10-02 01:35:27.339730: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6403a300 next 896 of size 256
2019-10-02 01:35:27.339736: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6403a400 next 897 of size 8192
2019-10-02 01:35:27.339741: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6403c400 next 898 of size 8192
2019-10-02 01:35:27.339747: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6403e400 next 899 of size 256
2019-10-02 01:35:27.339753: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6403e500 next 900 of size 8192
2019-10-02 01:35:27.339759: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe64040500 next 901 of size 16777216
2019-10-02 01:35:27.339764: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe65040500 next 902 of size 67108864
2019-10-02 01:35:27.339775: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe69040500 next 903 of size 16777216
2019-10-02 01:35:27.339781: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6a040500 next 904 of size 256
2019-10-02 01:35:27.339787: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6a040600 next 905 of size 32768
2019-10-02 01:35:27.339794: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6a048600 next 906 of size 256
2019-10-02 01:35:27.339806: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6a048700 next 907 of size 32768
2019-10-02 01:35:27.339816: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6a050700 next 908 of size 16777216
2019-10-02 01:35:27.339830: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6b050700 next 909 of size 256
2019-10-02 01:35:27.339846: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6b050800 next 910 of size 8192
2019-10-02 01:35:27.339862: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6b052800 next 911 of size 256
2019-10-02 01:35:27.339878: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6b052900 next 912 of size 8192
2019-10-02 01:35:27.339893: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6b054900 next 913 of size 8192
2019-10-02 01:35:27.339909: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6b056900 next 914 of size 256
2019-10-02 01:35:27.339924: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6b056a00 next 915 of size 8192
2019-10-02 01:35:27.339939: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6b058a00 next 916 of size 67108864
2019-10-02 01:35:27.339953: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6f058a00 next 917 of size 16777216
2019-10-02 01:35:27.339969: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe70058a00 next 918 of size 16777216
2019-10-02 01:35:27.339984: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe71058a00 next 919 of size 256
2019-10-02 01:35:27.339999: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe71058b00 next 920 of size 8192
2019-10-02 01:35:27.340015: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7105ab00 next 921 of size 256
2019-10-02 01:35:27.340030: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7105ac00 next 922 of size 8192
2019-10-02 01:35:27.340044: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7105cc00 next 923 of size 8192
2019-10-02 01:35:27.340058: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7105ec00 next 924 of size 256
2019-10-02 01:35:27.340073: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7105ed00 next 925 of size 8192
2019-10-02 01:35:27.340089: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe71060d00 next 926 of size 16777216
2019-10-02 01:35:27.340104: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe72060d00 next 927 of size 256
2019-10-02 01:35:27.340119: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe72060e00 next 928 of size 67108864
2019-10-02 01:35:27.340134: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe76060e00 next 929 of size 67108864
2019-10-02 01:35:27.340151: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7a060e00 next 930 of size 67108864
2019-10-02 01:35:27.340166: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e060e00 next 931 of size 8192
2019-10-02 01:35:27.340189: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e062e00 next 932 of size 256
2019-10-02 01:35:27.340206: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e062f00 next 933 of size 8192
2019-10-02 01:35:27.340224: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e064f00 next 934 of size 256
2019-10-02 01:35:27.340244: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e065000 next 935 of size 256
2019-10-02 01:35:27.340261: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e065100 next 622 of size 8388608
2019-10-02 01:35:27.340278: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e865100 next 1016 of size 256
2019-10-02 01:35:27.340293: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e865200 next 564 of size 512
2019-10-02 01:35:27.340311: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e865400 next 604 of size 256
2019-10-02 01:35:27.340327: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e865500 next 624 of size 256
2019-10-02 01:35:27.340342: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e865600 next 613 of size 512
2019-10-02 01:35:27.340358: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e865800 next 1031 of size 256
2019-10-02 01:35:27.340374: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e865900 next 1033 of size 256
2019-10-02 01:35:27.340390: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e865a00 next 1034 of size 256
2019-10-02 01:35:27.340405: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e865b00 next 619 of size 256
2019-10-02 01:35:27.340422: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e865c00 next 611 of size 256
2019-10-02 01:35:27.340436: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e865d00 next 614 of size 256
2019-10-02 01:35:27.340451: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e865e00 next 680 of size 256
2019-10-02 01:35:27.340468: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e865f00 next 659 of size 256
2019-10-02 01:35:27.340483: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e866000 next 566 of size 256
2019-10-02 01:35:27.340498: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e866100 next 649 of size 256
2019-10-02 01:35:27.340514: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e866200 next 617 of size 768
2019-10-02 01:35:27.340531: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e866500 next 608 of size 256
2019-10-02 01:35:27.340547: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e866600 next 588 of size 256
2019-10-02 01:35:27.340562: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e866700 next 644 of size 256
2019-10-02 01:35:27.340579: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e866800 next 661 of size 256
2019-10-02 01:35:27.340595: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e866900 next 591 of size 256
2019-10-02 01:35:27.340611: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e866a00 next 944 of size 256
2019-10-02 01:35:27.340628: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e866b00 next 945 of size 256
2019-10-02 01:35:27.340644: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e866c00 next 946 of size 256
2019-10-02 01:35:27.340661: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e866d00 next 947 of size 256
2019-10-02 01:35:27.340677: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e866e00 next 948 of size 8192
2019-10-02 01:35:27.340692: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e868e00 next 949 of size 256
2019-10-02 01:35:27.340709: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e868f00 next 950 of size 8192
2019-10-02 01:35:27.340726: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e86af00 next 952 of size 256
2019-10-02 01:35:27.340747: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e86b000 next 953 of size 256
2019-10-02 01:35:27.340765: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e86b100 next 954 of size 256
2019-10-02 01:35:27.340781: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e86b200 next 956 of size 256
2019-10-02 01:35:27.340795: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e86b300 next 961 of size 256
2019-10-02 01:35:27.340810: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e86b400 next 962 of size 256
2019-10-02 01:35:27.340829: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e86b500 next 963 of size 256
2019-10-02 01:35:27.340846: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e86b600 next 964 of size 8192
2019-10-02 01:35:27.340862: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e86d600 next 965 of size 8192
2019-10-02 01:35:27.340878: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e86f600 next 966 of size 256
2019-10-02 01:35:27.340893: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e86f700 next 967 of size 8192
2019-10-02 01:35:27.340908: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e871700 next 968 of size 256
2019-10-02 01:35:27.340924: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e871800 next 969 of size 8192
2019-10-02 01:35:27.340940: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e873800 next 971 of size 256
2019-10-02 01:35:27.340955: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e873900 next 972 of size 256
2019-10-02 01:35:27.340971: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e873a00 next 973 of size 256
2019-10-02 01:35:27.340987: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e873b00 next 974 of size 8192
2019-10-02 01:35:27.341004: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e875b00 next 975 of size 8192
2019-10-02 01:35:27.341021: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e877b00 next 633 of size 61184
2019-10-02 01:35:27.341037: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e886a00 next 937 of size 67108864
2019-10-02 01:35:27.341055: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe82886a00 next 938 of size 256
2019-10-02 01:35:27.341072: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe82886b00 next 939 of size 256
2019-10-02 01:35:27.341086: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe82886c00 next 940 of size 256
2019-10-02 01:35:27.341102: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe82886d00 next 941 of size 256
2019-10-02 01:35:27.341117: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe82886e00 next 942 of size 256
2019-10-02 01:35:27.341134: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe82886f00 next 943 of size 16777216
2019-10-02 01:35:27.341151: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe83886f00 next 951 of size 16777216
2019-10-02 01:35:27.341169: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe84886f00 next 955 of size 67108864
2019-10-02 01:35:27.341185: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe88886f00 next 957 of size 67108864
2019-10-02 01:35:27.341202: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe8c886f00 next 958 of size 16777216
2019-10-02 01:35:27.341218: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe8d886f00 next 959 of size 67108864
2019-10-02 01:35:27.341234: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe91886f00 next 960 of size 16777216
2019-10-02 01:35:27.341251: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe92886f00 next 970 of size 16777216
2019-10-02 01:35:27.341276: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe93886f00 next 976 of size 256
2019-10-02 01:35:27.341297: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe93887000 next 977 of size 32768
2019-10-02 01:35:27.341314: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9388f000 next 978 of size 67108864
2019-10-02 01:35:27.341330: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9788f000 next 979 of size 16777216
2019-10-02 01:35:27.341346: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9888f000 next 980 of size 256
2019-10-02 01:35:27.341366: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9888f100 next 981 of size 16777216
2019-10-02 01:35:27.341384: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9988f100 next 982 of size 256
2019-10-02 01:35:27.341400: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9988f200 next 983 of size 256
2019-10-02 01:35:27.341416: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9988f300 next 984 of size 16777216
2019-10-02 01:35:27.341432: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9a88f300 next 985 of size 67108864
2019-10-02 01:35:27.341448: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9e88f300 next 986 of size 256
2019-10-02 01:35:27.341464: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9e88f400 next 987 of size 256
2019-10-02 01:35:27.341480: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9e88f500 next 988 of size 256
2019-10-02 01:35:27.341497: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9e88f600 next 989 of size 256
2019-10-02 01:35:27.341513: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9e88f700 next 990 of size 256
2019-10-02 01:35:27.341545: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9e88f800 next 991 of size 256
2019-10-02 01:35:27.341563: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9e88f900 next 992 of size 16777216
2019-10-02 01:35:27.341581: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9f88f900 next 993 of size 8192
2019-10-02 01:35:27.341597: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9f891900 next 994 of size 256
2019-10-02 01:35:27.341613: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9f891a00 next 995 of size 8192
2019-10-02 01:35:27.341628: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9f893a00 next 996 of size 8192
2019-10-02 01:35:27.341643: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9f895a00 next 997 of size 256
2019-10-02 01:35:27.341660: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9f895b00 next 998 of size 8192
2019-10-02 01:35:27.341677: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9f897b00 next 999 of size 67108864
2019-10-02 01:35:27.341694: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efea3897b00 next 1000 of size 67108864
2019-10-02 01:35:27.341710: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efea7897b00 next 1001 of size 256
2019-10-02 01:35:27.341726: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efea7897c00 next 1002 of size 256
2019-10-02 01:35:27.341742: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efea7897d00 next 1003 of size 256
2019-10-02 01:35:27.341758: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efea7897e00 next 1004 of size 256
2019-10-02 01:35:27.341774: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efea7897f00 next 1005 of size 67108864
2019-10-02 01:35:27.341790: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efeab897f00 next 1006 of size 256
2019-10-02 01:35:27.341808: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efeab898000 next 1007 of size 256
2019-10-02 01:35:27.341835: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efeab898100 next 601 of size 16777216
2019-10-02 01:35:27.341854: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efeac898100 next 594 of size 8388608
2019-10-02 01:35:27.341874: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efead098100 next 620 of size 268435456
2019-10-02 01:35:27.341891: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd098100 next 629 of size 131072
2019-10-02 01:35:27.341909: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b8100 next 572 of size 256
2019-10-02 01:35:27.341930: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b8200 next 587 of size 256
2019-10-02 01:35:27.341945: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b8300 next 696 of size 256
2019-10-02 01:35:27.341961: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b8400 next 623 of size 256
2019-10-02 01:35:27.341978: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b8500 next 1037 of size 512
2019-10-02 01:35:27.341994: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b8700 next 1041 of size 512
2019-10-02 01:35:27.342010: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b8900 next 1047 of size 256
2019-10-02 01:35:27.342028: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b8a00 next 1059 of size 256
2019-10-02 01:35:27.342045: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b8b00 next 1060 of size 256
2019-10-02 01:35:27.342061: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b8c00 next 1061 of size 256
2019-10-02 01:35:27.342079: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b8d00 next 1076 of size 512
2019-10-02 01:35:27.342096: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b8f00 next 1077 of size 512
2019-10-02 01:35:27.342112: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b9100 next 1079 of size 256
2019-10-02 01:35:27.342129: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b9200 next 1098 of size 256
2019-10-02 01:35:27.342145: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b9300 next 1102 of size 256
2019-10-02 01:35:27.342161: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b9400 next 1106 of size 256
2019-10-02 01:35:27.342176: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b9500 next 1113 of size 512
2019-10-02 01:35:27.342191: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b9700 next 1121 of size 512
2019-10-02 01:35:27.342208: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b9900 next 1122 of size 256
2019-10-02 01:35:27.342223: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b9a00 next 1123 of size 256
2019-10-02 01:35:27.342238: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b9b00 next 1125 of size 256
2019-10-02 01:35:27.342255: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b9c00 next 1126 of size 256
2019-10-02 01:35:27.342271: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b9d00 next 1127 of size 256
2019-10-02 01:35:27.342287: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b9e00 next 1021 of size 256
2019-10-02 01:35:27.342305: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b9f00 next 1022 of size 256
2019-10-02 01:35:27.342322: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0ba000 next 1023 of size 256
2019-10-02 01:35:27.342339: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0ba100 next 1024 of size 256
2019-10-02 01:35:27.342367: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0ba200 next 1025 of size 256
2019-10-02 01:35:27.342387: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0ba300 next 1026 of size 256
2019-10-02 01:35:27.342404: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0ba400 next 596 of size 131072000
2019-10-02 01:35:27.342421: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efec4dba400 next 571 of size 268435456
2019-10-02 01:35:27.342438: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efed4dba400 next 558 of size 131072
2019-10-02 01:35:27.342456: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efed4dda400 next 597 of size 4194304000
2019-10-02 01:35:27.342475: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effcedda400 next 595 of size 67108864
2019-10-02 01:35:27.342492: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effd2dda400 next 638 of size 268435456
2019-10-02 01:35:27.342508: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effe2dda400 next 626 of size 131072
2019-10-02 01:35:27.342524: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effe2dfa400 next 648 of size 16777216
2019-10-02 01:35:27.342541: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effe3dfa400 next 674 of size 16777216
2019-10-02 01:35:27.342558: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effe4dfa400 next 589 of size 8192
2019-10-02 01:35:27.342574: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effe4dfc400 next 615 of size 8192
2019-10-02 01:35:27.342590: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effe4dfe400 next 658 of size 8192
2019-10-02 01:35:27.342606: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effe4e00400 next 698 of size 8192
2019-10-02 01:35:27.342622: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effe4e02400 next 676 of size 67108864
2019-10-02 01:35:27.342639: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effe8e02400 next 646 of size 8192
2019-10-02 01:35:27.342656: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effe8e04400 next 692 of size 16777216
2019-10-02 01:35:27.342672: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effe9e04400 next 682 of size 16384000
2019-10-02 01:35:27.342689: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effeada4400 next 681 of size 32768
2019-10-02 01:35:27.342705: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effeadac400 next 675 of size 8192
2019-10-02 01:35:27.342723: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effeadae400 next 612 of size 16384000
2019-10-02 01:35:27.342741: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effebd4e400 next 568 of size 16384000
2019-10-02 01:35:27.342757: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effeccee400 next 573 of size 16384000
2019-10-02 01:35:27.342772: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effedc8e400 next 625 of size 16384000
2019-10-02 01:35:27.342788: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effeec2e400 next 565 of size 8192
2019-10-02 01:35:27.342803: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effeec30400 next 670 of size 16384000
2019-10-02 01:35:27.342819: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effefbd0400 next 560 of size 67108864
2019-10-02 01:35:27.342835: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efff3bd0400 next 640 of size 16384000
2019-10-02 01:35:27.342851: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efff4b70400 next 561 of size 16384000
2019-10-02 01:35:27.342867: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efff5b10400 next 651 of size 16384000
2019-10-02 01:35:27.342891: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efff6ab0400 next 656 of size 16384000
2019-10-02 01:35:27.342910: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efff7a50400 next 645 of size 16384000
2019-10-02 01:35:27.342926: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efff89f0400 next 641 of size 8192
2019-10-02 01:35:27.342943: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efff89f2400 next 654 of size 8192
2019-10-02 01:35:27.342961: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efff89f4400 next 556 of size 16384000
2019-10-02 01:35:27.342977: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efff9994400 next 574 of size 16384000
2019-10-02 01:35:27.342995: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffa934400 next 570 of size 8192
2019-10-02 01:35:27.343017: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffa936400 next 562 of size 8192
2019-10-02 01:35:27.343034: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffa938400 next 1008 of size 8192
2019-10-02 01:35:27.343050: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffa93a400 next 666 of size 8192
2019-10-02 01:35:27.343067: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffa93c400 next 567 of size 8192
2019-10-02 01:35:27.343085: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffa93e400 next 557 of size 16384000
2019-10-02 01:35:27.343102: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffb8de400 next 691 of size 16384000
2019-10-02 01:35:27.343119: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffc87e400 next 634 of size 8192
2019-10-02 01:35:27.343136: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffc880400 next 631 of size 8192
2019-10-02 01:35:27.343154: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffc882400 next 672 of size 8192
2019-10-02 01:35:27.343171: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffc884400 next 679 of size 8192
2019-10-02 01:35:27.343187: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffc886400 next 669 of size 16384000
2019-10-02 01:35:27.343205: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffd826400 next 628 of size 8192
2019-10-02 01:35:27.343221: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffd828400 next 610 of size 8192
2019-10-02 01:35:27.343241: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffd82a400 next 653 of size 8192
2019-10-02 01:35:27.343258: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffd82c400 next 650 of size 8192
2019-10-02 01:35:27.343276: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffd82e400 next 713 of size 8192
2019-10-02 01:35:27.343293: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffd830400 next 632 of size 8192
2019-10-02 01:35:27.343348: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffd832400 next 575 of size 16777216
2019-10-02 01:35:27.343366: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffe832400 next 664 of size 8192
2019-10-02 01:35:27.343384: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffe834400 next 618 of size 8192
2019-10-02 01:35:27.343397: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffe836400 next 621 of size 8192
2019-10-02 01:35:27.343414: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffe838400 next 555 of size 16777216
2019-10-02 01:35:27.343430: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effff838400 next 602 of size 8192
2019-10-02 01:35:27.343447: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effff83a400 next 606 of size 32768
2019-10-02 01:35:27.343462: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effff842400 next 600 of size 8192
2019-10-02 01:35:27.343491: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effff844400 next 609 of size 8192
2019-10-02 01:35:27.343508: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effff846400 next 593 of size 8192
2019-10-02 01:35:27.343524: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effff848400 next 592 of size 8192
2019-10-02 01:35:27.343542: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effff84a400 next 652 of size 8192
2019-10-02 01:35:27.343560: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effff84c400 next 1012 of size 16777216
2019-10-02 01:35:27.343578: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f000084c400 next 647 of size 8192
2019-10-02 01:35:27.343595: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f000084e400 next 660 of size 8192
2019-10-02 01:35:27.343613: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0000850400 next 603 of size 8192
2019-10-02 01:35:27.343631: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0000852400 next 673 of size 8192
2019-10-02 01:35:27.343648: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0000854400 next 1020 of size 8192
2019-10-02 01:35:27.343664: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0000856400 next 1015 of size 8192
2019-10-02 01:35:27.343681: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0000858400 next 643 of size 16777216
2019-10-02 01:35:27.343699: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0001858400 next 1009 of size 67108864
2019-10-02 01:35:27.343717: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0005858400 next 1010 of size 16777216
2019-10-02 01:35:27.343734: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0006858400 next 616 of size 16777216
2019-10-02 01:35:27.343753: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0007858400 next 663 of size 16777216
2019-10-02 01:35:27.343770: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0008858400 next 1013 of size 8192
2019-10-02 01:35:27.343787: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f000885a400 next 1014 of size 8192
2019-10-02 01:35:27.343804: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f000885c400 next 671 of size 16777216
2019-10-02 01:35:27.343820: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f000985c400 next 607 of size 67108864
2019-10-02 01:35:27.343837: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f000d85c400 next 635 of size 16777216
2019-10-02 01:35:27.343854: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f000e85c400 next 662 of size 16777216
2019-10-02 01:35:27.343872: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f000f85c400 next 636 of size 8192
2019-10-02 01:35:27.343889: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f000f85e400 next 668 of size 8192
2019-10-02 01:35:27.343906: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f000f860400 next 642 of size 8192
2019-10-02 01:35:27.343922: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f000f862400 next 667 of size 32768
2019-10-02 01:35:27.343938: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f000f86a400 next 569 of size 8192
2019-10-02 01:35:27.343953: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f000f86c400 next 639 of size 67108864
2019-10-02 01:35:27.343969: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f001386c400 next 1017 of size 8192
2019-10-02 01:35:27.343983: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f001386e400 next 1018 of size 8192
2019-10-02 01:35:27.343999: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0013870400 next 627 of size 8192
2019-10-02 01:35:27.344023: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0013872400 next 598 of size 8192
2019-10-02 01:35:27.344039: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0013874400 next 655 of size 8192
2019-10-02 01:35:27.344056: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0013876400 next 678 of size 8192
2019-10-02 01:35:27.344072: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0013878400 next 599 of size 8192
2019-10-02 01:35:27.344088: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f001387a400 next 689 of size 8192
2019-10-02 01:35:27.344103: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f001387c400 next 693 of size 8192
2019-10-02 01:35:27.344119: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f001387e400 next 563 of size 32768
2019-10-02 01:35:27.344135: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0013886400 next 1019 of size 16777216
2019-10-02 01:35:27.344152: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0014886400 next 719 of size 16777216
2019-10-02 01:35:27.344167: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0015886400 next 695 of size 8192
2019-10-02 01:35:27.344185: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0015888400 next 694 of size 16777216
2019-10-02 01:35:27.344201: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0016888400 next 1027 of size 16777216
2019-10-02 01:35:27.344218: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0017888400 next 1028 of size 16777216
2019-10-02 01:35:27.344234: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0018888400 next 1029 of size 16777216
2019-10-02 01:35:27.344250: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0019888400 next 1030 of size 67108864
2019-10-02 01:35:27.344268: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f001d888400 next 1032 of size 8192
2019-10-02 01:35:27.344292: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f001d88a400 next 1035 of size 67108864
2019-10-02 01:35:27.344308: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f002188a400 next 1036 of size 16777216
2019-10-02 01:35:27.344326: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f002288a400 next 1038 of size 8192
2019-10-02 01:35:27.344344: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f002288c400 next 1039 of size 67108864
2019-10-02 01:35:27.344361: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f002688c400 next 1040 of size 8192
2019-10-02 01:35:27.344377: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f002688e400 next 1042 of size 8192
2019-10-02 01:35:27.344393: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0026890400 next 1043 of size 16777216
2019-10-02 01:35:27.344409: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0027890400 next 1044 of size 8192
2019-10-02 01:35:27.344425: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0027892400 next 1045 of size 16777216
2019-10-02 01:35:27.344442: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0028892400 next 1046 of size 8192
2019-10-02 01:35:27.344458: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0028894400 next 1048 of size 16777216
2019-10-02 01:35:27.344474: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0029894400 next 1049 of size 8192
2019-10-02 01:35:27.344491: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0029896400 next 1050 of size 16777216
2019-10-02 01:35:27.344509: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f002a896400 next 1051 of size 8192
2019-10-02 01:35:27.344525: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f002a898400 next 1052 of size 8192
2019-10-02 01:35:27.344548: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f002a89a400 next 1053 of size 8192
2019-10-02 01:35:27.344565: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f002a89c400 next 1054 of size 67108864
2019-10-02 01:35:27.344580: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f002e89c400 next 1055 of size 16777216
2019-10-02 01:35:27.344597: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f002f89c400 next 1056 of size 8192
2019-10-02 01:35:27.344616: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f002f89e400 next 1057 of size 32768
2019-10-02 01:35:27.344634: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f002f8a6400 next 1058 of size 8192
2019-10-02 01:35:27.344652: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f002f8a8400 next 1062 of size 16777216
2019-10-02 01:35:27.344668: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00308a8400 next 1063 of size 8192
2019-10-02 01:35:27.344685: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00308aa400 next 1064 of size 32768
2019-10-02 01:35:27.344704: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00308b2400 next 1065 of size 8192
2019-10-02 01:35:27.344720: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00308b4400 next 1066 of size 67108864
2019-10-02 01:35:27.344736: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00348b4400 next 1067 of size 8192
2019-10-02 01:35:27.344754: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00348b6400 next 1068 of size 8192
2019-10-02 01:35:27.344772: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00348b8400 next 1069 of size 8192
2019-10-02 01:35:27.344789: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00348ba400 next 1070 of size 8192
2019-10-02 01:35:27.344805: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00348bc400 next 1071 of size 32768
2019-10-02 01:35:27.344826: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00348c4400 next 1072 of size 8192
2019-10-02 01:35:27.344845: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00348c6400 next 1073 of size 8192
2019-10-02 01:35:27.344862: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00348c8400 next 1074 of size 67108864
2019-10-02 01:35:27.344879: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00388c8400 next 1075 of size 8192
2019-10-02 01:35:27.344896: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00388ca400 next 1078 of size 8192
2019-10-02 01:35:27.344912: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00388cc400 next 1080 of size 16777216
2019-10-02 01:35:27.344928: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00398cc400 next 1081 of size 8192
2019-10-02 01:35:27.344944: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00398ce400 next 1082 of size 16777216
2019-10-02 01:35:27.344962: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f003a8ce400 next 1083 of size 8192
2019-10-02 01:35:27.344979: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f003a8d0400 next 1084 of size 8192
2019-10-02 01:35:27.344996: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f003a8d2400 next 1085 of size 8192
2019-10-02 01:35:27.345014: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f003a8d4400 next 1086 of size 8192
2019-10-02 01:35:27.345031: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f003a8d6400 next 1087 of size 16777216
2019-10-02 01:35:27.345047: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f003b8d6400 next 1088 of size 67108864
2019-10-02 01:35:27.345064: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f003f8d6400 next 1089 of size 67108864
2019-10-02 01:35:27.345085: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00438d6400 next 1090 of size 8192
2019-10-02 01:35:27.345103: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00438d8400 next 1091 of size 8192
2019-10-02 01:35:27.345120: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00438da400 next 1092 of size 8192
2019-10-02 01:35:27.345138: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00438dc400 next 1093 of size 16777216
2019-10-02 01:35:27.345154: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00448dc400 next 1094 of size 8192
2019-10-02 01:35:27.345165: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00448de400 next 1095 of size 16777216
2019-10-02 01:35:27.345174: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00458de400 next 1096 of size 8192
2019-10-02 01:35:27.345184: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00458e0400 next 1097 of size 8192
2019-10-02 01:35:27.345193: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00458e2400 next 1099 of size 67108864
2019-10-02 01:35:27.345209: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00498e2400 next 1100 of size 16777216
2019-10-02 01:35:27.345224: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f004a8e2400 next 1101 of size 8192
2019-10-02 01:35:27.345241: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f004a8e4400 next 1103 of size 16777216
2019-10-02 01:35:27.345258: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f004b8e4400 next 1104 of size 32768
2019-10-02 01:35:27.345274: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f004b8ec400 next 1105 of size 8192
2019-10-02 01:35:27.345291: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f004b8ee400 next 1107 of size 16777216
2019-10-02 01:35:27.345316: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f004c8ee400 next 1108 of size 32768
2019-10-02 01:35:27.345335: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f004c8f6400 next 1109 of size 16777216
2019-10-02 01:35:27.345352: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f004d8f6400 next 1110 of size 8192
2019-10-02 01:35:27.345369: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f004d8f8400 next 1111 of size 8192
2019-10-02 01:35:27.345387: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f004d8fa400 next 1112 of size 8192
2019-10-02 01:35:27.345404: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f004d8fc400 next 1114 of size 8192
2019-10-02 01:35:27.345420: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f004d8fe400 next 1115 of size 67108864
2019-10-02 01:35:27.345437: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00518fe400 next 1116 of size 8192
2019-10-02 01:35:27.345452: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0051900400 next 1117 of size 8192
2019-10-02 01:35:27.345468: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0051902400 next 1118 of size 16777216
2019-10-02 01:35:27.345485: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0052902400 next 1119 of size 8192
2019-10-02 01:35:27.345501: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0052904400 next 1120 of size 8192
2019-10-02 01:35:27.345516: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0052906400 next 1124 of size 67108864
2019-10-02 01:35:27.345534: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0056906400 next 18446744073709551615 of size 74560768
2019-10-02 01:35:27.345551: I tensorflow/core/common_runtime/bfc_allocator.cc:914]      Summary of in-use Chunks by size: 
2019-10-02 01:35:27.345584: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 242 Chunks of size 256 totalling 60.5KiB
2019-10-02 01:35:27.345607: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 45 Chunks of size 512 totalling 22.5KiB
2019-10-02 01:35:27.345638: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 768 totalling 768B
2019-10-02 01:35:27.345659: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 1280 totalling 1.2KiB
2019-10-02 01:35:27.345676: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 4096 totalling 4.0KiB
2019-10-02 01:35:27.345693: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 5376 totalling 5.2KiB
2019-10-02 01:35:27.345711: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 6912 totalling 6.8KiB
2019-10-02 01:35:27.345729: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 448 Chunks of size 8192 totalling 3.50MiB
2019-10-02 01:35:27.345749: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 9984 totalling 9.8KiB
2019-10-02 01:35:27.345768: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 11264 totalling 11.0KiB
2019-10-02 01:35:27.345786: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 13056 totalling 12.8KiB
2019-10-02 01:35:27.345806: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 45 Chunks of size 32768 totalling 1.41MiB
2019-10-02 01:35:27.345824: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 56064 totalling 54.8KiB
2019-10-02 01:35:27.345844: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 61184 totalling 59.8KiB
2019-10-02 01:35:27.345865: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 7 Chunks of size 131072 totalling 896.0KiB
2019-10-02 01:35:27.345884: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 3 Chunks of size 8388608 totalling 24.00MiB
2019-10-02 01:35:27.345904: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 32 Chunks of size 16384000 totalling 500.00MiB
2019-10-02 01:35:27.345923: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 192 Chunks of size 16777216 totalling 3.00GiB
2019-10-02 01:35:27.345940: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 97 Chunks of size 67108864 totalling 6.06GiB
2019-10-02 01:35:27.345958: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 74560768 totalling 71.11MiB
2019-10-02 01:35:27.345976: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 131072000 totalling 125.00MiB
2019-10-02 01:35:27.345994: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 262144000 totalling 250.00MiB
2019-10-02 01:35:27.346012: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 3 Chunks of size 268435456 totalling 768.00MiB
2019-10-02 01:35:27.346031: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 4194304000 totalling 3.91GiB
2019-10-02 01:35:27.346050: I tensorflow/core/common_runtime/bfc_allocator.cc:921] Sum Total of in-use chunks: 14.67GiB
2019-10-02 01:35:27.346067: I tensorflow/core/common_runtime/bfc_allocator.cc:923] total_region_allocated_bytes_: 15753943296 memory_limit_: 15753943450 available bytes: 154 curr_region_allocation_bytes_: 31507887104
2019-10-02 01:35:27.346090: I tensorflow/core/common_runtime/bfc_allocator.cc:929] Stats: 
Limit:                 15753943450
InUse:                 15753943296
MaxInUse:              15753943296
NumAllocs:                    2276
MaxAllocSize:           4194304000

2019-10-02 01:35:27.346218: W tensorflow/core/common_runtime/bfc_allocator.cc:424] ****************************************************************************************************
2019-10-02 01:35:29.722716: W tensorflow/core/common_runtime/bfc_allocator.cc:419] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8B (rounded to 256).  Current allocation summary follows.
2019-10-02 01:35:29.722824: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (256): 	Total Chunks: 242, Chunks in use: 242. 60.5KiB allocated for chunks. 60.5KiB in use in bin. 4.2KiB client-requested in use in bin.
2019-10-02 01:35:29.722842: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (512): 	Total Chunks: 46, Chunks in use: 46. 23.2KiB allocated for chunks. 23.2KiB in use in bin. 23.0KiB client-requested in use in bin.
2019-10-02 01:35:29.722862: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (1024): 	Total Chunks: 1, Chunks in use: 1. 1.2KiB allocated for chunks. 1.2KiB in use in bin. 1.0KiB client-requested in use in bin.
2019-10-02 01:35:29.722870: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (2048): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-10-02 01:35:29.722879: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (4096): 	Total Chunks: 3, Chunks in use: 3. 16.0KiB allocated for chunks. 16.0KiB in use in bin. 12.0KiB client-requested in use in bin.
2019-10-02 01:35:29.722887: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (8192): 	Total Chunks: 451, Chunks in use: 451. 3.53MiB allocated for chunks. 3.53MiB in use in bin. 3.52MiB client-requested in use in bin.
2019-10-02 01:35:29.722895: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (16384): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-10-02 01:35:29.722902: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (32768): 	Total Chunks: 47, Chunks in use: 47. 1.52MiB allocated for chunks. 1.52MiB in use in bin. 1.47MiB client-requested in use in bin.
2019-10-02 01:35:29.722908: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (65536): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-10-02 01:35:29.722919: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (131072): 	Total Chunks: 7, Chunks in use: 7. 896.0KiB allocated for chunks. 896.0KiB in use in bin. 896.0KiB client-requested in use in bin.
2019-10-02 01:35:29.722925: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (262144): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-10-02 01:35:29.722932: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (524288): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-10-02 01:35:29.722938: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (1048576): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-10-02 01:35:29.722945: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (2097152): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-10-02 01:35:29.722951: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (4194304): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-10-02 01:35:29.722958: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (8388608): 	Total Chunks: 35, Chunks in use: 35. 524.00MiB allocated for chunks. 524.00MiB in use in bin. 524.00MiB client-requested in use in bin.
2019-10-02 01:35:29.722965: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (16777216): 	Total Chunks: 192, Chunks in use: 192. 3.00GiB allocated for chunks. 3.00GiB in use in bin. 3.00GiB client-requested in use in bin.
2019-10-02 01:35:29.722972: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (33554432): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-10-02 01:35:29.722978: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (67108864): 	Total Chunks: 99, Chunks in use: 99. 6.25GiB allocated for chunks. 6.25GiB in use in bin. 6.25GiB client-requested in use in bin.
2019-10-02 01:35:29.722986: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (134217728): 	Total Chunks: 1, Chunks in use: 1. 250.00MiB allocated for chunks. 250.00MiB in use in bin. 250.00MiB client-requested in use in bin.
2019-10-02 01:35:29.722996: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (268435456): 	Total Chunks: 4, Chunks in use: 4. 4.66GiB allocated for chunks. 4.66GiB in use in bin. 4.66GiB client-requested in use in bin.
2019-10-02 01:35:29.723004: I tensorflow/core/common_runtime/bfc_allocator.cc:885] Bin for 256B was 256B, Chunk State: 
2019-10-02 01:35:29.723010: I tensorflow/core/common_runtime/bfc_allocator.cc:898] Next region of size 15753943296
2019-10-02 01:35:29.723019: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0000000 next 1 of size 1280
2019-10-02 01:35:29.723028: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0000500 next 2 of size 256
2019-10-02 01:35:29.723034: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0000600 next 3 of size 256
2019-10-02 01:35:29.723039: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0000700 next 4 of size 256
2019-10-02 01:35:29.723045: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0000800 next 5 of size 256
2019-10-02 01:35:29.723051: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0000900 next 6 of size 256
2019-10-02 01:35:29.723057: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0000a00 next 7 of size 256
2019-10-02 01:35:29.723062: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0000b00 next 8 of size 256
2019-10-02 01:35:29.723068: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0000c00 next 9 of size 256
2019-10-02 01:35:29.723075: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0000d00 next 10 of size 256
2019-10-02 01:35:29.723080: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0000e00 next 11 of size 256
2019-10-02 01:35:29.723086: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0000f00 next 12 of size 256
2019-10-02 01:35:29.723092: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0001000 next 13 of size 256
2019-10-02 01:35:29.723097: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0001100 next 14 of size 256
2019-10-02 01:35:29.723103: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0001200 next 15 of size 256
2019-10-02 01:35:29.723109: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0001300 next 16 of size 256
2019-10-02 01:35:29.723115: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0001400 next 17 of size 256
2019-10-02 01:35:29.723121: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0001500 next 18 of size 256
2019-10-02 01:35:29.723126: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0001600 next 19 of size 256
2019-10-02 01:35:29.723132: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0001700 next 20 of size 256
2019-10-02 01:35:29.723137: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0001800 next 21 of size 256
2019-10-02 01:35:29.723143: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0001900 next 22 of size 256
2019-10-02 01:35:29.723149: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0001a00 next 23 of size 256
2019-10-02 01:35:29.723154: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0001b00 next 24 of size 256
2019-10-02 01:35:29.723160: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0001c00 next 25 of size 256
2019-10-02 01:35:29.723166: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0001d00 next 26 of size 256
2019-10-02 01:35:29.723171: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0001e00 next 27 of size 256
2019-10-02 01:35:29.723177: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0001f00 next 28 of size 256
2019-10-02 01:35:29.723188: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0002000 next 29 of size 256
2019-10-02 01:35:29.723194: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0002100 next 30 of size 256
2019-10-02 01:35:29.723199: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0002200 next 31 of size 256
2019-10-02 01:35:29.723205: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0002300 next 32 of size 256
2019-10-02 01:35:29.723211: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0002400 next 33 of size 256
2019-10-02 01:35:29.723217: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0002500 next 34 of size 256
2019-10-02 01:35:29.723223: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0002600 next 35 of size 256
2019-10-02 01:35:29.723228: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0002700 next 36 of size 256
2019-10-02 01:35:29.723234: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0002800 next 37 of size 256
2019-10-02 01:35:29.723240: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0002900 next 38 of size 256
2019-10-02 01:35:29.723245: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0002a00 next 39 of size 256
2019-10-02 01:35:29.723252: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0002b00 next 40 of size 256
2019-10-02 01:35:29.723258: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0002c00 next 41 of size 256
2019-10-02 01:35:29.723264: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0002d00 next 42 of size 256
2019-10-02 01:35:29.723270: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0002e00 next 43 of size 256
2019-10-02 01:35:29.723275: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0002f00 next 44 of size 8192
2019-10-02 01:35:29.723281: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb0004f00 next 45 of size 67108864
2019-10-02 01:35:29.723288: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb4004f00 next 46 of size 67108864
2019-10-02 01:35:29.723293: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb8004f00 next 47 of size 8192
2019-10-02 01:35:29.723341: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb8006f00 next 48 of size 32768
2019-10-02 01:35:29.723348: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb800ef00 next 49 of size 8192
2019-10-02 01:35:29.723354: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb8010f00 next 50 of size 16777216
2019-10-02 01:35:29.723360: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb9010f00 next 51 of size 8192
2019-10-02 01:35:29.723365: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb9012f00 next 52 of size 8192
2019-10-02 01:35:29.723371: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb9014f00 next 53 of size 32768
2019-10-02 01:35:29.723377: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb901cf00 next 54 of size 8192
2019-10-02 01:35:29.723383: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb901ef00 next 55 of size 8192
2019-10-02 01:35:29.723389: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb9020f00 next 56 of size 32768
2019-10-02 01:35:29.723394: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb9028f00 next 57 of size 8192
2019-10-02 01:35:29.723400: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb902af00 next 58 of size 8192
2019-10-02 01:35:29.723407: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb902cf00 next 59 of size 512
2019-10-02 01:35:29.723413: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb902d100 next 60 of size 8192
2019-10-02 01:35:29.723419: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcb902f100 next 61 of size 16777216
2019-10-02 01:35:29.723426: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcba02f100 next 62 of size 8192
2019-10-02 01:35:29.723432: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcba031100 next 63 of size 16777216
2019-10-02 01:35:29.723437: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcbb031100 next 64 of size 8192
2019-10-02 01:35:29.723443: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcbb033100 next 65 of size 8192
2019-10-02 01:35:29.723449: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcbb035100 next 66 of size 16777216
2019-10-02 01:35:29.723455: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcbc035100 next 67 of size 32768
2019-10-02 01:35:29.723460: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcbc03d100 next 68 of size 16777216
2019-10-02 01:35:29.723466: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcbd03d100 next 69 of size 8192
2019-10-02 01:35:29.723472: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcbd03f100 next 70 of size 8192
2019-10-02 01:35:29.723477: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcbd041100 next 71 of size 8192
2019-10-02 01:35:29.723483: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcbd043100 next 72 of size 67108864
2019-10-02 01:35:29.723489: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcc1043100 next 73 of size 8192
2019-10-02 01:35:29.723495: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcc1045100 next 74 of size 67108864
2019-10-02 01:35:29.723501: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcc5045100 next 75 of size 8192
2019-10-02 01:35:29.723507: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcc5047100 next 76 of size 8192
2019-10-02 01:35:29.723513: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcc5049100 next 77 of size 8192
2019-10-02 01:35:29.723518: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcc504b100 next 78 of size 8192
2019-10-02 01:35:29.723525: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcc504d100 next 79 of size 67108864
2019-10-02 01:35:29.723531: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcc904d100 next 80 of size 16777216
2019-10-02 01:35:29.723536: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcca04d100 next 81 of size 8192
2019-10-02 01:35:29.723542: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcca04f100 next 82 of size 16777216
2019-10-02 01:35:29.723547: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efccb04f100 next 83 of size 8192
2019-10-02 01:35:29.723553: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efccb051100 next 84 of size 16777216
2019-10-02 01:35:29.723559: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efccc051100 next 85 of size 8192
2019-10-02 01:35:29.723564: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efccc053100 next 86 of size 67108864
2019-10-02 01:35:29.723570: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd0053100 next 87 of size 8192
2019-10-02 01:35:29.723576: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd0055100 next 88 of size 32768
2019-10-02 01:35:29.723582: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd005d100 next 89 of size 8192
2019-10-02 01:35:29.723587: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd005f100 next 90 of size 8192
2019-10-02 01:35:29.723593: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd0061100 next 91 of size 8192
2019-10-02 01:35:29.723598: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd0063100 next 92 of size 8192
2019-10-02 01:35:29.723611: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd0065100 next 93 of size 8192
2019-10-02 01:35:29.723617: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd0067100 next 94 of size 16777216
2019-10-02 01:35:29.723623: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd1067100 next 95 of size 8192
2019-10-02 01:35:29.723629: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd1069100 next 96 of size 16384000
2019-10-02 01:35:29.723636: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd2009100 next 97 of size 16384000
2019-10-02 01:35:29.723642: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd2fa9100 next 98 of size 8192
2019-10-02 01:35:29.723647: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd2fab100 next 99 of size 512
2019-10-02 01:35:29.723653: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd2fab300 next 100 of size 16777216
2019-10-02 01:35:29.723659: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd3fab300 next 101 of size 16777216
2019-10-02 01:35:29.723665: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd4fab300 next 102 of size 67108864
2019-10-02 01:35:29.723671: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd8fab300 next 103 of size 16777216
2019-10-02 01:35:29.723677: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcd9fab300 next 104 of size 16777216
2019-10-02 01:35:29.723683: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcdafab300 next 105 of size 8192
2019-10-02 01:35:29.723689: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcdafad300 next 106 of size 512
2019-10-02 01:35:29.723694: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcdafad500 next 107 of size 16777216
2019-10-02 01:35:29.723700: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcdbfad500 next 108 of size 512
2019-10-02 01:35:29.723706: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcdbfad700 next 109 of size 16777216
2019-10-02 01:35:29.723712: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcdcfad700 next 110 of size 8192
2019-10-02 01:35:29.723717: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcdcfaf700 next 111 of size 8192
2019-10-02 01:35:29.723723: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcdcfb1700 next 112 of size 8192
2019-10-02 01:35:29.723729: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcdcfb3700 next 113 of size 8192
2019-10-02 01:35:29.723734: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcdcfb5700 next 114 of size 8192
2019-10-02 01:35:29.723740: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcdcfb7700 next 115 of size 16777216
2019-10-02 01:35:29.723745: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcddfb7700 next 116 of size 67108864
2019-10-02 01:35:29.723751: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce1fb7700 next 117 of size 512
2019-10-02 01:35:29.723756: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce1fb7900 next 118 of size 8192
2019-10-02 01:35:29.723762: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce1fb9900 next 119 of size 8192
2019-10-02 01:35:29.723768: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce1fbb900 next 120 of size 8192
2019-10-02 01:35:29.723773: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce1fbd900 next 121 of size 16777216
2019-10-02 01:35:29.723779: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce2fbd900 next 122 of size 8192
2019-10-02 01:35:29.723785: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce2fbf900 next 123 of size 8192
2019-10-02 01:35:29.723791: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce2fc1900 next 124 of size 16777216
2019-10-02 01:35:29.723802: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce3fc1900 next 125 of size 512
2019-10-02 01:35:29.723808: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce3fc1b00 next 126 of size 67108864
2019-10-02 01:35:29.723814: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce7fc1b00 next 127 of size 8192
2019-10-02 01:35:29.723819: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce7fc3b00 next 128 of size 8192
2019-10-02 01:35:29.723825: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce7fc5b00 next 129 of size 8192
2019-10-02 01:35:29.723831: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce7fc7b00 next 130 of size 8192
2019-10-02 01:35:29.723837: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce7fc9b00 next 131 of size 16777216
2019-10-02 01:35:29.723842: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce8fc9b00 next 132 of size 8192
2019-10-02 01:35:29.723848: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce8fcbb00 next 133 of size 16777216
2019-10-02 01:35:29.723853: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce9fcbb00 next 134 of size 8192
2019-10-02 01:35:29.723859: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efce9fcdb00 next 135 of size 16777216
2019-10-02 01:35:29.723865: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efceafcdb00 next 136 of size 8192
2019-10-02 01:35:29.723870: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efceafcfb00 next 137 of size 8192
2019-10-02 01:35:29.723876: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efceafd1b00 next 138 of size 8192
2019-10-02 01:35:29.723882: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efceafd3b00 next 139 of size 8192
2019-10-02 01:35:29.723888: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efceafd5b00 next 140 of size 8192
2019-10-02 01:35:29.723893: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efceafd7b00 next 141 of size 8192
2019-10-02 01:35:29.723899: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efceafd9b00 next 142 of size 16777216
2019-10-02 01:35:29.723906: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcebfd9b00 next 143 of size 16777216
2019-10-02 01:35:29.723911: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcecfd9b00 next 144 of size 8192
2019-10-02 01:35:29.723917: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcecfdbb00 next 145 of size 8192
2019-10-02 01:35:29.723923: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcecfddb00 next 146 of size 8192
2019-10-02 01:35:29.723928: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcecfdfb00 next 147 of size 32768
2019-10-02 01:35:29.723934: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcecfe7b00 next 148 of size 8192
2019-10-02 01:35:29.723939: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcecfe9b00 next 149 of size 512
2019-10-02 01:35:29.723945: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcecfe9d00 next 150 of size 8192
2019-10-02 01:35:29.723951: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcecfebd00 next 151 of size 8192
2019-10-02 01:35:29.723956: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcecfedd00 next 152 of size 16777216
2019-10-02 01:35:29.723962: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcedfedd00 next 153 of size 8192
2019-10-02 01:35:29.723968: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcedfefd00 next 154 of size 16777216
2019-10-02 01:35:29.723974: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efceefefd00 next 155 of size 8192
2019-10-02 01:35:29.723986: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efceeff1d00 next 156 of size 8192
2019-10-02 01:35:29.723992: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efceeff3d00 next 157 of size 8192
2019-10-02 01:35:29.723997: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efceeff5d00 next 158 of size 67108864
2019-10-02 01:35:29.724003: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcf2ff5d00 next 159 of size 8192
2019-10-02 01:35:29.724009: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcf2ff7d00 next 160 of size 32768
2019-10-02 01:35:29.724014: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcf2fffd00 next 161 of size 16777216
2019-10-02 01:35:29.724020: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcf3fffd00 next 162 of size 16777216
2019-10-02 01:35:29.724026: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcf4fffd00 next 163 of size 67108864
2019-10-02 01:35:29.724031: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcf8fffd00 next 164 of size 8192
2019-10-02 01:35:29.724037: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcf9001d00 next 165 of size 8192
2019-10-02 01:35:29.724043: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcf9003d00 next 166 of size 16384000
2019-10-02 01:35:29.724049: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcf9fa3d00 next 167 of size 8192
2019-10-02 01:35:29.724054: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcf9fa5d00 next 168 of size 67108864
2019-10-02 01:35:29.724060: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcfdfa5d00 next 169 of size 8192
2019-10-02 01:35:29.724066: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efcfdfa7d00 next 170 of size 67108864
2019-10-02 01:35:29.724072: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd01fa7d00 next 171 of size 8192
2019-10-02 01:35:29.724079: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd01fa9d00 next 172 of size 67108864
2019-10-02 01:35:29.724085: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd05fa9d00 next 173 of size 16777216
2019-10-02 01:35:29.724090: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd06fa9d00 next 174 of size 512
2019-10-02 01:35:29.724096: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd06fa9f00 next 175 of size 8192
2019-10-02 01:35:29.724102: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd06fabf00 next 176 of size 8192
2019-10-02 01:35:29.724108: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd06fadf00 next 177 of size 67108864
2019-10-02 01:35:29.724114: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd0afadf00 next 178 of size 67108864
2019-10-02 01:35:29.724119: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd0efadf00 next 179 of size 8192
2019-10-02 01:35:29.724125: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd0efaff00 next 180 of size 16777216
2019-10-02 01:35:29.724131: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd0ffaff00 next 181 of size 8192
2019-10-02 01:35:29.724137: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd0ffb1f00 next 182 of size 8192
2019-10-02 01:35:29.724143: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd0ffb3f00 next 183 of size 16384000
2019-10-02 01:35:29.724149: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd10f53f00 next 184 of size 16384000
2019-10-02 01:35:29.724154: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd11ef3f00 next 185 of size 8192
2019-10-02 01:35:29.724160: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd11ef5f00 next 186 of size 8192
2019-10-02 01:35:29.724172: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd11ef7f00 next 187 of size 16777216
2019-10-02 01:35:29.724179: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd12ef7f00 next 188 of size 32768
2019-10-02 01:35:29.724184: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd12efff00 next 189 of size 16777216
2019-10-02 01:35:29.724190: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd13efff00 next 190 of size 8192
2019-10-02 01:35:29.724195: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd13f01f00 next 191 of size 8192
2019-10-02 01:35:29.724202: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd13f03f00 next 192 of size 8192
2019-10-02 01:35:29.724207: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd13f05f00 next 193 of size 16777216
2019-10-02 01:35:29.724213: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd14f05f00 next 194 of size 8192
2019-10-02 01:35:29.724219: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd14f07f00 next 195 of size 8192
2019-10-02 01:35:29.724224: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd14f09f00 next 196 of size 67108864
2019-10-02 01:35:29.724230: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd18f09f00 next 197 of size 16777216
2019-10-02 01:35:29.724236: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd19f09f00 next 198 of size 8192
2019-10-02 01:35:29.724242: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd19f0bf00 next 199 of size 8192
2019-10-02 01:35:29.724247: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd19f0df00 next 200 of size 16777216
2019-10-02 01:35:29.724253: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd1af0df00 next 201 of size 8192
2019-10-02 01:35:29.724260: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd1af0ff00 next 202 of size 8192
2019-10-02 01:35:29.724266: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd1af11f00 next 203 of size 8192
2019-10-02 01:35:29.724271: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd1af13f00 next 204 of size 67108864
2019-10-02 01:35:29.724277: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd1ef13f00 next 205 of size 32768
2019-10-02 01:35:29.724284: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd1ef1bf00 next 206 of size 16777216
2019-10-02 01:35:29.724290: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd1ff1bf00 next 207 of size 8192
2019-10-02 01:35:29.724295: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd1ff1df00 next 208 of size 8192
2019-10-02 01:35:29.724301: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd1ff1ff00 next 209 of size 8192
2019-10-02 01:35:29.724307: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd1ff21f00 next 210 of size 16777216
2019-10-02 01:35:29.724313: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd20f21f00 next 211 of size 32768
2019-10-02 01:35:29.724318: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd20f29f00 next 212 of size 8192
2019-10-02 01:35:29.724324: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd20f2bf00 next 213 of size 8192
2019-10-02 01:35:29.724331: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd20f2df00 next 214 of size 8192
2019-10-02 01:35:29.724337: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd20f2ff00 next 215 of size 8192
2019-10-02 01:35:29.724343: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd20f31f00 next 216 of size 8192
2019-10-02 01:35:29.724348: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd20f33f00 next 217 of size 8192
2019-10-02 01:35:29.724354: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd20f35f00 next 218 of size 16777216
2019-10-02 01:35:29.724366: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd21f35f00 next 219 of size 512
2019-10-02 01:35:29.724373: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd21f36100 next 220 of size 16777216
2019-10-02 01:35:29.724379: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd22f36100 next 221 of size 8192
2019-10-02 01:35:29.724385: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd22f38100 next 222 of size 67108864
2019-10-02 01:35:29.724391: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd26f38100 next 223 of size 16777216
2019-10-02 01:35:29.724397: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd27f38100 next 224 of size 8192
2019-10-02 01:35:29.724403: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd27f3a100 next 225 of size 16384000
2019-10-02 01:35:29.724409: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd28eda100 next 226 of size 512
2019-10-02 01:35:29.724415: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd28eda300 next 227 of size 8192
2019-10-02 01:35:29.724421: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd28edc300 next 228 of size 8192
2019-10-02 01:35:29.724427: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd28ede300 next 229 of size 8192
2019-10-02 01:35:29.724432: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd28ee0300 next 230 of size 8192
2019-10-02 01:35:29.724438: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd28ee2300 next 231 of size 8192
2019-10-02 01:35:29.724444: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd28ee4300 next 232 of size 16777216
2019-10-02 01:35:29.724449: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd29ee4300 next 233 of size 8192
2019-10-02 01:35:29.724455: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd29ee6300 next 234 of size 67108864
2019-10-02 01:35:29.724461: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd2dee6300 next 235 of size 16384000
2019-10-02 01:35:29.724467: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd2ee86300 next 236 of size 16777216
2019-10-02 01:35:29.724473: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd2fe86300 next 237 of size 8192
2019-10-02 01:35:29.724479: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd2fe88300 next 238 of size 8192
2019-10-02 01:35:29.724485: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd2fe8a300 next 239 of size 512
2019-10-02 01:35:29.724490: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd2fe8a500 next 240 of size 16777216
2019-10-02 01:35:29.724496: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd30e8a500 next 241 of size 8192
2019-10-02 01:35:29.724502: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd30e8c500 next 242 of size 8192
2019-10-02 01:35:29.724508: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd30e8e500 next 243 of size 32768
2019-10-02 01:35:29.724513: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd30e96500 next 244 of size 16384000
2019-10-02 01:35:29.724519: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd31e36500 next 245 of size 16384000
2019-10-02 01:35:29.724525: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd32dd6500 next 246 of size 8192
2019-10-02 01:35:29.724531: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd32dd8500 next 247 of size 32768
2019-10-02 01:35:29.724537: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd32de0500 next 248 of size 8192
2019-10-02 01:35:29.724543: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd32de2500 next 249 of size 8192
2019-10-02 01:35:29.724552: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd32de4500 next 250 of size 32768
2019-10-02 01:35:29.724558: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd32dec500 next 251 of size 8192
2019-10-02 01:35:29.724564: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd32dee500 next 252 of size 8192
2019-10-02 01:35:29.724570: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd32df0500 next 253 of size 16777216
2019-10-02 01:35:29.724575: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd33df0500 next 254 of size 16384000
2019-10-02 01:35:29.724581: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd34d90500 next 255 of size 16384000
2019-10-02 01:35:29.724587: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd35d30500 next 256 of size 512
2019-10-02 01:35:29.724593: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd35d30700 next 257 of size 8192
2019-10-02 01:35:29.724599: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd35d32700 next 258 of size 8192
2019-10-02 01:35:29.724605: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd35d34700 next 259 of size 8192
2019-10-02 01:35:29.724611: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd35d36700 next 260 of size 67108864
2019-10-02 01:35:29.724617: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd39d36700 next 261 of size 8192
2019-10-02 01:35:29.724623: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd39d38700 next 262 of size 67108864
2019-10-02 01:35:29.724630: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd3dd38700 next 263 of size 512
2019-10-02 01:35:29.724635: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd3dd38900 next 264 of size 8192
2019-10-02 01:35:29.724641: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd3dd3a900 next 265 of size 8192
2019-10-02 01:35:29.724646: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd3dd3c900 next 266 of size 67108864
2019-10-02 01:35:29.724652: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd41d3c900 next 267 of size 16384000
2019-10-02 01:35:29.724658: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd42cdc900 next 268 of size 8192
2019-10-02 01:35:29.724664: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd42cde900 next 269 of size 8192
2019-10-02 01:35:29.724669: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd42ce0900 next 270 of size 8192
2019-10-02 01:35:29.724675: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd42ce2900 next 271 of size 67108864
2019-10-02 01:35:29.724681: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd46ce2900 next 272 of size 16777216
2019-10-02 01:35:29.724686: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd47ce2900 next 273 of size 16777216
2019-10-02 01:35:29.724692: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd48ce2900 next 274 of size 16777216
2019-10-02 01:35:29.724698: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd49ce2900 next 275 of size 16777216
2019-10-02 01:35:29.724704: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd4ace2900 next 276 of size 8192
2019-10-02 01:35:29.724710: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd4ace4900 next 277 of size 16384000
2019-10-02 01:35:29.724715: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd4bc84900 next 278 of size 16384000
2019-10-02 01:35:29.724721: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd4cc24900 next 279 of size 8192
2019-10-02 01:35:29.724727: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd4cc26900 next 280 of size 8192
2019-10-02 01:35:29.724733: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd4cc28900 next 281 of size 8192
2019-10-02 01:35:29.724741: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd4cc2a900 next 282 of size 8192
2019-10-02 01:35:29.724748: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd4cc2c900 next 283 of size 8192
2019-10-02 01:35:29.724754: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd4cc2e900 next 284 of size 67108864
2019-10-02 01:35:29.724759: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd50c2e900 next 285 of size 8192
2019-10-02 01:35:29.724765: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd50c30900 next 286 of size 16384000
2019-10-02 01:35:29.724771: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd51bd0900 next 287 of size 16384000
2019-10-02 01:35:29.724776: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd52b70900 next 288 of size 8192
2019-10-02 01:35:29.724782: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd52b72900 next 289 of size 8192
2019-10-02 01:35:29.724788: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd52b74900 next 290 of size 67108864
2019-10-02 01:35:29.724793: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd56b74900 next 291 of size 8192
2019-10-02 01:35:29.724800: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd56b76900 next 292 of size 8192
2019-10-02 01:35:29.724806: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd56b78900 next 293 of size 8192
2019-10-02 01:35:29.724812: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd56b7a900 next 294 of size 8192
2019-10-02 01:35:29.724818: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd56b7c900 next 295 of size 16777216
2019-10-02 01:35:29.724823: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd57b7c900 next 296 of size 8192
2019-10-02 01:35:29.724829: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd57b7e900 next 297 of size 32768
2019-10-02 01:35:29.724835: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd57b86900 next 298 of size 16777216
2019-10-02 01:35:29.724841: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd58b86900 next 299 of size 16777216
2019-10-02 01:35:29.724847: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd59b86900 next 300 of size 8192
2019-10-02 01:35:29.724852: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd59b88900 next 301 of size 8192
2019-10-02 01:35:29.724858: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd59b8a900 next 302 of size 16777216
2019-10-02 01:35:29.724864: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd5ab8a900 next 303 of size 512
2019-10-02 01:35:29.724869: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd5ab8ab00 next 304 of size 8192
2019-10-02 01:35:29.724875: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd5ab8cb00 next 305 of size 67108864
2019-10-02 01:35:29.724881: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd5eb8cb00 next 306 of size 8192
2019-10-02 01:35:29.724887: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd5eb8eb00 next 307 of size 16777216
2019-10-02 01:35:29.724893: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd5fb8eb00 next 308 of size 67108864
2019-10-02 01:35:29.724899: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd63b8eb00 next 309 of size 8192
2019-10-02 01:35:29.724904: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd63b90b00 next 310 of size 8192
2019-10-02 01:35:29.724910: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd63b92b00 next 311 of size 8192
2019-10-02 01:35:29.724916: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd63b94b00 next 312 of size 8192
2019-10-02 01:35:29.724927: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd63b96b00 next 313 of size 16777216
2019-10-02 01:35:29.724933: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd64b96b00 next 314 of size 16777216
2019-10-02 01:35:29.724939: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd65b96b00 next 315 of size 67108864
2019-10-02 01:35:29.724945: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd69b96b00 next 316 of size 8192
2019-10-02 01:35:29.724951: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd69b98b00 next 317 of size 8192
2019-10-02 01:35:29.724956: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd69b9ab00 next 318 of size 256
2019-10-02 01:35:29.724964: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd69b9ac00 next 319 of size 8192
2019-10-02 01:35:29.724970: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd69b9cc00 next 320 of size 512
2019-10-02 01:35:29.724976: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd69b9ce00 next 321 of size 67108864
2019-10-02 01:35:29.724982: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd6db9ce00 next 322 of size 32768
2019-10-02 01:35:29.724988: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd6dba4e00 next 323 of size 16777216
2019-10-02 01:35:29.724994: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd6eba4e00 next 324 of size 512
2019-10-02 01:35:29.725000: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd6eba5000 next 325 of size 32768
2019-10-02 01:35:29.725006: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd6ebad000 next 326 of size 8192
2019-10-02 01:35:29.725012: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd6ebaf000 next 327 of size 8192
2019-10-02 01:35:29.725018: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd6ebb1000 next 328 of size 8192
2019-10-02 01:35:29.725024: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd6ebb3000 next 329 of size 8192
2019-10-02 01:35:29.725030: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd6ebb5000 next 330 of size 16777216
2019-10-02 01:35:29.725036: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd6fbb5000 next 331 of size 32768
2019-10-02 01:35:29.725042: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd6fbbd000 next 332 of size 16777216
2019-10-02 01:35:29.725047: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd70bbd000 next 333 of size 8192
2019-10-02 01:35:29.725053: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd70bbf000 next 334 of size 16777216
2019-10-02 01:35:29.725059: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd71bbf000 next 335 of size 8192
2019-10-02 01:35:29.725064: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd71bc1000 next 336 of size 8192
2019-10-02 01:35:29.725070: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd71bc3000 next 337 of size 8192
2019-10-02 01:35:29.725076: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd71bc5000 next 338 of size 16777216
2019-10-02 01:35:29.725082: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd72bc5000 next 339 of size 8192
2019-10-02 01:35:29.725089: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd72bc7000 next 340 of size 16777216
2019-10-02 01:35:29.725095: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd73bc7000 next 341 of size 8192
2019-10-02 01:35:29.725101: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd73bc9000 next 342 of size 8192
2019-10-02 01:35:29.725107: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd73bcb000 next 343 of size 8192
2019-10-02 01:35:29.725118: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd73bcd000 next 344 of size 67108864
2019-10-02 01:35:29.725124: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd77bcd000 next 345 of size 8192
2019-10-02 01:35:29.725130: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd77bcf000 next 346 of size 16777216
2019-10-02 01:35:29.725136: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd78bcf000 next 347 of size 8192
2019-10-02 01:35:29.725141: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd78bd1000 next 348 of size 8192
2019-10-02 01:35:29.725147: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd78bd3000 next 349 of size 8192
2019-10-02 01:35:29.725153: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd78bd5000 next 350 of size 262144000
2019-10-02 01:35:29.725160: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd885d5000 next 351 of size 512
2019-10-02 01:35:29.725166: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd885d5200 next 352 of size 8192
2019-10-02 01:35:29.725172: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd885d7200 next 353 of size 8192
2019-10-02 01:35:29.725177: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd885d9200 next 354 of size 16777216
2019-10-02 01:35:29.725183: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd895d9200 next 355 of size 16777216
2019-10-02 01:35:29.725189: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd8a5d9200 next 356 of size 16777216
2019-10-02 01:35:29.725195: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd8b5d9200 next 357 of size 8192
2019-10-02 01:35:29.725201: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd8b5db200 next 358 of size 16777216
2019-10-02 01:35:29.725206: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd8c5db200 next 359 of size 32768
2019-10-02 01:35:29.725212: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd8c5e3200 next 360 of size 8192
2019-10-02 01:35:29.725218: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd8c5e5200 next 361 of size 8192
2019-10-02 01:35:29.725223: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd8c5e7200 next 362 of size 8192
2019-10-02 01:35:29.725229: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd8c5e9200 next 363 of size 512
2019-10-02 01:35:29.725235: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd8c5e9400 next 364 of size 16777216
2019-10-02 01:35:29.725240: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd8d5e9400 next 365 of size 8192
2019-10-02 01:35:29.725246: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd8d5eb400 next 366 of size 8192
2019-10-02 01:35:29.725252: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd8d5ed400 next 367 of size 16777216
2019-10-02 01:35:29.725257: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd8e5ed400 next 368 of size 16777216
2019-10-02 01:35:29.725263: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd8f5ed400 next 369 of size 67108864
2019-10-02 01:35:29.725269: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd935ed400 next 370 of size 16777216
2019-10-02 01:35:29.725275: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd945ed400 next 371 of size 67108864
2019-10-02 01:35:29.725280: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd985ed400 next 372 of size 8192
2019-10-02 01:35:29.725287: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd985ef400 next 373 of size 67108864
2019-10-02 01:35:29.725293: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd9c5ef400 next 374 of size 16777216
2019-10-02 01:35:29.725299: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd9d5ef400 next 375 of size 8192
2019-10-02 01:35:29.725308: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd9d5f1400 next 376 of size 8192
2019-10-02 01:35:29.725314: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd9d5f3400 next 377 of size 512
2019-10-02 01:35:29.725320: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd9d5f3600 next 378 of size 8192
2019-10-02 01:35:29.725326: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd9d5f5600 next 379 of size 8192
2019-10-02 01:35:29.725332: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd9d5f7600 next 380 of size 8192
2019-10-02 01:35:29.725338: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd9d5f9600 next 381 of size 8192
2019-10-02 01:35:29.725344: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd9d5fb600 next 382 of size 8192
2019-10-02 01:35:29.725350: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd9d5fd600 next 383 of size 16777216
2019-10-02 01:35:29.725356: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd9e5fd600 next 384 of size 32768
2019-10-02 01:35:29.725362: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efd9e605600 next 385 of size 67108864
2019-10-02 01:35:29.725368: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda2605600 next 386 of size 8192
2019-10-02 01:35:29.725373: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda2607600 next 387 of size 8192
2019-10-02 01:35:29.725379: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda2609600 next 388 of size 512
2019-10-02 01:35:29.725385: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda2609800 next 389 of size 8192
2019-10-02 01:35:29.725390: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda260b800 next 390 of size 8192
2019-10-02 01:35:29.725396: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda260d800 next 391 of size 512
2019-10-02 01:35:29.725402: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda260da00 next 392 of size 8192
2019-10-02 01:35:29.725407: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda260fa00 next 393 of size 8192
2019-10-02 01:35:29.725413: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda2611a00 next 394 of size 8192
2019-10-02 01:35:29.725419: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda2613a00 next 395 of size 8192
2019-10-02 01:35:29.725425: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda2615a00 next 396 of size 8192
2019-10-02 01:35:29.725430: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda2617a00 next 397 of size 8192
2019-10-02 01:35:29.725436: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda2619a00 next 398 of size 8192
2019-10-02 01:35:29.725442: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda261ba00 next 399 of size 8192
2019-10-02 01:35:29.725449: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda261da00 next 400 of size 8192
2019-10-02 01:35:29.725455: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda261fa00 next 401 of size 8192
2019-10-02 01:35:29.725460: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda2621a00 next 402 of size 67108864
2019-10-02 01:35:29.725467: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda6621a00 next 403 of size 8192
2019-10-02 01:35:29.725473: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda6623a00 next 404 of size 16777216
2019-10-02 01:35:29.725479: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efda7623a00 next 405 of size 67108864
2019-10-02 01:35:29.725485: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdab623a00 next 406 of size 8192
2019-10-02 01:35:29.725522: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdab625a00 next 407 of size 8192
2019-10-02 01:35:29.725530: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdab627a00 next 408 of size 8192
2019-10-02 01:35:29.725536: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdab629a00 next 409 of size 67108864
2019-10-02 01:35:29.725542: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdaf629a00 next 410 of size 16777216
2019-10-02 01:35:29.725548: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdb0629a00 next 411 of size 8192
2019-10-02 01:35:29.725554: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdb062ba00 next 412 of size 512
2019-10-02 01:35:29.725559: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdb062bc00 next 413 of size 67108864
2019-10-02 01:35:29.725565: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdb462bc00 next 414 of size 16777216
2019-10-02 01:35:29.725571: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdb562bc00 next 415 of size 8192
2019-10-02 01:35:29.725577: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdb562dc00 next 416 of size 8192
2019-10-02 01:35:29.725583: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdb562fc00 next 417 of size 16777216
2019-10-02 01:35:29.725589: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdb662fc00 next 418 of size 8192
2019-10-02 01:35:29.725595: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdb6631c00 next 419 of size 8192
2019-10-02 01:35:29.725601: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdb6633c00 next 420 of size 32768
2019-10-02 01:35:29.725607: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdb663bc00 next 421 of size 16777216
2019-10-02 01:35:29.725612: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdb763bc00 next 422 of size 16777216
2019-10-02 01:35:29.725618: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdb863bc00 next 423 of size 32768
2019-10-02 01:35:29.725624: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdb8643c00 next 424 of size 16777216
2019-10-02 01:35:29.725630: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdb9643c00 next 425 of size 8192
2019-10-02 01:35:29.725635: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdb9645c00 next 426 of size 16777216
2019-10-02 01:35:29.725641: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdba645c00 next 427 of size 8192
2019-10-02 01:35:29.725647: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdba647c00 next 428 of size 67108864
2019-10-02 01:35:29.725653: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdbe647c00 next 429 of size 8192
2019-10-02 01:35:29.725658: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdbe649c00 next 430 of size 8192
2019-10-02 01:35:29.725664: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdbe64bc00 next 431 of size 16777216
2019-10-02 01:35:29.725670: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdbf64bc00 next 432 of size 8192
2019-10-02 01:35:29.725675: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdbf64dc00 next 433 of size 8192
2019-10-02 01:35:29.725681: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdbf64fc00 next 434 of size 512
2019-10-02 01:35:29.725687: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdbf64fe00 next 435 of size 16777216
2019-10-02 01:35:29.725693: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc064fe00 next 436 of size 8192
2019-10-02 01:35:29.725698: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc0651e00 next 437 of size 16777216
2019-10-02 01:35:29.725704: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc1651e00 next 438 of size 8192
2019-10-02 01:35:29.725713: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc1653e00 next 439 of size 8192
2019-10-02 01:35:29.725719: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc1655e00 next 440 of size 67108864
2019-10-02 01:35:29.725725: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc5655e00 next 441 of size 16777216
2019-10-02 01:35:29.725731: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc6655e00 next 442 of size 8192
2019-10-02 01:35:29.725736: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc6657e00 next 443 of size 16777216
2019-10-02 01:35:29.725742: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc7657e00 next 444 of size 8192
2019-10-02 01:35:29.725748: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc7659e00 next 445 of size 8192
2019-10-02 01:35:29.725753: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc765be00 next 446 of size 8192
2019-10-02 01:35:29.725760: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc765de00 next 447 of size 8192
2019-10-02 01:35:29.725765: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc765fe00 next 448 of size 8192
2019-10-02 01:35:29.725771: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc7661e00 next 449 of size 8192
2019-10-02 01:35:29.725777: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc7663e00 next 450 of size 8192
2019-10-02 01:35:29.725783: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc7665e00 next 451 of size 16777216
2019-10-02 01:35:29.725789: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc8665e00 next 452 of size 512
2019-10-02 01:35:29.725794: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc8666000 next 453 of size 8192
2019-10-02 01:35:29.725800: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc8668000 next 454 of size 8192
2019-10-02 01:35:29.725806: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc866a000 next 455 of size 32768
2019-10-02 01:35:29.725811: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc8672000 next 456 of size 512
2019-10-02 01:35:29.725817: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc8672200 next 457 of size 8192
2019-10-02 01:35:29.725823: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc8674200 next 458 of size 8192
2019-10-02 01:35:29.725829: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc8676200 next 459 of size 16777216
2019-10-02 01:35:29.725835: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdc9676200 next 460 of size 16777216
2019-10-02 01:35:29.725841: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdca676200 next 461 of size 8192
2019-10-02 01:35:29.725846: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdca678200 next 462 of size 8192
2019-10-02 01:35:29.725852: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdca67a200 next 463 of size 8192
2019-10-02 01:35:29.725858: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdca67c200 next 464 of size 512
2019-10-02 01:35:29.725863: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdca67c400 next 465 of size 16777216
2019-10-02 01:35:29.725870: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdcb67c400 next 466 of size 32768
2019-10-02 01:35:29.725876: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdcb684400 next 467 of size 8192
2019-10-02 01:35:29.725881: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdcb686400 next 468 of size 8192
2019-10-02 01:35:29.725887: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdcb688400 next 469 of size 16777216
2019-10-02 01:35:29.725899: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdcc688400 next 470 of size 32768
2019-10-02 01:35:29.725905: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdcc690400 next 471 of size 8192
2019-10-02 01:35:29.725911: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdcc692400 next 472 of size 16777216
2019-10-02 01:35:29.725917: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdcd692400 next 473 of size 8192
2019-10-02 01:35:29.725923: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdcd694400 next 474 of size 8192
2019-10-02 01:35:29.725929: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdcd696400 next 475 of size 67108864
2019-10-02 01:35:29.725935: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdd1696400 next 476 of size 8192
2019-10-02 01:35:29.725941: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdd1698400 next 477 of size 67108864
2019-10-02 01:35:29.725947: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdd5698400 next 478 of size 256
2019-10-02 01:35:29.725953: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdd5698500 next 479 of size 8192
2019-10-02 01:35:29.725959: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdd569a500 next 480 of size 512
2019-10-02 01:35:29.725965: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdd569a700 next 481 of size 67108864
2019-10-02 01:35:29.725971: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdd969a700 next 482 of size 256
2019-10-02 01:35:29.725977: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdd969a800 next 514 of size 256
2019-10-02 01:35:29.725982: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdd969a900 next 484 of size 256
2019-10-02 01:35:29.725988: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdd969aa00 next 485 of size 8192
2019-10-02 01:35:29.725994: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdd969ca00 next 486 of size 8192
2019-10-02 01:35:29.726000: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdd969ea00 next 487 of size 16777216
2019-10-02 01:35:29.726006: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdda69ea00 next 515 of size 8192
2019-10-02 01:35:29.726011: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdda6a0a00 next 512 of size 512
2019-10-02 01:35:29.726017: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdda6a0c00 next 513 of size 8192
2019-10-02 01:35:29.726023: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdda6a2c00 next 510 of size 16777216
2019-10-02 01:35:29.726029: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efddb6a2c00 next 511 of size 8192
2019-10-02 01:35:29.726034: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efddb6a4c00 next 500 of size 16777216
2019-10-02 01:35:29.726040: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efddc6a4c00 next 501 of size 16777216
2019-10-02 01:35:29.726046: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efddd6a4c00 next 509 of size 8192
2019-10-02 01:35:29.726052: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efddd6a6c00 next 504 of size 8192
2019-10-02 01:35:29.726058: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efddd6a8c00 next 505 of size 67108864
2019-10-02 01:35:29.726065: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde16a8c00 next 489 of size 32768
2019-10-02 01:35:29.726071: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde16b0c00 next 490 of size 8192
2019-10-02 01:35:29.726076: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde16b2c00 next 488 of size 8192
2019-10-02 01:35:29.726082: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde16b4c00 next 508 of size 16777216
2019-10-02 01:35:29.726091: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde26b4c00 next 503 of size 8192
2019-10-02 01:35:29.726098: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde26b6c00 next 502 of size 8192
2019-10-02 01:35:29.726104: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde26b8c00 next 507 of size 8192
2019-10-02 01:35:29.726110: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde26bac00 next 497 of size 16777216
2019-10-02 01:35:29.726116: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde36bac00 next 498 of size 67108864
2019-10-02 01:35:29.726122: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde76bac00 next 506 of size 8192
2019-10-02 01:35:29.726128: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde76bcc00 next 496 of size 8192
2019-10-02 01:35:29.726134: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde76bec00 next 492 of size 16777216
2019-10-02 01:35:29.726140: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde86bec00 next 493 of size 512
2019-10-02 01:35:29.726145: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde86bee00 next 483 of size 8192
2019-10-02 01:35:29.726151: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde86c0e00 next 495 of size 16777216
2019-10-02 01:35:29.726157: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde96c0e00 next 491 of size 8192
2019-10-02 01:35:29.726162: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde96c2e00 next 499 of size 8192
2019-10-02 01:35:29.726168: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde96c4e00 next 494 of size 32768
2019-10-02 01:35:29.726174: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efde96cce00 next 516 of size 16777216
2019-10-02 01:35:29.726180: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdea6cce00 next 517 of size 8192
2019-10-02 01:35:29.726186: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdea6cee00 next 518 of size 8192
2019-10-02 01:35:29.726192: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdea6d0e00 next 519 of size 16777216
2019-10-02 01:35:29.726198: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdeb6d0e00 next 520 of size 67108864
2019-10-02 01:35:29.726204: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdef6d0e00 next 521 of size 8192
2019-10-02 01:35:29.726209: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdef6d2e00 next 522 of size 16777216
2019-10-02 01:35:29.726215: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf06d2e00 next 523 of size 8192
2019-10-02 01:35:29.726221: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf06d4e00 next 524 of size 8192
2019-10-02 01:35:29.726226: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf06d6e00 next 525 of size 8192
2019-10-02 01:35:29.726232: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf06d8e00 next 526 of size 16777216
2019-10-02 01:35:29.726238: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16d8e00 next 527 of size 8192
2019-10-02 01:35:29.726243: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16dae00 next 528 of size 8192
2019-10-02 01:35:29.726249: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16dce00 next 529 of size 8192
2019-10-02 01:35:29.726255: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16dee00 next 530 of size 256
2019-10-02 01:35:29.726261: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16def00 next 545 of size 256
2019-10-02 01:35:29.726267: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16df000 next 547 of size 8192
2019-10-02 01:35:29.726280: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16e1000 next 548 of size 256
2019-10-02 01:35:29.726286: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16e1100 next 549 of size 8192
2019-10-02 01:35:29.726293: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16e3100 next 546 of size 256
2019-10-02 01:35:29.726298: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16e3200 next 550 of size 8192
2019-10-02 01:35:29.726304: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16e5200 next 551 of size 256
2019-10-02 01:35:29.726310: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16e5300 next 552 of size 8192
2019-10-02 01:35:29.726316: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16e7300 next 554 of size 256
2019-10-02 01:35:29.726322: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16e7400 next 586 of size 4096
2019-10-02 01:35:29.726328: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16e8400 next 577 of size 256
2019-10-02 01:35:29.726334: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16e8500 next 590 of size 5376
2019-10-02 01:35:29.726340: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16e9a00 next 559 of size 8192
2019-10-02 01:35:29.726346: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16eba00 next 657 of size 9984
2019-10-02 01:35:29.726353: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16ee100 next 684 of size 6912
2019-10-02 01:35:29.726358: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16efc00 next 685 of size 256
2019-10-02 01:35:29.726364: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16efd00 next 686 of size 256
2019-10-02 01:35:29.726370: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16efe00 next 687 of size 256
2019-10-02 01:35:29.726376: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16eff00 next 688 of size 256
2019-10-02 01:35:29.726381: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f0000 next 683 of size 256
2019-10-02 01:35:29.726387: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f0100 next 690 of size 256
2019-10-02 01:35:29.726393: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f0200 next 665 of size 256
2019-10-02 01:35:29.726398: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f0300 next 605 of size 256
2019-10-02 01:35:29.726404: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f0400 next 697 of size 256
2019-10-02 01:35:29.726410: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f0500 next 936 of size 256
2019-10-02 01:35:29.726416: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f0600 next 630 of size 512
2019-10-02 01:35:29.726422: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f0800 next 1011 of size 256
2019-10-02 01:35:29.726427: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f0900 next 699 of size 512
2019-10-02 01:35:29.726433: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f0b00 next 700 of size 256
2019-10-02 01:35:29.726439: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f0c00 next 701 of size 256
2019-10-02 01:35:29.726445: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f0d00 next 703 of size 256
2019-10-02 01:35:29.726450: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f0e00 next 705 of size 256
2019-10-02 01:35:29.726456: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f0f00 next 706 of size 256
2019-10-02 01:35:29.726462: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f1000 next 707 of size 256
2019-10-02 01:35:29.726473: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f1100 next 712 of size 256
2019-10-02 01:35:29.726480: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f1200 next 637 of size 256
2019-10-02 01:35:29.726486: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f1300 next 714 of size 256
2019-10-02 01:35:29.726492: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16f1400 next 531 of size 56064
2019-10-02 01:35:29.726498: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf16fef00 next 532 of size 67108864
2019-10-02 01:35:29.726504: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf56fef00 next 533 of size 256
2019-10-02 01:35:29.726510: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf56ff000 next 534 of size 16777216
2019-10-02 01:35:29.726516: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf66ff000 next 535 of size 131072
2019-10-02 01:35:29.726522: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf671f000 next 715 of size 32768
2019-10-02 01:35:29.726528: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6727000 next 718 of size 256
2019-10-02 01:35:29.726534: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6727100 next 677 of size 256
2019-10-02 01:35:29.726540: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6727200 next 720 of size 256
2019-10-02 01:35:29.726546: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6727300 next 721 of size 8192
2019-10-02 01:35:29.726552: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6729300 next 722 of size 8192
2019-10-02 01:35:29.726558: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf672b300 next 724 of size 256
2019-10-02 01:35:29.726564: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf672b400 next 725 of size 256
2019-10-02 01:35:29.726570: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf672b500 next 726 of size 256
2019-10-02 01:35:29.726576: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf672b600 next 727 of size 256
2019-10-02 01:35:29.726581: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf672b700 next 728 of size 8192
2019-10-02 01:35:29.726587: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf672d700 next 729 of size 8192
2019-10-02 01:35:29.726593: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf672f700 next 730 of size 256
2019-10-02 01:35:29.726599: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf672f800 next 731 of size 256
2019-10-02 01:35:29.726605: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf672f900 next 732 of size 256
2019-10-02 01:35:29.726611: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf672fa00 next 733 of size 256
2019-10-02 01:35:29.726617: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf672fb00 next 734 of size 8192
2019-10-02 01:35:29.726624: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6731b00 next 735 of size 8192
2019-10-02 01:35:29.726629: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6733b00 next 736 of size 8192
2019-10-02 01:35:29.726635: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6735b00 next 737 of size 256
2019-10-02 01:35:29.726641: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6735c00 next 738 of size 8192
2019-10-02 01:35:29.726646: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6737c00 next 739 of size 8192
2019-10-02 01:35:29.726652: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6739c00 next 740 of size 256
2019-10-02 01:35:29.726658: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6739d00 next 741 of size 8192
2019-10-02 01:35:29.726667: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf673bd00 next 536 of size 13056
2019-10-02 01:35:29.726674: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf673f000 next 537 of size 8192
2019-10-02 01:35:29.726680: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6741000 next 538 of size 256
2019-10-02 01:35:29.726686: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6741100 next 539 of size 8192
2019-10-02 01:35:29.726691: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6743100 next 540 of size 131072
2019-10-02 01:35:29.726697: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6763100 next 541 of size 256
2019-10-02 01:35:29.726703: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdf6763200 next 542 of size 67108864
2019-10-02 01:35:29.726709: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdfa763200 next 543 of size 8192
2019-10-02 01:35:29.726715: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdfa765200 next 544 of size 67108864
2019-10-02 01:35:29.726721: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdfe765200 next 553 of size 131072
2019-10-02 01:35:29.726727: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdfe785200 next 582 of size 16777216
2019-10-02 01:35:29.726732: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff785200 next 742 of size 256
2019-10-02 01:35:29.726738: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff785300 next 743 of size 8192
2019-10-02 01:35:29.726744: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff787300 next 744 of size 8192
2019-10-02 01:35:29.726751: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff789300 next 745 of size 256
2019-10-02 01:35:29.726757: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff789400 next 746 of size 8192
2019-10-02 01:35:29.726762: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff78b400 next 747 of size 8192
2019-10-02 01:35:29.726768: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff78d400 next 748 of size 256
2019-10-02 01:35:29.726774: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff78d500 next 749 of size 8192
2019-10-02 01:35:29.726780: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff78f500 next 750 of size 8192
2019-10-02 01:35:29.726786: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff791500 next 751 of size 256
2019-10-02 01:35:29.726792: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff791600 next 752 of size 8192
2019-10-02 01:35:29.726798: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff793600 next 753 of size 256
2019-10-02 01:35:29.726804: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff793700 next 754 of size 256
2019-10-02 01:35:29.726810: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff793800 next 756 of size 256
2019-10-02 01:35:29.726816: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff793900 next 758 of size 8192
2019-10-02 01:35:29.726822: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff795900 next 759 of size 256
2019-10-02 01:35:29.726827: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff795a00 next 760 of size 8192
2019-10-02 01:35:29.726833: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff797a00 next 761 of size 8192
2019-10-02 01:35:29.726839: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff799a00 next 762 of size 256
2019-10-02 01:35:29.726844: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff799b00 next 763 of size 8192
2019-10-02 01:35:29.726850: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff79bb00 next 764 of size 8192
2019-10-02 01:35:29.726862: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff79db00 next 765 of size 256
2019-10-02 01:35:29.726869: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff79dc00 next 766 of size 8192
2019-10-02 01:35:29.726875: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff79fc00 next 769 of size 256
2019-10-02 01:35:29.726880: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff79fd00 next 771 of size 256
2019-10-02 01:35:29.726886: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff79fe00 next 773 of size 256
2019-10-02 01:35:29.726892: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff79ff00 next 776 of size 256
2019-10-02 01:35:29.726898: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff7a0000 next 778 of size 256
2019-10-02 01:35:29.726904: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff7a0100 next 780 of size 256
2019-10-02 01:35:29.726910: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff7a0200 next 782 of size 256
2019-10-02 01:35:29.726915: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff7a0300 next 785 of size 256
2019-10-02 01:35:29.726921: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff7a0400 next 786 of size 8192
2019-10-02 01:35:29.726927: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff7a2400 next 787 of size 512
2019-10-02 01:35:29.726933: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff7a2600 next 581 of size 11264
2019-10-02 01:35:29.726939: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efdff7a5200 next 580 of size 16777216
2019-10-02 01:35:29.726945: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe007a5200 next 579 of size 67108864
2019-10-02 01:35:29.726950: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe047a5200 next 578 of size 131072
2019-10-02 01:35:29.726956: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe047c5200 next 585 of size 16777216
2019-10-02 01:35:29.726962: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe057c5200 next 584 of size 8388608
2019-10-02 01:35:29.726968: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe05fc5200 next 583 of size 16777216
2019-10-02 01:35:29.726974: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe06fc5200 next 576 of size 16777216
2019-10-02 01:35:29.726979: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe07fc5200 next 702 of size 67108864
2019-10-02 01:35:29.726985: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe0bfc5200 next 704 of size 16777216
2019-10-02 01:35:29.726991: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe0cfc5200 next 708 of size 67108864
2019-10-02 01:35:29.726996: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe10fc5200 next 709 of size 16777216
2019-10-02 01:35:29.727002: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe11fc5200 next 710 of size 67108864
2019-10-02 01:35:29.727008: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe15fc5200 next 711 of size 16777216
2019-10-02 01:35:29.727013: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe16fc5200 next 716 of size 16777216
2019-10-02 01:35:29.727019: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe17fc5200 next 717 of size 67108864
2019-10-02 01:35:29.727025: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe1bfc5200 next 723 of size 67108864
2019-10-02 01:35:29.727031: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe1ffc5200 next 755 of size 16777216
2019-10-02 01:35:29.727037: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe20fc5200 next 757 of size 16777216
2019-10-02 01:35:29.727045: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe21fc5200 next 767 of size 16777216
2019-10-02 01:35:29.727051: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe22fc5200 next 768 of size 16777216
2019-10-02 01:35:29.727058: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe23fc5200 next 770 of size 67108864
2019-10-02 01:35:29.727064: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe27fc5200 next 772 of size 32768
2019-10-02 01:35:29.727071: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe27fcd200 next 774 of size 32768
2019-10-02 01:35:29.727077: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe27fd5200 next 775 of size 16777216
2019-10-02 01:35:29.727083: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe28fd5200 next 777 of size 16777216
2019-10-02 01:35:29.727089: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe29fd5200 next 779 of size 16777216
2019-10-02 01:35:29.727095: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe2afd5200 next 781 of size 67108864
2019-10-02 01:35:29.727101: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe2efd5200 next 783 of size 16777216
2019-10-02 01:35:29.727106: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe2ffd5200 next 784 of size 67108864
2019-10-02 01:35:29.727112: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe33fd5200 next 788 of size 512
2019-10-02 01:35:29.727118: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe33fd5400 next 789 of size 256
2019-10-02 01:35:29.727124: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe33fd5500 next 790 of size 256
2019-10-02 01:35:29.727130: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe33fd5600 next 791 of size 256
2019-10-02 01:35:29.727136: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe33fd5700 next 792 of size 256
2019-10-02 01:35:29.727142: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe33fd5800 next 793 of size 512
2019-10-02 01:35:29.727148: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe33fd5a00 next 794 of size 8192
2019-10-02 01:35:29.727153: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe33fd7a00 next 795 of size 512
2019-10-02 01:35:29.727159: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe33fd7c00 next 796 of size 8192
2019-10-02 01:35:29.727165: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe33fd9c00 next 797 of size 8192
2019-10-02 01:35:29.727171: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe33fdbc00 next 798 of size 8192
2019-10-02 01:35:29.727177: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe33fddc00 next 799 of size 16777216
2019-10-02 01:35:29.727182: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe34fddc00 next 800 of size 256
2019-10-02 01:35:29.727188: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe34fddd00 next 801 of size 16777216
2019-10-02 01:35:29.727194: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe35fddd00 next 802 of size 67108864
2019-10-02 01:35:29.727200: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe39fddd00 next 803 of size 16777216
2019-10-02 01:35:29.727206: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3afddd00 next 804 of size 256
2019-10-02 01:35:29.727212: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3afdde00 next 805 of size 8192
2019-10-02 01:35:29.727218: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3afdfe00 next 806 of size 256
2019-10-02 01:35:29.727223: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3afdff00 next 807 of size 8192
2019-10-02 01:35:29.727232: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3afe1f00 next 808 of size 256
2019-10-02 01:35:29.727238: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3afe2000 next 809 of size 8192
2019-10-02 01:35:29.727245: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3afe4000 next 810 of size 256
2019-10-02 01:35:29.727251: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3afe4100 next 811 of size 8192
2019-10-02 01:35:29.727257: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3afe6100 next 812 of size 8192
2019-10-02 01:35:29.727264: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3afe8100 next 813 of size 256
2019-10-02 01:35:29.727269: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3afe8200 next 814 of size 8192
2019-10-02 01:35:29.727275: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3afea200 next 815 of size 8192
2019-10-02 01:35:29.727281: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3afec200 next 816 of size 256
2019-10-02 01:35:29.727287: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3afec300 next 817 of size 8192
2019-10-02 01:35:29.727293: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3afee300 next 818 of size 32768
2019-10-02 01:35:29.727312: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3aff6300 next 819 of size 256
2019-10-02 01:35:29.727319: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3aff6400 next 820 of size 32768
2019-10-02 01:35:29.727324: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3affe400 next 821 of size 32768
2019-10-02 01:35:29.727331: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3b006400 next 822 of size 256
2019-10-02 01:35:29.727337: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3b006500 next 823 of size 32768
2019-10-02 01:35:29.727343: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3b00e500 next 824 of size 8192
2019-10-02 01:35:29.727349: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3b010500 next 825 of size 256
2019-10-02 01:35:29.727355: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3b010600 next 826 of size 8192
2019-10-02 01:35:29.727361: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3b012600 next 827 of size 16777216
2019-10-02 01:35:29.727367: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3c012600 next 828 of size 256
2019-10-02 01:35:29.727372: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3c012700 next 829 of size 16777216
2019-10-02 01:35:29.727378: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3d012700 next 830 of size 16777216
2019-10-02 01:35:29.727384: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3e012700 next 831 of size 8192
2019-10-02 01:35:29.727389: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3e014700 next 832 of size 256
2019-10-02 01:35:29.727395: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3e014800 next 833 of size 8192
2019-10-02 01:35:29.727400: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3e016800 next 834 of size 8192
2019-10-02 01:35:29.727406: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3e018800 next 835 of size 256
2019-10-02 01:35:29.727412: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3e018900 next 836 of size 8192
2019-10-02 01:35:29.727418: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3e01a900 next 837 of size 16777216
2019-10-02 01:35:29.727423: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe3f01a900 next 838 of size 67108864
2019-10-02 01:35:29.727429: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4301a900 next 839 of size 8192
2019-10-02 01:35:29.727444: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4301c900 next 840 of size 256
2019-10-02 01:35:29.727451: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4301ca00 next 841 of size 8192
2019-10-02 01:35:29.727457: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4301ea00 next 842 of size 256
2019-10-02 01:35:29.727463: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4301eb00 next 843 of size 8192
2019-10-02 01:35:29.727470: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe43020b00 next 844 of size 256
2019-10-02 01:35:29.727476: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe43020c00 next 845 of size 8192
2019-10-02 01:35:29.727481: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe43022c00 next 846 of size 8192
2019-10-02 01:35:29.727487: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe43024c00 next 847 of size 256
2019-10-02 01:35:29.727493: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe43024d00 next 848 of size 8192
2019-10-02 01:35:29.727499: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe43026d00 next 849 of size 16777216
2019-10-02 01:35:29.727505: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe44026d00 next 850 of size 16777216
2019-10-02 01:35:29.727511: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe45026d00 next 851 of size 256
2019-10-02 01:35:29.727516: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe45026e00 next 852 of size 256
2019-10-02 01:35:29.727522: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe45026f00 next 853 of size 16777216
2019-10-02 01:35:29.727527: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe46026f00 next 854 of size 16777216
2019-10-02 01:35:29.727533: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe47026f00 next 855 of size 67108864
2019-10-02 01:35:29.727539: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4b026f00 next 856 of size 256
2019-10-02 01:35:29.727545: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4b027000 next 857 of size 256
2019-10-02 01:35:29.727551: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4b027100 next 858 of size 512
2019-10-02 01:35:29.727557: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4b027300 next 859 of size 256
2019-10-02 01:35:29.727562: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4b027400 next 860 of size 512
2019-10-02 01:35:29.727568: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4b027600 next 861 of size 16777216
2019-10-02 01:35:29.727574: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4c027600 next 862 of size 16777216
2019-10-02 01:35:29.727579: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4d027600 next 863 of size 256
2019-10-02 01:35:29.727585: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4d027700 next 864 of size 8192
2019-10-02 01:35:29.727591: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4d029700 next 865 of size 256
2019-10-02 01:35:29.727597: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4d029800 next 866 of size 256
2019-10-02 01:35:29.727603: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4d029900 next 867 of size 8192
2019-10-02 01:35:29.727609: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4d02b900 next 868 of size 256
2019-10-02 01:35:29.727615: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4d02ba00 next 869 of size 8192
2019-10-02 01:35:29.727621: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4d02da00 next 870 of size 256
2019-10-02 01:35:29.727626: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4d02db00 next 871 of size 8192
2019-10-02 01:35:29.727636: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4d02fb00 next 872 of size 8192
2019-10-02 01:35:29.727643: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4d031b00 next 873 of size 256
2019-10-02 01:35:29.727649: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4d031c00 next 874 of size 8192
2019-10-02 01:35:29.727655: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe4d033c00 next 875 of size 67108864
2019-10-02 01:35:29.727661: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe51033c00 next 876 of size 16777216
2019-10-02 01:35:29.727667: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe52033c00 next 877 of size 256
2019-10-02 01:35:29.727673: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe52033d00 next 878 of size 67108864
2019-10-02 01:35:29.727680: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe56033d00 next 879 of size 256
2019-10-02 01:35:29.727686: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe56033e00 next 880 of size 8192
2019-10-02 01:35:29.727692: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe56035e00 next 881 of size 256
2019-10-02 01:35:29.727698: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe56035f00 next 882 of size 16777216
2019-10-02 01:35:29.727704: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe57035f00 next 883 of size 8192
2019-10-02 01:35:29.727710: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe57037f00 next 884 of size 256
2019-10-02 01:35:29.727716: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe57038000 next 885 of size 67108864
2019-10-02 01:35:29.727722: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe5b038000 next 886 of size 16777216
2019-10-02 01:35:29.727728: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe5c038000 next 887 of size 16777216
2019-10-02 01:35:29.727734: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe5d038000 next 888 of size 16777216
2019-10-02 01:35:29.727739: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe5e038000 next 889 of size 16777216
2019-10-02 01:35:29.727745: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe5f038000 next 890 of size 67108864
2019-10-02 01:35:29.727751: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe63038000 next 891 of size 256
2019-10-02 01:35:29.727757: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe63038100 next 892 of size 16777216
2019-10-02 01:35:29.727763: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe64038100 next 893 of size 256
2019-10-02 01:35:29.727768: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe64038200 next 894 of size 256
2019-10-02 01:35:29.727774: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe64038300 next 895 of size 8192
2019-10-02 01:35:29.727780: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6403a300 next 896 of size 256
2019-10-02 01:35:29.727787: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6403a400 next 897 of size 8192
2019-10-02 01:35:29.727793: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6403c400 next 898 of size 8192
2019-10-02 01:35:29.727798: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6403e400 next 899 of size 256
2019-10-02 01:35:29.727805: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6403e500 next 900 of size 8192
2019-10-02 01:35:29.727810: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe64040500 next 901 of size 16777216
2019-10-02 01:35:29.727816: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe65040500 next 902 of size 67108864
2019-10-02 01:35:29.727830: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe69040500 next 903 of size 16777216
2019-10-02 01:35:29.727837: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6a040500 next 904 of size 256
2019-10-02 01:35:29.727843: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6a040600 next 905 of size 32768
2019-10-02 01:35:29.727849: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6a048600 next 906 of size 256
2019-10-02 01:35:29.727855: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6a048700 next 907 of size 32768
2019-10-02 01:35:29.727860: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6a050700 next 908 of size 16777216
2019-10-02 01:35:29.727866: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6b050700 next 909 of size 256
2019-10-02 01:35:29.727872: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6b050800 next 910 of size 8192
2019-10-02 01:35:29.727878: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6b052800 next 911 of size 256
2019-10-02 01:35:29.727884: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6b052900 next 912 of size 8192
2019-10-02 01:35:29.727890: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6b054900 next 913 of size 8192
2019-10-02 01:35:29.727896: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6b056900 next 914 of size 256
2019-10-02 01:35:29.727902: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6b056a00 next 915 of size 8192
2019-10-02 01:35:29.727908: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6b058a00 next 916 of size 67108864
2019-10-02 01:35:29.727914: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe6f058a00 next 917 of size 16777216
2019-10-02 01:35:29.727920: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe70058a00 next 918 of size 16777216
2019-10-02 01:35:29.727926: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe71058a00 next 919 of size 256
2019-10-02 01:35:29.727932: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe71058b00 next 920 of size 8192
2019-10-02 01:35:29.727938: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7105ab00 next 921 of size 256
2019-10-02 01:35:29.727943: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7105ac00 next 922 of size 8192
2019-10-02 01:35:29.727949: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7105cc00 next 923 of size 8192
2019-10-02 01:35:29.727956: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7105ec00 next 924 of size 256
2019-10-02 01:35:29.727962: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7105ed00 next 925 of size 8192
2019-10-02 01:35:29.727967: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe71060d00 next 926 of size 16777216
2019-10-02 01:35:29.727973: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe72060d00 next 927 of size 256
2019-10-02 01:35:29.727979: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe72060e00 next 928 of size 67108864
2019-10-02 01:35:29.727985: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe76060e00 next 929 of size 67108864
2019-10-02 01:35:29.727990: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7a060e00 next 930 of size 67108864
2019-10-02 01:35:29.727996: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e060e00 next 931 of size 8192
2019-10-02 01:35:29.728002: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e062e00 next 932 of size 256
2019-10-02 01:35:29.728008: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e062f00 next 933 of size 8192
2019-10-02 01:35:29.728013: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e064f00 next 934 of size 256
2019-10-02 01:35:29.728021: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e065000 next 935 of size 256
2019-10-02 01:35:29.728027: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e065100 next 622 of size 8388608
2019-10-02 01:35:29.728035: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e865100 next 1016 of size 256
2019-10-02 01:35:29.728041: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e865200 next 564 of size 512
2019-10-02 01:35:29.728047: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e865400 next 604 of size 256
2019-10-02 01:35:29.728055: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e865500 next 624 of size 256
2019-10-02 01:35:29.728061: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e865600 next 613 of size 512
2019-10-02 01:35:29.728068: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e865800 next 1031 of size 256
2019-10-02 01:35:29.728074: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e865900 next 1033 of size 256
2019-10-02 01:35:29.728080: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e865a00 next 1034 of size 256
2019-10-02 01:35:29.728086: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e865b00 next 619 of size 256
2019-10-02 01:35:29.728091: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e865c00 next 611 of size 256
2019-10-02 01:35:29.728097: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e865d00 next 614 of size 256
2019-10-02 01:35:29.728103: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e865e00 next 680 of size 256
2019-10-02 01:35:29.728108: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e865f00 next 659 of size 256
2019-10-02 01:35:29.728114: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e866000 next 566 of size 256
2019-10-02 01:35:29.728122: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e866100 next 649 of size 256
2019-10-02 01:35:29.728129: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e866200 next 617 of size 768
2019-10-02 01:35:29.728135: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e866500 next 608 of size 256
2019-10-02 01:35:29.728141: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e866600 next 588 of size 256
2019-10-02 01:35:29.728147: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e866700 next 644 of size 256
2019-10-02 01:35:29.728153: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e866800 next 661 of size 256
2019-10-02 01:35:29.728158: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e866900 next 591 of size 256
2019-10-02 01:35:29.728164: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e866a00 next 944 of size 256
2019-10-02 01:35:29.728170: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e866b00 next 945 of size 256
2019-10-02 01:35:29.728176: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e866c00 next 946 of size 256
2019-10-02 01:35:29.728182: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e866d00 next 947 of size 256
2019-10-02 01:35:29.728188: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e866e00 next 948 of size 8192
2019-10-02 01:35:29.728196: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e868e00 next 949 of size 256
2019-10-02 01:35:29.728202: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e868f00 next 950 of size 8192
2019-10-02 01:35:29.728208: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e86af00 next 952 of size 256
2019-10-02 01:35:29.728219: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e86b000 next 953 of size 256
2019-10-02 01:35:29.728225: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e86b100 next 954 of size 256
2019-10-02 01:35:29.728231: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e86b200 next 956 of size 256
2019-10-02 01:35:29.728237: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e86b300 next 961 of size 256
2019-10-02 01:35:29.728243: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e86b400 next 962 of size 256
2019-10-02 01:35:29.728249: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e86b500 next 963 of size 256
2019-10-02 01:35:29.728255: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e86b600 next 964 of size 8192
2019-10-02 01:35:29.728261: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e86d600 next 965 of size 8192
2019-10-02 01:35:29.728267: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e86f600 next 966 of size 256
2019-10-02 01:35:29.728273: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e86f700 next 967 of size 8192
2019-10-02 01:35:29.728279: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e871700 next 968 of size 256
2019-10-02 01:35:29.728285: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e871800 next 969 of size 8192
2019-10-02 01:35:29.728293: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e873800 next 971 of size 256
2019-10-02 01:35:29.728302: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e873900 next 972 of size 256
2019-10-02 01:35:29.728309: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e873a00 next 973 of size 256
2019-10-02 01:35:29.728315: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e873b00 next 974 of size 8192
2019-10-02 01:35:29.728321: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e875b00 next 975 of size 8192
2019-10-02 01:35:29.728328: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e877b00 next 633 of size 61184
2019-10-02 01:35:29.728334: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe7e886a00 next 937 of size 67108864
2019-10-02 01:35:29.728340: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe82886a00 next 938 of size 256
2019-10-02 01:35:29.728346: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe82886b00 next 939 of size 256
2019-10-02 01:35:29.728352: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe82886c00 next 940 of size 256
2019-10-02 01:35:29.728358: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe82886d00 next 941 of size 256
2019-10-02 01:35:29.728364: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe82886e00 next 942 of size 256
2019-10-02 01:35:29.728369: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe82886f00 next 943 of size 16777216
2019-10-02 01:35:29.728375: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe83886f00 next 951 of size 16777216
2019-10-02 01:35:29.728381: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe84886f00 next 955 of size 67108864
2019-10-02 01:35:29.728387: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe88886f00 next 957 of size 67108864
2019-10-02 01:35:29.728393: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe8c886f00 next 958 of size 16777216
2019-10-02 01:35:29.728399: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe8d886f00 next 959 of size 67108864
2019-10-02 01:35:29.728405: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe91886f00 next 960 of size 16777216
2019-10-02 01:35:29.728411: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe92886f00 next 970 of size 16777216
2019-10-02 01:35:29.728440: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe93886f00 next 976 of size 256
2019-10-02 01:35:29.728448: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe93887000 next 977 of size 32768
2019-10-02 01:35:29.728454: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9388f000 next 978 of size 67108864
2019-10-02 01:35:29.728460: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9788f000 next 979 of size 16777216
2019-10-02 01:35:29.728466: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9888f000 next 980 of size 256
2019-10-02 01:35:29.728472: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9888f100 next 981 of size 16777216
2019-10-02 01:35:29.728477: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9988f100 next 982 of size 256
2019-10-02 01:35:29.728483: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9988f200 next 983 of size 256
2019-10-02 01:35:29.728489: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9988f300 next 984 of size 16777216
2019-10-02 01:35:29.728495: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9a88f300 next 985 of size 67108864
2019-10-02 01:35:29.728501: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9e88f300 next 986 of size 256
2019-10-02 01:35:29.728507: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9e88f400 next 987 of size 256
2019-10-02 01:35:29.728513: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9e88f500 next 988 of size 256
2019-10-02 01:35:29.728519: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9e88f600 next 989 of size 256
2019-10-02 01:35:29.728525: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9e88f700 next 990 of size 256
2019-10-02 01:35:29.728531: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9e88f800 next 991 of size 256
2019-10-02 01:35:29.728537: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9e88f900 next 992 of size 16777216
2019-10-02 01:35:29.728542: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9f88f900 next 993 of size 8192
2019-10-02 01:35:29.728548: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9f891900 next 994 of size 256
2019-10-02 01:35:29.728554: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9f891a00 next 995 of size 8192
2019-10-02 01:35:29.728560: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9f893a00 next 996 of size 8192
2019-10-02 01:35:29.728565: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9f895a00 next 997 of size 256
2019-10-02 01:35:29.728571: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9f895b00 next 998 of size 8192
2019-10-02 01:35:29.728577: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efe9f897b00 next 999 of size 67108864
2019-10-02 01:35:29.728583: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efea3897b00 next 1000 of size 67108864
2019-10-02 01:35:29.728589: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efea7897b00 next 1001 of size 256
2019-10-02 01:35:29.728595: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efea7897c00 next 1002 of size 256
2019-10-02 01:35:29.728600: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efea7897d00 next 1003 of size 256
2019-10-02 01:35:29.728606: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efea7897e00 next 1004 of size 256
2019-10-02 01:35:29.728612: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efea7897f00 next 1005 of size 67108864
2019-10-02 01:35:29.728618: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efeab897f00 next 1006 of size 256
2019-10-02 01:35:29.728624: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efeab898000 next 1007 of size 256
2019-10-02 01:35:29.728636: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efeab898100 next 601 of size 16777216
2019-10-02 01:35:29.728642: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efeac898100 next 594 of size 8388608
2019-10-02 01:35:29.728648: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efead098100 next 620 of size 268435456
2019-10-02 01:35:29.728654: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd098100 next 629 of size 131072
2019-10-02 01:35:29.728659: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b8100 next 572 of size 256
2019-10-02 01:35:29.728665: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b8200 next 587 of size 256
2019-10-02 01:35:29.728672: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b8300 next 696 of size 256
2019-10-02 01:35:29.728678: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b8400 next 623 of size 256
2019-10-02 01:35:29.728685: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b8500 next 1037 of size 512
2019-10-02 01:35:29.728690: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b8700 next 1041 of size 512
2019-10-02 01:35:29.728696: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b8900 next 1047 of size 256
2019-10-02 01:35:29.728702: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b8a00 next 1059 of size 256
2019-10-02 01:35:29.728708: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b8b00 next 1060 of size 256
2019-10-02 01:35:29.728713: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b8c00 next 1061 of size 256
2019-10-02 01:35:29.728719: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b8d00 next 1076 of size 512
2019-10-02 01:35:29.728725: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b8f00 next 1077 of size 512
2019-10-02 01:35:29.728732: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b9100 next 1079 of size 256
2019-10-02 01:35:29.728738: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b9200 next 1098 of size 256
2019-10-02 01:35:29.728743: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b9300 next 1102 of size 256
2019-10-02 01:35:29.728749: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b9400 next 1106 of size 256
2019-10-02 01:35:29.728755: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b9500 next 1113 of size 512
2019-10-02 01:35:29.728761: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b9700 next 1121 of size 512
2019-10-02 01:35:29.728767: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b9900 next 1122 of size 256
2019-10-02 01:35:29.728772: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b9a00 next 1123 of size 256
2019-10-02 01:35:29.728778: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b9b00 next 1125 of size 256
2019-10-02 01:35:29.728784: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b9c00 next 1126 of size 256
2019-10-02 01:35:29.728789: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b9d00 next 1127 of size 256
2019-10-02 01:35:29.728796: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b9e00 next 1021 of size 256
2019-10-02 01:35:29.728801: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0b9f00 next 1022 of size 256
2019-10-02 01:35:29.728807: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0ba000 next 1023 of size 256
2019-10-02 01:35:29.728813: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0ba100 next 1024 of size 256
2019-10-02 01:35:29.728826: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0ba200 next 1025 of size 256
2019-10-02 01:35:29.728832: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0ba300 next 1026 of size 256
2019-10-02 01:35:29.728839: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efebd0ba400 next 596 of size 131072000
2019-10-02 01:35:29.728845: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efec4dba400 next 571 of size 268435456
2019-10-02 01:35:29.728851: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efed4dba400 next 558 of size 131072
2019-10-02 01:35:29.728857: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efed4dda400 next 597 of size 4194304000
2019-10-02 01:35:29.728863: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effcedda400 next 595 of size 67108864
2019-10-02 01:35:29.728869: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effd2dda400 next 638 of size 268435456
2019-10-02 01:35:29.728875: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effe2dda400 next 626 of size 131072
2019-10-02 01:35:29.728882: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effe2dfa400 next 648 of size 16777216
2019-10-02 01:35:29.728888: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effe3dfa400 next 674 of size 16777216
2019-10-02 01:35:29.728894: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effe4dfa400 next 589 of size 8192
2019-10-02 01:35:29.728900: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effe4dfc400 next 615 of size 8192
2019-10-02 01:35:29.728906: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effe4dfe400 next 658 of size 8192
2019-10-02 01:35:29.728912: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effe4e00400 next 698 of size 8192
2019-10-02 01:35:29.728918: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effe4e02400 next 676 of size 67108864
2019-10-02 01:35:29.728923: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effe8e02400 next 646 of size 8192
2019-10-02 01:35:29.728929: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effe8e04400 next 692 of size 16777216
2019-10-02 01:35:29.728935: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effe9e04400 next 682 of size 16384000
2019-10-02 01:35:29.728941: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effeada4400 next 681 of size 32768
2019-10-02 01:35:29.728947: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effeadac400 next 675 of size 8192
2019-10-02 01:35:29.728953: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effeadae400 next 612 of size 16384000
2019-10-02 01:35:29.728960: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effebd4e400 next 568 of size 16384000
2019-10-02 01:35:29.728968: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effeccee400 next 573 of size 16384000
2019-10-02 01:35:29.728974: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effedc8e400 next 625 of size 16384000
2019-10-02 01:35:29.728980: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effeec2e400 next 565 of size 8192
2019-10-02 01:35:29.728985: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effeec30400 next 670 of size 16384000
2019-10-02 01:35:29.728991: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effefbd0400 next 560 of size 67108864
2019-10-02 01:35:29.728997: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efff3bd0400 next 640 of size 16384000
2019-10-02 01:35:29.729003: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efff4b70400 next 561 of size 16384000
2019-10-02 01:35:29.729008: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efff5b10400 next 651 of size 16384000
2019-10-02 01:35:29.729020: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efff6ab0400 next 656 of size 16384000
2019-10-02 01:35:29.729026: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efff7a50400 next 645 of size 16384000
2019-10-02 01:35:29.729032: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efff89f0400 next 641 of size 8192
2019-10-02 01:35:29.729037: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efff89f2400 next 654 of size 8192
2019-10-02 01:35:29.729043: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efff89f4400 next 556 of size 16384000
2019-10-02 01:35:29.729049: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efff9994400 next 574 of size 16384000
2019-10-02 01:35:29.729055: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffa934400 next 570 of size 8192
2019-10-02 01:35:29.729061: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffa936400 next 562 of size 8192
2019-10-02 01:35:29.729067: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffa938400 next 1008 of size 8192
2019-10-02 01:35:29.729072: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffa93a400 next 666 of size 8192
2019-10-02 01:35:29.729078: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffa93c400 next 567 of size 8192
2019-10-02 01:35:29.729084: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffa93e400 next 557 of size 16384000
2019-10-02 01:35:29.729090: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffb8de400 next 691 of size 16384000
2019-10-02 01:35:29.729096: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffc87e400 next 634 of size 8192
2019-10-02 01:35:29.729101: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffc880400 next 631 of size 8192
2019-10-02 01:35:29.729107: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffc882400 next 672 of size 8192
2019-10-02 01:35:29.729113: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffc884400 next 679 of size 8192
2019-10-02 01:35:29.729119: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffc886400 next 669 of size 16384000
2019-10-02 01:35:29.729124: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffd826400 next 628 of size 8192
2019-10-02 01:35:29.729130: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffd828400 next 610 of size 8192
2019-10-02 01:35:29.729135: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffd82a400 next 653 of size 8192
2019-10-02 01:35:29.729141: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffd82c400 next 650 of size 8192
2019-10-02 01:35:29.729147: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffd82e400 next 713 of size 8192
2019-10-02 01:35:29.729152: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffd830400 next 632 of size 8192
2019-10-02 01:35:29.729159: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffd832400 next 575 of size 16777216
2019-10-02 01:35:29.729165: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffe832400 next 664 of size 8192
2019-10-02 01:35:29.729171: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffe834400 next 618 of size 8192
2019-10-02 01:35:29.729177: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffe836400 next 621 of size 8192
2019-10-02 01:35:29.729183: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7efffe838400 next 555 of size 16777216
2019-10-02 01:35:29.729188: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effff838400 next 602 of size 8192
2019-10-02 01:35:29.729194: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effff83a400 next 606 of size 32768
2019-10-02 01:35:29.729200: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effff842400 next 600 of size 8192
2019-10-02 01:35:29.729208: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effff844400 next 609 of size 8192
2019-10-02 01:35:29.729214: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effff846400 next 593 of size 8192
2019-10-02 01:35:29.729220: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effff848400 next 592 of size 8192
2019-10-02 01:35:29.729226: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effff84a400 next 652 of size 8192
2019-10-02 01:35:29.729232: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7effff84c400 next 1012 of size 16777216
2019-10-02 01:35:29.729238: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f000084c400 next 647 of size 8192
2019-10-02 01:35:29.729243: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f000084e400 next 660 of size 8192
2019-10-02 01:35:29.729249: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0000850400 next 603 of size 8192
2019-10-02 01:35:29.729255: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0000852400 next 673 of size 8192
2019-10-02 01:35:29.729260: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0000854400 next 1020 of size 8192
2019-10-02 01:35:29.729266: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0000856400 next 1015 of size 8192
2019-10-02 01:35:29.729272: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0000858400 next 643 of size 16777216
2019-10-02 01:35:29.729277: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0001858400 next 1009 of size 67108864
2019-10-02 01:35:29.729283: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0005858400 next 1010 of size 16777216
2019-10-02 01:35:29.729289: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0006858400 next 616 of size 16777216
2019-10-02 01:35:29.729294: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0007858400 next 663 of size 16777216
2019-10-02 01:35:29.729300: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0008858400 next 1013 of size 8192
2019-10-02 01:35:29.729306: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f000885a400 next 1014 of size 8192
2019-10-02 01:35:29.729311: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f000885c400 next 671 of size 16777216
2019-10-02 01:35:29.729317: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f000985c400 next 607 of size 67108864
2019-10-02 01:35:29.729323: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f000d85c400 next 635 of size 16777216
2019-10-02 01:35:29.729328: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f000e85c400 next 662 of size 16777216
2019-10-02 01:35:29.729334: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f000f85c400 next 636 of size 8192
2019-10-02 01:35:29.729340: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f000f85e400 next 668 of size 8192
2019-10-02 01:35:29.729347: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f000f860400 next 642 of size 8192
2019-10-02 01:35:29.729352: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f000f862400 next 667 of size 32768
2019-10-02 01:35:29.729358: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f000f86a400 next 569 of size 8192
2019-10-02 01:35:29.729364: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f000f86c400 next 639 of size 67108864
2019-10-02 01:35:29.729370: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f001386c400 next 1017 of size 8192
2019-10-02 01:35:29.729376: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f001386e400 next 1018 of size 8192
2019-10-02 01:35:29.729381: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0013870400 next 627 of size 8192
2019-10-02 01:35:29.729393: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0013872400 next 598 of size 8192
2019-10-02 01:35:29.729399: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0013874400 next 655 of size 8192
2019-10-02 01:35:29.729404: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0013876400 next 678 of size 8192
2019-10-02 01:35:29.729411: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0013878400 next 599 of size 8192
2019-10-02 01:35:29.729416: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f001387a400 next 689 of size 8192
2019-10-02 01:35:29.729422: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f001387c400 next 693 of size 8192
2019-10-02 01:35:29.729428: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f001387e400 next 563 of size 32768
2019-10-02 01:35:29.729433: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0013886400 next 1019 of size 16777216
2019-10-02 01:35:29.729439: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0014886400 next 719 of size 16777216
2019-10-02 01:35:29.729445: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0015886400 next 695 of size 8192
2019-10-02 01:35:29.729451: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0015888400 next 694 of size 16777216
2019-10-02 01:35:29.729457: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0016888400 next 1027 of size 16777216
2019-10-02 01:35:29.729462: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0017888400 next 1028 of size 16777216
2019-10-02 01:35:29.729468: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0018888400 next 1029 of size 16777216
2019-10-02 01:35:29.729474: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0019888400 next 1030 of size 67108864
2019-10-02 01:35:29.729480: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f001d888400 next 1032 of size 8192
2019-10-02 01:35:29.729485: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f001d88a400 next 1035 of size 67108864
2019-10-02 01:35:29.729491: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f002188a400 next 1036 of size 16777216
2019-10-02 01:35:29.729497: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f002288a400 next 1038 of size 8192
2019-10-02 01:35:29.729502: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f002288c400 next 1039 of size 67108864
2019-10-02 01:35:29.729508: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f002688c400 next 1040 of size 8192
2019-10-02 01:35:29.729514: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f002688e400 next 1042 of size 8192
2019-10-02 01:35:29.729519: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0026890400 next 1043 of size 16777216
2019-10-02 01:35:29.729525: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0027890400 next 1044 of size 8192
2019-10-02 01:35:29.729531: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0027892400 next 1045 of size 16777216
2019-10-02 01:35:29.729538: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0028892400 next 1046 of size 8192
2019-10-02 01:35:29.729545: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0028894400 next 1048 of size 16777216
2019-10-02 01:35:29.729554: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0029894400 next 1049 of size 8192
2019-10-02 01:35:29.729566: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0029896400 next 1050 of size 16777216
2019-10-02 01:35:29.729575: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f002a896400 next 1051 of size 8192
2019-10-02 01:35:29.729584: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f002a898400 next 1052 of size 8192
2019-10-02 01:35:29.729609: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f002a89a400 next 1053 of size 8192
2019-10-02 01:35:29.729618: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f002a89c400 next 1054 of size 67108864
2019-10-02 01:35:29.729627: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f002e89c400 next 1055 of size 16777216
2019-10-02 01:35:29.729637: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f002f89c400 next 1056 of size 8192
2019-10-02 01:35:29.729645: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f002f89e400 next 1057 of size 32768
2019-10-02 01:35:29.729656: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f002f8a6400 next 1058 of size 8192
2019-10-02 01:35:29.729666: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f002f8a8400 next 1062 of size 16777216
2019-10-02 01:35:29.729676: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00308a8400 next 1063 of size 8192
2019-10-02 01:35:29.729692: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00308aa400 next 1064 of size 32768
2019-10-02 01:35:29.729699: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00308b2400 next 1065 of size 8192
2019-10-02 01:35:29.729705: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00308b4400 next 1066 of size 67108864
2019-10-02 01:35:29.729710: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00348b4400 next 1067 of size 8192
2019-10-02 01:35:29.729716: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00348b6400 next 1068 of size 8192
2019-10-02 01:35:29.729722: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00348b8400 next 1069 of size 8192
2019-10-02 01:35:29.729727: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00348ba400 next 1070 of size 8192
2019-10-02 01:35:29.729733: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00348bc400 next 1071 of size 32768
2019-10-02 01:35:29.729739: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00348c4400 next 1072 of size 8192
2019-10-02 01:35:29.729744: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00348c6400 next 1073 of size 8192
2019-10-02 01:35:29.729750: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00348c8400 next 1074 of size 67108864
2019-10-02 01:35:29.729756: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00388c8400 next 1075 of size 8192
2019-10-02 01:35:29.729761: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00388ca400 next 1078 of size 8192
2019-10-02 01:35:29.729767: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00388cc400 next 1080 of size 16777216
2019-10-02 01:35:29.729773: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00398cc400 next 1081 of size 8192
2019-10-02 01:35:29.729778: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00398ce400 next 1082 of size 16777216
2019-10-02 01:35:29.729784: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f003a8ce400 next 1083 of size 8192
2019-10-02 01:35:29.729789: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f003a8d0400 next 1084 of size 8192
2019-10-02 01:35:29.729797: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f003a8d2400 next 1085 of size 8192
2019-10-02 01:35:29.729802: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f003a8d4400 next 1086 of size 8192
2019-10-02 01:35:29.729809: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f003a8d6400 next 1087 of size 16777216
2019-10-02 01:35:29.729815: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f003b8d6400 next 1088 of size 67108864
2019-10-02 01:35:29.729820: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f003f8d6400 next 1089 of size 67108864
2019-10-02 01:35:29.729832: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00438d6400 next 1090 of size 8192
2019-10-02 01:35:29.729838: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00438d8400 next 1091 of size 8192
2019-10-02 01:35:29.729844: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00438da400 next 1092 of size 8192
2019-10-02 01:35:29.729849: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00438dc400 next 1093 of size 16777216
2019-10-02 01:35:29.729855: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00448dc400 next 1094 of size 8192
2019-10-02 01:35:29.729861: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00448de400 next 1095 of size 16777216
2019-10-02 01:35:29.729867: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00458de400 next 1096 of size 8192
2019-10-02 01:35:29.729873: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00458e0400 next 1097 of size 8192
2019-10-02 01:35:29.729878: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00458e2400 next 1099 of size 67108864
2019-10-02 01:35:29.729884: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00498e2400 next 1100 of size 16777216
2019-10-02 01:35:29.729890: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f004a8e2400 next 1101 of size 8192
2019-10-02 01:35:29.729895: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f004a8e4400 next 1103 of size 16777216
2019-10-02 01:35:29.729902: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f004b8e4400 next 1104 of size 32768
2019-10-02 01:35:29.729908: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f004b8ec400 next 1105 of size 8192
2019-10-02 01:35:29.729913: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f004b8ee400 next 1107 of size 16777216
2019-10-02 01:35:29.729919: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f004c8ee400 next 1108 of size 32768
2019-10-02 01:35:29.729925: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f004c8f6400 next 1109 of size 16777216
2019-10-02 01:35:29.729931: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f004d8f6400 next 1110 of size 8192
2019-10-02 01:35:29.729937: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f004d8f8400 next 1111 of size 8192
2019-10-02 01:35:29.729943: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f004d8fa400 next 1112 of size 8192
2019-10-02 01:35:29.729948: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f004d8fc400 next 1114 of size 8192
2019-10-02 01:35:29.729954: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f004d8fe400 next 1115 of size 67108864
2019-10-02 01:35:29.729960: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f00518fe400 next 1116 of size 8192
2019-10-02 01:35:29.729966: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0051900400 next 1117 of size 8192
2019-10-02 01:35:29.729972: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0051902400 next 1118 of size 16777216
2019-10-02 01:35:29.729977: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0052902400 next 1119 of size 8192
2019-10-02 01:35:29.729984: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0052904400 next 1120 of size 8192
2019-10-02 01:35:29.729991: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0052906400 next 1124 of size 67108864
2019-10-02 01:35:29.729998: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7f0056906400 next 18446744073709551615 of size 74560768
2019-10-02 01:35:29.730006: I tensorflow/core/common_runtime/bfc_allocator.cc:914]      Summary of in-use Chunks by size: 
2019-10-02 01:35:29.730015: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 242 Chunks of size 256 totalling 60.5KiB
2019-10-02 01:35:29.730023: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 45 Chunks of size 512 totalling 22.5KiB
2019-10-02 01:35:29.730032: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 768 totalling 768B
2019-10-02 01:35:29.730041: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 1280 totalling 1.2KiB
2019-10-02 01:35:29.730047: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 4096 totalling 4.0KiB
2019-10-02 01:35:29.730053: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 5376 totalling 5.2KiB
2019-10-02 01:35:29.730059: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 6912 totalling 6.8KiB
2019-10-02 01:35:29.730065: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 448 Chunks of size 8192 totalling 3.50MiB
2019-10-02 01:35:29.730071: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 9984 totalling 9.8KiB
2019-10-02 01:35:29.730077: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 11264 totalling 11.0KiB
2019-10-02 01:35:29.730083: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 13056 totalling 12.8KiB
2019-10-02 01:35:29.730090: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 45 Chunks of size 32768 totalling 1.41MiB
2019-10-02 01:35:29.730096: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 56064 totalling 54.8KiB
2019-10-02 01:35:29.730102: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 61184 totalling 59.8KiB
2019-10-02 01:35:29.730108: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 7 Chunks of size 131072 totalling 896.0KiB
2019-10-02 01:35:29.730114: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 3 Chunks of size 8388608 totalling 24.00MiB
2019-10-02 01:35:29.730121: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 32 Chunks of size 16384000 totalling 500.00MiB
2019-10-02 01:35:29.730127: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 192 Chunks of size 16777216 totalling 3.00GiB
2019-10-02 01:35:29.730133: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 97 Chunks of size 67108864 totalling 6.06GiB
2019-10-02 01:35:29.730139: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 74560768 totalling 71.11MiB
2019-10-02 01:35:29.730145: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 131072000 totalling 125.00MiB
2019-10-02 01:35:29.730151: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 262144000 totalling 250.00MiB
2019-10-02 01:35:29.730158: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 3 Chunks of size 268435456 totalling 768.00MiB
2019-10-02 01:35:29.730163: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 4194304000 totalling 3.91GiB
2019-10-02 01:35:29.730170: I tensorflow/core/common_runtime/bfc_allocator.cc:921] Sum Total of in-use chunks: 14.67GiB
2019-10-02 01:35:29.730176: I tensorflow/core/common_runtime/bfc_allocator.cc:923] total_region_allocated_bytes_: 15753943296 memory_limit_: 15753943450 available bytes: 154 curr_region_allocation_bytes_: 31507887104
2019-10-02 01:35:29.730186: I tensorflow/core/common_runtime/bfc_allocator.cc:929] Stats: 
Limit:                 15753943450
InUse:                 15753943296
MaxInUse:              15753943296
NumAllocs:                    2276
MaxAllocSize:           4194304000

2019-10-02 01:35:29.730237: W tensorflow/core/common_runtime/bfc_allocator.cc:424] ****************************************************************************************************
2019-10-02 01:35:29.730275: W tensorflow/core/framework/op_kernel.cc:1599] OP_REQUIRES failed at constant_op.cc:77 : Resource exhausted: OOM when allocating tensor of shape [] and type int64
2019-10-02 01:35:29.730319: E tensorflow/core/common_runtime/executor.cc:642] Executor failed to create kernel. Resource exhausted: OOM when allocating tensor of shape [] and type int64
	 [[{{node Add/y}}]]
I1002 01:35:29.733419 139640791607040 trainer.py:380] Steps/second: 0.000000, Examples/second: 0.000000
2019-10-02 01:35:29.771785: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2019-10-02 01:35:30.963614: I lingvo/core/ops/record_yielder.cc:353] 0x7ef943186510Basic record yielder exit
I1002 01:35:34.971351 139640791607040 trainer.py:380] Steps/second: 0.000000, Examples/second: 0.000000
E1002 01:35:35.415446 139640783214336 base_runner.py:212] trainer done (fatal error): <class 'tensorflow.python.framework.errors_impl.InternalError'>
I1002 01:35:35.421288 139640783214336 base_runner.py:106] trainer exception: From /job:local/replica:0/task:0:
Dst tensor is not initialized.
	 [[node mul_511 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1748) ]]

Original stack trace for 'mul_511':
  File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 1837, in <module>
    tf.app.run(main)
  File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File "usr/local/lib/python3.6/dist-packages/absl/app.py", line 299, in run
    _run_main(main, args)
  File "usr/local/lib/python3.6/dist-packages/absl/app.py", line 250, in _run_main
    sys.exit(main(argv))
  File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 1833, in main
    RunnerManager(FLAGS.model).Start()
  File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 1825, in Start
    self.StartRunners(self.CreateRunners(FLAGS.job.split(','), FLAGS.logdir))
  File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 1573, in CreateRunners
    trial)
  File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 1532, in _CreateRunner
    return self.Trainer(cfg, *common_args)
  File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 395, in __init__
    self._model.ConstructFPropBPropGraph()
  File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/base_model.py", line 1221, in ConstructFPropBPropGraph
    self._task.BProp()
  File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/base_model.py", line 563, in BProp
    self._BPropForVariables(self.vars)
  File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/base_model.py", line 594, in _BPropForVariables
    gradient_adjuster=self.AdjustGradients)
  File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/learner.py", line 172, in Apply
    var_grads, p.l2_regularizer_weight, p=2.0)
  File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/py_utils.py", line 2035, in AdjustGradientsWithLpLoss
    return lp_loss, var_grads.Transform(LpGrad)
  File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/py_utils.py", line 636, in Transform
    return Transform(self, fn)
  File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/py_utils.py", line 499, in Transform
    values = [Transform(v[k], fn) for k in keys]
  File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/py_utils.py", line 499, in <listcomp>
    values = [Transform(v[k], fn) for k in keys]
  File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/py_utils.py", line 499, in Transform
    values = [Transform(v[k], fn) for k in keys]
  File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/py_utils.py", line 499, in <listcomp>
    values = [Transform(v[k], fn) for k in keys]
  File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/py_utils.py", line 499, in Transform
    values = [Transform(v[k], fn) for k in keys]
  File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/py_utils.py", line 499, in <listcomp>
    values = [Transform(v[k], fn) for k in keys]
  File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/py_utils.py", line 499, in Transform
    values = [Transform(v[k], fn) for k in keys]
  File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/py_utils.py", line 499, in <listcomp>
    values = [Transform(v[k], fn) for k in keys]
  File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/py_utils.py", line 499, in Transform
    values = [Transform(v[k], fn) for k in keys]
  File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/py_utils.py", line 499, in <listcomp>
    values = [Transform(v[k], fn) for k in keys]
  File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/py_utils.py", line 499, in Transform
    values = [Transform(v[k], fn) for k in keys]
  File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/py_utils.py", line 499, in <listcomp>
    values = [Transform(v[k], fn) for k in keys]
  File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/py_utils.py", line 499, in Transform
    values = [Transform(v[k], fn) for k in keys]
  File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/py_utils.py", line 499, in <listcomp>
    values = [Transform(v[k], fn) for k in keys]
  File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/py_utils.py", line 502, in Transform
    return fn(v)
  File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/py_utils.py", line 2030, in LpGrad
    delta = lp_regularizer_weight * grad_v
  File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/variables.py", line 1078, in _run_op
    return tensor_oper(a.value(), *args, **kwargs)
  File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_ops.py", line 925, in r_binary_op_wrapper
    return func(x, y, name=name)
  File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_ops.py", line 1206, in _mul_dispatch
    return gen_math_ops.mul(x, y, name=name)
  File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/gen_math_ops.py", line 6704, in mul
    "Mul", x=x, y=y, name=name)
  File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/op_def_library.py", line 793, in _apply_op_helper
    op_def=op_def)
  File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py", line 507, in new_func
    return func(*args, **kwargs)
  File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py", line 3357, in create_op
    attrs, op_def, compute_device)
  File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py", line 3426, in _create_op_internal
    op_def=op_def)
  File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py", line 1748, in __init__
    self._traceback = tf_stack.extract_stack()


E1002 01:35:35.428283 139640783214336 base_runner.py:219] Traceback (most recent call last):
E1002 01:35:35.428386 139640783214336 base_runner.py:219]   File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py", line 1369, in _do_call
E1002 01:35:35.428457 139640783214336 base_runner.py:219]     return fn(*args)
E1002 01:35:35.428519 139640783214336 base_runner.py:219]   File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py", line 1352, in _run_fn
E1002 01:35:35.428577 139640783214336 base_runner.py:219]     target_list, run_metadata)
E1002 01:35:35.428633 139640783214336 base_runner.py:219]   File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py", line 1447, in _call_tf_sessionrun
E1002 01:35:35.428690 139640783214336 base_runner.py:219]     run_metadata)
E1002 01:35:35.428744 139640783214336 base_runner.py:219] tensorflow.python.framework.errors_impl.InternalError: From /job:local/replica:0/task:0:
E1002 01:35:35.428804 139640783214336 base_runner.py:219] Dst tensor is not initialized.
E1002 01:35:35.428860 139640783214336 base_runner.py:219] 	 [[{{node mul_511}}]]
E1002 01:35:35.428922 139640783214336 base_runner.py:219] 
E1002 01:35:35.428979 139640783214336 base_runner.py:219] During handling of the above exception, another exception occurred:
E1002 01:35:35.429034 139640783214336 base_runner.py:219] 
E1002 01:35:35.429089 139640783214336 base_runner.py:219] Traceback (most recent call last):
E1002 01:35:35.429144 139640783214336 base_runner.py:219]   File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/base_runner.py", line 168, in _RunLoop
E1002 01:35:35.429198 139640783214336 base_runner.py:219]     loop_func(*loop_args)
E1002 01:35:35.429253 139640783214336 base_runner.py:219]   File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 536, in _Loop
E1002 01:35:35.429307 139640783214336 base_runner.py:219]     model_task.per_example_tensors,
E1002 01:35:35.429361 139640783214336 base_runner.py:219]   File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py", line 956, in run
E1002 01:35:35.429415 139640783214336 base_runner.py:219]     run_metadata_ptr)
E1002 01:35:35.429470 139640783214336 base_runner.py:219]   File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py", line 1180, in _run
E1002 01:35:35.429524 139640783214336 base_runner.py:219]     feed_dict_tensor, options, run_metadata)
E1002 01:35:35.429579 139640783214336 base_runner.py:219]   File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py", line 1361, in _do_run
E1002 01:35:35.429634 139640783214336 base_runner.py:219]     run_metadata)
E1002 01:35:35.429687 139640783214336 base_runner.py:219]   File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py", line 1388, in _do_call
E1002 01:35:35.429742 139640783214336 base_runner.py:219]     raise type(e)(node_def, op, message)
E1002 01:35:35.429797 139640783214336 base_runner.py:219] tensorflow.python.framework.errors_impl.InternalError: From /job:local/replica:0/task:0:
E1002 01:35:35.429851 139640783214336 base_runner.py:219] Dst tensor is not initialized.
E1002 01:35:35.429905 139640783214336 base_runner.py:219] 	 [[node mul_511 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1748) ]]
E1002 01:35:35.429960 139640783214336 base_runner.py:219] 
E1002 01:35:35.430013 139640783214336 base_runner.py:219] Original stack trace for 'mul_511':
E1002 01:35:35.430067 139640783214336 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 1837, in <module>
E1002 01:35:35.430121 139640783214336 base_runner.py:219]     tf.app.run(main)
E1002 01:35:35.430176 139640783214336 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py", line 40, in run
E1002 01:35:35.430230 139640783214336 base_runner.py:219]     _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
E1002 01:35:35.430285 139640783214336 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/absl/app.py", line 299, in run
E1002 01:35:35.430339 139640783214336 base_runner.py:219]     _run_main(main, args)
E1002 01:35:35.430393 139640783214336 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/absl/app.py", line 250, in _run_main
E1002 01:35:35.430447 139640783214336 base_runner.py:219]     sys.exit(main(argv))
E1002 01:35:35.430501 139640783214336 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 1833, in main
E1002 01:35:35.430555 139640783214336 base_runner.py:219]     RunnerManager(FLAGS.model).Start()
E1002 01:35:35.430609 139640783214336 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 1825, in Start
E1002 01:35:35.430662 139640783214336 base_runner.py:219]     self.StartRunners(self.CreateRunners(FLAGS.job.split(','), FLAGS.logdir))
E1002 01:35:35.430716 139640783214336 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 1573, in CreateRunners
E1002 01:35:35.430775 139640783214336 base_runner.py:219]     trial)
E1002 01:35:35.430831 139640783214336 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 1532, in _CreateRunner
E1002 01:35:35.430885 139640783214336 base_runner.py:219]     return self.Trainer(cfg, *common_args)
E1002 01:35:35.430940 139640783214336 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 395, in __init__
E1002 01:35:35.430994 139640783214336 base_runner.py:219]     self._model.ConstructFPropBPropGraph()
E1002 01:35:35.431048 139640783214336 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/base_model.py", line 1221, in ConstructFPropBPropGraph
E1002 01:35:35.431103 139640783214336 base_runner.py:219]     self._task.BProp()
E1002 01:35:35.431157 139640783214336 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/base_model.py", line 563, in BProp
E1002 01:35:35.431212 139640783214336 base_runner.py:219]     self._BPropForVariables(self.vars)
E1002 01:35:35.431266 139640783214336 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/base_model.py", line 594, in _BPropForVariables
E1002 01:35:35.431338 139640783214336 base_runner.py:219]     gradient_adjuster=self.AdjustGradients)
E1002 01:35:35.431395 139640783214336 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/learner.py", line 172, in Apply
E1002 01:35:35.431450 139640783214336 base_runner.py:219]     var_grads, p.l2_regularizer_weight, p=2.0)
E1002 01:35:35.431505 139640783214336 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/py_utils.py", line 2035, in AdjustGradientsWithLpLoss
E1002 01:35:35.431560 139640783214336 base_runner.py:219]     return lp_loss, var_grads.Transform(LpGrad)
E1002 01:35:35.431614 139640783214336 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/py_utils.py", line 636, in Transform
E1002 01:35:35.431669 139640783214336 base_runner.py:219]     return Transform(self, fn)
E1002 01:35:35.431723 139640783214336 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/py_utils.py", line 499, in Transform
E1002 01:35:35.431777 139640783214336 base_runner.py:219]     values = [Transform(v[k], fn) for k in keys]
E1002 01:35:35.431832 139640783214336 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/py_utils.py", line 499, in <listcomp>
E1002 01:35:35.431886 139640783214336 base_runner.py:219]     values = [Transform(v[k], fn) for k in keys]
E1002 01:35:35.431940 139640783214336 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/py_utils.py", line 499, in Transform
E1002 01:35:35.431995 139640783214336 base_runner.py:219]     values = [Transform(v[k], fn) for k in keys]
E1002 01:35:35.432049 139640783214336 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/py_utils.py", line 499, in <listcomp>
E1002 01:35:35.432103 139640783214336 base_runner.py:219]     values = [Transform(v[k], fn) for k in keys]
E1002 01:35:35.432157 139640783214336 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/py_utils.py", line 499, in Transform
E1002 01:35:35.432211 139640783214336 base_runner.py:219]     values = [Transform(v[k], fn) for k in keys]
E1002 01:35:35.432266 139640783214336 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/py_utils.py", line 499, in <listcomp>
E1002 01:35:35.432320 139640783214336 base_runner.py:219]     values = [Transform(v[k], fn) for k in keys]
E1002 01:35:35.432379 139640783214336 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/py_utils.py", line 499, in Transform
E1002 01:35:35.432434 139640783214336 base_runner.py:219]     values = [Transform(v[k], fn) for k in keys]
E1002 01:35:35.432488 139640783214336 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/py_utils.py", line 499, in <listcomp>
E1002 01:35:35.432543 139640783214336 base_runner.py:219]     values = [Transform(v[k], fn) for k in keys]
E1002 01:35:35.432597 139640783214336 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/py_utils.py", line 499, in Transform
E1002 01:35:35.432651 139640783214336 base_runner.py:219]     values = [Transform(v[k], fn) for k in keys]
E1002 01:35:35.432706 139640783214336 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/py_utils.py", line 499, in <listcomp>
E1002 01:35:35.432760 139640783214336 base_runner.py:219]     values = [Transform(v[k], fn) for k in keys]
E1002 01:35:35.432814 139640783214336 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/py_utils.py", line 499, in Transform
E1002 01:35:35.432868 139640783214336 base_runner.py:219]     values = [Transform(v[k], fn) for k in keys]
E1002 01:35:35.432922 139640783214336 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/py_utils.py", line 499, in <listcomp>
E1002 01:35:35.432976 139640783214336 base_runner.py:219]     values = [Transform(v[k], fn) for k in keys]
E1002 01:35:35.433030 139640783214336 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/py_utils.py", line 499, in Transform
E1002 01:35:35.433085 139640783214336 base_runner.py:219]     values = [Transform(v[k], fn) for k in keys]
E1002 01:35:35.433138 139640783214336 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/py_utils.py", line 499, in <listcomp>
E1002 01:35:35.433192 139640783214336 base_runner.py:219]     values = [Transform(v[k], fn) for k in keys]
E1002 01:35:35.433247 139640783214336 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/py_utils.py", line 502, in Transform
E1002 01:35:35.433300 139640783214336 base_runner.py:219]     return fn(v)
E1002 01:35:35.433355 139640783214336 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/py_utils.py", line 2030, in LpGrad
E1002 01:35:35.433408 139640783214336 base_runner.py:219]     delta = lp_regularizer_weight * grad_v
E1002 01:35:35.433462 139640783214336 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/variables.py", line 1078, in _run_op
E1002 01:35:35.433516 139640783214336 base_runner.py:219]     return tensor_oper(a.value(), *args, **kwargs)
E1002 01:35:35.433570 139640783214336 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_ops.py", line 925, in r_binary_op_wrapper
E1002 01:35:35.433624 139640783214336 base_runner.py:219]     return func(x, y, name=name)
E1002 01:35:35.433678 139640783214336 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_ops.py", line 1206, in _mul_dispatch
E1002 01:35:35.433732 139640783214336 base_runner.py:219]     return gen_math_ops.mul(x, y, name=name)
E1002 01:35:35.433787 139640783214336 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/gen_math_ops.py", line 6704, in mul
E1002 01:35:35.433842 139640783214336 base_runner.py:219]     "Mul", x=x, y=y, name=name)
E1002 01:35:35.433896 139640783214336 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/op_def_library.py", line 793, in _apply_op_helper
E1002 01:35:35.433950 139640783214336 base_runner.py:219]     op_def=op_def)
E1002 01:35:35.434009 139640783214336 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py", line 507, in new_func
E1002 01:35:35.434065 139640783214336 base_runner.py:219]     return func(*args, **kwargs)
E1002 01:35:35.434119 139640783214336 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py", line 3357, in create_op
E1002 01:35:35.434174 139640783214336 base_runner.py:219]     attrs, op_def, compute_device)
E1002 01:35:35.434228 139640783214336 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py", line 3426, in _create_op_internal
E1002 01:35:35.434282 139640783214336 base_runner.py:219]     op_def=op_def)
E1002 01:35:35.434336 139640783214336 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py", line 1748, in __init__
E1002 01:35:35.434391 139640783214336 base_runner.py:219]     self._traceback = tf_stack.extract_stack()
E1002 01:35:35.434446 139640783214336 base_runner.py:219] 
E1002 01:35:35.434500 139640783214336 base_runner.py:219] 
I1002 01:35:41.733031 139640791607040 trainer.py:380] Steps/second: 0.000000, Examples/second: 0.000000
