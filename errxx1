WARNING:tensorflow:

  TensorFlow's `tf-nightly` package will soon be updated to TensorFlow 2.0.

  Please upgrade your code to TensorFlow 2.0:
    * https://www.tensorflow.org/beta/guide/migration_guide

  Or install the latest stable TensorFlow 1.X release:
    * `pip install -U "tensorflow==1.*"`

  Otherwise your code may be broken by the change.

  
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/compat/v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
I1001 15:21:31.055776 139818304546624 model_imports.py:47] Importing lingvo.tasks.asr.params
I1001 15:21:31.086439 139818304546624 model_registry.py:124] Registering models from module: lingvo.tasks.asr.params.librispeech
I1001 15:21:31.092577 139818304546624 model_imports.py:47] Importing lingvo.tasks.car.params
I1001 15:21:31.121421 139818304546624 model_registry.py:124] Registering models from module: lingvo.tasks.car.params.kitti
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/waymo_open_dataset/metrics/ops/py_metrics_ops.py:24: The name tf.resource_loader.get_path_to_datafile is deprecated. Please use tf.compat.v1.resource_loader.get_path_to_datafile instead.

W1001 15:21:31.132356 139818304546624 module_wrapper.py:137] From /usr/local/lib/python3.6/dist-packages/waymo_open_dataset/metrics/ops/py_metrics_ops.py:24: The name tf.resource_loader.get_path_to_datafile is deprecated. Please use tf.compat.v1.resource_loader.get_path_to_datafile instead.

I1001 15:21:31.149682 139818304546624 model_registry.py:124] Registering models from module: lingvo.tasks.car.params.waymo
I1001 15:21:31.155215 139818304546624 model_imports.py:47] Importing lingvo.tasks.image.params
I1001 15:21:31.157969 139818304546624 model_registry.py:124] Registering models from module: lingvo.tasks.image.params.mnist
I1001 15:21:31.158098 139818304546624 model_imports.py:47] Importing lingvo.tasks.lm.params
I1001 15:21:31.160392 139818304546624 model_registry.py:124] Registering models from module: lingvo.tasks.lm.params.one_billion_wds
I1001 15:21:31.163670 139818304546624 model_imports.py:47] Importing lingvo.tasks.mt.params
I1001 15:21:31.169934 139818304546624 model_registry.py:124] Registering models from module: lingvo.tasks.mt.params.wmt14_en_de
I1001 15:21:31.177171 139818304546624 model_registry.py:124] Registering models from module: lingvo.tasks.mt.params.wmtm16_en_de
I1001 15:21:31.178230 139818304546624 model_imports.py:47] Importing lingvo.tasks.punctuator.params
I1001 15:21:31.180585 139818304546624 model_registry.py:124] Registering models from module: lingvo.tasks.punctuator.params.codelab
2019-10-01 15:21:31.181037: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-10-01 15:21:31.187553: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300045000 Hz
2019-10-01 15:21:31.187766: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x6a3e5a0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2019-10-01 15:21:31.187786: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2019-10-01 15:21:31.190082: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2019-10-01 15:21:31.262328: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-10-01 15:21:31.263161: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x6b061d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2019-10-01 15:21:31.263185: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7
2019-10-01 15:21:31.263375: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-10-01 15:21:31.264086: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235
pciBusID: 0000:00:1e.0
2019-10-01 15:21:31.264370: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2019-10-01 15:21:31.265809: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2019-10-01 15:21:31.267042: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2019-10-01 15:21:31.267341: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2019-10-01 15:21:31.268975: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2019-10-01 15:21:31.270202: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2019-10-01 15:21:31.274038: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2019-10-01 15:21:31.274137: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-10-01 15:21:31.274918: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-10-01 15:21:31.275623: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2019-10-01 15:21:31.275666: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2019-10-01 15:21:31.277198: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-10-01 15:21:31.277214: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2019-10-01 15:21:31.277221: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2019-10-01 15:21:31.277341: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-10-01 15:21:31.278097: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-10-01 15:21:31.278838: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:local/replica:0/task:0/device:GPU:0 with 10805 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7)
2019-10-01 15:21:31.281059: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:258] Initialize GrpcChannelCache for job local -> {0 -> localhost:44465}
2019-10-01 15:21:31.282219: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:365] Started server with target: grpc://localhost:44465
I1001 15:21:31.282562 139818304546624 trainer.py:1515] Job controller start
I1001 15:21:31.318697 139818304546624 base_runner.py:57] ============================================================
I1001 15:21:31.339689 139818304546624 base_runner.py:59] allow_implicit_capture : NoneType
I1001 15:21:31.340054 139818304546624 base_runner.py:59] cls : type/lingvo.core.base_model/SingleTaskModel
I1001 15:21:31.340137 139818304546624 base_runner.py:59] cluster.add_summary : NoneType
I1001 15:21:31.340216 139818304546624 base_runner.py:59] cluster.cls : type/lingvo.core.cluster/_Cluster
I1001 15:21:31.340300 139818304546624 base_runner.py:59] cluster.controller.cpus_per_replica : 1
I1001 15:21:31.340379 139818304546624 base_runner.py:59] cluster.controller.devices_per_split : 1
I1001 15:21:31.340449 139818304546624 base_runner.py:59] cluster.controller.gpus_per_replica : 0
I1001 15:21:31.340518 139818304546624 base_runner.py:59] cluster.controller.name : '/job:local'
I1001 15:21:31.340588 139818304546624 base_runner.py:59] cluster.controller.num_tpu_hosts : 0
I1001 15:21:31.340655 139818304546624 base_runner.py:59] cluster.controller.replicas : 1
I1001 15:21:31.340723 139818304546624 base_runner.py:59] cluster.controller.targets : ''
I1001 15:21:31.340792 139818304546624 base_runner.py:59] cluster.controller.tpus_per_replica : 0
I1001 15:21:31.340860 139818304546624 base_runner.py:59] cluster.decoder.cpus_per_replica : 1
I1001 15:21:31.340936 139818304546624 base_runner.py:59] cluster.decoder.devices_per_split : 1
I1001 15:21:31.340995 139818304546624 base_runner.py:59] cluster.decoder.gpus_per_replica : 1
I1001 15:21:31.341073 139818304546624 base_runner.py:59] cluster.decoder.name : '/job:local'
I1001 15:21:31.341131 139818304546624 base_runner.py:59] cluster.decoder.num_tpu_hosts : 0
I1001 15:21:31.341218 139818304546624 base_runner.py:59] cluster.decoder.replicas : 1
I1001 15:21:31.341305 139818304546624 base_runner.py:59] cluster.decoder.targets : ''
I1001 15:21:31.341376 139818304546624 base_runner.py:59] cluster.decoder.tpus_per_replica : 0
I1001 15:21:31.341439 139818304546624 base_runner.py:59] cluster.evaler.cpus_per_replica : 1
I1001 15:21:31.341506 139818304546624 base_runner.py:59] cluster.evaler.devices_per_split : 1
I1001 15:21:31.341577 139818304546624 base_runner.py:59] cluster.evaler.gpus_per_replica : 1
I1001 15:21:31.341643 139818304546624 base_runner.py:59] cluster.evaler.name : '/job:local'
I1001 15:21:31.341712 139818304546624 base_runner.py:59] cluster.evaler.num_tpu_hosts : 0
I1001 15:21:31.341780 139818304546624 base_runner.py:59] cluster.evaler.replicas : 1
I1001 15:21:31.341846 139818304546624 base_runner.py:59] cluster.evaler.targets : ''
I1001 15:21:31.341928 139818304546624 base_runner.py:59] cluster.evaler.tpus_per_replica : 0
I1001 15:21:31.341993 139818304546624 base_runner.py:59] cluster.input.cpus_per_replica : 1
I1001 15:21:31.342072 139818304546624 base_runner.py:59] cluster.input.devices_per_split : 1
I1001 15:21:31.342143 139818304546624 base_runner.py:59] cluster.input.gpus_per_replica : 0
I1001 15:21:31.342216 139818304546624 base_runner.py:59] cluster.input.name : '/job:local'
I1001 15:21:31.342278 139818304546624 base_runner.py:59] cluster.input.num_tpu_hosts : 0
I1001 15:21:31.342367 139818304546624 base_runner.py:59] cluster.input.replicas : 0
I1001 15:21:31.342454 139818304546624 base_runner.py:59] cluster.input.targets : ''
I1001 15:21:31.342518 139818304546624 base_runner.py:59] cluster.input.tpus_per_replica : 0
I1001 15:21:31.342622 139818304546624 base_runner.py:59] cluster.job : 'controller'
I1001 15:21:31.342678 139818304546624 base_runner.py:59] cluster.logdir : ''
I1001 15:21:31.342758 139818304546624 base_runner.py:59] cluster.mode : 'sync'
I1001 15:21:31.342816 139818304546624 base_runner.py:59] cluster.ps.cpus_per_replica : 1
I1001 15:21:31.342891 139818304546624 base_runner.py:59] cluster.ps.devices_per_split : 1
I1001 15:21:31.342977 139818304546624 base_runner.py:59] cluster.ps.gpus_per_replica : 0
I1001 15:21:31.343058 139818304546624 base_runner.py:59] cluster.ps.name : '/job:local'
I1001 15:21:31.343128 139818304546624 base_runner.py:59] cluster.ps.num_tpu_hosts : 0
I1001 15:21:31.343198 139818304546624 base_runner.py:59] cluster.ps.replicas : 1
I1001 15:21:31.343256 139818304546624 base_runner.py:59] cluster.ps.targets : ''
I1001 15:21:31.343334 139818304546624 base_runner.py:59] cluster.ps.tpus_per_replica : 0
I1001 15:21:31.343394 139818304546624 base_runner.py:59] cluster.task : 0
I1001 15:21:31.343472 139818304546624 base_runner.py:59] cluster.worker.cpus_per_replica : 1
I1001 15:21:31.343526 139818304546624 base_runner.py:59] cluster.worker.devices_per_split : 1
I1001 15:21:31.343614 139818304546624 base_runner.py:59] cluster.worker.gpus_per_replica : 1
I1001 15:21:31.343675 139818304546624 base_runner.py:59] cluster.worker.name : '/job:local'
I1001 15:21:31.343743 139818304546624 base_runner.py:59] cluster.worker.num_tpu_hosts : 0
I1001 15:21:31.343817 139818304546624 base_runner.py:59] cluster.worker.replicas : 1
I1001 15:21:31.343881 139818304546624 base_runner.py:59] cluster.worker.targets : ''
I1001 15:21:31.343948 139818304546624 base_runner.py:59] cluster.worker.tpus_per_replica : 0
I1001 15:21:31.344037 139818304546624 base_runner.py:59] dtype : float32
I1001 15:21:31.344126 139818304546624 base_runner.py:59] fprop_dtype : NoneType
I1001 15:21:31.344203 139818304546624 base_runner.py:59] inference_driver_name : NoneType
I1001 15:21:31.344291 139818304546624 base_runner.py:59] input.allow_implicit_capture : NoneType
I1001 15:21:31.344369 139818304546624 base_runner.py:59] input.bucket_adjust_every_n : 0
I1001 15:21:31.344456 139818304546624 base_runner.py:59] input.bucket_batch_limit : [32]
I1001 15:21:31.344535 139818304546624 base_runner.py:59] input.bucket_upper_bound : [1024]
I1001 15:21:31.344622 139818304546624 base_runner.py:59] input.cls : type/lingvo.tasks.lm.input_generator/LmInput
I1001 15:21:31.344709 139818304546624 base_runner.py:59] input.dtype : float32
I1001 15:21:31.344796 139818304546624 base_runner.py:59] input.file_buffer_size : 10000000
I1001 15:21:31.344883 139818304546624 base_runner.py:59] input.file_datasource : NoneType
I1001 15:21:31.344963 139818304546624 base_runner.py:59] input.file_parallelism : 10
I1001 15:21:31.345043 139818304546624 base_runner.py:59] input.file_pattern : 'text:/tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en*'
I1001 15:21:31.345114 139818304546624 base_runner.py:59] input.file_random_seed : 301
I1001 15:21:31.345186 139818304546624 base_runner.py:59] input.fixed_input_shape : True
I1001 15:21:31.345268 139818304546624 base_runner.py:59] input.flush_every_n : 0
I1001 15:21:31.345355 139818304546624 base_runner.py:59] input.fprop_dtype : NoneType
I1001 15:21:31.345439 139818304546624 base_runner.py:59] input.inference_driver_name : NoneType
I1001 15:21:31.345527 139818304546624 base_runner.py:59] input.is_eval : NoneType
I1001 15:21:31.345613 139818304546624 base_runner.py:59] input.is_inference : NoneType
I1001 15:21:31.345702 139818304546624 base_runner.py:59] input.name : '1bwds_train_set'
I1001 15:21:31.345789 139818304546624 base_runner.py:59] input.num_batcher_threads : 16
I1001 15:21:31.345875 139818304546624 base_runner.py:59] input.num_samples : 0
I1001 15:21:31.345960 139818304546624 base_runner.py:59] input.pad_to_max_seq_length : False
I1001 15:21:31.346042 139818304546624 base_runner.py:59] input.params_init.method : 'xavier'
I1001 15:21:31.346130 139818304546624 base_runner.py:59] input.params_init.scale : 1.000001
I1001 15:21:31.346217 139818304546624 base_runner.py:59] input.params_init.seed : NoneType
I1001 15:21:31.346298 139818304546624 base_runner.py:59] input.random_seed : NoneType
I1001 15:21:31.346380 139818304546624 base_runner.py:59] input.remote.max_inflights_per_target : 32
I1001 15:21:31.346448 139818304546624 base_runner.py:59] input.remote.shardable_batch : False
I1001 15:21:31.346508 139818304546624 base_runner.py:59] input.require_sequential_order : False
I1001 15:21:31.346604 139818304546624 base_runner.py:59] input.skip_lp_regularization : NoneType
I1001 15:21:31.346683 139818304546624 base_runner.py:59] input.source_max_length : NoneType
I1001 15:21:31.346743 139818304546624 base_runner.py:59] input.target_max_length : 1024
I1001 15:21:31.346824 139818304546624 base_runner.py:59] input.tokenizer.allow_implicit_capture : NoneType
I1001 15:21:31.346910 139818304546624 base_runner.py:59] input.tokenizer.append_eos : True
I1001 15:21:31.346997 139818304546624 base_runner.py:59] input.tokenizer.cls : type/lingvo.core.tokenizers/AsciiTokenizer
I1001 15:21:31.347078 139818304546624 base_runner.py:59] input.tokenizer.dtype : float32
I1001 15:21:31.347168 139818304546624 base_runner.py:59] input.tokenizer.fprop_dtype : NoneType
I1001 15:21:31.347256 139818304546624 base_runner.py:59] input.tokenizer.inference_driver_name : NoneType
I1001 15:21:31.347337 139818304546624 base_runner.py:59] input.tokenizer.is_eval : NoneType
I1001 15:21:31.347425 139818304546624 base_runner.py:59] input.tokenizer.is_inference : NoneType
I1001 15:21:31.347510 139818304546624 base_runner.py:59] input.tokenizer.name : 'tokenizer'
I1001 15:21:31.347596 139818304546624 base_runner.py:59] input.tokenizer.pad_to_max_length : True
I1001 15:21:31.347683 139818304546624 base_runner.py:59] input.tokenizer.params_init.method : 'xavier'
I1001 15:21:31.347767 139818304546624 base_runner.py:59] input.tokenizer.params_init.scale : 1.000001
I1001 15:21:31.347854 139818304546624 base_runner.py:59] input.tokenizer.params_init.seed : NoneType
I1001 15:21:31.347940 139818304546624 base_runner.py:59] input.tokenizer.random_seed : NoneType
I1001 15:21:31.348021 139818304546624 base_runner.py:59] input.tokenizer.skip_lp_regularization : NoneType
I1001 15:21:31.348106 139818304546624 base_runner.py:59] input.tokenizer.target_eos_id : 2
I1001 15:21:31.348194 139818304546624 base_runner.py:59] input.tokenizer.target_sos_id : 1
I1001 15:21:31.348282 139818304546624 base_runner.py:59] input.tokenizer.target_unk_id : 0
I1001 15:21:31.348361 139818304546624 base_runner.py:59] input.tokenizer.vn.global_vn : False
I1001 15:21:31.348448 139818304546624 base_runner.py:59] input.tokenizer.vn.per_step_vn : False
I1001 15:21:31.348534 139818304546624 base_runner.py:59] input.tokenizer.vn.scale : NoneType
I1001 15:21:31.348621 139818304546624 base_runner.py:59] input.tokenizer.vn.seed : NoneType
I1001 15:21:31.348708 139818304546624 base_runner.py:59] input.tokenizer.vocab_size : 32000
I1001 15:21:31.348785 139818304546624 base_runner.py:59] input.tokenizer_dict : {}
I1001 15:21:31.348872 139818304546624 base_runner.py:59] input.tpu_infeed_parallelism : 1
I1001 15:21:31.348950 139818304546624 base_runner.py:59] input.use_chaining : False
I1001 15:21:31.349038 139818304546624 base_runner.py:59] input.use_per_host_infeed : False
I1001 15:21:31.349124 139818304546624 base_runner.py:59] input.use_within_batch_mixing : False
I1001 15:21:31.349204 139818304546624 base_runner.py:59] input.vn.global_vn : False
I1001 15:21:31.349279 139818304546624 base_runner.py:59] input.vn.per_step_vn : False
I1001 15:21:31.349348 139818304546624 base_runner.py:59] input.vn.scale : NoneType
I1001 15:21:31.349417 139818304546624 base_runner.py:59] input.vn.seed : NoneType
I1001 15:21:31.349490 139818304546624 base_runner.py:59] is_eval : NoneType
I1001 15:21:31.349554 139818304546624 base_runner.py:59] is_inference : NoneType
I1001 15:21:31.349632 139818304546624 base_runner.py:59] model : 'lm.one_billion_wds.OneBWdsGPipeTransformerWPM@/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/tasks/lm/params/one_billion_wds.py:188'
I1001 15:21:31.349696 139818304546624 base_runner.py:59] name : ''
I1001 15:21:31.349774 139818304546624 base_runner.py:59] params_init.method : 'xavier'
I1001 15:21:31.349840 139818304546624 base_runner.py:59] params_init.scale : 1.000001
I1001 15:21:31.349912 139818304546624 base_runner.py:59] params_init.seed : NoneType
I1001 15:21:31.349982 139818304546624 base_runner.py:59] random_seed : NoneType
I1001 15:21:31.350049 139818304546624 base_runner.py:59] skip_lp_regularization : NoneType
I1001 15:21:31.350126 139818304546624 base_runner.py:59] task.allow_implicit_capture : NoneType
I1001 15:21:31.350189 139818304546624 base_runner.py:59] task.cls : type/lingvo.tasks.lm.model/FixedShapeInputLanguageModel
I1001 15:21:31.350269 139818304546624 base_runner.py:59] task.decoder : NoneType
I1001 15:21:31.350325 139818304546624 base_runner.py:59] task.dtype : float32
I1001 15:21:31.350411 139818304546624 base_runner.py:59] task.encoder : NoneType
I1001 15:21:31.350495 139818304546624 base_runner.py:59] task.eval.decoder_samples_per_summary : 0
I1001 15:21:31.350589 139818304546624 base_runner.py:59] task.eval.load_checkpoint_from : NoneType
I1001 15:21:31.350667 139818304546624 base_runner.py:59] task.eval.samples_per_summary : 0
I1001 15:21:31.350728 139818304546624 base_runner.py:59] task.eval.start_decoder_after : 0
I1001 15:21:31.350791 139818304546624 base_runner.py:59] task.eval.start_eval_after : 0
I1001 15:21:31.350879 139818304546624 base_runner.py:59] task.fprop_dtype : NoneType
I1001 15:21:31.350964 139818304546624 base_runner.py:59] task.inference_driver_name : NoneType
I1001 15:21:31.351038 139818304546624 base_runner.py:59] task.input : NoneType
I1001 15:21:31.351109 139818304546624 base_runner.py:59] task.is_eval : NoneType
I1001 15:21:31.351166 139818304546624 base_runner.py:59] task.is_inference : NoneType
I1001 15:21:31.351245 139818304546624 base_runner.py:59] task.lm.allow_implicit_capture : NoneType
I1001 15:21:31.351299 139818304546624 base_runner.py:59] task.lm.cls : type/lingvo.tasks.lm.layers/GPipeTransformerLm
I1001 15:21:31.351377 139818304546624 base_runner.py:59] task.lm.dtype : float32
I1001 15:21:31.351436 139818304546624 base_runner.py:59] task.lm.fprop_dtype : NoneType
I1001 15:21:31.351516 139818304546624 base_runner.py:59] task.lm.inference_driver_name : NoneType
I1001 15:21:31.351600 139818304546624 base_runner.py:59] task.lm.is_eval : NoneType
I1001 15:21:31.351678 139818304546624 base_runner.py:59] task.lm.is_inference : NoneType
I1001 15:21:31.351748 139818304546624 base_runner.py:59] task.lm.name : 'transformerlm'
I1001 15:21:31.351818 139818304546624 base_runner.py:59] task.lm.params_init.method : 'xavier'
I1001 15:21:31.351884 139818304546624 base_runner.py:59] task.lm.params_init.scale : 1.000001
I1001 15:21:31.351956 139818304546624 base_runner.py:59] task.lm.params_init.seed : NoneType
I1001 15:21:31.352020 139818304546624 base_runner.py:59] task.lm.random_seed : NoneType
I1001 15:21:31.352095 139818304546624 base_runner.py:59] task.lm.skip_lp_regularization : NoneType
I1001 15:21:31.352174 139818304546624 base_runner.py:59] task.lm.stack.allow_implicit_capture : NoneType
I1001 15:21:31.352237 139818304546624 base_runner.py:59] task.lm.stack.batch_dim : 1
I1001 15:21:31.352315 139818304546624 base_runner.py:59] task.lm.stack.cls : type/lingvo.core.layers_with_gpipe/GPipeTransformerStack
I1001 15:21:31.352380 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.allow_implicit_capture : NoneType
I1001 15:21:31.352454 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.cls : type/lingvo.core.layers_with_gpipe/GPipeTransformerLayer
I1001 15:21:31.352537 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.dtype : float32
I1001 15:21:31.352622 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.final_enc_layer : False
I1001 15:21:31.352710 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.fprop_dtype : NoneType
I1001 15:21:31.352797 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.has_aux_atten : True
I1001 15:21:31.352873 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.inference_driver_name : NoneType
I1001 15:21:31.352931 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.is_decoder : False
I1001 15:21:31.352995 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.is_eval : NoneType
I1001 15:21:31.353064 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.is_inference : NoneType
I1001 15:21:31.353134 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.is_transparent : False
I1001 15:21:31.353204 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.allow_implicit_capture : NoneType
I1001 15:21:31.353274 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.cls : type/lingvo.core.layers/LayerNorm
I1001 15:21:31.353347 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.dtype : float32
I1001 15:21:31.353435 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.epsilon : 1e-06
I1001 15:21:31.353523 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.fprop_dtype : NoneType
I1001 15:21:31.353599 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.inference_driver_name : NoneType
I1001 15:21:31.353693 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.input_dim : 0
I1001 15:21:31.353775 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.is_eval : NoneType
I1001 15:21:31.353863 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.is_inference : NoneType
I1001 15:21:31.353951 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.name : ''
I1001 15:21:31.354038 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.params_init.method : 'xavier'
I1001 15:21:31.354124 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.params_init.scale : 1.000001
I1001 15:21:31.354209 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.params_init.seed : NoneType
I1001 15:21:31.354296 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.random_seed : NoneType
I1001 15:21:31.354382 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.skip_lp_regularization : NoneType
I1001 15:21:31.354466 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.vn.global_vn : False
I1001 15:21:31.354579 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.vn.per_step_vn : False
I1001 15:21:31.354670 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.vn.scale : NoneType
I1001 15:21:31.354748 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.vn.seed : NoneType
I1001 15:21:31.354835 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.mask_self_atten : True
I1001 15:21:31.354914 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.name : ''
I1001 15:21:31.355001 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.normalize_output : False
I1001 15:21:31.355082 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.output_dim : 0
I1001 15:21:31.355164 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.packed_input : False
I1001 15:21:31.355247 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.params_init.method : 'xavier'
I1001 15:21:31.355335 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.params_init.scale : 1.000001
I1001 15:21:31.355423 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.params_init.seed : NoneType
I1001 15:21:31.355509 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.random_seed : NoneType
I1001 15:21:31.355597 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.skip_lp_regularization : NoneType
I1001 15:21:31.355684 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.source_dim : 0
I1001 15:21:31.355770 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.add_unnormalized_input : False
I1001 15:21:31.355857 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.allow_implicit_capture : NoneType
I1001 15:21:31.355936 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_dropout_prob : 0.0
I1001 15:21:31.356023 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_hidden_dim : 0
I1001 15:21:31.356111 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.allow_implicit_capture : NoneType
I1001 15:21:31.356198 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.atten_dropout_deterministic : False
I1001 15:21:31.356286 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.atten_dropout_prob : 0.0
I1001 15:21:31.356372 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.cls : type/lingvo.core.attention/MultiHeadedAttention
I1001 15:21:31.356459 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.context_dim : 0
I1001 15:21:31.356547 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.ctx_post_proj_dim : 0
I1001 15:21:31.356632 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.dtype : float32
I1001 15:21:31.356725 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.enable_ctx_post_proj : True
I1001 15:21:31.356814 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.enable_ctx_pre_proj : False
I1001 15:21:31.356900 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.enable_query_proj : True
I1001 15:21:31.356985 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.enable_source_proj : True
I1001 15:21:31.357065 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.fprop_dtype : NoneType
I1001 15:21:31.357145 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.hidden_dim : 0
I1001 15:21:31.357231 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inference_driver_name : NoneType
I1001 15:21:31.357312 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.allow_implicit_capture : NoneType
I1001 15:21:31.357398 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.atten_dropout_deterministic : False
I1001 15:21:31.357475 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.atten_dropout_prob : 0.0
I1001 15:21:31.357563 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.cls : type/lingvo.core.attention/DotProductAttention
I1001 15:21:31.357647 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.dtype : float32
I1001 15:21:31.357732 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.fprop_dtype : NoneType
I1001 15:21:31.357811 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.hidden_dim : 0
I1001 15:21:31.357892 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.inference_driver_name : NoneType
I1001 15:21:31.357978 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.is_eval : NoneType
I1001 15:21:31.358066 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.is_inference : NoneType
I1001 15:21:31.358153 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.name : ''
I1001 15:21:31.358232 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.packed_input : False
I1001 15:21:31.358319 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.params_init.method : 'xavier'
I1001 15:21:31.358400 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.params_init.scale : 1.000001
I1001 15:21:31.358487 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.params_init.seed : NoneType
I1001 15:21:31.358593 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.qdomain.default : NoneType
I1001 15:21:31.358681 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.qdomain.fullyconnected : NoneType
I1001 15:21:31.358769 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.qdomain.softmax : NoneType
I1001 15:21:31.358857 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.query_dim : 0
I1001 15:21:31.358944 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.random_seed : NoneType
I1001 15:21:31.359033 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.skip_lp_regularization : NoneType
I1001 15:21:31.359122 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.source_dim : 0
I1001 15:21:31.359206 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.global_vn : False
I1001 15:21:31.359292 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.per_step_vn : False
I1001 15:21:31.359380 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.scale : NoneType
I1001 15:21:31.359467 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.seed : NoneType
I1001 15:21:31.359549 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.is_eval : NoneType
I1001 15:21:31.359632 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.is_inference : NoneType
I1001 15:21:31.359716 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.name : ''
I1001 15:21:31.359801 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.num_attention_heads : 2
I1001 15:21:31.359889 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.packed_input : False
I1001 15:21:31.359967 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.params_init.method : 'xavier'
I1001 15:21:31.360054 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.params_init.scale : 1.0
I1001 15:21:31.360141 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.params_init.seed : NoneType
I1001 15:21:31.360222 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.qdomain.atten_context : NoneType
I1001 15:21:31.360308 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.qdomain.default : NoneType
I1001 15:21:31.360396 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.qdomain.fullyconnected : NoneType
I1001 15:21:31.360483 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.qdomain.softmax : NoneType
I1001 15:21:31.360563 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.query_dim : 0
I1001 15:21:31.360645 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.random_seed : NoneType
I1001 15:21:31.360719 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.skip_lp_regularization : NoneType
I1001 15:21:31.360777 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.source_dim : 0
I1001 15:21:31.360862 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.use_source_vec_as_attention_value : False
I1001 15:21:31.360948 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.vn.global_vn : False
I1001 15:21:31.361023 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.vn.per_step_vn : False
I1001 15:21:31.361085 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.vn.scale : NoneType
I1001 15:21:31.361154 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.vn.seed : NoneType
I1001 15:21:31.361234 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.cls : type/lingvo.core.layers_with_attention/TransformerAttentionLayer
I1001 15:21:31.361297 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.context_dim : 0
I1001 15:21:31.361373 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.dtype : float32
I1001 15:21:31.361442 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.fprop_dtype : NoneType
I1001 15:21:31.361511 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.inference_driver_name : NoneType
I1001 15:21:31.361588 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.is_eval : NoneType
I1001 15:21:31.361671 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.is_inference : NoneType
I1001 15:21:31.361757 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.is_masked : False
I1001 15:21:31.361835 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.allow_implicit_capture : NoneType
I1001 15:21:31.361923 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.cls : type/lingvo.core.layers/LayerNorm
I1001 15:21:31.362008 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.dtype : float32
I1001 15:21:31.362095 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.epsilon : 1e-06
I1001 15:21:31.362176 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.fprop_dtype : NoneType
I1001 15:21:31.362264 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.inference_driver_name : NoneType
I1001 15:21:31.362349 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.input_dim : 0
I1001 15:21:31.362421 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.is_eval : NoneType
I1001 15:21:31.362482 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.is_inference : NoneType
I1001 15:21:31.362572 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.name : ''
I1001 15:21:31.362644 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.params_init.method : 'xavier'
I1001 15:21:31.362719 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.params_init.scale : 1.000001
I1001 15:21:31.362783 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.params_init.seed : NoneType
I1001 15:21:31.362859 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.random_seed : NoneType
I1001 15:21:31.362945 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.skip_lp_regularization : NoneType
I1001 15:21:31.363032 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.vn.global_vn : False
I1001 15:21:31.363118 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.vn.per_step_vn : False
I1001 15:21:31.363193 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.vn.scale : NoneType
I1001 15:21:31.363246 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.vn.seed : NoneType
I1001 15:21:31.363314 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.mask_type : 'future'
I1001 15:21:31.363382 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.name : ''
I1001 15:21:31.363452 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.num_attention_heads : 8
I1001 15:21:31.363512 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.packed_input : False
I1001 15:21:31.363587 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.params_init.method : 'xavier'
I1001 15:21:31.363643 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.params_init.scale : 1.000001
I1001 15:21:31.363722 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.params_init.seed : NoneType
I1001 15:21:31.363790 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.random_seed : NoneType
I1001 15:21:31.363854 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_prob : 0.0
I1001 15:21:31.363928 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.allow_implicit_capture : NoneType
I1001 15:21:31.363989 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.cls : type/lingvo.core.layers/DropoutLayer
I1001 15:21:31.364073 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.dropout_at_eval : False
I1001 15:21:31.364148 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.dtype : float32
I1001 15:21:31.364230 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.fprop_dtype : NoneType
I1001 15:21:31.364313 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.inference_driver_name : NoneType
I1001 15:21:31.364379 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.is_eval : NoneType
I1001 15:21:31.364440 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.is_inference : NoneType
I1001 15:21:31.364505 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.keep_prob : 1.0
I1001 15:21:31.364579 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.name : ''
I1001 15:21:31.364642 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.noise_shape : NoneType
I1001 15:21:31.364708 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.noise_shape_broadcast_dims : NoneType
I1001 15:21:31.364778 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.params_init.method : 'xavier'
I1001 15:21:31.364842 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.params_init.scale : 1.000001
I1001 15:21:31.364924 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.params_init.seed : NoneType
I1001 15:21:31.365008 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.random_seed : NoneType
I1001 15:21:31.365090 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.skip_lp_regularization : NoneType
I1001 15:21:31.365175 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.global_vn : False
I1001 15:21:31.365261 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.per_step_vn : False
I1001 15:21:31.365344 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.scale : NoneType
I1001 15:21:31.365415 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.seed : NoneType
I1001 15:21:31.365484 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.skip_lp_regularization : NoneType
I1001 15:21:31.365539 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.source_dim : 0
I1001 15:21:31.365619 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.vn.global_vn : False
I1001 15:21:31.365698 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.vn.per_step_vn : False
I1001 15:21:31.365785 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.vn.scale : NoneType
I1001 15:21:31.365869 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.vn.seed : NoneType
I1001 15:21:31.365951 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_aux_atten_tpl : NoneType
I1001 15:21:31.366012 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.activation : 'RELU'
I1001 15:21:31.366081 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.allow_implicit_capture : NoneType
I1001 15:21:31.366158 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.cls : type/lingvo.core.layers_with_attention/TransformerFeedForwardLayer
I1001 15:21:31.366220 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.dtype : float32
I1001 15:21:31.366307 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.activation : ['RELU', 'NONE']
I1001 15:21:31.366366 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.allow_implicit_capture : NoneType
I1001 15:21:31.366450 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.batch_norm : False
I1001 15:21:31.366531 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.bn_fold_weights : NoneType
I1001 15:21:31.366646 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.cls : type/lingvo.core.layers/FeedForwardNet
I1001 15:21:31.366733 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.allow_implicit_capture : NoneType
I1001 15:21:31.366811 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.cls : type/lingvo.core.layers/DropoutLayer
I1001 15:21:31.366886 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.dropout_at_eval : False
I1001 15:21:31.366951 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.dtype : float32
I1001 15:21:31.367023 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.fprop_dtype : NoneType
I1001 15:21:31.367094 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.inference_driver_name : NoneType
I1001 15:21:31.367212 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.is_eval : NoneType
I1001 15:21:31.367311 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.is_inference : NoneType
I1001 15:21:31.367401 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.keep_prob : 1.0
I1001 15:21:31.367485 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.name : ''
I1001 15:21:31.367560 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.noise_shape : NoneType
I1001 15:21:31.367624 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.noise_shape_broadcast_dims : NoneType
I1001 15:21:31.367694 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.params_init.method : 'xavier'
I1001 15:21:31.367762 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.params_init.scale : 1.000001
I1001 15:21:31.367832 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.params_init.seed : NoneType
I1001 15:21:31.367899 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.random_seed : NoneType
I1001 15:21:31.367969 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.skip_lp_regularization : NoneType
I1001 15:21:31.368039 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.global_vn : False
I1001 15:21:31.368106 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.per_step_vn : False
I1001 15:21:31.368183 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.scale : NoneType
I1001 15:21:31.368244 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.seed : NoneType
I1001 15:21:31.368323 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dtype : float32
I1001 15:21:31.368381 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.fprop_dtype : NoneType
I1001 15:21:31.368466 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.inference_driver_name : NoneType
I1001 15:21:31.368553 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.input_dim : 0
I1001 15:21:31.368640 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.is_eval : NoneType
I1001 15:21:31.368723 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.is_inference : NoneType
I1001 15:21:31.368808 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.name : ''
I1001 15:21:31.368893 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.params_init.method : 'xavier'
I1001 15:21:31.368978 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.params_init.scale : 1.000001
I1001 15:21:31.369063 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.params_init.seed : NoneType
I1001 15:21:31.369149 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.activation : 'RELU'
I1001 15:21:31.369229 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.affine_last : False
I1001 15:21:31.369312 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.allow_implicit_capture : NoneType
I1001 15:21:31.369382 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.batch_norm : True
I1001 15:21:31.369451 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.bias_init : 0.0
I1001 15:21:31.369515 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.bn_fold_weights : NoneType
I1001 15:21:31.369588 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.cls : type/lingvo.core.layers/ProjectionLayer
I1001 15:21:31.369650 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.dtype : float32
I1001 15:21:31.369724 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.fprop_dtype : NoneType
I1001 15:21:31.369788 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.has_bias : False
I1001 15:21:31.369863 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.inference_driver_name : NoneType
I1001 15:21:31.369933 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.input_dim : 0
I1001 15:21:31.370001 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.is_eval : NoneType
I1001 15:21:31.370079 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.is_inference : NoneType
I1001 15:21:31.370156 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.name : ''
I1001 15:21:31.370215 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.output_dim : 0
I1001 15:21:31.370301 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.params_init.method : 'xavier'
I1001 15:21:31.370382 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.params_init.scale : 1.000001
I1001 15:21:31.370469 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.params_init.seed : NoneType
I1001 15:21:31.370569 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.qdomain.default : NoneType
I1001 15:21:31.370661 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.random_seed : NoneType
I1001 15:21:31.370758 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.skip_lp_regularization : NoneType
I1001 15:21:31.370837 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.global_vn : False
I1001 15:21:31.370906 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.per_step_vn : False
I1001 15:21:31.370976 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.scale : NoneType
I1001 15:21:31.371045 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.seed : NoneType
I1001 15:21:31.371114 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.weight_norm : False
I1001 15:21:31.371200 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.qdomain.default : NoneType
I1001 15:21:31.371284 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.random_seed : NoneType
I1001 15:21:31.371370 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.skip_connections : NoneType
I1001 15:21:31.371454 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.skip_lp_regularization : NoneType
I1001 15:21:31.371540 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.global_vn : False
I1001 15:21:31.371625 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.per_step_vn : False
I1001 15:21:31.371710 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.scale : NoneType
I1001 15:21:31.371793 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.seed : NoneType
I1001 15:21:31.371878 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.weight_norm : False
I1001 15:21:31.371962 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fprop_dtype : NoneType
I1001 15:21:31.372046 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.hidden_dim : 2048
I1001 15:21:31.372132 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.inference_driver_name : NoneType
I1001 15:21:31.372216 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.input_dim : 0
I1001 15:21:31.372303 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.is_eval : NoneType
I1001 15:21:31.372387 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.is_inference : NoneType
I1001 15:21:31.372473 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.allow_implicit_capture : NoneType
I1001 15:21:31.372557 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.cls : type/lingvo.core.layers/LayerNorm
I1001 15:21:31.372644 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.dtype : float32
I1001 15:21:31.372728 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.epsilon : 1e-06
I1001 15:21:31.372815 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.fprop_dtype : NoneType
I1001 15:21:31.372900 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.inference_driver_name : NoneType
I1001 15:21:31.372970 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.input_dim : 0
I1001 15:21:31.373039 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.is_eval : NoneType
I1001 15:21:31.373101 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.is_inference : NoneType
I1001 15:21:31.373176 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.name : ''
I1001 15:21:31.373238 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.params_init.method : 'xavier'
I1001 15:21:31.373318 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.params_init.scale : 1.000001
I1001 15:21:31.373396 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.params_init.seed : NoneType
I1001 15:21:31.373480 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.random_seed : NoneType
I1001 15:21:31.373566 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.skip_lp_regularization : NoneType
I1001 15:21:31.373651 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.vn.global_vn : False
I1001 15:21:31.373738 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.vn.per_step_vn : False
I1001 15:21:31.373824 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.vn.scale : NoneType
I1001 15:21:31.373908 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.vn.seed : NoneType
I1001 15:21:31.373978 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.name : ''
I1001 15:21:31.374046 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.output_dim : 0
I1001 15:21:31.374107 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.params_init.method : 'xavier'
I1001 15:21:31.374180 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.params_init.scale : 1.000001
I1001 15:21:31.374240 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.params_init.seed : NoneType
I1001 15:21:31.374319 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.random_seed : NoneType
I1001 15:21:31.374387 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.relu_dropout_prob : 0.0
I1001 15:21:31.374474 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.activation : 'RELU'
I1001 15:21:31.374577 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.affine_last : False
I1001 15:21:31.374660 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.allow_implicit_capture : NoneType
I1001 15:21:31.374746 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.batch_norm : True
I1001 15:21:31.374815 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.bias_init : 0.0
I1001 15:21:31.374884 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.bn_fold_weights : NoneType
I1001 15:21:31.374947 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.cls : type/lingvo.core.layers/ProjectionLayer
I1001 15:21:31.375022 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.dtype : float32
I1001 15:21:31.375078 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.fprop_dtype : NoneType
I1001 15:21:31.375164 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.has_bias : False
I1001 15:21:31.375248 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.inference_driver_name : NoneType
I1001 15:21:31.375335 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.input_dim : 0
I1001 15:21:31.375419 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.is_eval : NoneType
I1001 15:21:31.375507 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.is_inference : NoneType
I1001 15:21:31.375591 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.name : ''
I1001 15:21:31.375679 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.output_dim : 0
I1001 15:21:31.375763 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.params_init.method : 'xavier'
I1001 15:21:31.375856 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.params_init.scale : 1.000001
I1001 15:21:31.375943 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.params_init.seed : NoneType
I1001 15:21:31.376019 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.qdomain.default : NoneType
I1001 15:21:31.376089 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.random_seed : NoneType
I1001 15:21:31.376159 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.skip_lp_regularization : NoneType
I1001 15:21:31.376228 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.global_vn : False
I1001 15:21:31.376301 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.per_step_vn : False
I1001 15:21:31.376380 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.scale : NoneType
I1001 15:21:31.376450 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.seed : NoneType
I1001 15:21:31.376537 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.weight_norm : False
I1001 15:21:31.376621 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_prob : 0.0
I1001 15:21:31.376707 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.allow_implicit_capture : NoneType
I1001 15:21:31.376792 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.cls : type/lingvo.core.layers/DropoutLayer
I1001 15:21:31.376879 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.dropout_at_eval : False
I1001 15:21:31.376963 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.dtype : float32
I1001 15:21:31.377050 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.fprop_dtype : NoneType
I1001 15:21:31.377134 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.inference_driver_name : NoneType
I1001 15:21:31.377220 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.is_eval : NoneType
I1001 15:21:31.377305 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.is_inference : NoneType
I1001 15:21:31.377391 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.keep_prob : 1.0
I1001 15:21:31.377476 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.name : ''
I1001 15:21:31.377546 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.noise_shape : NoneType
I1001 15:21:31.377613 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.noise_shape_broadcast_dims : NoneType
I1001 15:21:31.377679 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.params_init.method : 'xavier'
I1001 15:21:31.377755 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.params_init.scale : 1.000001
I1001 15:21:31.377817 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.params_init.seed : NoneType
I1001 15:21:31.377893 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.random_seed : NoneType
I1001 15:21:31.377955 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.skip_lp_regularization : NoneType
I1001 15:21:31.378034 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.global_vn : False
I1001 15:21:31.378108 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.per_step_vn : False
I1001 15:21:31.378196 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.scale : NoneType
I1001 15:21:31.378280 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.seed : NoneType
I1001 15:21:31.378367 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.skip_lp_regularization : NoneType
I1001 15:21:31.378452 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.vn.global_vn : False
I1001 15:21:31.378561 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.vn.per_step_vn : False
I1001 15:21:31.378651 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.vn.scale : NoneType
I1001 15:21:31.378738 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.vn.seed : NoneType
I1001 15:21:31.378823 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.transparent_merger_tpl : NoneType
I1001 15:21:31.378892 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.vn.global_vn : False
I1001 15:21:31.378961 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.vn.per_step_vn : False
I1001 15:21:31.379022 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.vn.scale : NoneType
I1001 15:21:31.379097 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.vn.seed : NoneType
I1001 15:21:31.379154 139818304546624 base_runner.py:59] task.lm.stack.dtype : float32
I1001 15:21:31.379232 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.add_tgt_embedding_layer : False
I1001 15:21:31.379318 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.allow_implicit_capture : NoneType
I1001 15:21:31.379405 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.batch_dim : 1
I1001 15:21:31.379491 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.cls : type/lingvo.core.layers_with_gpipe/GPipeTransformerEmbeddingLayer
I1001 15:21:31.379577 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.dec_task_emb : NoneType
I1001 15:21:31.379662 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.allow_implicit_capture : NoneType
I1001 15:21:31.379745 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.cls : type/lingvo.core.layers/DropoutLayer
I1001 15:21:31.379814 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.dropout_at_eval : False
I1001 15:21:31.379883 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.dtype : float32
I1001 15:21:31.379949 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.fprop_dtype : NoneType
I1001 15:21:31.380022 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.inference_driver_name : NoneType
I1001 15:21:31.380088 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.is_eval : NoneType
I1001 15:21:31.380153 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.is_inference : NoneType
I1001 15:21:31.380225 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.keep_prob : 1.0
I1001 15:21:31.380290 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.name : ''
I1001 15:21:31.380365 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.noise_shape : NoneType
I1001 15:21:31.380427 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.noise_shape_broadcast_dims : NoneType
I1001 15:21:31.380502 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.params_init.method : 'xavier'
I1001 15:21:31.380569 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.params_init.scale : 1.000001
I1001 15:21:31.380639 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.params_init.seed : NoneType
I1001 15:21:31.380717 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.random_seed : NoneType
I1001 15:21:31.380808 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.skip_lp_regularization : NoneType
I1001 15:21:31.380895 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.vn.global_vn : False
I1001 15:21:31.380980 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.vn.per_step_vn : False
I1001 15:21:31.381068 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.vn.scale : NoneType
I1001 15:21:31.381152 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.vn.seed : NoneType
I1001 15:21:31.381238 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.dtype : float32
I1001 15:21:31.381325 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.enc_task_emb : NoneType
I1001 15:21:31.381409 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.fprop_dtype : NoneType
I1001 15:21:31.381479 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.inference_driver_name : NoneType
I1001 15:21:31.381548 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.input_dropout_prob : 0.0
I1001 15:21:31.381609 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.is_eval : NoneType
I1001 15:21:31.381684 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.is_inference : NoneType
I1001 15:21:31.381748 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.is_transparent : False
I1001 15:21:31.381833 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.max_seq_len : 300
I1001 15:21:31.381918 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.name : ''
I1001 15:21:31.382005 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.packed_input : False
I1001 15:21:31.382089 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.params_init.method : 'xavier'
I1001 15:21:31.382177 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.params_init.scale : 1.000001
I1001 15:21:31.382262 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.params_init.seed : NoneType
I1001 15:21:31.382334 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.allow_implicit_capture : NoneType
I1001 15:21:31.382402 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.cls : type/lingvo.core.layers/PositionalEmbeddingLayer
I1001 15:21:31.382470 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.dtype : float32
I1001 15:21:31.382562 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.embedding_dim : 2048
I1001 15:21:31.382629 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.fprop_dtype : NoneType
I1001 15:21:31.382704 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.inference_driver_name : NoneType
I1001 15:21:31.382764 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.is_eval : NoneType
I1001 15:21:31.382845 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.is_inference : NoneType
I1001 15:21:31.382907 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.max_timescale : 10000
I1001 15:21:31.382981 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.min_timescale : 1
I1001 15:21:31.383037 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.name : ''
I1001 15:21:31.383122 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.params_init.method : 'xavier'
I1001 15:21:31.383207 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.params_init.scale : 1.000001
I1001 15:21:31.383293 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.params_init.seed : NoneType
I1001 15:21:31.383377 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.random_seed : NoneType
I1001 15:21:31.383463 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.skip_lp_regularization : NoneType
I1001 15:21:31.383547 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.trainable_scaling : False
I1001 15:21:31.383639 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.trainable_scaling_init : 1.0
I1001 15:21:31.383724 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.vn.global_vn : False
I1001 15:21:31.383812 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.vn.per_step_vn : False
I1001 15:21:31.383896 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.vn.scale : NoneType
I1001 15:21:31.383983 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.vn.seed : NoneType
I1001 15:21:31.384068 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.random_seed : NoneType
I1001 15:21:31.384155 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.skip_lp_regularization : NoneType
I1001 15:21:31.384240 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.allow_implicit_capture : NoneType
I1001 15:21:31.384325 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.apply_pruning : False
I1001 15:21:31.384411 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.cls : type/lingvo.core.layers/SimpleEmbeddingLayer
I1001 15:21:31.384496 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.dtype : float32
I1001 15:21:31.384582 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.embedding_dim : 2048
I1001 15:21:31.384668 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.fprop_dtype : NoneType
I1001 15:21:31.384753 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.fprop_mode : NoneType
I1001 15:21:31.384836 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.inference_driver_name : NoneType
I1001 15:21:31.384904 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.is_eval : NoneType
I1001 15:21:31.384973 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.is_inference : NoneType
I1001 15:21:31.385036 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.name : ''
I1001 15:21:31.385109 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.params_init.method : 'gaussian'
I1001 15:21:31.385179 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.params_init.scale : 0.022097086912079608
I1001 15:21:31.385246 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.params_init.seed : NoneType
I1001 15:21:31.385317 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.qdomain.default : NoneType
I1001 15:21:31.385379 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.random_seed : NoneType
I1001 15:21:31.385454 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.skip_lp_regularization : NoneType
I1001 15:21:31.385534 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.use_3d_weight_tensor : False
I1001 15:21:31.385597 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.use_matmul : False
I1001 15:21:31.385663 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.vn.global_vn : False
I1001 15:21:31.385738 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.vn.per_step_vn : False
I1001 15:21:31.385802 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.vn.scale : NoneType
I1001 15:21:31.385876 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.vn.seed : NoneType
I1001 15:21:31.385940 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.vocab_size : 32000
I1001 15:21:31.386020 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.vn.global_vn : False
I1001 15:21:31.386082 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.vn.per_step_vn : False
I1001 15:21:31.386161 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.vn.scale : NoneType
I1001 15:21:31.386230 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.vn.seed : NoneType
I1001 15:21:31.386317 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.allow_implicit_capture : NoneType
I1001 15:21:31.386401 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.cls : type/lingvo.core.layers_with_gpipe/GPipeTransformerLayer
I1001 15:21:31.386494 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.dtype : float32
I1001 15:21:31.386599 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.final_enc_layer : False
I1001 15:21:31.386687 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.fprop_dtype : NoneType
I1001 15:21:31.386771 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.has_aux_atten : False
I1001 15:21:31.386840 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.inference_driver_name : NoneType
I1001 15:21:31.386896 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.is_decoder : False
I1001 15:21:31.386964 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.is_eval : NoneType
I1001 15:21:31.387031 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.is_inference : NoneType
I1001 15:21:31.387100 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.is_transparent : False
I1001 15:21:31.387156 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.allow_implicit_capture : NoneType
I1001 15:21:31.387235 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.cls : type/lingvo.core.layers/LayerNorm
I1001 15:21:31.387289 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.dtype : float32
I1001 15:21:31.387367 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.epsilon : 1e-06
I1001 15:21:31.387427 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.fprop_dtype : NoneType
I1001 15:21:31.387500 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.inference_driver_name : NoneType
I1001 15:21:31.387565 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.input_dim : 0
I1001 15:21:31.387629 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.is_eval : NoneType
I1001 15:21:31.387701 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.is_inference : NoneType
I1001 15:21:31.387756 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.name : ''
I1001 15:21:31.387835 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.params_init.method : 'xavier'
I1001 15:21:31.387889 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.params_init.scale : 1.000001
I1001 15:21:31.387959 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.params_init.seed : NoneType
I1001 15:21:31.388025 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.random_seed : NoneType
I1001 15:21:31.388092 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.skip_lp_regularization : NoneType
I1001 15:21:31.388161 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.vn.global_vn : False
I1001 15:21:31.388228 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.vn.per_step_vn : False
I1001 15:21:31.388297 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.vn.scale : NoneType
I1001 15:21:31.388357 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.vn.seed : NoneType
I1001 15:21:31.388430 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.mask_self_atten : True
I1001 15:21:31.388487 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.name : ''
I1001 15:21:31.388563 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.normalize_output : False
I1001 15:21:31.388625 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.output_dim : 0
I1001 15:21:31.388697 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.packed_input : False
I1001 15:21:31.388761 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.params_init.method : 'xavier'
I1001 15:21:31.388828 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.params_init.scale : 1.000001
I1001 15:21:31.388899 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.params_init.seed : NoneType
I1001 15:21:31.388957 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.random_seed : NoneType
I1001 15:21:31.389038 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.skip_lp_regularization : NoneType
I1001 15:21:31.389095 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.source_dim : 2048
I1001 15:21:31.389173 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.add_unnormalized_input : False
I1001 15:21:31.389228 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.allow_implicit_capture : NoneType
I1001 15:21:31.389305 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_dropout_prob : 0.0
I1001 15:21:31.389365 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_hidden_dim : 0
I1001 15:21:31.389438 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.allow_implicit_capture : NoneType
I1001 15:21:31.389502 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.atten_dropout_deterministic : False
I1001 15:21:31.389569 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.atten_dropout_prob : 0.0
I1001 15:21:31.389639 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.cls : type/lingvo.core.attention/MultiHeadedAttention
I1001 15:21:31.389704 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.context_dim : 0
I1001 15:21:31.389777 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.ctx_post_proj_dim : 0
I1001 15:21:31.389837 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.dtype : float32
I1001 15:21:31.389913 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.enable_ctx_post_proj : True
I1001 15:21:31.389970 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.enable_ctx_pre_proj : True
I1001 15:21:31.390049 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.enable_query_proj : True
I1001 15:21:31.390110 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.enable_source_proj : True
I1001 15:21:31.390183 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.fprop_dtype : NoneType
I1001 15:21:31.390240 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.hidden_dim : 0
I1001 15:21:31.390310 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inference_driver_name : NoneType
I1001 15:21:31.390379 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.allow_implicit_capture : NoneType
I1001 15:21:31.390446 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.atten_dropout_deterministic : False
I1001 15:21:31.390517 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.atten_dropout_prob : 0.0
I1001 15:21:31.390606 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.cls : type/lingvo.core.attention/DotProductAttention
I1001 15:21:31.390677 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.dtype : float32
I1001 15:21:31.390739 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.fprop_dtype : NoneType
I1001 15:21:31.390816 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.hidden_dim : 0
I1001 15:21:31.390871 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.inference_driver_name : NoneType
I1001 15:21:31.390949 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.is_eval : NoneType
I1001 15:21:31.391008 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.is_inference : NoneType
I1001 15:21:31.391085 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.name : ''
I1001 15:21:31.391149 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.packed_input : False
I1001 15:21:31.391259 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.params_init.method : 'xavier'
I1001 15:21:31.391328 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.params_init.scale : 1.000001
I1001 15:21:31.391397 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.params_init.seed : NoneType
I1001 15:21:31.391467 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.qdomain.default : NoneType
I1001 15:21:31.391530 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.qdomain.fullyconnected : NoneType
I1001 15:21:31.391603 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.qdomain.softmax : NoneType
I1001 15:21:31.391657 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.query_dim : 0
I1001 15:21:31.391735 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.random_seed : NoneType
I1001 15:21:31.391796 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.skip_lp_regularization : NoneType
I1001 15:21:31.391868 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.source_dim : 0
I1001 15:21:31.391933 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.global_vn : False
I1001 15:21:31.392002 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.per_step_vn : False
I1001 15:21:31.392072 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.scale : NoneType
I1001 15:21:31.392135 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.seed : NoneType
I1001 15:21:31.392209 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.is_eval : NoneType
I1001 15:21:31.392263 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.is_inference : NoneType
I1001 15:21:31.392341 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.name : ''
I1001 15:21:31.392401 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.num_attention_heads : 2
I1001 15:21:31.392472 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.packed_input : False
I1001 15:21:31.392538 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.params_init.method : 'xavier'
I1001 15:21:31.392605 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.params_init.scale : 1.0
I1001 15:21:31.392674 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.params_init.seed : NoneType
I1001 15:21:31.392734 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.qdomain.atten_context : NoneType
I1001 15:21:31.392810 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.qdomain.default : NoneType
I1001 15:21:31.392867 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.qdomain.fullyconnected : NoneType
I1001 15:21:31.392945 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.qdomain.softmax : NoneType
I1001 15:21:31.392999 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.query_dim : 0
I1001 15:21:31.393087 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.random_seed : NoneType
I1001 15:21:31.393145 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.skip_lp_regularization : NoneType
I1001 15:21:31.393218 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.source_dim : 0
I1001 15:21:31.393281 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.use_source_vec_as_attention_value : False
I1001 15:21:31.393348 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.vn.global_vn : False
I1001 15:21:31.393417 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.vn.per_step_vn : False
I1001 15:21:31.393484 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.vn.scale : NoneType
I1001 15:21:31.393554 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.vn.seed : NoneType
I1001 15:21:31.393607 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.cls : type/lingvo.core.layers_with_attention/TransformerAttentionLayer
I1001 15:21:31.393684 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.context_dim : 0
I1001 15:21:31.393743 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.dtype : float32
I1001 15:21:31.393811 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.fprop_dtype : NoneType
I1001 15:21:31.393880 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.inference_driver_name : NoneType
I1001 15:21:31.393944 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.is_eval : NoneType
I1001 15:21:31.394017 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.is_inference : NoneType
I1001 15:21:31.394084 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.is_masked : True
I1001 15:21:31.394154 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.allow_implicit_capture : NoneType
I1001 15:21:31.394211 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.cls : type/lingvo.core.layers/LayerNorm
I1001 15:21:31.394284 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.dtype : float32
I1001 15:21:31.394345 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.epsilon : 1e-06
I1001 15:21:31.394415 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.fprop_dtype : NoneType
I1001 15:21:31.394482 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.inference_driver_name : NoneType
I1001 15:21:31.394550 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.input_dim : 0
I1001 15:21:31.394635 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.is_eval : NoneType
I1001 15:21:31.394704 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.is_inference : NoneType
I1001 15:21:31.394773 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.name : ''
I1001 15:21:31.394833 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.params_init.method : 'xavier'
I1001 15:21:31.394905 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.params_init.scale : 1.000001
I1001 15:21:31.394963 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.params_init.seed : NoneType
I1001 15:21:31.395034 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.random_seed : NoneType
I1001 15:21:31.395099 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.skip_lp_regularization : NoneType
I1001 15:21:31.395165 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.vn.global_vn : False
I1001 15:21:31.395240 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.vn.per_step_vn : False
I1001 15:21:31.395304 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.vn.scale : NoneType
I1001 15:21:31.395377 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.vn.seed : NoneType
I1001 15:21:31.395435 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.mask_type : 'future'
I1001 15:21:31.395512 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.name : ''
I1001 15:21:31.395567 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.num_attention_heads : 16
I1001 15:21:31.395645 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.packed_input : False
I1001 15:21:31.395704 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.params_init.method : 'xavier'
I1001 15:21:31.395774 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.params_init.scale : 1.000001
I1001 15:21:31.395840 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.params_init.seed : NoneType
I1001 15:21:31.395905 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.random_seed : NoneType
I1001 15:21:31.395977 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_prob : 0.0
I1001 15:21:31.396037 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.allow_implicit_capture : NoneType
I1001 15:21:31.396112 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.cls : type/lingvo.core.layers/DropoutLayer
I1001 15:21:31.396167 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.dropout_at_eval : False
I1001 15:21:31.396246 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.dtype : float32
I1001 15:21:31.396303 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.fprop_dtype : NoneType
I1001 15:21:31.396375 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.inference_driver_name : NoneType
I1001 15:21:31.396440 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.is_eval : NoneType
I1001 15:21:31.396508 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.is_inference : NoneType
I1001 15:21:31.396578 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.keep_prob : 1.0
I1001 15:21:31.396640 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.name : ''
I1001 15:21:31.396714 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.noise_shape : NoneType
I1001 15:21:31.396778 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.noise_shape_broadcast_dims : NoneType
I1001 15:21:31.396851 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.params_init.method : 'xavier'
I1001 15:21:31.396905 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.params_init.scale : 1.000001
I1001 15:21:31.396976 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.params_init.seed : NoneType
I1001 15:21:31.397040 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.random_seed : NoneType
I1001 15:21:31.397109 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.skip_lp_regularization : NoneType
I1001 15:21:31.397179 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.global_vn : False
I1001 15:21:31.397238 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.per_step_vn : False
I1001 15:21:31.397319 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.scale : NoneType
I1001 15:21:31.397382 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.seed : NoneType
I1001 15:21:31.397455 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.skip_lp_regularization : NoneType
I1001 15:21:31.397510 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.source_dim : 0
I1001 15:21:31.397579 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.vn.global_vn : False
I1001 15:21:31.397647 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.vn.per_step_vn : False
I1001 15:21:31.397710 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.vn.scale : NoneType
I1001 15:21:31.397782 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.vn.seed : NoneType
I1001 15:21:31.397841 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_aux_atten_tpl : NoneType
I1001 15:21:31.397918 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.activation : 'RELU'
I1001 15:21:31.397972 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.allow_implicit_capture : NoneType
I1001 15:21:31.398050 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.cls : type/lingvo.core.layers_with_attention/TransformerFeedForwardLayer
I1001 15:21:31.398109 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.dtype : float32
I1001 15:21:31.398181 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.activation : ['RELU', 'NONE']
I1001 15:21:31.398248 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.allow_implicit_capture : NoneType
I1001 15:21:31.398315 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.batch_norm : False
I1001 15:21:31.398385 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.bn_fold_weights : NoneType
I1001 15:21:31.398448 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.cls : type/lingvo.core.layers/FeedForwardNet
I1001 15:21:31.398522 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.allow_implicit_capture : NoneType
I1001 15:21:31.398606 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.cls : type/lingvo.core.layers/DropoutLayer
I1001 15:21:31.398677 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.dropout_at_eval : False
I1001 15:21:31.398741 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.dtype : float32
I1001 15:21:31.398814 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.fprop_dtype : NoneType
I1001 15:21:31.398874 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.inference_driver_name : NoneType
I1001 15:21:31.398950 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.is_eval : NoneType
I1001 15:21:31.399014 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.is_inference : NoneType
I1001 15:21:31.399087 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.keep_prob : 1.0
I1001 15:21:31.399142 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.name : ''
I1001 15:21:31.399214 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.noise_shape : NoneType
I1001 15:21:31.399281 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.noise_shape_broadcast_dims : NoneType
I1001 15:21:31.399353 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.params_init.method : 'xavier'
I1001 15:21:31.399422 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.params_init.scale : 1.000001
I1001 15:21:31.399491 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.params_init.seed : NoneType
I1001 15:21:31.399560 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.random_seed : NoneType
I1001 15:21:31.399624 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.skip_lp_regularization : NoneType
I1001 15:21:31.399695 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.global_vn : False
I1001 15:21:31.399755 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.per_step_vn : False
I1001 15:21:31.399828 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.scale : NoneType
I1001 15:21:31.399884 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.seed : NoneType
I1001 15:21:31.399952 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dtype : float32
I1001 15:21:31.400021 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.fprop_dtype : NoneType
I1001 15:21:31.400089 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.inference_driver_name : NoneType
I1001 15:21:31.400163 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.input_dim : 0
I1001 15:21:31.400227 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.is_eval : NoneType
I1001 15:21:31.400299 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.is_inference : NoneType
I1001 15:21:31.400353 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.name : ''
I1001 15:21:31.400432 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.params_init.method : 'xavier'
I1001 15:21:31.400490 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.params_init.scale : 1.000001
I1001 15:21:31.400562 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.params_init.seed : NoneType
I1001 15:21:31.400626 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.activation : 'RELU'
I1001 15:21:31.400694 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.affine_last : False
I1001 15:21:31.400763 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.allow_implicit_capture : NoneType
I1001 15:21:31.400828 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.batch_norm : True
I1001 15:21:31.400904 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.bias_init : 0.0
I1001 15:21:31.400957 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.bn_fold_weights : NoneType
I1001 15:21:31.401036 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.cls : type/lingvo.core.layers/ProjectionLayer
I1001 15:21:31.401094 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.dtype : float32
I1001 15:21:31.401167 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.fprop_dtype : NoneType
I1001 15:21:31.401230 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.has_bias : False
I1001 15:21:31.401303 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.inference_driver_name : NoneType
I1001 15:21:31.401377 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.input_dim : 0
I1001 15:21:31.401432 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.is_eval : NoneType
I1001 15:21:31.401498 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.is_inference : NoneType
I1001 15:21:31.401567 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.name : ''
I1001 15:21:31.401636 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.output_dim : 0
I1001 15:21:31.401691 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.params_init.method : 'xavier'
I1001 15:21:31.401769 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.params_init.scale : 1.000001
I1001 15:21:31.401828 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.params_init.seed : NoneType
I1001 15:21:31.401899 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.qdomain.default : NoneType
I1001 15:21:31.401961 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.random_seed : NoneType
I1001 15:21:31.402023 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.skip_lp_regularization : NoneType
I1001 15:21:31.402096 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.global_vn : False
I1001 15:21:31.402158 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.per_step_vn : False
I1001 15:21:31.402230 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.scale : NoneType
I1001 15:21:31.402286 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.seed : NoneType
I1001 15:21:31.402361 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.weight_norm : False
I1001 15:21:31.402422 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.qdomain.default : NoneType
I1001 15:21:31.402489 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.random_seed : NoneType
I1001 15:21:31.402577 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.skip_connections : NoneType
I1001 15:21:31.402652 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.skip_lp_regularization : NoneType
I1001 15:21:31.402717 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.global_vn : False
I1001 15:21:31.402778 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.per_step_vn : False
I1001 15:21:31.402852 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.scale : NoneType
I1001 15:21:31.402911 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.seed : NoneType
I1001 15:21:31.402984 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.weight_norm : False
I1001 15:21:31.403041 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fprop_dtype : NoneType
I1001 15:21:31.403113 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.hidden_dim : 8192
I1001 15:21:31.403178 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.inference_driver_name : NoneType
I1001 15:21:31.403245 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.input_dim : 0
I1001 15:21:31.403319 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.is_eval : NoneType
I1001 15:21:31.403386 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.is_inference : NoneType
I1001 15:21:31.403458 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.allow_implicit_capture : NoneType
I1001 15:21:31.403518 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.cls : type/lingvo.core.layers/LayerNorm
I1001 15:21:31.403594 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.dtype : float32
I1001 15:21:31.403655 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.epsilon : 1e-06
I1001 15:21:31.403727 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.fprop_dtype : NoneType
I1001 15:21:31.403783 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.inference_driver_name : NoneType
I1001 15:21:31.403852 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.input_dim : 0
I1001 15:21:31.403920 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.is_eval : NoneType
I1001 15:21:31.403984 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.is_inference : NoneType
I1001 15:21:31.404056 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.name : ''
I1001 15:21:31.404114 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.params_init.method : 'xavier'
I1001 15:21:31.404191 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.params_init.scale : 1.000001
I1001 15:21:31.404250 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.params_init.seed : NoneType
I1001 15:21:31.404323 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.random_seed : NoneType
I1001 15:21:31.404382 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.skip_lp_regularization : NoneType
I1001 15:21:31.404446 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.vn.global_vn : False
I1001 15:21:31.404517 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.vn.per_step_vn : False
I1001 15:21:31.404570 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.vn.scale : NoneType
I1001 15:21:31.404650 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.vn.seed : NoneType
I1001 15:21:31.404704 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.name : ''
I1001 15:21:31.404776 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.output_dim : 0
I1001 15:21:31.404841 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.params_init.method : 'xavier'
I1001 15:21:31.404908 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.params_init.scale : 1.000001
I1001 15:21:31.404977 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.params_init.seed : NoneType
I1001 15:21:31.405044 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.random_seed : NoneType
I1001 15:21:31.405113 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.relu_dropout_prob : 0.0
I1001 15:21:31.405165 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.activation : 'RELU'
I1001 15:21:31.405240 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.affine_last : False
I1001 15:21:31.405301 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.allow_implicit_capture : NoneType
I1001 15:21:31.405369 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.batch_norm : True
I1001 15:21:31.405444 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.bias_init : 0.0
I1001 15:21:31.405500 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.bn_fold_weights : NoneType
I1001 15:21:31.405585 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.cls : type/lingvo.core.layers/ProjectionLayer
I1001 15:21:31.405668 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.dtype : float32
I1001 15:21:31.405755 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.fprop_dtype : NoneType
I1001 15:21:31.405838 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.has_bias : False
I1001 15:21:31.405919 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.inference_driver_name : NoneType
I1001 15:21:31.405988 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.input_dim : 0
I1001 15:21:31.406049 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.is_eval : NoneType
I1001 15:21:31.406111 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.is_inference : NoneType
I1001 15:21:31.406178 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.name : ''
I1001 15:21:31.406247 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.output_dim : 0
I1001 15:21:31.406306 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.params_init.method : 'xavier'
I1001 15:21:31.406383 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.params_init.scale : 1.000001
I1001 15:21:31.406443 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.params_init.seed : NoneType
I1001 15:21:31.406516 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.qdomain.default : NoneType
I1001 15:21:31.406589 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.random_seed : NoneType
I1001 15:21:31.406666 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.skip_lp_regularization : NoneType
I1001 15:21:31.406726 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.global_vn : False
I1001 15:21:31.406794 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.per_step_vn : False
I1001 15:21:31.406863 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.scale : NoneType
I1001 15:21:31.406922 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.seed : NoneType
I1001 15:21:31.406997 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.weight_norm : False
I1001 15:21:31.407052 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_prob : 0.0
I1001 15:21:31.407131 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.allow_implicit_capture : NoneType
I1001 15:21:31.407186 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.cls : type/lingvo.core.layers/DropoutLayer
I1001 15:21:31.407262 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.dropout_at_eval : False
I1001 15:21:31.407322 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.dtype : float32
I1001 15:21:31.407389 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.fprop_dtype : NoneType
I1001 15:21:31.407455 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.inference_driver_name : NoneType
I1001 15:21:31.407527 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.is_eval : NoneType
I1001 15:21:31.407598 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.is_inference : NoneType
I1001 15:21:31.407662 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.keep_prob : 1.0
I1001 15:21:31.407733 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.name : ''
I1001 15:21:31.407792 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.noise_shape : NoneType
I1001 15:21:31.407867 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.noise_shape_broadcast_dims : NoneType
I1001 15:21:31.407921 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.params_init.method : 'xavier'
I1001 15:21:31.408000 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.params_init.scale : 1.000001
I1001 15:21:31.408055 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.params_init.seed : NoneType
I1001 15:21:31.408131 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.random_seed : NoneType
I1001 15:21:31.408191 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.skip_lp_regularization : NoneType
I1001 15:21:31.408262 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.global_vn : False
I1001 15:21:31.408327 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.per_step_vn : False
I1001 15:21:31.408393 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.scale : NoneType
I1001 15:21:31.408462 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.seed : NoneType
I1001 15:21:31.408526 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.skip_lp_regularization : NoneType
I1001 15:21:31.408598 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.vn.global_vn : False
I1001 15:21:31.408652 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.vn.per_step_vn : False
I1001 15:21:31.408726 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.vn.scale : NoneType
I1001 15:21:31.408788 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.vn.seed : NoneType
I1001 15:21:31.408855 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.transparent_merger_tpl : NoneType
I1001 15:21:31.408924 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.vn.global_vn : False
I1001 15:21:31.408985 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.vn.per_step_vn : False
I1001 15:21:31.409058 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.vn.scale : NoneType
I1001 15:21:31.409123 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.vn.seed : NoneType
I1001 15:21:31.409195 139818304546624 base_runner.py:59] task.lm.stack.fprop_dtype : NoneType
I1001 15:21:31.409252 139818304546624 base_runner.py:59] task.lm.stack.inference_driver_name : NoneType
I1001 15:21:31.409325 139818304546624 base_runner.py:59] task.lm.stack.is_eval : NoneType
I1001 15:21:31.409385 139818304546624 base_runner.py:59] task.lm.stack.is_inference : NoneType
I1001 15:21:31.409457 139818304546624 base_runner.py:59] task.lm.stack.is_transparent : False
I1001 15:21:31.409523 139818304546624 base_runner.py:59] task.lm.stack.label_smoothing : NoneType
I1001 15:21:31.409590 139818304546624 base_runner.py:59] task.lm.stack.model_dim : 2048
I1001 15:21:31.409660 139818304546624 base_runner.py:59] task.lm.stack.name : ''
I1001 15:21:31.409726 139818304546624 base_runner.py:59] task.lm.stack.normalize_encoder : False
I1001 15:21:31.409800 139818304546624 base_runner.py:59] task.lm.stack.num_decoder_layers : 0
I1001 15:21:31.409860 139818304546624 base_runner.py:59] task.lm.stack.num_encoder_layers : 32
I1001 15:21:31.409932 139818304546624 base_runner.py:59] task.lm.stack.num_micro_batches : 32
I1001 15:21:31.409990 139818304546624 base_runner.py:59] task.lm.stack.packed_input : False
I1001 15:21:31.410064 139818304546624 base_runner.py:59] task.lm.stack.params_init.method : 'xavier'
I1001 15:21:31.410126 139818304546624 base_runner.py:59] task.lm.stack.params_init.scale : 1.000001
I1001 15:21:31.410195 139818304546624 base_runner.py:59] task.lm.stack.params_init.seed : NoneType
I1001 15:21:31.410263 139818304546624 base_runner.py:59] task.lm.stack.random_seed : NoneType
I1001 15:21:31.410330 139818304546624 base_runner.py:59] task.lm.stack.skip_lp_regularization : NoneType
I1001 15:21:31.410399 139818304546624 base_runner.py:59] task.lm.stack.softmax_tpl.allow_implicit_capture : NoneType
I1001 15:21:31.410457 139818304546624 base_runner.py:59] task.lm.stack.softmax_tpl.apply_pruning : False
I1001 15:21:31.410548 139818304546624 base_runner.py:59] task.lm.stack.softmax_tpl.chunk_size : 4194
I1001 15:21:31.410617 139818304546624 base_runner.py:59] task.lm.stack.softmax_tpl.cls : type/lingvo.core.layers_with_gpipe/GPipeTransformerSoftmaxLayer
I1001 15:21:31.410689 139818304546624 base_runner.py:59] task.lm.stack.softmax_tpl.dtype : float32
I1001 15:21:31.410743 139818304546624 base_runner.py:59] task.lm.stack.softmax_tpl.fprop_dtype : NoneType
I1001 15:21:31.410816 139818304546624 base_runner.py:59] task.lm.stack.softmax_tpl.inference_driver_name : NoneType
I1001 15:21:31.410882 139818304546624 base_runner.py:59] task.lm.stack.softmax_tpl.input_dim : 2048
I1001 15:21:31.410951 139818304546624 base_runner.py:59] task.lm.stack.softmax_tpl.inputs_from_decoder : False
I1001 15:21:31.411019 139818304546624 base_runner.py:59] task.lm.stack.softmax_tpl.is_eval : NoneType
I1001 15:21:31.411088 139818304546624 base_runner.py:59] task.lm.stack.softmax_tpl.is_inference : NoneType
I1001 15:21:31.411156 139818304546624 base_runner.py:59] task.lm.stack.softmax_tpl.logits_abs_max : NoneType
I1001 15:21:31.411220 139818304546624 base_runner.py:59] task.lm.stack.softmax_tpl.name : ''
I1001 15:21:31.411293 139818304546624 base_runner.py:59] task.lm.stack.softmax_tpl.num_classes : 32000
I1001 15:21:31.411346 139818304546624 base_runner.py:59] task.lm.stack.softmax_tpl.num_sampled : 0
I1001 15:21:31.411445 139818304546624 base_runner.py:59] task.lm.stack.softmax_tpl.num_shards : 16
I1001 15:21:31.411500 139818304546624 base_runner.py:59] task.lm.stack.softmax_tpl.params_init.method : 'xavier'
I1001 15:21:31.411550 139818304546624 base_runner.py:59] task.lm.stack.softmax_tpl.params_init.scale : 1.000001
I1001 15:21:31.411599 139818304546624 base_runner.py:59] task.lm.stack.softmax_tpl.params_init.seed : NoneType
I1001 15:21:31.411648 139818304546624 base_runner.py:59] task.lm.stack.softmax_tpl.qdomain.default : NoneType
I1001 15:21:31.411697 139818304546624 base_runner.py:59] task.lm.stack.softmax_tpl.random_seed : NoneType
I1001 15:21:31.411746 139818304546624 base_runner.py:59] task.lm.stack.softmax_tpl.skip_lp_regularization : NoneType
I1001 15:21:31.411794 139818304546624 base_runner.py:59] task.lm.stack.softmax_tpl.vn.global_vn : False
I1001 15:21:31.411843 139818304546624 base_runner.py:59] task.lm.stack.softmax_tpl.vn.per_step_vn : False
I1001 15:21:31.411891 139818304546624 base_runner.py:59] task.lm.stack.softmax_tpl.vn.scale : NoneType
I1001 15:21:31.411939 139818304546624 base_runner.py:59] task.lm.stack.softmax_tpl.vn.seed : NoneType
I1001 15:21:31.411987 139818304546624 base_runner.py:59] task.lm.stack.splits : [8, 16, 24, 32]
I1001 15:21:31.412035 139818304546624 base_runner.py:59] task.lm.stack.state_dtype : float32
I1001 15:21:31.412084 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_dropout_prob : 0.1
I1001 15:21:31.412133 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.allow_implicit_capture : NoneType
I1001 15:21:31.412187 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.cls : type/lingvo.core.layers_with_gpipe/DeterministicWeightsLayer
I1001 15:21:31.412237 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.allow_implicit_capture : NoneType
I1001 15:21:31.412286 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.cls : type/lingvo.core.layers/DeterministicDropoutLayer
I1001 15:21:31.412335 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.dropout_at_eval : False
I1001 15:21:31.412384 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.dtype : float32
I1001 15:21:31.412433 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.fprop_dtype : NoneType
I1001 15:21:31.412481 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.inference_driver_name : NoneType
I1001 15:21:31.412530 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.is_eval : NoneType
I1001 15:21:31.412578 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.is_inference : NoneType
I1001 15:21:31.412626 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.keep_prob : 1.0
I1001 15:21:31.412674 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.name : ''
I1001 15:21:31.412722 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.noise_shape : NoneType
I1001 15:21:31.412770 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.noise_shape_broadcast_dims : NoneType
I1001 15:21:31.412818 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.params_init.method : 'xavier'
I1001 15:21:31.412867 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.params_init.scale : 1.000001
I1001 15:21:31.412916 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.params_init.seed : NoneType
I1001 15:21:31.412965 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.random_seed : NoneType
I1001 15:21:31.413013 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.skip_lp_regularization : NoneType
I1001 15:21:31.413061 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.vn.global_vn : False
I1001 15:21:31.413110 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.vn.per_step_vn : False
I1001 15:21:31.413158 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.vn.scale : NoneType
I1001 15:21:31.413206 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.vn.seed : NoneType
I1001 15:21:31.413255 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dtype : float32
I1001 15:21:31.413304 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.fprop_dtype : NoneType
I1001 15:21:31.413352 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.global_weight_scale : 1.0
I1001 15:21:31.413401 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.inference_driver_name : NoneType
I1001 15:21:31.413449 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.is_eval : NoneType
I1001 15:21:31.413498 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.is_inference : NoneType
I1001 15:21:31.413546 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.minimal_prob : 0.0
I1001 15:21:31.413594 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.name : ''
I1001 15:21:31.413643 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.num_sources : 0
I1001 15:21:31.413691 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.params_init.method : 'xavier'
I1001 15:21:31.413746 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.params_init.scale : 1.000001
I1001 15:21:31.413796 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.params_init.seed : NoneType
I1001 15:21:31.413845 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.random_seed : NoneType
I1001 15:21:31.413894 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.skip_lp_regularization : NoneType
I1001 15:21:31.413942 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.vn.global_vn : False
I1001 15:21:31.413990 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.vn.per_step_vn : False
I1001 15:21:31.414039 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.vn.scale : NoneType
I1001 15:21:31.414087 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.vn.seed : NoneType
I1001 15:21:31.414135 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.weighted_merger_dropout_prob : 0.0
I1001 15:21:31.414183 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.weighted_merger_softmax : True
I1001 15:21:31.414232 139818304546624 base_runner.py:59] task.lm.stack.use_pipelined_embeddings : True
I1001 15:21:31.414280 139818304546624 base_runner.py:59] task.lm.stack.vn.global_vn : False
I1001 15:21:31.414329 139818304546624 base_runner.py:59] task.lm.stack.vn.per_step_vn : False
I1001 15:21:31.414377 139818304546624 base_runner.py:59] task.lm.stack.vn.scale : NoneType
I1001 15:21:31.414426 139818304546624 base_runner.py:59] task.lm.stack.vn.seed : NoneType
I1001 15:21:31.414475 139818304546624 base_runner.py:59] task.lm.vn.global_vn : False
I1001 15:21:31.414524 139818304546624 base_runner.py:59] task.lm.vn.per_step_vn : False
I1001 15:21:31.414596 139818304546624 base_runner.py:59] task.lm.vn.scale : NoneType
I1001 15:21:31.414646 139818304546624 base_runner.py:59] task.lm.vn.seed : NoneType
I1001 15:21:31.414695 139818304546624 base_runner.py:59] task.lm.vocab_size : 32000
I1001 15:21:31.414744 139818304546624 base_runner.py:59] task.name : '1bwds_wpm_level_lm'
I1001 15:21:31.414792 139818304546624 base_runner.py:59] task.online_encoder : NoneType
I1001 15:21:31.414841 139818304546624 base_runner.py:59] task.params_init.method : 'xavier'
I1001 15:21:31.414890 139818304546624 base_runner.py:59] task.params_init.scale : 1.000001
I1001 15:21:31.414939 139818304546624 base_runner.py:59] task.params_init.seed : NoneType
I1001 15:21:31.414987 139818304546624 base_runner.py:59] task.random_seed : NoneType
I1001 15:21:31.415035 139818304546624 base_runner.py:59] task.skip_lp_regularization : NoneType
I1001 15:21:31.415084 139818304546624 base_runner.py:59] task.train.bprop_variable_exclusion : NoneType
I1001 15:21:31.415132 139818304546624 base_runner.py:59] task.train.bprop_variable_filter : NoneType
I1001 15:21:31.415180 139818304546624 base_runner.py:59] task.train.clip_gradient_norm_to_value : 0.0
I1001 15:21:31.415228 139818304546624 base_runner.py:59] task.train.clip_gradient_single_norm_to_value : 0.0
I1001 15:21:31.415277 139818304546624 base_runner.py:59] task.train.colocate_gradients_with_ops : True
I1001 15:21:31.415324 139818304546624 base_runner.py:59] task.train.early_stop.metric_history.jobname : 'eval_dev'
I1001 15:21:31.415373 139818304546624 base_runner.py:59] task.train.early_stop.metric_history.local_filesystem : False
I1001 15:21:31.415421 139818304546624 base_runner.py:59] task.train.early_stop.metric_history.logdir : ''
I1001 15:21:31.415470 139818304546624 base_runner.py:59] task.train.early_stop.metric_history.metric : 'log_pplx'
I1001 15:21:31.415518 139818304546624 base_runner.py:59] task.train.early_stop.metric_history.minimize : True
I1001 15:21:31.415567 139818304546624 base_runner.py:59] task.train.early_stop.metric_history.name : 'MetricHistory'
I1001 15:21:31.415615 139818304546624 base_runner.py:59] task.train.early_stop.metric_history.tfevent_file : False
I1001 15:21:31.415668 139818304546624 base_runner.py:59] task.train.early_stop.min_steps : 0
I1001 15:21:31.415718 139818304546624 base_runner.py:59] task.train.early_stop.name : 'EarlyStop'
I1001 15:21:31.415766 139818304546624 base_runner.py:59] task.train.early_stop.tolerance : 0.0
I1001 15:21:31.415815 139818304546624 base_runner.py:59] task.train.early_stop.verbose : True
I1001 15:21:31.415864 139818304546624 base_runner.py:59] task.train.early_stop.window : 0
I1001 15:21:31.415912 139818304546624 base_runner.py:59] task.train.ema_decay : 0.0
I1001 15:21:31.415960 139818304546624 base_runner.py:59] task.train.enqueue_max_steps : -1
I1001 15:21:31.416009 139818304546624 base_runner.py:59] task.train.gate_gradients : False
I1001 15:21:31.416058 139818304546624 base_runner.py:59] task.train.grad_aggregation_method : 1
I1001 15:21:31.416106 139818304546624 base_runner.py:59] task.train.grad_norm_to_clip_to_zero : 0.0
I1001 15:21:31.416155 139818304546624 base_runner.py:59] task.train.grad_norm_tracker : NoneType
I1001 15:21:31.416203 139818304546624 base_runner.py:59] task.train.init_from_checkpoint_rules : {}
I1001 15:21:31.416251 139818304546624 base_runner.py:59] task.train.l1_regularizer_weight : NoneType
I1001 15:21:31.416300 139818304546624 base_runner.py:59] task.train.l2_regularizer_weight : 1e-06
I1001 15:21:31.416347 139818304546624 base_runner.py:59] task.train.learner : NoneType
I1001 15:21:31.416396 139818304546624 base_runner.py:59] task.train.learning_rate : 0.5
I1001 15:21:31.416444 139818304546624 base_runner.py:59] task.train.lr_schedule.allow_implicit_capture : NoneType
I1001 15:21:31.416492 139818304546624 base_runner.py:59] task.train.lr_schedule.cls : type/lingvo.core.schedule/TransformerLearningRateSchedule
I1001 15:21:31.416540 139818304546624 base_runner.py:59] task.train.lr_schedule.decay_end : NoneType
I1001 15:21:31.416589 139818304546624 base_runner.py:59] task.train.lr_schedule.dtype : float32
I1001 15:21:31.416637 139818304546624 base_runner.py:59] task.train.lr_schedule.fprop_dtype : NoneType
I1001 15:21:31.416686 139818304546624 base_runner.py:59] task.train.lr_schedule.inference_driver_name : NoneType
I1001 15:21:31.416734 139818304546624 base_runner.py:59] task.train.lr_schedule.is_eval : NoneType
I1001 15:21:31.416782 139818304546624 base_runner.py:59] task.train.lr_schedule.is_inference : NoneType
I1001 15:21:31.416831 139818304546624 base_runner.py:59] task.train.lr_schedule.model_dim : 2048
I1001 15:21:31.416879 139818304546624 base_runner.py:59] task.train.lr_schedule.name : 'LRSched'
I1001 15:21:31.416928 139818304546624 base_runner.py:59] task.train.lr_schedule.params_init.method : 'xavier'
I1001 15:21:31.416976 139818304546624 base_runner.py:59] task.train.lr_schedule.params_init.scale : 1.000001
I1001 15:21:31.417024 139818304546624 base_runner.py:59] task.train.lr_schedule.params_init.seed : NoneType
I1001 15:21:31.417072 139818304546624 base_runner.py:59] task.train.lr_schedule.random_seed : NoneType
I1001 15:21:31.417120 139818304546624 base_runner.py:59] task.train.lr_schedule.skip_lp_regularization : NoneType
I1001 15:21:31.417168 139818304546624 base_runner.py:59] task.train.lr_schedule.vn.global_vn : False
I1001 15:21:31.417217 139818304546624 base_runner.py:59] task.train.lr_schedule.vn.per_step_vn : False
I1001 15:21:31.417265 139818304546624 base_runner.py:59] task.train.lr_schedule.vn.scale : NoneType
I1001 15:21:31.417354 139818304546624 base_runner.py:59] task.train.lr_schedule.vn.seed : NoneType
I1001 15:21:31.417408 139818304546624 base_runner.py:59] task.train.lr_schedule.warmup_steps : 40000
I1001 15:21:31.417457 139818304546624 base_runner.py:59] task.train.lr_schedule.worker_replicas : 1
I1001 15:21:31.417506 139818304546624 base_runner.py:59] task.train.max_lstm_gradient_norm : 0.0
I1001 15:21:31.417555 139818304546624 base_runner.py:59] task.train.max_steps : 4000000
I1001 15:21:31.417603 139818304546624 base_runner.py:59] task.train.optimizer.allow_implicit_capture : NoneType
I1001 15:21:31.417652 139818304546624 base_runner.py:59] task.train.optimizer.beta1 : 0.9
I1001 15:21:31.417706 139818304546624 base_runner.py:59] task.train.optimizer.beta2 : 0.997
I1001 15:21:31.417756 139818304546624 base_runner.py:59] task.train.optimizer.cls : type/lingvo.core.optimizer/Adam
I1001 15:21:31.417805 139818304546624 base_runner.py:59] task.train.optimizer.dtype : float32
I1001 15:21:31.417854 139818304546624 base_runner.py:59] task.train.optimizer.epsilon : 1e-09
I1001 15:21:31.417903 139818304546624 base_runner.py:59] task.train.optimizer.fprop_dtype : NoneType
I1001 15:21:31.417952 139818304546624 base_runner.py:59] task.train.optimizer.inference_driver_name : NoneType
I1001 15:21:31.418001 139818304546624 base_runner.py:59] task.train.optimizer.is_eval : NoneType
I1001 15:21:31.418050 139818304546624 base_runner.py:59] task.train.optimizer.is_inference : NoneType
I1001 15:21:31.418099 139818304546624 base_runner.py:59] task.train.optimizer.name : 'Adam'
I1001 15:21:31.418148 139818304546624 base_runner.py:59] task.train.optimizer.params_init.method : 'xavier'
I1001 15:21:31.418196 139818304546624 base_runner.py:59] task.train.optimizer.params_init.scale : 1.000001
I1001 15:21:31.418245 139818304546624 base_runner.py:59] task.train.optimizer.params_init.seed : NoneType
I1001 15:21:31.418293 139818304546624 base_runner.py:59] task.train.optimizer.random_seed : NoneType
I1001 15:21:31.418342 139818304546624 base_runner.py:59] task.train.optimizer.skip_lp_regularization : NoneType
I1001 15:21:31.418390 139818304546624 base_runner.py:59] task.train.optimizer.vn.global_vn : False
I1001 15:21:31.418438 139818304546624 base_runner.py:59] task.train.optimizer.vn.per_step_vn : False
I1001 15:21:31.418487 139818304546624 base_runner.py:59] task.train.optimizer.vn.scale : NoneType
I1001 15:21:31.418549 139818304546624 base_runner.py:59] task.train.optimizer.vn.seed : NoneType
I1001 15:21:31.418602 139818304546624 base_runner.py:59] task.train.pruning_hparams_dict : NoneType
I1001 15:21:31.418651 139818304546624 base_runner.py:59] task.train.save_interval_seconds : 600
I1001 15:21:31.418700 139818304546624 base_runner.py:59] task.train.save_keep_checkpoint_every_n_hours : 0.5
I1001 15:21:31.418749 139818304546624 base_runner.py:59] task.train.save_max_to_keep : 100
I1001 15:21:31.418798 139818304546624 base_runner.py:59] task.train.start_up_delay_steps : 200
I1001 15:21:31.418847 139818304546624 base_runner.py:59] task.train.sum_loss_across_tokens_in_batch : False
I1001 15:21:31.418896 139818304546624 base_runner.py:59] task.train.summary_interval_steps : 100
I1001 15:21:31.418946 139818304546624 base_runner.py:59] task.train.tpu_steps_per_loop : 100
I1001 15:21:31.418995 139818304546624 base_runner.py:59] task.train.vn_start_step : 20000
I1001 15:21:31.419044 139818304546624 base_runner.py:59] task.train.vn_std : 0.0
I1001 15:21:31.419093 139818304546624 base_runner.py:59] task.vn.global_vn : False
I1001 15:21:31.419141 139818304546624 base_runner.py:59] task.vn.per_step_vn : False
I1001 15:21:31.419190 139818304546624 base_runner.py:59] task.vn.scale : NoneType
I1001 15:21:31.419239 139818304546624 base_runner.py:59] task.vn.seed : NoneType
I1001 15:21:31.419288 139818304546624 base_runner.py:59] train.early_stop.metric_history.jobname : 'eval_dev'
I1001 15:21:31.419337 139818304546624 base_runner.py:59] train.early_stop.metric_history.local_filesystem : False
I1001 15:21:31.419386 139818304546624 base_runner.py:59] train.early_stop.metric_history.logdir : ''
I1001 15:21:31.419434 139818304546624 base_runner.py:59] train.early_stop.metric_history.metric : 'log_pplx'
I1001 15:21:31.419483 139818304546624 base_runner.py:59] train.early_stop.metric_history.minimize : True
I1001 15:21:31.419532 139818304546624 base_runner.py:59] train.early_stop.metric_history.name : 'MetricHistory'
I1001 15:21:31.419581 139818304546624 base_runner.py:59] train.early_stop.metric_history.tfevent_file : False
I1001 15:21:31.419630 139818304546624 base_runner.py:59] train.early_stop.min_steps : 0
I1001 15:21:31.419679 139818304546624 base_runner.py:59] train.early_stop.name : 'EarlyStop'
I1001 15:21:31.419733 139818304546624 base_runner.py:59] train.early_stop.tolerance : 0.0
I1001 15:21:31.419783 139818304546624 base_runner.py:59] train.early_stop.verbose : True
I1001 15:21:31.419832 139818304546624 base_runner.py:59] train.early_stop.window : 0
I1001 15:21:31.419882 139818304546624 base_runner.py:59] train.ema_decay : 0.0
I1001 15:21:31.419931 139818304546624 base_runner.py:59] train.enqueue_max_steps : -1
I1001 15:21:31.419979 139818304546624 base_runner.py:59] train.init_from_checkpoint_rules : {}
I1001 15:21:31.420028 139818304546624 base_runner.py:59] train.max_steps : 4000000
I1001 15:21:31.420077 139818304546624 base_runner.py:59] train.save_interval_seconds : 600
I1001 15:21:31.420125 139818304546624 base_runner.py:59] train.save_keep_checkpoint_every_n_hours : 0.5
I1001 15:21:31.420174 139818304546624 base_runner.py:59] train.save_max_to_keep : 100
I1001 15:21:31.420223 139818304546624 base_runner.py:59] train.start_up_delay_steps : 200
I1001 15:21:31.420272 139818304546624 base_runner.py:59] train.summary_interval_steps : 100
I1001 15:21:31.420321 139818304546624 base_runner.py:59] train.tpu_steps_per_loop : 100
I1001 15:21:31.420370 139818304546624 base_runner.py:59] vn.global_vn : False
I1001 15:21:31.420419 139818304546624 base_runner.py:59] vn.per_step_vn : False
I1001 15:21:31.420467 139818304546624 base_runner.py:59] vn.scale : NoneType
I1001 15:21:31.420516 139818304546624 base_runner.py:59] vn.seed : NoneType
I1001 15:21:31.420565 139818304546624 base_runner.py:59] 
I1001 15:21:31.420770 139818304546624 base_runner.py:60] ============================================================
I1001 15:21:31.425423 139818304546624 base_runner.py:106] Starting ...
I1001 15:21:31.429635 139818304546624 cluster.py:497] _LeastLoadedPlacer : ['/job:local/replica:0/task:0/device:CPU:0']
I1001 15:21:31.451945 139818304546624 cluster.py:515] Place variable global_step on /job:local/replica:0/task:0/device:CPU:0 8
I1001 15:21:31.465636 139818304546624 base_model.py:1093] Training parameters for <class 'lingvo.core.base_model.SingleTaskModel'>: {
  early_stop: {
    metric_history: {
"eval_dev"
      local_filesystem: False
"/tmp/mnist/log"
"log_pplx"
      minimize: True
"MetricHistory"
      tfevent_file: False
    }
    min_steps: 0
"EarlyStop"
    tolerance: 0.0
    verbose: True
    window: 0
  }
  ema_decay: 0.0
  enqueue_max_steps: -1
  init_from_checkpoint_rules: {}
  max_steps: 4000000
  save_interval_seconds: 600
  save_keep_checkpoint_every_n_hours: 0.5
  save_max_to_keep: 100
  start_up_delay_steps: 200
  summary_interval_steps: 100
  tpu_steps_per_loop: 100
}
I1001 15:21:31.481684 139818304546624 base_model.py:301] input_params: {
  allow_implicit_capture: None
  bucket_adjust_every_n: 0
  bucket_batch_limit: [32]
  bucket_upper_bound: [1024]
  cls: <class 'lingvo.tasks.lm.input_generator.LmInput'>
  dtype: <dtype: 'float32'>
  file_buffer_size: 10000000
  file_datasource: None
  file_parallelism: 10
"text:/tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en*"
  file_random_seed: 301
  fixed_input_shape: True
  flush_every_n: 0
  fprop_dtype: None
  inference_driver_name: None
  is_eval: None
  is_inference: None
"1bwds_train_set"
  num_batcher_threads: 16
  num_samples: 0
  pad_to_max_seq_length: False
  params_init: {
"xavier"
    scale: 1.000001
    seed: None
  }
  random_seed: None
  remote: {
    max_inflights_per_target: 32
    shardable_batch: False
  }
  require_sequential_order: False
  skip_lp_regularization: None
  source_max_length: None
  target_max_length: 1024
  tokenizer: {
    allow_implicit_capture: None
    append_eos: True
    cls: <class 'lingvo.core.tokenizers.AsciiTokenizer'>
    dtype: <dtype: 'float32'>
    fprop_dtype: None
    inference_driver_name: None
    is_eval: None
    is_inference: None
"tokenizer"
    pad_to_max_length: True
    params_init: {
"xavier"
      scale: 1.000001
      seed: None
    }
    random_seed: None
    skip_lp_regularization: None
    target_eos_id: 2
    target_sos_id: 1
    target_unk_id: 0
    vn: {
      global_vn: False
      per_step_vn: False
      scale: None
      seed: None
    }
    vocab_size: 32000
  }
  tokenizer_dict: {}
  tpu_infeed_parallelism: 1
  use_chaining: False
  use_per_host_infeed: False
  use_within_batch_mixing: False
  vn: {
    global_vn: False
    per_step_vn: False
    scale: None
    seed: None
  }
}
I1001 15:21:31.485605 139818304546624 base_input_generator.py:624] bucket_batch_limit [32]
I1001 15:21:31.543335 139818304546624 learner.py:351] Ignoring legacy param start_up_delay_steps=200 for optimization program
I1001 15:21:31.543460 139818304546624 learner.py:351] Ignoring legacy param max_steps=4000000 for optimization program
I1001 15:21:31.543527 139818304546624 learner.py:351] Ignoring legacy param tpu_steps_per_loop=100 for optimization program
I1001 15:21:31.543584 139818304546624 learner.py:351] Ignoring legacy param vn_start_step=20000 for optimization program
I1001 15:21:31.543637 139818304546624 learner.py:351] Ignoring legacy param vn_std=0.0 for optimization program
I1001 15:21:31.543689 139818304546624 learner.py:351] Ignoring legacy param early_stop={
  metric_history: {
"eval_dev"
    local_filesystem: False
"/tmp/mnist/log"
"log_pplx"
    minimize: True
"MetricHistory"
    tfevent_file: False
  }
  min_steps: 0
"EarlyStop"
  tolerance: 0.0
  verbose: True
  window: 0
} for optimization program
I1001 15:21:31.543800 139818304546624 learner.py:351] Ignoring legacy param ema_decay=0.0 for optimization program
I1001 15:21:31.543856 139818304546624 learner.py:351] Ignoring legacy param init_from_checkpoint_rules={} for optimization program
I1001 15:21:31.543908 139818304546624 learner.py:351] Ignoring legacy param pruning_hparams_dict=None for optimization program
I1001 15:21:31.543957 139818304546624 learner.py:351] Ignoring legacy param enqueue_max_steps=-1 for optimization program
I1001 15:21:31.544006 139818304546624 learner.py:351] Ignoring legacy param save_interval_seconds=600 for optimization program
I1001 15:21:31.544054 139818304546624 learner.py:351] Ignoring legacy param save_max_to_keep=100 for optimization program
I1001 15:21:31.544102 139818304546624 learner.py:351] Ignoring legacy param save_keep_checkpoint_every_n_hours=0.5 for optimization program
I1001 15:21:31.544152 139818304546624 learner.py:351] Ignoring legacy param summary_interval_steps=100 for optimization program
I1001 15:21:31.544201 139818304546624 learner.py:351] Ignoring legacy param learner=None for optimization program
I1001 15:21:31.544282 139818304546624 learner.py:351] Ignoring legacy param max_lstm_gradient_norm=0.0 for optimization program
I1001 15:21:31.544334 139818304546624 learner.py:351] Ignoring legacy param sum_loss_across_tokens_in_batch=False for optimization program
I1001 15:21:31.544771 139818304546624 learner.py:356] Learner params: allow_implicit_capture : NoneType
I1001 15:21:31.544851 139818304546624 learner.py:356] Learner params: bprop_variable_exclusion : NoneType
I1001 15:21:31.544913 139818304546624 learner.py:356] Learner params: bprop_variable_filter : NoneType
I1001 15:21:31.544968 139818304546624 learner.py:356] Learner params: clip_gradient_norm_to_value : 0.0
I1001 15:21:31.545022 139818304546624 learner.py:356] Learner params: clip_gradient_single_norm_to_value : 0.0
I1001 15:21:31.545073 139818304546624 learner.py:356] Learner params: cls : type/lingvo.core.learner/Learner
I1001 15:21:31.545124 139818304546624 learner.py:356] Learner params: colocate_gradients_with_ops : True
I1001 15:21:31.545175 139818304546624 learner.py:356] Learner params: dtype : float32
I1001 15:21:31.545225 139818304546624 learner.py:356] Learner params: fprop_dtype : NoneType
I1001 15:21:31.545276 139818304546624 learner.py:356] Learner params: gate_gradients : False
I1001 15:21:31.545326 139818304546624 learner.py:356] Learner params: grad_aggregation_method : 1
I1001 15:21:31.545377 139818304546624 learner.py:356] Learner params: grad_norm_to_clip_to_zero : 0.0
I1001 15:21:31.545427 139818304546624 learner.py:356] Learner params: grad_norm_tracker : NoneType
I1001 15:21:31.545484 139818304546624 learner.py:356] Learner params: inference_driver_name : NoneType
I1001 15:21:31.545535 139818304546624 learner.py:356] Learner params: is_eval : NoneType
I1001 15:21:31.545585 139818304546624 learner.py:356] Learner params: is_inference : NoneType
I1001 15:21:31.545635 139818304546624 learner.py:356] Learner params: l1_regularizer_weight : NoneType
I1001 15:21:31.545685 139818304546624 learner.py:356] Learner params: l2_regularizer_weight : 1e-06
I1001 15:21:31.545734 139818304546624 learner.py:356] Learner params: learning_rate : 0.5
I1001 15:21:31.545783 139818304546624 learner.py:356] Learner params: lr_schedule.allow_implicit_capture : NoneType
I1001 15:21:31.545833 139818304546624 learner.py:356] Learner params: lr_schedule.cls : type/lingvo.core.schedule/TransformerLearningRateSchedule
I1001 15:21:31.545883 139818304546624 learner.py:356] Learner params: lr_schedule.decay_end : NoneType
I1001 15:21:31.545933 139818304546624 learner.py:356] Learner params: lr_schedule.dtype : float32
I1001 15:21:31.545983 139818304546624 learner.py:356] Learner params: lr_schedule.fprop_dtype : NoneType
I1001 15:21:31.546033 139818304546624 learner.py:356] Learner params: lr_schedule.inference_driver_name : NoneType
I1001 15:21:31.546082 139818304546624 learner.py:356] Learner params: lr_schedule.is_eval : NoneType
I1001 15:21:31.546132 139818304546624 learner.py:356] Learner params: lr_schedule.is_inference : NoneType
I1001 15:21:31.546181 139818304546624 learner.py:356] Learner params: lr_schedule.model_dim : 2048
I1001 15:21:31.546231 139818304546624 learner.py:356] Learner params: lr_schedule.name : 'LRSched'
I1001 15:21:31.546280 139818304546624 learner.py:356] Learner params: lr_schedule.params_init.method : 'xavier'
I1001 15:21:31.546329 139818304546624 learner.py:356] Learner params: lr_schedule.params_init.scale : 1.000001
I1001 15:21:31.546379 139818304546624 learner.py:356] Learner params: lr_schedule.params_init.seed : NoneType
I1001 15:21:31.546428 139818304546624 learner.py:356] Learner params: lr_schedule.random_seed : NoneType
I1001 15:21:31.546478 139818304546624 learner.py:356] Learner params: lr_schedule.skip_lp_regularization : NoneType
I1001 15:21:31.546527 139818304546624 learner.py:356] Learner params: lr_schedule.vn.global_vn : False
I1001 15:21:31.546607 139818304546624 learner.py:356] Learner params: lr_schedule.vn.per_step_vn : False
I1001 15:21:31.546658 139818304546624 learner.py:356] Learner params: lr_schedule.vn.scale : NoneType
I1001 15:21:31.546709 139818304546624 learner.py:356] Learner params: lr_schedule.vn.seed : NoneType
I1001 15:21:31.546758 139818304546624 learner.py:356] Learner params: lr_schedule.warmup_steps : 40000
I1001 15:21:31.546809 139818304546624 learner.py:356] Learner params: lr_schedule.worker_replicas : 1
I1001 15:21:31.546859 139818304546624 learner.py:356] Learner params: name : 'loss'
I1001 15:21:31.546908 139818304546624 learner.py:356] Learner params: optimizer.allow_implicit_capture : NoneType
I1001 15:21:31.546958 139818304546624 learner.py:356] Learner params: optimizer.beta1 : 0.9
I1001 15:21:31.547008 139818304546624 learner.py:356] Learner params: optimizer.beta2 : 0.997
I1001 15:21:31.547057 139818304546624 learner.py:356] Learner params: optimizer.cls : type/lingvo.core.optimizer/Adam
I1001 15:21:31.547107 139818304546624 learner.py:356] Learner params: optimizer.dtype : float32
I1001 15:21:31.547157 139818304546624 learner.py:356] Learner params: optimizer.epsilon : 1e-09
I1001 15:21:31.547206 139818304546624 learner.py:356] Learner params: optimizer.fprop_dtype : NoneType
I1001 15:21:31.547255 139818304546624 learner.py:356] Learner params: optimizer.inference_driver_name : NoneType
I1001 15:21:31.547305 139818304546624 learner.py:356] Learner params: optimizer.is_eval : NoneType
I1001 15:21:31.547354 139818304546624 learner.py:356] Learner params: optimizer.is_inference : NoneType
I1001 15:21:31.547404 139818304546624 learner.py:356] Learner params: optimizer.name : 'Adam'
I1001 15:21:31.547452 139818304546624 learner.py:356] Learner params: optimizer.params_init.method : 'xavier'
I1001 15:21:31.547507 139818304546624 learner.py:356] Learner params: optimizer.params_init.scale : 1.000001
I1001 15:21:31.547558 139818304546624 learner.py:356] Learner params: optimizer.params_init.seed : NoneType
I1001 15:21:31.547607 139818304546624 learner.py:356] Learner params: optimizer.random_seed : NoneType
I1001 15:21:31.547656 139818304546624 learner.py:356] Learner params: optimizer.skip_lp_regularization : NoneType
I1001 15:21:31.547705 139818304546624 learner.py:356] Learner params: optimizer.vn.global_vn : False
I1001 15:21:31.547755 139818304546624 learner.py:356] Learner params: optimizer.vn.per_step_vn : False
I1001 15:21:31.547804 139818304546624 learner.py:356] Learner params: optimizer.vn.scale : NoneType
I1001 15:21:31.547853 139818304546624 learner.py:356] Learner params: optimizer.vn.seed : NoneType
I1001 15:21:31.547903 139818304546624 learner.py:356] Learner params: params_init.method : 'xavier'
I1001 15:21:31.547952 139818304546624 learner.py:356] Learner params: params_init.scale : 1.000001
I1001 15:21:31.548002 139818304546624 learner.py:356] Learner params: params_init.seed : NoneType
I1001 15:21:31.548051 139818304546624 learner.py:356] Learner params: random_seed : NoneType
I1001 15:21:31.548100 139818304546624 learner.py:356] Learner params: skip_lp_regularization : NoneType
I1001 15:21:31.548150 139818304546624 learner.py:356] Learner params: vn.global_vn : False
I1001 15:21:31.548200 139818304546624 learner.py:356] Learner params: vn.per_step_vn : False
I1001 15:21:31.548250 139818304546624 learner.py:356] Learner params: vn.scale : NoneType
I1001 15:21:31.548299 139818304546624 learner.py:356] Learner params: vn.seed : NoneType
I1001 15:21:31.548348 139818304546624 learner.py:356] Learner params: 
I1001 15:21:31.887086 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var on /job:local/replica:0/task:0/device:CPU:0 262144008
I1001 15:21:31.888967 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var:0 shape=(32000, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:31.907872 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 278921224
I1001 15:21:31.909766 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:31.912387 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 278929416
I1001 15:21:31.913998 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:31.920872 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 295706632
I1001 15:21:31.922777 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:31.925375 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 295714824
I1001 15:21:31.926979 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:31.933880 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 312492040
I1001 15:21:31.935823 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:31.938337 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 312500232
I1001 15:21:31.939942 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:31.946828 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 329277448
I1001 15:21:31.948702 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:31.951334 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 329285640
I1001 15:21:31.952933 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:31.957005 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 329286152
I1001 15:21:31.958627 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:31.962410 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 329294344
I1001 15:21:31.964022 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:31.966655 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 329302536
I1001 15:21:31.968253 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:21:31.977710 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:21:31.983878 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 396411400
I1001 15:21:31.985822 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:31.988374 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 396444168
I1001 15:21:31.989970 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:21:31.991910 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:21:31.998077 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 463553032
I1001 15:21:31.999985 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.002606 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 463561224
I1001 15:21:32.004204 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.008806 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 463569416
I1001 15:21:32.010390 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.013098 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 463577608
I1001 15:21:32.014719 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.034780 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 480354824
I1001 15:21:32.036656 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.039200 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 480363016
I1001 15:21:32.040908 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.047735 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 497140232
I1001 15:21:32.049621 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.052263 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 497148424
I1001 15:21:32.053857 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.060714 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 513925640
I1001 15:21:32.063192 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.065729 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 513933832
I1001 15:21:32.067351 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.074265 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 530711048
I1001 15:21:32.076186 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.078759 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 530719240
I1001 15:21:32.080474 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.084134 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 530719752
I1001 15:21:32.085741 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.089577 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 530727944
I1001 15:21:32.091189 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.093799 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 530736136
I1001 15:21:32.095437 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:21:32.104892 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:21:32.111042 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 597845000
I1001 15:21:32.113037 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.115594 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 597877768
I1001 15:21:32.117190 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:21:32.119628 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:21:32.125766 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 664986632
I1001 15:21:32.127689 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.130290 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 664994824
I1001 15:21:32.131918 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.136509 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 665003016
I1001 15:21:32.138092 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.140769 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 665011208
I1001 15:21:32.142365 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.162121 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 681788424
I1001 15:21:32.164037 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.166587 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 681796616
I1001 15:21:32.168783 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.175662 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 698573832
I1001 15:21:32.177549 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.180196 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 698582024
I1001 15:21:32.181827 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.188758 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 715359240
I1001 15:21:32.190730 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.193252 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 715367432
I1001 15:21:32.194900 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.201806 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 732144648
I1001 15:21:32.203758 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.206318 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 732152840
I1001 15:21:32.208060 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.211716 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 732153352
I1001 15:21:32.213361 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.217411 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 732161544
I1001 15:21:32.219051 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.221717 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 732169736
I1001 15:21:32.223357 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:21:32.233539 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:21:32.239768 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 799278600
I1001 15:21:32.241725 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.244292 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 799311368
I1001 15:21:32.245913 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:21:32.247876 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:21:32.254015 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 866420232
I1001 15:21:32.255926 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.258571 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 866428424
I1001 15:21:32.260191 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.264811 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 866436616
I1001 15:21:32.266429 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.269120 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 866444808
I1001 15:21:32.270784 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.409344 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 883222024
I1001 15:21:32.411390 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.414102 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 883230216
I1001 15:21:32.415748 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.422711 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 900007432
I1001 15:21:32.424705 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.427334 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 900015624
I1001 15:21:32.428970 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.435953 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 916792840
I1001 15:21:32.437864 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.440436 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 916801032
I1001 15:21:32.442203 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.449127 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 933578248
I1001 15:21:32.451071 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.453747 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 933586440
I1001 15:21:32.455408 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.459102 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 933586952
I1001 15:21:32.460738 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.464637 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 933595144
I1001 15:21:32.466953 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.469492 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 933603336
I1001 15:21:32.471184 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:21:32.480827 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:21:32.487078 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 1000712200
I1001 15:21:32.488991 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.491561 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 1000744968
I1001 15:21:32.493181 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:21:32.495158 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:21:32.501341 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 1067853832
I1001 15:21:32.503352 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.505900 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 1067862024
I1001 15:21:32.507562 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.512234 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1067870216
I1001 15:21:32.514061 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.516622 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1067878408
I1001 15:21:32.518250 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.538625 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 1084655624
I1001 15:21:32.540526 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.543101 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1084663816
I1001 15:21:32.544829 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.551705 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 1101441032
I1001 15:21:32.553606 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.556270 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1101449224
I1001 15:21:32.557893 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.564800 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 1118226440
I1001 15:21:32.566755 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.569317 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1118234632
I1001 15:21:32.570992 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.578310 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 1135011848
I1001 15:21:32.580263 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.582985 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1135020040
I1001 15:21:32.584623 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.588319 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 1135020552
I1001 15:21:32.589959 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.593866 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1135028744
I1001 15:21:32.595515 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.598191 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1135036936
I1001 15:21:32.599839 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:21:32.609424 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:21:32.615711 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 1202145800
I1001 15:21:32.617711 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.620283 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 1202178568
I1001 15:21:32.621941 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:21:32.623929 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:21:32.630668 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 1269287432
I1001 15:21:32.632645 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.635355 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 1269295624
I1001 15:21:32.636981 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.641655 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1269303816
I1001 15:21:32.643290 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.645976 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1269312008
I1001 15:21:32.647627 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.667533 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 1286089224
I1001 15:21:32.669611 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.672200 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1286097416
I1001 15:21:32.673943 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.681364 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 1302874632
I1001 15:21:32.683326 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.685967 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1302882824
I1001 15:21:32.687637 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.694473 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 1319660040
I1001 15:21:32.696480 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.699060 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1319668232
I1001 15:21:32.700693 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.707637 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 1336445448
I1001 15:21:32.709551 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.712160 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1336453640
I1001 15:21:32.713921 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.717632 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 1336454152
I1001 15:21:32.719329 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.723276 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1336462344
I1001 15:21:32.724911 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.727597 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1336470536
I1001 15:21:32.729234 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:21:32.739392 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:21:32.745623 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 1403579400
I1001 15:21:32.747660 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.750220 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 1403612168
I1001 15:21:32.751897 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:21:32.753896 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:21:32.760076 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 1470721032
I1001 15:21:32.762009 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.764668 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 1470729224
I1001 15:21:32.766308 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.771054 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1470737416
I1001 15:21:32.772714 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.775454 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1470745608
I1001 15:21:32.777086 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.797663 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 1487522824
I1001 15:21:32.799648 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.802227 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1487531016
I1001 15:21:32.804027 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.810958 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 1504308232
I1001 15:21:32.812900 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.815638 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1504316424
I1001 15:21:32.817279 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.824251 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 1521093640
I1001 15:21:32.826258 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.828852 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1521101832
I1001 15:21:32.830502 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.837464 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 1537879048
I1001 15:21:32.839420 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.842010 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1537887240
I1001 15:21:32.843783 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.847546 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 1537887752
I1001 15:21:32.849192 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.853152 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1537895944
I1001 15:21:32.854812 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.857941 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1537904136
I1001 15:21:32.859609 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:21:32.869381 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:21:32.875607 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 1605013000
I1001 15:21:32.877604 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.880226 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 1605045768
I1001 15:21:32.881873 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:21:32.883880 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:21:32.890061 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 1672154632
I1001 15:21:32.892028 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.894719 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 1672162824
I1001 15:21:32.896396 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.901106 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1672171016
I1001 15:21:32.902765 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.905455 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1672179208
I1001 15:21:32.907110 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.927576 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 1688956424
I1001 15:21:32.929554 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.932134 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1688964616
I1001 15:21:32.933874 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.940807 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 1705741832
I1001 15:21:32.942768 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.945446 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1705750024
I1001 15:21:32.947112 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.954036 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 1722527240
I1001 15:21:32.956057 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.958663 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1722535432
I1001 15:21:32.960299 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.967715 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 1739312648
I1001 15:21:32.969657 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.972347 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1739320840
I1001 15:21:32.974112 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.977833 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 1739321352
I1001 15:21:32.979526 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.983493 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1739329544
I1001 15:21:32.985123 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:32.987813 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1739337736
I1001 15:21:32.989461 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:21:32.999150 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:21:33.005400 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 1806446600
I1001 15:21:33.007417 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.010002 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 1806479368
I1001 15:21:33.011684 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:21:33.013704 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:21:33.020477 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 1873588232
I1001 15:21:33.022455 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.025181 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 1873596424
I1001 15:21:33.026858 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.031621 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1873604616
I1001 15:21:33.033256 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.035978 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1873612808
I1001 15:21:33.037623 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.109843 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 1890390024
I1001 15:21:33.111837 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.114489 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1890398216
I1001 15:21:33.116181 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.123158 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 1907175432
I1001 15:21:33.125548 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.128138 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1907183624
I1001 15:21:33.129799 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.136743 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 1923960840
I1001 15:21:33.138712 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.141292 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1923969032
I1001 15:21:33.143078 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.149963 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 1940746248
I1001 15:21:33.151920 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.154658 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1940754440
I1001 15:21:33.156324 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.160012 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 1940754952
I1001 15:21:33.161672 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.165645 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1940763144
I1001 15:21:33.167328 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.170008 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1940771336
I1001 15:21:33.171679 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:21:33.181323 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:21:33.188062 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 2007880200
I1001 15:21:33.320052 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.323043 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 2007912968
I1001 15:21:33.324784 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:21:33.326871 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:21:33.333177 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 2075021832
I1001 15:21:33.335171 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.337860 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 2075030024
I1001 15:21:33.339551 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.344376 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2075038216
I1001 15:21:33.346039 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.348800 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2075046408
I1001 15:21:33.350463 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.370915 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 2091823624
I1001 15:21:33.372861 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.375575 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2091831816
I1001 15:21:33.377234 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.384165 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 2108609032
I1001 15:21:33.386175 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.388769 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2108617224
I1001 15:21:33.390434 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.397455 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 2125394440
I1001 15:21:33.399437 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.402018 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2125402632
I1001 15:21:33.403804 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.410731 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 2142179848
I1001 15:21:33.412682 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.415421 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2142188040
I1001 15:21:33.417116 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.420837 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 2142188552
I1001 15:21:33.422504 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.426558 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2142196744
I1001 15:21:33.428207 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.430923 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2142204936
I1001 15:21:33.432589 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:21:33.442835 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:21:33.449038 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 2209313800
I1001 15:21:33.451084 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.453669 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 2209346568
I1001 15:21:33.455371 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:21:33.457389 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:21:33.463621 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 2276455432
I1001 15:21:33.465624 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.468329 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 2276463624
I1001 15:21:33.469991 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.474835 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2276471816
I1001 15:21:33.476495 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.479219 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2276480008
I1001 15:21:33.480882 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.501475 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 2293257224
I1001 15:21:33.503442 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.506015 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2293265416
I1001 15:21:33.507813 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.514709 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 2310042632
I1001 15:21:33.516700 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.519402 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2310050824
I1001 15:21:33.521071 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.528072 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 2326828040
I1001 15:21:33.530096 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.532715 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2326836232
I1001 15:21:33.534375 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.541369 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 2343613448
I1001 15:21:33.543341 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.545946 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2343621640
I1001 15:21:33.548202 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.551974 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 2343622152
I1001 15:21:33.553639 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.557660 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2343630344
I1001 15:21:33.559335 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.562023 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2343638536
I1001 15:21:33.563714 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:21:33.573354 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:21:33.579655 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 2410747400
I1001 15:21:33.581674 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.584299 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 2410780168
I1001 15:21:33.585954 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:21:33.588015 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:21:33.594202 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 2477889032
I1001 15:21:33.596162 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.598872 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 2477897224
I1001 15:21:33.600538 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.605328 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2477905416
I1001 15:21:33.607003 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.610176 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2477913608
I1001 15:21:33.612191 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.632297 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 2494690824
I1001 15:21:33.634333 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.636947 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2494699016
I1001 15:21:33.638748 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.645664 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 2511476232
I1001 15:21:33.647651 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.650376 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2511484424
I1001 15:21:33.652071 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.659490 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 2528261640
I1001 15:21:33.661499 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.664140 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2528269832
I1001 15:21:33.665801 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.672827 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 2545047048
I1001 15:21:33.674844 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.677482 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2545055240
I1001 15:21:33.679285 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.683129 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 2545055752
I1001 15:21:33.684810 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.688823 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2545063944
I1001 15:21:33.690479 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.693200 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2545072136
I1001 15:21:33.694893 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:21:33.704653 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:21:33.710897 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 2612181000
I1001 15:21:33.713458 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.716121 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 2612213768
I1001 15:21:33.717830 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:21:33.719914 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:21:33.726170 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 2679322632
I1001 15:21:33.728173 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.730881 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 2679330824
I1001 15:21:33.732568 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.737415 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2679339016
I1001 15:21:33.739099 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.741827 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2679347208
I1001 15:21:33.743523 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.764223 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 2696124424
I1001 15:21:33.766212 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.768844 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2696132616
I1001 15:21:33.770640 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.777581 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 2712909832
I1001 15:21:33.779564 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.782260 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2712918024
I1001 15:21:33.783950 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.790879 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 2729695240
I1001 15:21:33.792916 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.795548 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2729703432
I1001 15:21:33.797222 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.804211 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 2746480648
I1001 15:21:33.806193 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.808849 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2746488840
I1001 15:21:33.810648 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.814396 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 2746489352
I1001 15:21:33.816097 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.820168 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2746497544
I1001 15:21:33.821833 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.824568 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2746505736
I1001 15:21:33.826323 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:21:33.836600 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:21:33.842872 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 2813614600
I1001 15:21:33.844906 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.847543 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 2813647368
I1001 15:21:33.849213 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:21:33.851316 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:21:33.857568 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 2880756232
I1001 15:21:33.859563 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.862283 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 2880764424
I1001 15:21:33.863980 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.868803 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2880772616
I1001 15:21:33.870465 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.873239 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2880780808
I1001 15:21:33.874964 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.895472 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 2897558024
I1001 15:21:33.897447 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.900082 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2897566216
I1001 15:21:33.901876 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.908813 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 2914343432
I1001 15:21:33.910805 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.913517 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2914351624
I1001 15:21:33.915218 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.922168 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 2931128840
I1001 15:21:33.924236 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.926909 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2931137032
I1001 15:21:33.928571 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.935595 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 2947914248
I1001 15:21:33.937578 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.940282 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2947922440
I1001 15:21:33.942521 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.946419 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 2947922952
I1001 15:21:33.948124 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.952218 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2947931144
I1001 15:21:33.953889 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.956620 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2947939336
I1001 15:21:33.958286 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:21:33.968042 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:21:33.974332 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 3015048200
I1001 15:21:33.976430 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.979075 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 3015080968
I1001 15:21:33.980768 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:21:33.982881 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:21:33.989108 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 3082189832
I1001 15:21:33.991106 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:33.993814 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 3082198024
I1001 15:21:33.995531 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.000379 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3082206216
I1001 15:21:34.002053 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.005293 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3082214408
I1001 15:21:34.007008 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.027256 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 3098991624
I1001 15:21:34.029224 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.031872 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3098999816
I1001 15:21:34.033676 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.040695 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 3115777032
I1001 15:21:34.042708 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.045435 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3115785224
I1001 15:21:34.047141 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.054619 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 3132562440
I1001 15:21:34.056690 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.059347 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3132570632
I1001 15:21:34.061020 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.068045 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 3149347848
I1001 15:21:34.070029 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.072700 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3149356040
I1001 15:21:34.074499 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.078363 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 3149356552
I1001 15:21:34.080082 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.084195 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3149364744
I1001 15:21:34.085875 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.088630 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3149372936
I1001 15:21:34.090326 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:21:34.100157 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:21:34.106430 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 3216481800
I1001 15:21:34.109039 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.111696 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 3216514568
I1001 15:21:34.113373 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:21:34.115518 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:21:34.121796 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 3283623432
I1001 15:21:34.123815 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.126632 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 3283631624
I1001 15:21:34.128341 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.133232 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3283639816
I1001 15:21:34.134956 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.137713 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3283648008
I1001 15:21:34.139438 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.160210 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 3300425224
I1001 15:21:34.162202 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.164860 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3300433416
I1001 15:21:34.166689 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.173639 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 3317210632
I1001 15:21:34.175650 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.178406 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3317218824
I1001 15:21:34.180124 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.187119 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 3333996040
I1001 15:21:34.189172 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.191835 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3334004232
I1001 15:21:34.193550 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.200626 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 3350781448
I1001 15:21:34.202632 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.205309 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3350789640
I1001 15:21:34.207128 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.210966 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 3350790152
I1001 15:21:34.212669 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.216781 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3350798344
I1001 15:21:34.218498 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.221251 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3350806536
I1001 15:21:34.222995 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:21:34.233427 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:21:34.239761 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 3417915400
I1001 15:21:34.241807 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.244471 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 3417948168
I1001 15:21:34.246207 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:21:34.248352 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:21:34.254617 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 3485057032
I1001 15:21:34.256614 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.259373 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 3485065224
I1001 15:21:34.261073 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.266008 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3485073416
I1001 15:21:34.267718 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.270467 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3485081608
I1001 15:21:34.272206 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.491893 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 3501858824
I1001 15:21:34.493973 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.496635 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3501867016
I1001 15:21:34.498465 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.505410 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 3518644232
I1001 15:21:34.507425 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.510143 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3518652424
I1001 15:21:34.511869 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.518859 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 3535429640
I1001 15:21:34.520913 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.523589 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3535437832
I1001 15:21:34.525285 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.532359 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 3552215048
I1001 15:21:34.534351 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.537086 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3552223240
I1001 15:21:34.538919 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.543289 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 3552223752
I1001 15:21:34.545011 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.549217 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3552231944
I1001 15:21:34.550939 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.553691 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3552240136
I1001 15:21:34.555440 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:21:34.565454 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:21:34.571956 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 3619349000
I1001 15:21:34.574075 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.576777 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 3619381768
I1001 15:21:34.578520 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:21:34.580671 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:21:34.586976 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 3686490632
I1001 15:21:34.588990 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.591740 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 3686498824
I1001 15:21:34.593448 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.598402 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3686507016
I1001 15:21:34.600143 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.602931 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3686515208
I1001 15:21:34.604637 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.625358 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 3703292424
I1001 15:21:34.627416 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.630064 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3703300616
I1001 15:21:34.631907 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.638994 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 3720077832
I1001 15:21:34.641007 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.643782 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3720086024
I1001 15:21:34.645492 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.652513 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 3736863240
I1001 15:21:34.655074 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.657738 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3736871432
I1001 15:21:34.659462 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.666488 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 3753648648
I1001 15:21:34.668527 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.671224 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3753656840
I1001 15:21:34.673035 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.676890 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 3753657352
I1001 15:21:34.678657 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.682833 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3753665544
I1001 15:21:34.684537 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.687308 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3753673736
I1001 15:21:34.689030 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:21:34.698906 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:21:34.705234 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 3820782600
I1001 15:21:34.707338 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.709996 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 3820815368
I1001 15:21:34.711738 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:21:34.714370 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:21:34.720723 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 3887924232
I1001 15:21:34.722747 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.725492 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 3887932424
I1001 15:21:34.727244 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.732224 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3887940616
I1001 15:21:34.733932 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.736741 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3887948808
I1001 15:21:34.738447 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.758711 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 3904726024
I1001 15:21:34.760723 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.763397 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3904734216
I1001 15:21:34.765704 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.772694 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 3921511432
I1001 15:21:34.774727 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.777514 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3921519624
I1001 15:21:34.779277 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.786220 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 3938296840
I1001 15:21:34.788330 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.791015 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3938305032
I1001 15:21:34.792726 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.799806 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 3955082248
I1001 15:21:34.801810 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.804547 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3955090440
I1001 15:21:34.806383 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.810244 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 3955090952
I1001 15:21:34.812008 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.816187 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3955099144
I1001 15:21:34.817904 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.820709 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3955107336
I1001 15:21:34.822423 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:21:34.832938 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:21:34.839302 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 4022216200
I1001 15:21:34.841374 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.844067 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 4022248968
I1001 15:21:34.845810 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:21:34.848011 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:21:34.854282 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 4089357832
I1001 15:21:34.856322 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.859107 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 4089366024
I1001 15:21:34.860830 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.865816 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4089374216
I1001 15:21:34.867546 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.870326 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4089382408
I1001 15:21:34.872076 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.892859 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 4106159624
I1001 15:21:34.894903 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.897554 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4106167816
I1001 15:21:34.899403 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.906366 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 4122945032
I1001 15:21:34.908430 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.911213 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4122953224
I1001 15:21:34.912926 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.919952 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 4139730440
I1001 15:21:34.922064 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.924752 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4139738632
I1001 15:21:34.926463 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.933607 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 4156515848
I1001 15:21:34.935650 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.938356 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4156524040
I1001 15:21:34.940211 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.944558 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 4156524552
I1001 15:21:34.946296 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.950481 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4156532744
I1001 15:21:34.952214 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.954987 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4156540936
I1001 15:21:34.956711 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:21:34.966700 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:21:34.973038 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 4223649800
I1001 15:21:34.975144 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.977817 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 4223682568
I1001 15:21:34.979600 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:21:34.981751 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:21:34.988089 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 4290791432
I1001 15:21:34.990116 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.992893 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 4290799624
I1001 15:21:34.994656 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:34.999642 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4290807816
I1001 15:21:35.001360 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.004165 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4290816008
I1001 15:21:35.005902 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.026769 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 4307593224
I1001 15:21:35.028834 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.031528 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4307601416
I1001 15:21:35.033389 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.040423 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 4324378632
I1001 15:21:35.042426 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.045221 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4324386824
I1001 15:21:35.046973 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.054151 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 4341164040
I1001 15:21:35.056708 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.059408 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4341172232
I1001 15:21:35.061144 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.068210 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 4357949448
I1001 15:21:35.070229 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.072954 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4357957640
I1001 15:21:35.074810 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.078697 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 4357958152
I1001 15:21:35.080443 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.084662 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4357966344
I1001 15:21:35.086390 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.089205 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4357974536
I1001 15:21:35.090964 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:21:35.100920 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:21:35.107263 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 4425083400
I1001 15:21:35.109376 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.112093 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 4425116168
I1001 15:21:35.113826 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:21:35.116521 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:21:35.122891 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 4492225032
I1001 15:21:35.124926 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.127727 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 4492233224
I1001 15:21:35.129496 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.134495 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4492241416
I1001 15:21:35.136245 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.139062 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4492249608
I1001 15:21:35.140815 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.161276 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 4509026824
I1001 15:21:35.163329 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.166010 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4509035016
I1001 15:21:35.168363 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.175366 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 4525812232
I1001 15:21:35.177384 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.180217 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4525820424
I1001 15:21:35.181939 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.188917 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 4542597640
I1001 15:21:35.191029 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.193715 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4542605832
I1001 15:21:35.195487 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.203747 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 4559383048
I1001 15:21:35.206032 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.208987 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4559391240
I1001 15:21:35.210851 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.214751 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 4559391752
I1001 15:21:35.216502 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.220798 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4559399944
I1001 15:21:35.222523 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.225310 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4559408136
I1001 15:21:35.227063 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:21:35.237674 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:21:35.244059 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 4626517000
I1001 15:21:35.246149 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.248864 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 4626549768
I1001 15:21:35.250628 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:21:35.252821 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:21:35.259148 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 4693658632
I1001 15:21:35.261190 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.263996 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 4693666824
I1001 15:21:35.265730 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.270794 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4693675016
I1001 15:21:35.272523 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.275348 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4693683208
I1001 15:21:35.277082 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.298433 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 4710460424
I1001 15:21:35.300543 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.303271 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4710468616
I1001 15:21:35.305113 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.312220 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 4727245832
I1001 15:21:35.314258 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.317086 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4727254024
I1001 15:21:35.318846 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.325901 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 4744031240
I1001 15:21:35.328051 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.330832 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4744039432
I1001 15:21:35.332571 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.339670 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 4760816648
I1001 15:21:35.341725 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.344508 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4760824840
I1001 15:21:35.346344 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.350848 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 4760825352
I1001 15:21:35.352605 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.356907 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4760833544
I1001 15:21:35.358663 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.361455 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4760841736
I1001 15:21:35.363239 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:21:35.373443 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:21:35.379980 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 4827950600
I1001 15:21:35.382116 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.384849 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 4827983368
I1001 15:21:35.386629 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:21:35.388870 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:21:35.395259 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 4895092232
I1001 15:21:35.397322 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.400151 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 4895100424
I1001 15:21:35.401892 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.407032 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4895108616
I1001 15:21:35.408780 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.411639 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4895116808
I1001 15:21:35.413372 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.434804 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 4911894024
I1001 15:21:35.437000 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.439818 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4911902216
I1001 15:21:35.441723 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.448790 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 4928679432
I1001 15:21:35.450853 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.453647 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4928687624
I1001 15:21:35.455402 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.462373 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 4945464840
I1001 15:21:35.465078 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.467822 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4945473032
I1001 15:21:35.469559 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.476716 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 4962250248
I1001 15:21:35.478784 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.481557 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4962258440
I1001 15:21:35.483437 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.487421 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 4962258952
I1001 15:21:35.489172 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.493475 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4962267144
I1001 15:21:35.495241 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.498024 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4962275336
I1001 15:21:35.499817 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:21:35.510225 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:21:35.517233 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 5029384200
I1001 15:21:35.519590 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.522504 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 5029416968
I1001 15:21:35.524274 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:21:35.527227 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:21:35.533638 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 5096525832
I1001 15:21:35.535705 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.539316 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 5096534024
I1001 15:21:35.541067 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.546134 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5096542216
I1001 15:21:35.547904 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.550834 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5096550408
I1001 15:21:35.552598 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.626687 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 5113327624
I1001 15:21:35.628882 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.631649 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5113335816
I1001 15:21:35.633468 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.811793 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 5130113032
I1001 15:21:35.814111 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.817014 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5130121224
I1001 15:21:35.818908 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.826065 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 5146898440
I1001 15:21:35.828162 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.831035 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5146906632
I1001 15:21:35.832784 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.839855 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 5163683848
I1001 15:21:35.841969 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.844758 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5163692040
I1001 15:21:35.846518 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.850451 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 5163692552
I1001 15:21:35.852334 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.856710 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5163700744
I1001 15:21:35.858458 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.861267 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5163708936
I1001 15:21:35.863047 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:21:35.874072 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:21:35.881051 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 5230817800
I1001 15:21:35.883337 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.886206 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 5230850568
I1001 15:21:35.887992 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:21:35.890308 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:21:35.896754 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 5297959432
I1001 15:21:35.898952 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.901674 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 5297967624
I1001 15:21:35.903453 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.908634 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5297975816
I1001 15:21:35.910591 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.913270 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5297984008
I1001 15:21:35.915054 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.936334 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 5314761224
I1001 15:21:35.938394 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.941233 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5314769416
I1001 15:21:35.943230 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.950407 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 5331546632
I1001 15:21:35.952507 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.955356 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5331554824
I1001 15:21:35.957098 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.964255 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 5348332040
I1001 15:21:35.966334 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.969123 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5348340232
I1001 15:21:35.970901 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.978004 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 5365117448
I1001 15:21:35.980083 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.983006 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5365125640
I1001 15:21:35.984768 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.988767 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 5365126152
I1001 15:21:35.990533 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.994931 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5365134344
I1001 15:21:35.996673 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:35.999962 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5365142536
I1001 15:21:36.001728 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:21:36.011942 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:21:36.018699 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 5432251400
I1001 15:21:36.021023 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.024026 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 5432284168
I1001 15:21:36.025892 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:21:36.028328 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:21:36.034939 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 5499393032
I1001 15:21:36.037088 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.040010 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 5499401224
I1001 15:21:36.041773 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.047025 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5499409416
I1001 15:21:36.048786 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.051657 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5499417608
I1001 15:21:36.053423 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.074668 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 5516194824
I1001 15:21:36.076734 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.079494 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5516203016
I1001 15:21:36.081349 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.088462 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 5532980232
I1001 15:21:36.090526 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.093367 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5532988424
I1001 15:21:36.095148 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.102157 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 5549765640
I1001 15:21:36.104300 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.107107 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5549773832
I1001 15:21:36.108895 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.117466 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 5566551048
I1001 15:21:36.119882 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.122852 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5566559240
I1001 15:21:36.124716 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.128721 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 5566559752
I1001 15:21:36.130507 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.134981 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5566567944
I1001 15:21:36.136727 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.139576 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5566576136
I1001 15:21:36.141358 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:21:36.151709 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:21:36.158130 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 5633685000
I1001 15:21:36.160283 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.163051 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 5633717768
I1001 15:21:36.164817 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:21:36.167087 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:21:36.174011 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 5700826632
I1001 15:21:36.176131 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.179065 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 5700834824
I1001 15:21:36.180856 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.186134 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5700843016
I1001 15:21:36.187918 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.190804 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5700851208
I1001 15:21:36.192572 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.213284 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 5717628424
I1001 15:21:36.215399 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.218179 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5717636616
I1001 15:21:36.220077 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.227950 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 5734413832
I1001 15:21:36.230019 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.233139 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5734422024
I1001 15:21:36.234920 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.241916 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 5751199240
I1001 15:21:36.244048 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.246795 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5751207432
I1001 15:21:36.248548 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.255594 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 5767984648
I1001 15:21:36.257653 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.260470 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5767992840
I1001 15:21:36.262352 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.266437 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 5767993352
I1001 15:21:36.268237 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.272606 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5768001544
I1001 15:21:36.274369 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.277210 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5768009736
I1001 15:21:36.279031 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:21:36.290162 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:21:36.296776 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 5835118600
I1001 15:21:36.298955 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.301715 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 5835151368
I1001 15:21:36.303518 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:21:36.305788 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:21:36.312201 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 5902260232
I1001 15:21:36.314280 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.317124 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 5902268424
I1001 15:21:36.318916 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.324122 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5902276616
I1001 15:21:36.325911 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.328871 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5902284808
I1001 15:21:36.330733 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.352397 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 5919062024
I1001 15:21:36.354592 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.357458 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5919070216
I1001 15:21:36.359357 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.366446 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 5935847432
I1001 15:21:36.368572 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.371540 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5935855624
I1001 15:21:36.373320 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.380381 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 5952632840
I1001 15:21:36.382528 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.385299 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5952641032
I1001 15:21:36.387106 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.394230 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 5969418248
I1001 15:21:36.396352 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.399163 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5969426440
I1001 15:21:36.401047 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.405088 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 5969426952
I1001 15:21:36.406911 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.411360 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5969435144
I1001 15:21:36.413115 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.416447 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5969443336
I1001 15:21:36.418241 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:21:36.428652 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:21:36.435456 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 6036552200
I1001 15:21:36.437628 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.440405 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 6036584968
I1001 15:21:36.442189 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:21:36.444485 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:21:36.450850 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 6103693832
I1001 15:21:36.452921 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.455816 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 6103702024
I1001 15:21:36.457613 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.462881 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 6103710216
I1001 15:21:36.464708 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.467595 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 6103718408
I1001 15:21:36.469389 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.490916 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 6120495624
I1001 15:21:36.493097 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.495945 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6120503816
I1001 15:21:36.497847 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.505017 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 6137281032
I1001 15:21:36.507120 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.509967 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6137289224
I1001 15:21:36.511750 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.518785 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 6154066440
I1001 15:21:36.520933 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.523773 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6154074632
I1001 15:21:36.525560 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.533314 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 6170851848
I1001 15:21:36.535441 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.538225 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6170860040
I1001 15:21:36.540139 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.544192 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 6170860552
I1001 15:21:36.545991 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.550382 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 6170868744
I1001 15:21:36.552193 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.555070 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 6170876936
I1001 15:21:36.556852 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:21:36.567222 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:21:36.573730 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 6237985800
I1001 15:21:36.575915 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.578726 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 6238018568
I1001 15:21:36.580515 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:21:36.582819 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:21:36.589853 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 6305127432
I1001 15:21:36.591973 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.594896 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 6305135624
I1001 15:21:36.596682 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.601906 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 6305143816
I1001 15:21:36.603724 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.606642 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 6305152008
I1001 15:21:36.608434 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.629518 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 6321929224
I1001 15:21:36.631645 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.634494 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6321937416
I1001 15:21:36.636406 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.644249 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 6338714632
I1001 15:21:36.646370 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.649268 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6338722824
I1001 15:21:36.651092 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.658241 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 6355500040
I1001 15:21:36.660538 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.663398 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6355508232
I1001 15:21:36.665198 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.672401 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 6372285448
I1001 15:21:36.674492 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.677299 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6372293640
I1001 15:21:36.679211 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.683334 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 6372294152
I1001 15:21:36.685173 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.689650 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 6372302344
I1001 15:21:36.691455 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.694298 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 6372310536
I1001 15:21:36.696147 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:21:36.707114 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:21:36.713596 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 6439419400
I1001 15:21:36.715788 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.718586 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 6439452168
I1001 15:21:36.720402 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:21:36.722804 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:21:36.729228 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 6506561032
I1001 15:21:36.731348 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.734235 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 6506569224
I1001 15:21:36.736057 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.741315 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 6506577416
I1001 15:21:36.743145 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.746026 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 6506585608
I1001 15:21:36.747846 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.769318 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 6523362824
I1001 15:21:36.771471 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.774208 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6523371016
I1001 15:21:36.776121 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.783207 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 6540148232
I1001 15:21:36.785319 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.788201 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6540156424
I1001 15:21:36.790025 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.797128 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 6556933640
I1001 15:21:36.799314 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.802071 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6556941832
I1001 15:21:36.803903 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.811052 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 6573719048
I1001 15:21:36.813156 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.815974 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6573727240
I1001 15:21:36.817864 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.821947 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 6573727752
I1001 15:21:36.823806 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.828361 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 6573735944
I1001 15:21:36.830236 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.833587 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 6573744136
I1001 15:21:36.835444 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:21:36.845815 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:21:36.852274 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 6640853000
I1001 15:21:36.854435 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.857243 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 6640885768
I1001 15:21:36.859071 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:21:36.861397 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:21:36.867751 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 6707994632
I1001 15:21:36.869850 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.872710 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 6708002824
I1001 15:21:36.874504 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.879763 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 6708011016
I1001 15:21:36.881580 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:36.884532 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 6708019208
I1001 15:21:36.886331 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:21:36.891829 139818304546624 py_utils.py:1229] WARNING!!! var weight_0 is using the default xavier initializer. Make sure this is intended.
I1001 15:21:36.898134 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var on /job:local/replica:0/task:0/device:CPU:0 6724403208
I1001 15:21:36.900247 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:21:36.901080 139818304546624 py_utils.py:1229] WARNING!!! var weight_1 is using the default xavier initializer. Make sure this is intended.
I1001 15:21:36.907921 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var on /job:local/replica:0/task:0/device:CPU:0 6740787208
I1001 15:21:36.910001 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:21:36.910861 139818304546624 py_utils.py:1229] WARNING!!! var weight_2 is using the default xavier initializer. Make sure this is intended.
I1001 15:21:36.917099 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var on /job:local/replica:0/task:0/device:CPU:0 6757171208
I1001 15:21:36.919223 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:21:36.920059 139818304546624 py_utils.py:1229] WARNING!!! var weight_3 is using the default xavier initializer. Make sure this is intended.
I1001 15:21:36.926427 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var on /job:local/replica:0/task:0/device:CPU:0 6773555208
I1001 15:21:36.928531 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:21:36.929364 139818304546624 py_utils.py:1229] WARNING!!! var weight_4 is using the default xavier initializer. Make sure this is intended.
I1001 15:21:36.935653 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var on /job:local/replica:0/task:0/device:CPU:0 6789939208
I1001 15:21:36.937733 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:21:36.938583 139818304546624 py_utils.py:1229] WARNING!!! var weight_5 is using the default xavier initializer. Make sure this is intended.
I1001 15:21:36.944873 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var on /job:local/replica:0/task:0/device:CPU:0 6806323208
I1001 15:21:36.947005 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:21:36.947833 139818304546624 py_utils.py:1229] WARNING!!! var weight_6 is using the default xavier initializer. Make sure this is intended.
I1001 15:21:36.954091 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var on /job:local/replica:0/task:0/device:CPU:0 6822707208
I1001 15:21:36.956205 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:21:36.957032 139818304546624 py_utils.py:1229] WARNING!!! var weight_7 is using the default xavier initializer. Make sure this is intended.
I1001 15:21:36.963364 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var on /job:local/replica:0/task:0/device:CPU:0 6839091208
I1001 15:21:36.965464 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:21:36.966308 139818304546624 py_utils.py:1229] WARNING!!! var weight_8 is using the default xavier initializer. Make sure this is intended.
I1001 15:21:36.972604 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var on /job:local/replica:0/task:0/device:CPU:0 6855475208
I1001 15:21:36.974694 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:21:36.975523 139818304546624 py_utils.py:1229] WARNING!!! var weight_9 is using the default xavier initializer. Make sure this is intended.
I1001 15:21:36.982392 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var on /job:local/replica:0/task:0/device:CPU:0 6871859208
I1001 15:21:36.984539 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:21:36.985389 139818304546624 py_utils.py:1229] WARNING!!! var weight_10 is using the default xavier initializer. Make sure this is intended.
I1001 15:21:36.991628 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var on /job:local/replica:0/task:0/device:CPU:0 6888243208
I1001 15:21:36.993716 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:21:36.994574 139818304546624 py_utils.py:1229] WARNING!!! var weight_11 is using the default xavier initializer. Make sure this is intended.
I1001 15:21:37.000879 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var on /job:local/replica:0/task:0/device:CPU:0 6904627208
I1001 15:21:37.003020 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:21:37.003851 139818304546624 py_utils.py:1229] WARNING!!! var weight_12 is using the default xavier initializer. Make sure this is intended.
I1001 15:21:37.010089 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var on /job:local/replica:0/task:0/device:CPU:0 6921011208
I1001 15:21:37.012191 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:21:37.013013 139818304546624 py_utils.py:1229] WARNING!!! var weight_13 is using the default xavier initializer. Make sure this is intended.
I1001 15:21:37.019332 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var on /job:local/replica:0/task:0/device:CPU:0 6937395208
I1001 15:21:37.021408 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:21:37.022243 139818304546624 py_utils.py:1229] WARNING!!! var weight_14 is using the default xavier initializer. Make sure this is intended.
I1001 15:21:37.028543 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var on /job:local/replica:0/task:0/device:CPU:0 6953779208
I1001 15:21:37.030657 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:21:37.031493 139818304546624 py_utils.py:1229] WARNING!!! var weight_15 is using the default xavier initializer. Make sure this is intended.
I1001 15:21:37.037868 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var on /job:local/replica:0/task:0/device:CPU:0 6970163208
I1001 15:21:37.040034 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.043074 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var on /job:local/replica:0/task:0/device:CPU:0 6970171208
I1001 15:21:37.044963 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.047706 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var on /job:local/replica:0/task:0/device:CPU:0 6970179208
I1001 15:21:37.049504 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.052880 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var on /job:local/replica:0/task:0/device:CPU:0 6970187208
I1001 15:21:37.054708 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.057459 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var on /job:local/replica:0/task:0/device:CPU:0 6970195208
I1001 15:21:37.059259 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.062089 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var on /job:local/replica:0/task:0/device:CPU:0 6970203208
I1001 15:21:37.063913 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.066683 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var on /job:local/replica:0/task:0/device:CPU:0 6970211208
I1001 15:21:37.068480 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.071356 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var on /job:local/replica:0/task:0/device:CPU:0 6970219208
I1001 15:21:37.073129 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.075903 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var on /job:local/replica:0/task:0/device:CPU:0 6970227208
I1001 15:21:37.077788 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.080560 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var on /job:local/replica:0/task:0/device:CPU:0 6970235208
I1001 15:21:37.082360 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.085259 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var on /job:local/replica:0/task:0/device:CPU:0 6970243208
I1001 15:21:37.087084 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.089817 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var on /job:local/replica:0/task:0/device:CPU:0 6970251208
I1001 15:21:37.091625 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.094472 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var on /job:local/replica:0/task:0/device:CPU:0 6970259208
I1001 15:21:37.096282 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.099035 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var on /job:local/replica:0/task:0/device:CPU:0 6970267208
I1001 15:21:37.100848 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.103719 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var on /job:local/replica:0/task:0/device:CPU:0 6970275208
I1001 15:21:37.105495 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.108249 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var on /job:local/replica:0/task:0/device:CPU:0 6970283208
I1001 15:21:37.110129 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.112900 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var on /job:local/replica:0/task:0/device:CPU:0 6970291208
I1001 15:21:37.114726 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.959171 139818304546624 py_utils.py:1484] === worker 0 ===
I1001 15:21:37.973967 139818304546624 py_utils.py:1474] worker 0: global_step                                                           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.974058 139818304546624 py_utils.py:1474] worker 0: input._tokenizer_default.global_step                                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.974118 139818304546624 py_utils.py:1474] worker 0: input.global_step                                                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.974171 139818304546624 py_utils.py:1474] worker 0: learners[0].global_step                                               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.974220 139818304546624 py_utils.py:1474] worker 0: learners[0].lr_schedule.global_step                                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.974268 139818304546624 py_utils.py:1474] worker 0: learners[0].optimizer.global_step                                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.974323 139818304546624 py_utils.py:1474] worker 0: lm.global_step                                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.974372 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.emb.global_step                                       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.974418 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.emb.src_dropout.global_step                           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.974464 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.emb.src_pos_emb.global_step                           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.974509 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.emb.src_token_emb.global_step                         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.974574 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.emb.src_token_emb.wm                                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.974623 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.974669 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.974715 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.974761 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.974806 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.974852 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.974897 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.974943 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.974988 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.975032 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.global_step                         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.975078 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.975123 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.975168 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.975219 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.975265 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.global_step                                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.975311 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.975357 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.975403 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.975448 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.975494 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.975540 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.975585 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.975630 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.975676 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.975721 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.975767 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.975812 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.global_step                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.975858 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.975903 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.975948 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.975994 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.976043 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.976090 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.976135 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.976180 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.976226 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.976271 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.976317 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.976362 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.976407 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.976453 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.global_step                         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.976498 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.976543 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.976588 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.976633 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.976678 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.global_step                                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.976722 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.976767 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.976812 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.976857 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.976906 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.976951 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.976996 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.977041 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.977087 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.977132 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.977176 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.977221 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.global_step                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.977266 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.977311 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.977356 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.977401 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.977445 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.977489 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.977535 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.977580 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.977625 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.977670 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.977718 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.977763 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.977808 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.977854 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.global_step                         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.977899 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.977944 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.977989 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.978034 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.978080 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.global_step                                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.978125 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.978169 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.978214 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.978258 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.978303 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.978347 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.978392 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.978436 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.978481 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.978525 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.978598 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.978646 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.global_step                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.978692 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.978737 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.978782 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.978828 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.978873 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.978919 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.978963 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.979008 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.979053 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.979098 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.979143 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.979188 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.979233 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.979278 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.global_step                         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.979323 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.979368 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.979417 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.979463 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.979508 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.global_step                                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.979553 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.979598 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.979643 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.979688 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.979733 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.979778 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.979823 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.979868 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.979913 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.979958 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.980003 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.980048 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.global_step                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.980093 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.980138 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.980183 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.980228 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.980277 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.980323 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.980368 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.980414 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.980458 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.980504 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.980549 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.980594 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.980639 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.980684 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.global_step                         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.980729 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.980773 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.980818 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.980863 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.980906 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.global_step                                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.980951 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.980996 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.981040 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.981089 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.981135 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.981180 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.981224 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.981269 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.981313 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.981358 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.981404 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.981448 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.global_step                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.981493 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.981537 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.981582 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.981626 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.981671 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.981716 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.981761 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.981807 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.981853 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.981897 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.981946 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.981992 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.982037 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.982082 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.global_step                         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.982127 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.982172 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.982216 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.982261 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.982306 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.global_step                                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.982351 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.982395 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.982440 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.982484 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.982528 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.982593 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.982639 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.982684 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.982729 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.982778 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.982824 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.982869 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.global_step                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.982913 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.982959 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.983003 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.983048 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.983093 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.983138 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.983184 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.983229 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.983274 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.983319 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.983364 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.983409 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.983454 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.983500 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.global_step                         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.983545 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.983590 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.983640 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.983686 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.983732 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.global_step                                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.983777 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.983827 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.983872 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.983917 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.983963 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.984008 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.984054 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.984099 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.984144 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.984190 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.984235 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.984280 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.global_step                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.984325 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.984371 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.984416 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.984466 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.984511 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.984557 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.984602 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.984647 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.984692 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.984738 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.984783 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.984828 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.984874 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.984919 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.global_step                         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.984964 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.985010 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.985055 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.985101 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.985146 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.global_step                                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.985191 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.985237 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.985282 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.985331 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.985377 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.985421 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.985466 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.985512 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.985556 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.985602 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.985647 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.985692 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.global_step                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.985737 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.985782 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.985826 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.985871 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.985916 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.global_step                                           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.985961 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.986005 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.986050 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.986093 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.986138 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.986187 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.986233 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.986278 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.986323 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.986368 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.global_step                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.986413 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.986458 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.986503 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.986562 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.986613 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.global_step                                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.986658 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.986703 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.986747 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.986793 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.986838 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.986882 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.986927 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.global_step               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.986972 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.987021 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.987066 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.987111 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.987156 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.global_step                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.987201 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.987246 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.987291 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.987336 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.987381 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.987426 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.987470 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.987532 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.987593 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.987640 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.987685 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.987731 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.987775 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.987820 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.global_step                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.987864 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.987914 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.987960 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.988005 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.988050 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.global_step                                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.988095 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.988140 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.988184 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.988229 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.988274 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.988319 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.988363 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.global_step               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.988408 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.988453 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.988497 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.988542 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.988587 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.global_step                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.988632 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.988677 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.988724 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.988770 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.988815 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.988860 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.988904 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.988949 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.988994 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.989038 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.989082 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.989127 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.989172 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.989217 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.global_step                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.989261 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.989305 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.989350 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.989395 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.989439 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.global_step                                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.989485 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.989530 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.989578 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.989624 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.989669 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.989714 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.989759 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.global_step               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.989805 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.989850 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.989895 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.989940 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.989984 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.global_step                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.990029 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.990074 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.990119 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.990164 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.990209 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.990254 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.990299 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.990344 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.990393 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.990439 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.990484 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.990530 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.990599 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.990646 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.global_step                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.990691 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.990736 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.990781 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.990826 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.990871 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.global_step                                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.990916 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.990961 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.991005 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.991050 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.991095 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.991140 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.991185 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.global_step               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.991230 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.991281 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.991327 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.991372 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.991417 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.global_step                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.991462 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.991508 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.991553 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.991598 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.991643 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.991688 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.991733 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.991778 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.991823 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.991868 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.991913 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.991957 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.992002 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.992047 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.global_step                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.992096 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.992142 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.992187 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.992231 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.992276 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.global_step                                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.992321 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.992366 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.992411 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.992456 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.992501 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.992547 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.992592 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.global_step               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.992637 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.992682 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.992727 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.992773 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.992817 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.global_step                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.992863 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.992908 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.992957 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.993002 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.993048 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.993093 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.993138 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.993183 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.993228 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.993273 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.993318 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.993363 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.993408 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.993453 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.global_step                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.993497 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.993542 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.993587 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.993632 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.993676 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.global_step                                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.993722 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.993770 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.993817 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.993866 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.993912 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.993957 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.994002 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.global_step               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.994048 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.994092 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.994138 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.994183 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.994229 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.global_step                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.994273 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.994317 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.994362 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.994407 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.994452 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.994497 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.994554 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.994605 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.994655 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.994701 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.994746 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.994791 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.994835 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.994880 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.global_step                         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.994925 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.994970 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.995014 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.995059 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.995104 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.global_step                                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.995148 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.995193 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.995238 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.995282 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.995327 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.995372 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.995416 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.995466 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.995512 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.995557 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.995602 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.995648 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.global_step                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.995693 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.995738 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.995784 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.995829 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.995874 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.995919 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.995965 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.996010 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.996055 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.996100 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.996145 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.996190 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.996235 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.996280 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.global_step                         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.996330 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.996376 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.996421 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.996466 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.996511 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.global_step                                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.996556 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.996601 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.996646 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.996691 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.996737 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.996782 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.996827 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.996871 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.996916 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.996960 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.997005 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.997050 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.global_step                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.997095 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.997147 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.997193 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.997238 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.997284 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.global_step                                           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.997329 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.997375 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.997420 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.997465 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.997510 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.997556 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.997601 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.997647 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.997692 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.997736 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.global_step                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.997781 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.997827 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.997871 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.997917 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.997961 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.global_step                                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.998011 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.998057 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.998102 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.998147 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.998192 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.998237 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.998282 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.global_step               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.998327 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.998372 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.998417 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.998462 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.998507 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.global_step                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.998570 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.998618 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.998663 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.998708 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.998753 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.998798 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.998844 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.998893 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.998939 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.998984 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.999029 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.999074 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.999119 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.999164 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.global_step                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.999209 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.999254 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.999298 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.999343 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.999388 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.global_step                                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.999433 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.999477 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.999522 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.999567 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.999611 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.999656 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.999705 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.global_step               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.999750 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.999795 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.999841 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.999886 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.999931 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.global_step                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:37.999976 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.000021 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.000066 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.000118 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.000192 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.000266 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.000338 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.000408 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.000482 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.000556 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.000618 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.000667 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.000713 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.000766 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.global_step                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.000813 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.000858 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.000904 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.000949 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.000994 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.global_step                                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.001040 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.001085 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.001131 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.001176 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.001221 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.001266 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.001312 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.global_step               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.001357 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.001402 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.001447 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.001492 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.001538 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.global_step                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.001587 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.001633 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.001678 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.001723 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.001768 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.001812 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.001857 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.001902 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.001947 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.001992 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.002040 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.002087 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.002133 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.002178 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.global_step                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.002223 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.002269 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.002314 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.002359 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.002404 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.global_step                                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.002454 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.002501 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.002579 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.002637 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.002684 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.002731 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.002776 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.global_step               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.002822 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.002867 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.002912 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.002958 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.003003 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.global_step                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.003049 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.003095 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.003140 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.003185 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.003231 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.003276 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.003326 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.003373 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.003418 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.003463 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.003507 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.003552 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.003598 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.003643 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.global_step                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.003688 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.003733 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.003778 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.003823 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.003868 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.global_step                                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.003916 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.003963 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.004008 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.004053 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.004098 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.004142 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.004192 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.global_step               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.004238 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.004283 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.004329 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.004374 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.004419 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.global_step                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.004464 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.004509 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.004554 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.004599 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.004645 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.004691 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.004736 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.004781 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.004826 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.004872 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.004917 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.004962 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.005012 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.005059 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.global_step                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.005104 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.005150 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.005196 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.005242 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.005287 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.global_step                                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.005332 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.005378 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.005424 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.005469 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.005515 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.005560 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.005605 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.global_step               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.005650 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.005696 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.005742 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.005787 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.005834 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.global_step                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.005883 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.005929 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.005975 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.006020 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.006065 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.006110 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.006156 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.006201 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.006247 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.006292 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.006338 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.006383 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.006428 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.006473 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.global_step                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.006518 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.006596 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.006645 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.006692 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.006745 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.global_step                                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.006793 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.006839 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.006884 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.006929 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.006975 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.007020 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.007066 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.global_step               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.007111 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.007156 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.007201 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.007247 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.007292 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.global_step                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.007337 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.007382 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.007427 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.007472 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.007517 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.007562 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.007611 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.007658 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.007703 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.007747 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.007792 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.007837 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.007882 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.007927 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.global_step                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.007971 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.008017 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.008062 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.008107 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.008152 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.global_step                                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.008197 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.008242 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.008287 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.008332 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.008378 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.008427 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.008473 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.global_step               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.008519 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.008565 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.008610 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.008656 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.008701 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.global_step                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.008746 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.008791 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.008837 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.008883 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.008928 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.global_step                                           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.008974 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.009019 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.009064 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.009110 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.009155 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.009201 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.009247 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.009296 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.009343 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.009388 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.global_step                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.009435 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.009480 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.009526 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.009572 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.009617 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.global_step                                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.009662 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.009708 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.009754 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.009800 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.009845 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.009891 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.009936 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.global_step               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.009982 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.010028 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.010073 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.010124 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.010171 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.global_step                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.010217 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.010262 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.010308 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.010353 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.010398 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.010443 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.010488 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.010533 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.010615 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.010662 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.010707 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.010752 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.010797 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.010842 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.global_step                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.010887 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.010932 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.010977 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.011027 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.011073 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.global_step                                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.011119 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.011164 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.011210 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.011255 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.011300 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.011345 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.011390 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.global_step               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.011436 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.011481 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.011526 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.011571 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.011616 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.global_step                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.011661 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.011706 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.011752 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.011797 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.011842 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.011891 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.011937 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.011982 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.012027 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.012072 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.012117 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.012162 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.012207 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.012253 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.global_step                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.012298 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.012344 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.012389 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.012434 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.012479 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.global_step                                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.012523 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.012569 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.012613 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.012658 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.012708 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.012754 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.012799 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.global_step               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.012844 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.012890 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.012934 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.012979 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.013024 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.global_step                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.013069 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.013114 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.013158 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.013203 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.013248 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.013293 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.013338 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.013383 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.013428 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.013473 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.013518 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.013566 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.013612 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.013657 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.global_step                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.013701 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.013747 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.013792 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.013836 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.013881 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.global_step                                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.013926 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.013975 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.014021 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.014066 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.014111 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.014156 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.014201 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.global_step               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.014246 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.014291 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.014336 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.014386 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.014432 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.global_step                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.014477 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.014523 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.014605 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.014654 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.014701 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.014747 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.014794 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.014840 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.014886 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.014931 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.014977 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.015023 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.015068 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.015113 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.global_step                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.015158 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.015204 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.015249 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.015299 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.015346 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.global_step                                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.015392 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.015437 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.015482 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.015527 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.015572 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.015617 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.015662 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.global_step               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.015707 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.015752 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.015797 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.015842 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.015887 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.global_step                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.015931 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.015976 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.016021 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.016065 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.016114 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.016160 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.016205 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.016250 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.016295 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.016340 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.016385 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.016430 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.016474 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.016520 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.global_step                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.016565 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.016610 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.016655 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.016700 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.016745 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.global_step                                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.016790 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.016835 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.016880 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.016925 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.016974 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.017019 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.017065 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.global_step               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.017110 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.017155 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.017200 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.017246 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.017291 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.global_step                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.017336 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.017381 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.017427 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.017472 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.017517 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.017562 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.017607 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.017652 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.017698 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.017743 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.017792 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.017839 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.017884 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.017930 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.global_step                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.017975 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.018021 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.018066 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.018111 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.018157 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.global_step                                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.018202 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.018247 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.018292 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.018338 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.018383 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.018428 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.018474 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.global_step               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.018519 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.018602 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.018650 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.018702 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.018749 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.global_step                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.018794 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.018840 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.018885 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.018931 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.018976 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.019021 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.019066 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.019112 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.019158 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.019203 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.019248 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.019293 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.019338 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.019384 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.global_step                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.019429 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.019473 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.019522 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.019568 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.019613 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.global_step                                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.019658 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.019702 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.019747 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.019792 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.019837 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.019882 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.019927 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.global_step               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.019972 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.020016 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.020061 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.020105 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.020149 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.global_step                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.020194 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.020239 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.020284 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.020328 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.020378 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.global_step                                           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.020423 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_0                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.020469 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_1                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.020515 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_10                                       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.020560 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_11                                       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.020605 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_12                                       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.020651 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_13                                       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.020696 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_14                                       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.020741 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_15                                       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.020787 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_2                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.020832 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_3                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.020877 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_4                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.020922 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_5                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.020967 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_6                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.021013 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_7                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.021058 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_8                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.021103 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_9                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.021149 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.global_step                                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.021198 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_0                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.021244 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_1                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.021289 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_10                                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.021335 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_11                                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.021380 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_12                                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.021425 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_13                                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.021471 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_14                                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.021516 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_15                                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.021561 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_2                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.021605 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_3                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.021650 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_4                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.021695 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_5                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.021741 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_6                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.021785 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_7                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.021830 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_8                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.021876 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_9                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.021921 139818304546624 py_utils.py:1474] worker 0: lm.stack.global_step                                                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 15:21:38.021984 139818304546624 py_utils.py:1490] ==========
I1001 15:21:40.509019 139818304546624 gpipe.py:457] cell 0 input [<tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/GatherV2_1:0' shape=(1024, 1) dtype=int32>, <tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/GatherV2_2:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None, None, None]
I1001 15:21:42.764433 139818304546624 gpipe.py:457] cell 1 input [<tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/encoder_7/add:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/GatherV2_2:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I1001 15:21:45.190418 139818304546624 gpipe.py:457] cell 2 input [<tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/encoder_15/add:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/GatherV2_2:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I1001 15:21:47.362927 139818304546624 gpipe.py:457] cell 3 input [<tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/encoder_23/add:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/GatherV2_2:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I1001 15:21:49.965686 139818304546624 gpipe.py:457] cell 0 input [<tf.Tensor 'arg259:0' shape=(1024, 1) dtype=int32>, <tf.Tensor 'arg260:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None, None, None]
W1001 15:21:51.887149 139818304546624 recurrent.py:886] cell_fn contains stateful ops: [('emb/Assert/Assert', 'Assert'), ('emb/Assert_1/Assert', 'Assert'), ('encoder_0/fflayer_0/encoder_0/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_0/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_0/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_1/fflayer_0/encoder_1/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_1/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_1/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_2/fflayer_0/encoder_2/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_2/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_2/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_3/fflayer_0/encoder_3/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_3/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_3/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_4/fflayer_0/encoder_4/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_4/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_4/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_5/fflayer_0/encoder_5/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_5/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_5/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_6/fflayer_0/encoder_6/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_6/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_6/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_7/fflayer_0/encoder_7/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_7/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_7/fflayer_1/Assert/AssertGuard/Assert', 'Assert')]
I1001 15:21:52.012938 139818304546624 gpipe.py:457] cell 1 input [<tf.Tensor 'arg254:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'arg255:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
W1001 15:21:54.257925 139818304546624 recurrent.py:886] cell_fn contains stateful ops: [('encoder_8/fflayer_0/encoder_8/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_8/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_8/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_9/fflayer_0/encoder_9/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_9/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_9/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_10/fflayer_0/encoder_10/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_10/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_10/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_11/fflayer_0/encoder_11/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_11/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_11/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_12/fflayer_0/encoder_12/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_12/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_12/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_13/fflayer_0/encoder_13/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_13/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_13/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_14/fflayer_0/encoder_14/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_14/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_14/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_15/fflayer_0/encoder_15/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_15/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_15/fflayer_1/Assert/AssertGuard/Assert', 'Assert')]
I1001 15:21:54.383101 139818304546624 gpipe.py:457] cell 2 input [<tf.Tensor 'arg254:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'arg255:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
W1001 15:21:56.266147 139818304546624 recurrent.py:886] cell_fn contains stateful ops: [('encoder_16/fflayer_0/encoder_16/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_16/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_16/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_17/fflayer_0/encoder_17/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_17/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_17/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_18/fflayer_0/encoder_18/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_18/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_18/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_19/fflayer_0/encoder_19/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_19/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_19/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_20/fflayer_0/encoder_20/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_20/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_20/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_21/fflayer_0/encoder_21/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_21/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_21/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_22/fflayer_0/encoder_22/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_22/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_22/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_23/fflayer_0/encoder_23/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_23/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_23/fflayer_1/Assert/AssertGuard/Assert', 'Assert')]
I1001 15:21:56.406971 139818304546624 gpipe.py:457] cell 3 input [<tf.Tensor 'arg286:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'arg287:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
W1001 15:21:58.738798 139818304546624 recurrent.py:886] cell_fn contains stateful ops: [('encoder_24/fflayer_0/encoder_24/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_24/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_24/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_25/fflayer_0/encoder_25/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_25/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_25/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_26/fflayer_0/encoder_26/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_26/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_26/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_27/fflayer_0/encoder_27/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_27/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_27/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_28/fflayer_0/encoder_28/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_28/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_28/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_29/fflayer_0/encoder_29/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_29/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_29/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_30/fflayer_0/encoder_30/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_30/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_30/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_31/fflayer_0/encoder_31/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_31/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_31/fflayer_1/Assert/AssertGuard/Assert', 'Assert')]
I1001 15:21:59.230299 139818304546624 gpipe.py:457] cell 0 input [<tf.Tensor 'arg259:0' shape=(1024, 1) dtype=int32>, <tf.Tensor 'arg260:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None, None, None]
I1001 15:22:01.803689 139818304546624 gpipe.py:457] cell 1 input [<tf.Tensor 'Recv_1:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'Recv_2:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I1001 15:22:04.823028 139818304546624 gpipe.py:457] cell 2 input [<tf.Tensor 'Recv_1:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'Recv_2:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I1001 15:22:07.851616 139818304546624 gpipe.py:457] cell 3 input [<tf.Tensor 'Recv_1:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'Recv_2:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I1001 15:22:09.909427 139818304546624 gpipe.py:548] pipeline output = [<tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/Reshape_2:0' shape=(1024, 32, 32000) dtype=float32>]
I1001 15:22:09.913766 139818304546624 layers.py:2786] Using sparse_softmax_cross_entropy_with_logits() in SimpleFullSoftmax::_FProp2D logits_shape=[32768, 32000]
I1001 15:22:10.006881 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/total_samples/var on /job:local/replica:0/task:0/device:CPU:0 6970291216
I1001 15:22:10.008737 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/total_samples/var:0 shape=() on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:22:10.017442 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var:0
I1001 15:22:10.017541 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 15:22:10.017610 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 15:22:10.017669 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 15:22:10.017723 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 15:22:10.017776 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var:0
I1001 15:22:10.017829 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var:0
I1001 15:22:10.017880 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 15:22:10.017933 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 15:22:10.017984 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 15:22:10.018037 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 15:22:10.018089 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 15:22:10.018141 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 15:22:10.018203 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 15:22:10.018257 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 15:22:10.018307 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 15:22:10.018358 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var:0
I1001 15:22:10.018409 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var:0
I1001 15:22:10.018459 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 15:22:10.018510 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 15:22:10.018587 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 15:22:10.018642 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 15:22:10.018693 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var:0
I1001 15:22:10.018743 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var:0
I1001 15:22:10.018794 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 15:22:10.018845 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 15:22:10.018895 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 15:22:10.018947 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 15:22:10.018997 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 15:22:10.019048 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 15:22:10.019099 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 15:22:10.019150 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 15:22:10.019200 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 15:22:10.019251 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var:0
I1001 15:22:10.019302 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var:0
I1001 15:22:10.019353 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 15:22:10.019404 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 15:22:10.019462 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 15:22:10.019514 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 15:22:10.019565 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var:0
I1001 15:22:10.019615 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var:0
I1001 15:22:10.019665 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 15:22:10.019716 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 15:22:10.019766 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 15:22:10.019817 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 15:22:10.019867 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 15:22:10.019916 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 15:22:10.019967 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 15:22:10.020017 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 15:22:10.020066 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 15:22:10.020117 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var:0
I1001 15:22:10.020166 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var:0
I1001 15:22:10.020216 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 15:22:10.020266 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 15:22:10.020316 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 15:22:10.020366 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 15:22:10.020419 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var:0
I1001 15:22:10.020471 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var:0
I1001 15:22:10.020521 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 15:22:10.020572 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 15:22:10.020622 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 15:22:10.020672 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 15:22:10.020727 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 15:22:10.020780 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 15:22:10.020830 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 15:22:10.020881 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 15:22:10.020932 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 15:22:10.020983 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var:0
I1001 15:22:10.021034 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var:0
I1001 15:22:10.021085 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 15:22:10.021134 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 15:22:10.021185 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 15:22:10.021236 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 15:22:10.021286 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var:0
I1001 15:22:10.021336 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var:0
I1001 15:22:10.021387 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 15:22:10.021437 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 15:22:10.021487 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 15:22:10.021538 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 15:22:10.021588 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 15:22:10.021638 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 15:22:10.021688 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 15:22:10.021739 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 15:22:10.021789 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 15:22:10.021839 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var:0
I1001 15:22:10.021889 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var:0
I1001 15:22:10.021944 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 15:22:10.021996 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 15:22:10.022046 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 15:22:10.022097 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 15:22:10.022147 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var:0
I1001 15:22:10.022198 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var:0
I1001 15:22:10.022249 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 15:22:10.022300 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 15:22:10.022350 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 15:22:10.022402 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 15:22:10.022453 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 15:22:10.022503 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 15:22:10.022571 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 15:22:10.022627 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 15:22:10.022678 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 15:22:10.022729 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var:0
I1001 15:22:10.022780 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var:0
I1001 15:22:10.022830 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 15:22:10.022881 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 15:22:10.022930 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 15:22:10.022981 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 15:22:10.023030 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var:0
I1001 15:22:10.023081 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var:0
I1001 15:22:10.023131 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 15:22:10.023182 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 15:22:10.023236 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 15:22:10.023289 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 15:22:10.023339 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 15:22:10.023389 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 15:22:10.023439 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 15:22:10.023489 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 15:22:10.023540 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 15:22:10.023591 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var:0
I1001 15:22:10.023641 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var:0
I1001 15:22:10.023690 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 15:22:10.023740 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 15:22:10.023791 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 15:22:10.023841 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 15:22:10.023891 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var:0
I1001 15:22:10.023941 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var:0
I1001 15:22:10.023992 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 15:22:10.024042 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 15:22:10.024093 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 15:22:10.024144 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 15:22:10.024194 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 15:22:10.024245 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 15:22:10.024294 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 15:22:10.024345 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 15:22:10.024395 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 15:22:10.024450 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var:0
I1001 15:22:10.024502 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var:0
I1001 15:22:10.024553 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 15:22:10.024603 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 15:22:10.024654 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 15:22:10.024704 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 15:22:10.024755 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var:0
I1001 15:22:10.024805 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var:0
I1001 15:22:10.024855 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 15:22:10.024906 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 15:22:10.024956 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 15:22:10.025007 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 15:22:10.025057 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 15:22:10.025108 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 15:22:10.025159 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 15:22:10.025210 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 15:22:10.025261 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 15:22:10.025312 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var:0
I1001 15:22:10.025362 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var:0
I1001 15:22:10.025413 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 15:22:10.025463 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 15:22:10.025513 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 15:22:10.025563 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 15:22:10.025613 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var:0
I1001 15:22:10.025663 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var:0
I1001 15:22:10.025717 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 15:22:10.025769 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 15:22:10.025820 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 15:22:10.025869 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 15:22:10.025920 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 15:22:10.025971 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 15:22:10.026022 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 15:22:10.026079 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 15:22:10.026130 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 15:22:10.026181 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var:0
I1001 15:22:10.026232 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var:0
I1001 15:22:10.026282 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 15:22:10.026332 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 15:22:10.026383 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 15:22:10.026433 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 15:22:10.026484 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var:0
I1001 15:22:10.026546 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var:0
I1001 15:22:10.026605 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 15:22:10.026657 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 15:22:10.026707 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 15:22:10.026758 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 15:22:10.026808 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 15:22:10.026858 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 15:22:10.026908 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 15:22:10.026964 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 15:22:10.027016 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 15:22:10.027066 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var:0
I1001 15:22:10.027116 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var:0
I1001 15:22:10.027167 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 15:22:10.027217 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 15:22:10.027267 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 15:22:10.027317 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 15:22:10.027367 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var:0
I1001 15:22:10.027418 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var:0
I1001 15:22:10.027468 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 15:22:10.027518 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 15:22:10.027569 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 15:22:10.027620 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 15:22:10.027671 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 15:22:10.027721 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 15:22:10.027772 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 15:22:10.027822 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 15:22:10.027872 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 15:22:10.027923 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var:0
I1001 15:22:10.027973 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var:0
I1001 15:22:10.028023 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 15:22:10.028073 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 15:22:10.028123 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 15:22:10.028173 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 15:22:10.028227 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var:0
I1001 15:22:10.028279 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var:0
I1001 15:22:10.028328 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 15:22:10.028379 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 15:22:10.028430 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 15:22:10.028483 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 15:22:10.028543 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 15:22:10.028602 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 15:22:10.028660 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 15:22:10.028716 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 15:22:10.028769 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 15:22:10.028821 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var:0
I1001 15:22:10.028871 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var:0
I1001 15:22:10.028922 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 15:22:10.028972 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 15:22:10.029022 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 15:22:10.029072 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 15:22:10.029122 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var:0
I1001 15:22:10.029172 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var:0
I1001 15:22:10.029222 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 15:22:10.029274 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 15:22:10.029325 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 15:22:10.029376 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 15:22:10.029426 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 15:22:10.029507 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 15:22:10.029569 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 15:22:10.029621 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 15:22:10.029672 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 15:22:10.029723 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var:0
I1001 15:22:10.029773 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var:0
I1001 15:22:10.029824 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 15:22:10.029875 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 15:22:10.029926 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 15:22:10.029977 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 15:22:10.030028 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var:0
I1001 15:22:10.030079 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var:0
I1001 15:22:10.030129 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 15:22:10.030180 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 15:22:10.030231 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 15:22:10.030282 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 15:22:10.030333 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 15:22:10.030383 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 15:22:10.030434 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 15:22:10.030489 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 15:22:10.030559 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 15:22:10.030617 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var:0
I1001 15:22:10.030669 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var:0
I1001 15:22:10.030720 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 15:22:10.030771 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 15:22:10.030827 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 15:22:10.030879 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 15:22:10.030929 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var:0
I1001 15:22:10.030980 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var:0
I1001 15:22:10.031031 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 15:22:10.031082 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 15:22:10.031133 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 15:22:10.031184 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 15:22:10.031236 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 15:22:10.031286 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 15:22:10.031337 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 15:22:10.031388 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 15:22:10.031439 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 15:22:10.031489 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var:0
I1001 15:22:10.031539 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var:0
I1001 15:22:10.031590 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 15:22:10.031640 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 15:22:10.031691 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 15:22:10.031741 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 15:22:10.031791 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var:0
I1001 15:22:10.031841 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var:0
I1001 15:22:10.031892 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 15:22:10.031943 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 15:22:10.031994 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 15:22:10.032045 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 15:22:10.032100 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 15:22:10.032153 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 15:22:10.032204 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 15:22:10.032254 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 15:22:10.032304 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 15:22:10.032354 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var:0
I1001 15:22:10.032404 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var:0
I1001 15:22:10.032455 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 15:22:10.032505 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 15:22:10.032555 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 15:22:10.032605 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 15:22:10.032655 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var:0
I1001 15:22:10.032706 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var:0
I1001 15:22:10.032757 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 15:22:10.032808 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 15:22:10.032859 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 15:22:10.032911 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 15:22:10.032961 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 15:22:10.033011 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 15:22:10.033061 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 15:22:10.033111 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 15:22:10.033161 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 15:22:10.033211 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var:0
I1001 15:22:10.033261 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var:0
I1001 15:22:10.033316 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 15:22:10.033367 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 15:22:10.033417 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 15:22:10.033468 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 15:22:10.033518 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var:0
I1001 15:22:10.033568 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var:0
I1001 15:22:10.033617 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 15:22:10.033668 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 15:22:10.033719 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 15:22:10.033769 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 15:22:10.033820 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 15:22:10.033870 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 15:22:10.033921 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 15:22:10.033971 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 15:22:10.034021 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 15:22:10.034072 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var:0
I1001 15:22:10.034122 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var:0
I1001 15:22:10.034172 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 15:22:10.034223 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 15:22:10.034272 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 15:22:10.034322 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 15:22:10.034373 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var:0
I1001 15:22:10.034423 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var:0
I1001 15:22:10.034473 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 15:22:10.034523 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 15:22:10.034600 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 15:22:10.034653 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 15:22:10.034703 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 15:22:10.034753 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 15:22:10.034803 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 15:22:10.034853 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 15:22:10.034903 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 15:22:10.034953 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var:0
I1001 15:22:10.035003 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var:0
I1001 15:22:10.035053 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 15:22:10.035103 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 15:22:10.035152 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 15:22:10.035202 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 15:22:10.035251 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var:0
I1001 15:22:10.035301 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var:0
I1001 15:22:10.035351 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 15:22:10.035401 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 15:22:10.035451 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 15:22:10.035501 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 15:22:10.035551 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 15:22:10.035601 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 15:22:10.035651 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 15:22:10.035701 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 15:22:10.035751 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 15:22:10.035805 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var:0
I1001 15:22:10.035856 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var:0
I1001 15:22:10.035906 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 15:22:10.035956 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 15:22:10.036006 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 15:22:10.036056 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 15:22:10.036107 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var:0
I1001 15:22:10.036156 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var:0
I1001 15:22:10.036206 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 15:22:10.036256 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 15:22:10.036306 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 15:22:10.036357 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 15:22:10.036407 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 15:22:10.036457 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 15:22:10.036508 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 15:22:10.036558 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 15:22:10.036608 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 15:22:10.036659 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var:0
I1001 15:22:10.036709 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var:0
I1001 15:22:10.036759 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 15:22:10.036809 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 15:22:10.036860 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 15:22:10.036910 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 15:22:10.036959 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var:0
I1001 15:22:10.037013 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var:0
I1001 15:22:10.037064 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 15:22:10.037114 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 15:22:10.037165 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 15:22:10.037215 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 15:22:10.037266 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 15:22:10.037316 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 15:22:10.037367 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 15:22:10.037416 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 15:22:10.037466 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 15:22:10.037517 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var:0
I1001 15:22:10.037566 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var:0
I1001 15:22:10.037616 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 15:22:10.037667 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 15:22:10.037717 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 15:22:10.037767 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 15:22:10.037817 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var:0
I1001 15:22:10.037867 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var:0
I1001 15:22:10.037917 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 15:22:10.037967 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 15:22:10.038018 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 15:22:10.038068 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 15:22:10.038118 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 15:22:10.038168 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 15:22:10.038218 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 15:22:10.038273 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 15:22:10.038323 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 15:22:10.038374 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var:0
I1001 15:22:10.038425 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var:0
I1001 15:22:10.038475 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 15:22:10.038526 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 15:22:10.038595 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 15:22:10.038648 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 15:22:10.038699 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var:0
I1001 15:22:10.038750 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var:0
I1001 15:22:10.038801 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 15:22:10.038852 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 15:22:10.038903 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 15:22:10.038954 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 15:22:10.039005 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 15:22:10.039056 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 15:22:10.039106 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 15:22:10.039157 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 15:22:10.039207 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 15:22:10.039258 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var:0
I1001 15:22:10.039308 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var:0
I1001 15:22:10.039359 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 15:22:10.039408 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 15:22:10.039458 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 15:22:10.039512 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 15:22:10.039563 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var:0
I1001 15:22:10.039613 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var:0
I1001 15:22:10.039664 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 15:22:10.039715 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 15:22:10.039765 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 15:22:10.039815 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 15:22:10.039866 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 15:22:10.039916 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 15:22:10.039967 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 15:22:10.040016 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 15:22:10.040066 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 15:22:10.040117 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var:0
I1001 15:22:10.040167 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var:0
I1001 15:22:10.040216 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 15:22:10.040266 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 15:22:10.040316 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 15:22:10.040365 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 15:22:10.040415 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var:0
I1001 15:22:10.040465 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var:0
I1001 15:22:10.040518 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 15:22:10.040569 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 15:22:10.040620 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 15:22:10.040671 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 15:22:10.040722 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 15:22:10.040776 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 15:22:10.040827 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 15:22:10.040878 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 15:22:10.040927 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 15:22:10.040977 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var:0
I1001 15:22:10.041028 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var:0
I1001 15:22:10.041078 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 15:22:10.041129 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 15:22:10.041179 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 15:22:10.041229 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 15:22:10.041279 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var:0
I1001 15:22:10.041329 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var:0
I1001 15:22:10.041379 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 15:22:10.041430 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 15:22:10.041481 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 15:22:10.041530 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 15:22:10.041581 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 15:22:10.041631 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 15:22:10.041682 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 15:22:10.041732 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 15:22:10.041781 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 15:22:10.041832 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var:0
I1001 15:22:10.041883 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var:0
I1001 15:22:10.041933 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 15:22:10.041987 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 15:22:10.042038 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 15:22:10.042089 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 15:22:10.042139 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var:0
I1001 15:22:10.042189 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var:0
I1001 15:22:10.042239 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 15:22:10.042291 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 15:22:10.042342 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 15:22:10.042392 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 15:22:10.042443 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 15:22:10.042495 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 15:22:10.042564 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 15:22:10.042622 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 15:22:10.042674 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 15:22:10.042725 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var:0
I1001 15:22:10.042776 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var:0
I1001 15:22:10.042828 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 15:22:10.042878 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 15:22:10.042929 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 15:22:10.042980 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 15:22:10.043030 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var:0
I1001 15:22:10.043081 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var:0
I1001 15:22:10.043131 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 15:22:10.043182 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 15:22:10.043233 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 15:22:10.043290 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 15:22:10.043341 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 15:22:10.043392 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 15:22:10.043442 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 15:22:10.043491 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 15:22:10.043541 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 15:22:10.043591 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var:0
I1001 15:22:10.043641 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var:0
I1001 15:22:10.043691 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 15:22:10.043741 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 15:22:10.043791 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 15:22:10.043841 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 15:22:10.043890 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var:0
I1001 15:22:10.043939 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var:0
I1001 15:22:10.043990 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 15:22:10.044040 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 15:22:10.044090 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 15:22:10.044140 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 15:22:10.044191 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 15:22:10.044241 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 15:22:10.044291 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 15:22:10.044341 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 15:22:10.044392 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 15:22:10.044442 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var:0
I1001 15:22:10.044496 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var:0
I1001 15:22:10.044547 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 15:22:10.044597 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 15:22:10.044646 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 15:22:10.044697 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 15:22:10.044747 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var:0
I1001 15:22:10.044797 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var:0
I1001 15:22:10.044847 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 15:22:10.044898 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 15:22:10.044948 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 15:22:10.044999 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 15:22:10.045049 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 15:22:10.045099 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 15:22:10.045150 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 15:22:10.045199 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 15:22:10.045249 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 15:22:10.045299 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var:0
I1001 15:22:10.045349 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var:0
I1001 15:22:10.045398 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var:0
I1001 15:22:10.045449 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var:0
I1001 15:22:10.045498 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var:0
I1001 15:22:10.045548 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var:0
I1001 15:22:10.045598 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var:0
I1001 15:22:10.045648 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var:0
I1001 15:22:10.045697 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var:0
I1001 15:22:10.045747 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var:0
I1001 15:22:10.045802 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var:0
I1001 15:22:10.045853 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var:0
I1001 15:22:10.045903 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var:0
I1001 15:22:10.045953 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var:0
I1001 15:22:10.046003 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var:0
I1001 15:22:10.046052 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var:0
I1001 15:22:10.046102 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var:0
I1001 15:22:10.046152 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var:0
I1001 15:22:10.046202 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var:0
I1001 15:22:10.046252 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var:0
I1001 15:22:10.046303 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var:0
I1001 15:22:10.046353 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var:0
I1001 15:22:10.046403 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var:0
I1001 15:22:10.046453 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var:0
I1001 15:22:10.046503 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var:0
I1001 15:22:10.046570 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var:0
I1001 15:22:10.046625 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var:0
I1001 15:22:10.046676 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var:0
I1001 15:22:10.046727 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var:0
I1001 15:22:10.046778 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var:0
I1001 15:22:10.046828 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var:0
I1001 15:22:10.046879 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var:0
I1001 15:22:10.046929 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var:0
I1001 15:22:10.046979 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var:0
I1001 15:22:15.572587 139818304546624 gpipe.py:457] cell 3 input [<tf.Tensor 'arg287:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'arg288:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I1001 15:22:21.479412 139818304546624 gpipe.py:457] cell 2 input [<tf.Tensor 'arg255:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'arg256:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I1001 15:22:27.067893 139818304546624 gpipe.py:457] cell 1 input [<tf.Tensor 'arg255:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'arg256:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I1001 15:22:32.774996 139818304546624 gpipe.py:457] cell 0 input [<tf.Tensor 'arg259:0' shape=(1024, 1) dtype=int32>, <tf.Tensor 'arg260:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None, None, None]
I1001 15:22:43.929472 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.emb.src_token_emb.wm: <tf.Variable '1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var:0' shape=(32000, 2048) dtype=float32_ref>
I1001 15:22:43.929697 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 15:22:43.929785 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 15:22:43.929865 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.929935 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 15:22:43.930004 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.930068 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.930130 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 15:22:43.930194 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.930261 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.930323 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.930388 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.930450 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.930515 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.930598 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.930677 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.930740 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.930800 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.930860 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 15:22:43.930921 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 15:22:43.930984 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.931044 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 15:22:43.931107 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.931168 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.931228 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 15:22:43.931289 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.931353 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.931414 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.931478 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.931539 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.931608 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.931670 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.931733 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.931794 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.931853 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.931913 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 15:22:43.931980 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 15:22:43.932043 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.932104 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 15:22:43.932167 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.932227 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.932286 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 15:22:43.932346 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.932410 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.932471 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.932539 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.932600 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.932664 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.932725 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.932789 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.932849 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.932909 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.932970 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 15:22:43.933030 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 15:22:43.933094 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.933155 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 15:22:43.933218 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.933279 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.933339 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 15:22:43.933400 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.933469 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.933531 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.933595 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.933656 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.933721 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.933781 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.933845 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.933905 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.933966 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.934026 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 15:22:43.934086 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 15:22:43.934150 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.934210 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 15:22:43.934274 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.934335 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.934400 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 15:22:43.934462 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.934526 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.934613 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.934680 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.934742 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.934806 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.934874 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.934938 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.934998 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.935059 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.935119 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 15:22:43.935179 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 15:22:43.935242 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.935308 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 15:22:43.935372 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.935434 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.935494 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 15:22:43.935554 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.935619 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.935679 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.935744 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.935804 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.935869 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.935931 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.935994 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.936054 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.936114 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.936173 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 15:22:43.936239 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 15:22:43.936305 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.936366 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 15:22:43.936429 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.936489 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.936548 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 15:22:43.936609 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.936672 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.936733 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.936797 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.936857 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.936921 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.936981 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.937044 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.937109 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.937171 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.937231 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 15:22:43.937292 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 15:22:43.937355 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.937417 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 15:22:43.937480 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.937542 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.937603 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 15:22:43.937664 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.937729 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.937789 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.937853 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.937913 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.937977 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.938041 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.938105 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.938165 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.938226 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.938286 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 15:22:43.938347 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 15:22:43.938414 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.938475 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 15:22:43.938553 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.938619 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.938680 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 15:22:43.938741 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.938805 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.938865 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.938930 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.938997 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.939062 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.939123 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.939188 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.939250 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.939310 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.939371 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 15:22:43.939432 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 15:22:43.939496 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.939557 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 15:22:43.939620 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.939681 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.939741 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 15:22:43.939801 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.939865 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.939931 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.939995 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.940056 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.940120 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.940181 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.940245 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.940305 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.940366 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.940425 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 15:22:43.940485 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 15:22:43.940549 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.940609 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 15:22:43.940672 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.940732 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.940793 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 15:22:43.940857 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.940923 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.940984 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.941049 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.941110 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.941174 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.941235 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.941299 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.941360 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.941421 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.941482 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 15:22:43.941543 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 15:22:43.941606 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.941667 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 15:22:43.941735 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.941796 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.941856 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 15:22:43.941917 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.941982 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.942043 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.942107 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.942168 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.942232 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.942293 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.942356 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.942417 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.942478 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.942551 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 15:22:43.942620 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 15:22:43.942690 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.942752 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 15:22:43.942816 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.942876 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.942937 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 15:22:43.942998 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.943062 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.943124 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.943188 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.943249 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.943314 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.943375 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.943440 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.943501 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.943567 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.943629 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 15:22:43.943689 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 15:22:43.943753 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.943814 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 15:22:43.943877 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.943939 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.944000 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 15:22:43.944060 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.944124 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.944185 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.944250 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.944311 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.944375 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.944437 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.944506 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.944567 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.944628 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.944688 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 15:22:43.944749 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 15:22:43.944813 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.944874 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 15:22:43.944938 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.944998 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.945059 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 15:22:43.945119 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.945182 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.945243 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.945307 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.945368 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.945436 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.945498 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.945563 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.945623 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.945683 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.945744 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 15:22:43.945805 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 15:22:43.945868 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.945929 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 15:22:43.945992 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.946052 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.946114 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 15:22:43.946175 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.946239 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.946305 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.946371 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.946432 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.946496 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.946578 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.946647 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.946708 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.946774 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.946836 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 15:22:43.946896 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 15:22:43.946960 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.947021 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 15:22:43.947085 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.947145 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.947205 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 15:22:43.947271 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.947336 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.947398 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.947462 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.947523 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.947586 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.947648 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.947711 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.947772 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.947833 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.947893 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 15:22:43.947953 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 15:22:43.948017 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.948078 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 15:22:43.948141 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.948207 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.948268 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 15:22:43.948328 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.948392 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.948456 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.948520 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.948581 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.948645 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.948706 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.948770 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.948831 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.948891 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.948952 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 15:22:43.949013 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 15:22:43.949081 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.949143 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 15:22:43.949206 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.949268 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.949329 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 15:22:43.949389 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.949453 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.949514 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.949579 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.949639 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.949703 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.949764 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.949827 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.949888 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.949949 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.950012 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 15:22:43.950075 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 15:22:43.950138 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.950199 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 15:22:43.950262 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.950324 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.950383 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 15:22:43.950444 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.950508 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.950587 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.950655 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.950716 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.950781 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.950841 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.950909 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.950972 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.951031 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.951092 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 15:22:43.951152 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 15:22:43.951216 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.951277 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 15:22:43.951341 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.951402 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.951462 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 15:22:43.951522 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.951586 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.951648 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.951712 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.951773 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.951841 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.951903 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.951967 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.952027 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.952088 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.952149 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 15:22:43.952210 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 15:22:43.952274 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.952334 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 15:22:43.952397 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.952458 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.952518 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 15:22:43.952580 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.952644 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.952705 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.952775 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.952836 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.952899 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.952960 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.953024 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.953085 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.953145 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.953205 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 15:22:43.953266 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 15:22:43.953330 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.953391 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 15:22:43.953454 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.953515 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.953574 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 15:22:43.953635 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.953703 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.953766 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.953831 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.953892 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.953955 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.954016 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.954079 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.954140 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.954202 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.954262 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 15:22:43.954323 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 15:22:43.954385 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.954446 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 15:22:43.954508 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.954593 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.954657 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 15:22:43.954719 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.954783 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.954844 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.954908 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.954968 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.955032 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.955092 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.955156 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.955216 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.955276 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.955337 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 15:22:43.955397 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 15:22:43.955462 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.955528 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 15:22:43.955592 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.955654 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.955714 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 15:22:43.955775 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.955840 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.955901 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.955966 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.956027 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.956091 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.956152 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.956216 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.956276 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.956337 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.956402 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 15:22:43.956464 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 15:22:43.956528 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.956590 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 15:22:43.956653 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.956715 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.956775 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 15:22:43.956836 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.956900 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.956961 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.957026 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.957087 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.957151 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.957213 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.957277 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.957342 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.957403 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.957463 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 15:22:43.957524 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 15:22:43.957587 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.957649 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 15:22:43.957712 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.957773 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.957834 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 15:22:43.957894 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.957959 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.958020 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.958083 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.958143 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.958207 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.958272 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.958336 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.958397 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.958456 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.958520 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 15:22:43.958605 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 15:22:43.958671 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.958738 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 15:22:43.958801 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.958862 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.958922 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 15:22:43.958982 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.959046 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.959107 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.959176 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.959237 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.959301 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.959362 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.959425 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.959486 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.959545 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.959605 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 15:22:43.959666 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 15:22:43.959729 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.959791 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 15:22:43.959854 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.959915 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.959976 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 15:22:43.960035 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.960103 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.960165 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.960228 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.960289 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.960352 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.960412 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.960475 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.960536 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.960596 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.960656 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 15:22:43.960718 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 15:22:43.960781 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.960843 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 15:22:43.960906 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.960968 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.961032 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 15:22:43.961093 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.961158 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.961219 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.961283 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.961344 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.961407 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.961468 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.961533 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.961594 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.961654 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.961714 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 15:22:43.961774 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 15:22:43.961837 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.961902 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 15:22:43.961966 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.962027 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.962087 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 15:22:43.962148 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.962213 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.962274 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.962338 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.962399 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.962462 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.962524 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.962607 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.962669 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.962730 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.962789 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 15:22:43.962856 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 15:22:43.962920 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.962982 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 15:22:43.963046 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.963106 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.963166 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 15:22:43.963225 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.963290 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.963351 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.963415 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.963475 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.963539 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.963600 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:22:43.963664 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.963729 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.963790 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:22:43.963850 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_0: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var:0' shape=(2000,) dtype=float32_ref>
I1001 15:22:43.963912 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_1: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var:0' shape=(2000,) dtype=float32_ref>
I1001 15:22:43.963972 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_10: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var:0' shape=(2000,) dtype=float32_ref>
I1001 15:22:43.964033 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_11: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var:0' shape=(2000,) dtype=float32_ref>
I1001 15:22:43.964092 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_12: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var:0' shape=(2000,) dtype=float32_ref>
I1001 15:22:43.964152 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_13: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var:0' shape=(2000,) dtype=float32_ref>
I1001 15:22:43.964210 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_14: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var:0' shape=(2000,) dtype=float32_ref>
I1001 15:22:43.964269 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_15: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var:0' shape=(2000,) dtype=float32_ref>
I1001 15:22:43.964328 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_2: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var:0' shape=(2000,) dtype=float32_ref>
I1001 15:22:43.964387 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_3: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var:0' shape=(2000,) dtype=float32_ref>
I1001 15:22:43.964446 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_4: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var:0' shape=(2000,) dtype=float32_ref>
I1001 15:22:43.964505 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_5: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var:0' shape=(2000,) dtype=float32_ref>
I1001 15:22:43.964564 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_6: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var:0' shape=(2000,) dtype=float32_ref>
I1001 15:22:43.964623 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_7: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var:0' shape=(2000,) dtype=float32_ref>
I1001 15:22:43.964682 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_8: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var:0' shape=(2000,) dtype=float32_ref>
I1001 15:22:43.964740 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_9: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var:0' shape=(2000,) dtype=float32_ref>
I1001 15:22:43.964804 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_0: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var:0' shape=(2048, 2000) dtype=float32_ref>
I1001 15:22:43.964869 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_1: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var:0' shape=(2048, 2000) dtype=float32_ref>
I1001 15:22:43.964933 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_10: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var:0' shape=(2048, 2000) dtype=float32_ref>
I1001 15:22:43.964998 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_11: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var:0' shape=(2048, 2000) dtype=float32_ref>
I1001 15:22:43.965061 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_12: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var:0' shape=(2048, 2000) dtype=float32_ref>
I1001 15:22:43.965124 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_13: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var:0' shape=(2048, 2000) dtype=float32_ref>
I1001 15:22:43.965187 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_14: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var:0' shape=(2048, 2000) dtype=float32_ref>
I1001 15:22:43.965250 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_15: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var:0' shape=(2048, 2000) dtype=float32_ref>
I1001 15:22:43.965312 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_2: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var:0' shape=(2048, 2000) dtype=float32_ref>
I1001 15:22:43.965375 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_3: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var:0' shape=(2048, 2000) dtype=float32_ref>
I1001 15:22:43.965439 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_4: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var:0' shape=(2048, 2000) dtype=float32_ref>
I1001 15:22:43.965502 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_5: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var:0' shape=(2048, 2000) dtype=float32_ref>
I1001 15:22:43.965565 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_6: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var:0' shape=(2048, 2000) dtype=float32_ref>
I1001 15:22:43.965628 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_7: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var:0' shape=(2048, 2000) dtype=float32_ref>
I1001 15:22:43.965692 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_8: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var:0' shape=(2048, 2000) dtype=float32_ref>
I1001 15:22:43.965756 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_9: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var:0' shape=(2048, 2000) dtype=float32_ref>
I1001 15:22:53.851198 139818304546624 learner.py:279] gradient_adjuster=<bound method LanguageModel.AdjustGradients of <lingvo.tasks.lm.model.FixedShapeInputLanguageModel object at 0x7f29880f2b70>>
I1001 15:22:59.034436 139818304546624 cluster.py:515] Place variable beta1_power on /job:local/replica:0/task:0/device:CPU:0 6970291220
I1001 15:22:59.037409 139818304546624 cluster.py:515] Place variable beta2_power on /job:local/replica:0/task:0/device:CPU:0 6970291224
I1001 15:22:59.042516 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7232435224
I1001 15:22:59.047651 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7494579224
I1001 15:22:59.052759 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7494611992
I1001 15:22:59.057874 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7494644760
I1001 15:22:59.062934 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7561753624
I1001 15:22:59.068062 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7628862488
I1001 15:22:59.073070 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7628870680
I1001 15:22:59.078176 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7628878872
I1001 15:22:59.083211 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7695987736
I1001 15:22:59.088322 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7763096600
I1001 15:22:59.093343 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7763104792
I1001 15:22:59.098575 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7763112984
I1001 15:22:59.103629 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7763121176
I1001 15:22:59.108737 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7763129368
I1001 15:22:59.112584 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7763129880
I1001 15:22:59.116416 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7763130392
I1001 15:22:59.121366 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7779907608
I1001 15:22:59.126486 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7796684824
I1001 15:22:59.131555 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7796693016
I1001 15:22:59.136671 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7796701208
I1001 15:22:59.141707 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7813478424
I1001 15:22:59.147381 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7830255640
I1001 15:22:59.152496 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7830263832
I1001 15:22:59.157521 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7830272024
I1001 15:22:59.162664 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7847049240
I1001 15:22:59.167693 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7863826456
I1001 15:22:59.172851 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7863834648
I1001 15:22:59.177861 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7863842840
I1001 15:22:59.183006 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7880620056
I1001 15:22:59.188063 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7897397272
I1001 15:22:59.193267 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7897405464
I1001 15:22:59.198297 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7897413656
I1001 15:22:59.203447 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7897421848
I1001 15:22:59.208600 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7897430040
I1001 15:22:59.213642 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7897438232
I1001 15:22:59.218785 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7897446424
I1001 15:22:59.223871 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7897479192
I1001 15:22:59.228970 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7897511960
I1001 15:22:59.234003 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7964620824
I1001 15:22:59.239171 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8031729688
I1001 15:22:59.244213 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8031737880
I1001 15:22:59.249383 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8031746072
I1001 15:22:59.254394 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8098854936
I1001 15:22:59.260067 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8165963800
I1001 15:22:59.265169 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8165971992
I1001 15:22:59.270235 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8165980184
I1001 15:22:59.275472 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8165988376
I1001 15:22:59.280503 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8165996568
I1001 15:22:59.284338 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8165997080
I1001 15:22:59.288198 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8165997592
I1001 15:22:59.293149 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8182774808
I1001 15:22:59.298355 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8199552024
I1001 15:22:59.303581 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8199560216
I1001 15:22:59.308628 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8199568408
I1001 15:22:59.313777 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8216345624
I1001 15:22:59.318874 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8233122840
I1001 15:22:59.324018 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8233131032
I1001 15:22:59.329163 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8233139224
I1001 15:22:59.334321 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8249916440
I1001 15:22:59.339418 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8266693656
I1001 15:22:59.344576 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8266701848
I1001 15:22:59.349675 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8266710040
I1001 15:22:59.354853 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8283487256
I1001 15:22:59.360006 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8300264472
I1001 15:22:59.365036 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8300272664
I1001 15:22:59.370702 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8300280856
I1001 15:22:59.375730 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8300289048
I1001 15:22:59.380896 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8300297240
I1001 15:22:59.385993 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8300305432
I1001 15:22:59.391156 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8300313624
I1001 15:22:59.396184 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8300346392
I1001 15:22:59.401376 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8300379160
I1001 15:22:59.406421 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8367488024
I1001 15:22:59.411588 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8434596888
I1001 15:22:59.416706 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8434605080
I1001 15:22:59.421767 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8434613272
I1001 15:22:59.426941 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8501722136
I1001 15:22:59.432002 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8568831000
I1001 15:22:59.437149 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8568839192
I1001 15:22:59.442182 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8568847384
I1001 15:22:59.447384 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8568855576
I1001 15:22:59.452424 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8568863768
I1001 15:22:59.456401 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8568864280
I1001 15:22:59.460188 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8568864792
I1001 15:22:59.465127 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8585642008
I1001 15:22:59.470311 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8602419224
I1001 15:22:59.475500 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8602427416
I1001 15:22:59.480536 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8602435608
I1001 15:22:59.486176 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8619212824
I1001 15:22:59.491274 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8635990040
I1001 15:22:59.496421 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8635998232
I1001 15:22:59.501537 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8636006424
I1001 15:22:59.506718 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8652783640
I1001 15:22:59.511795 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8669560856
I1001 15:22:59.516964 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8669569048
I1001 15:22:59.522112 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8669577240
I1001 15:22:59.527186 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8686354456
I1001 15:22:59.532364 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8703131672
I1001 15:22:59.537430 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8703139864
I1001 15:22:59.542640 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8703148056
I1001 15:22:59.547735 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8703156248
I1001 15:22:59.552892 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8703164440
I1001 15:22:59.557961 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8703172632
I1001 15:22:59.563133 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8703180824
I1001 15:22:59.568197 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8703213592
I1001 15:22:59.573331 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8703246360
I1001 15:22:59.578380 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8770355224
I1001 15:22:59.583585 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8837464088
I1001 15:22:59.588773 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8837472280
I1001 15:22:59.593832 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8837480472
I1001 15:22:59.599558 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8904589336
I1001 15:22:59.604614 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8971698200
I1001 15:22:59.609766 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8971706392
I1001 15:22:59.614869 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8971714584
I1001 15:22:59.620035 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8971722776
I1001 15:22:59.625095 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8971730968
I1001 15:22:59.629039 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8971731480
I1001 15:22:59.632831 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8971731992
I1001 15:22:59.637794 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8988509208
I1001 15:22:59.642999 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9005286424
I1001 15:22:59.648149 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9005294616
I1001 15:22:59.653206 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9005302808
I1001 15:22:59.658366 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9022080024
I1001 15:22:59.663450 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9038857240
I1001 15:22:59.668602 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9038865432
I1001 15:22:59.673648 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9038873624
I1001 15:22:59.678811 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9055650840
I1001 15:22:59.683876 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9072428056
I1001 15:22:59.689025 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9072436248
I1001 15:22:59.694195 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9072444440
I1001 15:22:59.699352 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9089221656
I1001 15:22:59.704511 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9105998872
I1001 15:22:59.709565 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9106007064
I1001 15:22:59.715191 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9106015256
I1001 15:22:59.720221 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9106023448
I1001 15:22:59.725388 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9106031640
I1001 15:22:59.730442 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9106039832
I1001 15:22:59.735610 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9106048024
I1001 15:22:59.740661 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9106080792
I1001 15:22:59.745826 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9106113560
I1001 15:22:59.750873 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9173222424
I1001 15:22:59.756041 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9240331288
I1001 15:22:59.761196 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9240339480
I1001 15:22:59.766250 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9240347672
I1001 15:22:59.771445 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9307456536
I1001 15:22:59.776510 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9374565400
I1001 15:22:59.781653 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9374573592
I1001 15:22:59.786726 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9374581784
I1001 15:22:59.791862 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9374589976
I1001 15:22:59.796940 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9374598168
I1001 15:22:59.800956 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9374598680
I1001 15:22:59.804752 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9374599192
I1001 15:22:59.809700 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9391376408
I1001 15:22:59.814906 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9408153624
I1001 15:22:59.820067 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9408161816
I1001 15:22:59.825136 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9408170008
I1001 15:22:59.830773 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9424947224
I1001 15:22:59.835843 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9441724440
I1001 15:22:59.841009 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9441732632
I1001 15:22:59.846082 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9441740824
I1001 15:22:59.851335 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9458518040
I1001 15:22:59.856388 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9475295256
I1001 15:22:59.861566 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9475303448
I1001 15:22:59.866736 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9475311640
I1001 15:22:59.871816 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9492088856
I1001 15:22:59.877035 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9508866072
I1001 15:22:59.882101 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9508874264
I1001 15:22:59.887284 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9508882456
I1001 15:22:59.892318 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9508890648
I1001 15:22:59.897488 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9508898840
I1001 15:22:59.902768 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9508907032
I1001 15:22:59.907940 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9508915224
I1001 15:22:59.912998 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9508947992
I1001 15:22:59.918142 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9508980760
I1001 15:22:59.923227 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9576089624
I1001 15:22:59.928392 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9643198488
I1001 15:22:59.933565 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9643206680
I1001 15:22:59.938646 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9643214872
I1001 15:22:59.944285 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9710323736
I1001 15:22:59.949346 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9777432600
I1001 15:22:59.954504 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9777440792
I1001 15:22:59.959576 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9777448984
I1001 15:22:59.964770 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9777457176
I1001 15:22:59.969837 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9777465368
I1001 15:22:59.973811 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9777465880
I1001 15:22:59.977570 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9777466392
I1001 15:22:59.982579 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9794243608
I1001 15:22:59.987754 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9811020824
I1001 15:22:59.992924 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9811029016
I1001 15:22:59.998023 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9811037208
I1001 15:23:00.003201 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9827814424
I1001 15:23:00.008346 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9844591640
I1001 15:23:00.013515 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9844599832
I1001 15:23:00.018617 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9844608024
I1001 15:23:00.023779 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9861385240
I1001 15:23:00.028880 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9878162456
I1001 15:23:00.034054 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9878170648
I1001 15:23:00.039221 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9878178840
I1001 15:23:00.044304 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9894956056
I1001 15:23:00.049480 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9911733272
I1001 15:23:00.054562 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9911741464
I1001 15:23:00.060259 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9911749656
I1001 15:23:00.065322 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9911757848
I1001 15:23:00.070508 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9911766040
I1001 15:23:00.075597 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9911774232
I1001 15:23:00.080753 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9911782424
I1001 15:23:00.085835 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9911815192
I1001 15:23:00.091020 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9911847960
I1001 15:23:00.096072 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9978956824
I1001 15:23:00.101353 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10046065688
I1001 15:23:00.106518 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10046073880
I1001 15:23:00.111614 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10046082072
I1001 15:23:00.116805 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10113190936
I1001 15:23:00.121887 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10180299800
I1001 15:23:00.127093 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10180307992
I1001 15:23:00.132173 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10180316184
I1001 15:23:00.137349 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10180324376
I1001 15:23:00.142438 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10180332568
I1001 15:23:00.146416 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10180333080
I1001 15:23:00.150238 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10180333592
I1001 15:23:00.155275 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10197110808
I1001 15:23:00.160471 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10213888024
I1001 15:23:00.165644 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10213896216
I1001 15:23:00.170774 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10213904408
I1001 15:23:00.176391 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10230681624
I1001 15:23:00.181459 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10247458840
I1001 15:23:00.186656 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10247467032
I1001 15:23:00.191934 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10247475224
I1001 15:23:00.197115 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10264252440
I1001 15:23:00.202199 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10281029656
I1001 15:23:00.207437 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10281037848
I1001 15:23:00.212604 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10281046040
I1001 15:23:00.217713 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10297823256
I1001 15:23:00.222947 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10314600472
I1001 15:23:00.228015 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10314608664
I1001 15:23:00.233184 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10314616856
I1001 15:23:00.238268 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10314625048
I1001 15:23:00.243523 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10314633240
I1001 15:23:00.248645 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10314641432
I1001 15:23:00.253809 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10314649624
I1001 15:23:00.258924 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10314682392
I1001 15:23:00.264097 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10314715160
I1001 15:23:00.269186 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10381824024
I1001 15:23:00.274372 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10448932888
I1001 15:23:00.279587 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10448941080
I1001 15:23:00.284675 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10448949272
I1001 15:23:00.290355 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10516058136
I1001 15:23:00.295468 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10583167000
I1001 15:23:00.300791 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10583175192
I1001 15:23:00.305867 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10583183384
I1001 15:23:00.311073 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10583191576
I1001 15:23:00.316167 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10583199768
I1001 15:23:00.320162 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10583200280
I1001 15:23:00.324005 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10583200792
I1001 15:23:00.329049 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10599978008
I1001 15:23:00.334258 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10616755224
I1001 15:23:00.339519 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10616763416
I1001 15:23:00.344593 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10616771608
I1001 15:23:00.349835 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10633548824
I1001 15:23:00.354967 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10650326040
I1001 15:23:00.360160 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10650334232
I1001 15:23:00.365241 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10650342424
I1001 15:23:00.370436 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10667119640
I1001 15:23:00.375580 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10683896856
I1001 15:23:00.380766 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10683905048
I1001 15:23:00.385955 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10683913240
I1001 15:23:00.391078 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10700690456
I1001 15:23:00.396292 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10717467672
I1001 15:23:00.401462 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10717475864
I1001 15:23:00.407142 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10717484056
I1001 15:23:00.412227 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10717492248
I1001 15:23:00.417426 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10717500440
I1001 15:23:00.422520 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10717508632
I1001 15:23:00.427716 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10717516824
I1001 15:23:00.432809 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10717549592
I1001 15:23:00.437975 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10717582360
I1001 15:23:00.443070 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10784691224
I1001 15:23:00.448294 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10851800088
I1001 15:23:00.453457 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10851808280
I1001 15:23:00.458655 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10851816472
I1001 15:23:00.463833 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10918925336
I1001 15:23:00.468915 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10986034200
I1001 15:23:00.474094 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10986042392
I1001 15:23:00.479220 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10986050584
I1001 15:23:00.484397 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10986058776
I1001 15:23:00.489488 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10986066968
I1001 15:23:00.493469 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10986067480
I1001 15:23:00.497293 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10986067992
I1001 15:23:00.502362 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11002845208
I1001 15:23:00.507626 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11019622424
I1001 15:23:00.512792 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11019630616
I1001 15:23:00.517879 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11019638808
I1001 15:23:00.523537 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11036416024
I1001 15:23:00.528634 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11053193240
I1001 15:23:00.533840 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11053201432
I1001 15:23:00.538953 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11053209624
I1001 15:23:00.544138 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11069986840
I1001 15:23:00.549244 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11086764056
I1001 15:23:00.554474 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11086772248
I1001 15:23:00.559676 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11086780440
I1001 15:23:00.564791 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11103557656
I1001 15:23:00.570014 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11120334872
I1001 15:23:00.575131 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11120343064
I1001 15:23:00.580331 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11120351256
I1001 15:23:00.585433 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11120359448
I1001 15:23:00.590643 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11120367640
I1001 15:23:00.595765 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11120375832
I1001 15:23:00.600969 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11120384024
I1001 15:23:00.606069 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11120416792
I1001 15:23:00.611321 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11120449560
I1001 15:23:00.616423 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11187558424
I1001 15:23:00.621599 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11254667288
I1001 15:23:00.626827 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11254675480
I1001 15:23:00.631903 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11254683672
I1001 15:23:00.637586 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11321792536
I1001 15:23:00.642708 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11388901400
I1001 15:23:00.647898 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11388909592
I1001 15:23:00.652996 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11388917784
I1001 15:23:00.658190 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11388925976
I1001 15:23:00.663372 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11388934168
I1001 15:23:00.667404 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11388934680
I1001 15:23:00.671224 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11388935192
I1001 15:23:00.676235 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11405712408
I1001 15:23:00.681438 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11422489624
I1001 15:23:00.686682 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11422497816
I1001 15:23:00.691815 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11422506008
I1001 15:23:00.697016 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11439283224
I1001 15:23:00.702194 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11456060440
I1001 15:23:00.707405 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11456068632
I1001 15:23:00.712498 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11456076824
I1001 15:23:00.717725 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11472854040
I1001 15:23:00.722862 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11489631256
I1001 15:23:00.728069 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11489639448
I1001 15:23:00.733243 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11489647640
I1001 15:23:00.738355 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11506424856
I1001 15:23:00.743580 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11523202072
I1001 15:23:00.748687 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11523210264
I1001 15:23:00.754379 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11523218456
I1001 15:23:00.759513 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11523226648
I1001 15:23:00.764748 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11523234840
I1001 15:23:00.769889 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11523243032
I1001 15:23:00.775128 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11523251224
I1001 15:23:00.780228 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11523283992
I1001 15:23:00.785416 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11523316760
I1001 15:23:00.790525 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11590425624
I1001 15:23:00.795918 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11657534488
I1001 15:23:00.801121 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11657542680
I1001 15:23:00.806209 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11657550872
I1001 15:23:00.811471 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11724659736
I1001 15:23:00.816580 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11791768600
I1001 15:23:00.821796 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11791776792
I1001 15:23:00.826900 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11791784984
I1001 15:23:00.832110 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11791793176
I1001 15:23:00.837232 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11791801368
I1001 15:23:00.841249 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11791801880
I1001 15:23:00.845056 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11791802392
I1001 15:23:00.850115 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11808579608
I1001 15:23:00.855402 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11825356824
I1001 15:23:00.860615 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11825365016
I1001 15:23:00.865720 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11825373208
I1001 15:23:00.871472 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11842150424
I1001 15:23:00.876581 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11858927640
I1001 15:23:00.881777 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11858935832
I1001 15:23:00.886893 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11858944024
I1001 15:23:00.892092 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11875721240
I1001 15:23:00.897248 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11892498456
I1001 15:23:00.902701 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11892506648
I1001 15:23:00.907923 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11892514840
I1001 15:23:00.913005 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11909292056
I1001 15:23:00.918219 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11926069272
I1001 15:23:00.923330 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11926077464
I1001 15:23:00.928550 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11926085656
I1001 15:23:00.933661 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11926093848
I1001 15:23:00.939002 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11926102040
I1001 15:23:00.944147 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11926110232
I1001 15:23:00.949367 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11926118424
I1001 15:23:00.954486 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11926151192
I1001 15:23:00.959697 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11926183960
I1001 15:23:00.964810 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11993292824
I1001 15:23:00.970022 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12060401688
I1001 15:23:00.975232 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12060409880
I1001 15:23:00.980475 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12060418072
I1001 15:23:00.986170 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12127526936
I1001 15:23:00.991314 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12194635800
I1001 15:23:00.996539 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12194643992
I1001 15:23:01.001663 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12194652184
I1001 15:23:01.006885 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12194660376
I1001 15:23:01.011988 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12194668568
I1001 15:23:01.016027 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12194669080
I1001 15:23:01.019858 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12194669592
I1001 15:23:01.024885 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12211446808
I1001 15:23:01.030119 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12228224024
I1001 15:23:01.035421 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12228232216
I1001 15:23:01.040537 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12228240408
I1001 15:23:01.045749 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12245017624
I1001 15:23:01.050893 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12261794840
I1001 15:23:01.056097 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12261803032
I1001 15:23:01.061238 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12261811224
I1001 15:23:01.066447 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12278588440
I1001 15:23:01.071622 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12295365656
I1001 15:23:01.076820 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12295373848
I1001 15:23:01.082047 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12295382040
I1001 15:23:01.087183 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12312159256
I1001 15:23:01.092394 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12328936472
I1001 15:23:01.097520 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12328944664
I1001 15:23:01.103368 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12328952856
I1001 15:23:01.108505 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12328961048
I1001 15:23:01.113736 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12328969240
I1001 15:23:01.118893 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12328977432
I1001 15:23:01.124083 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12328985624
I1001 15:23:01.129194 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12329018392
I1001 15:23:01.134400 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12329051160
I1001 15:23:01.139580 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12396160024
I1001 15:23:01.144808 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12463268888
I1001 15:23:01.150002 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12463277080
I1001 15:23:01.155160 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12463285272
I1001 15:23:01.160374 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12530394136
I1001 15:23:01.165529 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12597503000
I1001 15:23:01.170794 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12597511192
I1001 15:23:01.175913 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12597519384
I1001 15:23:01.181114 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12597527576
I1001 15:23:01.186226 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12597535768
I1001 15:23:01.190249 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12597536280
I1001 15:23:01.194186 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12597536792
I1001 15:23:01.199234 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12614314008
I1001 15:23:01.204472 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12631091224
I1001 15:23:01.209682 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12631099416
I1001 15:23:01.214832 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12631107608
I1001 15:23:01.220518 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12647884824
I1001 15:23:01.225634 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12664662040
I1001 15:23:01.230916 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12664670232
I1001 15:23:01.236029 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12664678424
I1001 15:23:01.241253 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12681455640
I1001 15:23:01.246393 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12698232856
I1001 15:23:01.251640 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12698241048
I1001 15:23:01.256826 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12698249240
I1001 15:23:01.261907 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12715026456
I1001 15:23:01.267196 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12731803672
I1001 15:23:01.272360 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12731811864
I1001 15:23:01.277587 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12731820056
I1001 15:23:01.282747 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12731828248
I1001 15:23:01.288012 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12731836440
I1001 15:23:01.293167 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12731844632
I1001 15:23:01.298377 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12731852824
I1001 15:23:01.303593 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12731885592
I1001 15:23:01.308801 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12731918360
I1001 15:23:01.313905 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12799027224
I1001 15:23:01.319187 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12866136088
I1001 15:23:01.324394 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12866144280
I1001 15:23:01.329510 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12866152472
I1001 15:23:01.335227 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12933261336
I1001 15:23:01.340335 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13000370200
I1001 15:23:01.345573 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13000378392
I1001 15:23:01.350727 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13000386584
I1001 15:23:01.355969 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13000394776
I1001 15:23:01.361083 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13000402968
I1001 15:23:01.365118 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13000403480
I1001 15:23:01.368986 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13000403992
I1001 15:23:01.374095 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13017181208
I1001 15:23:01.379431 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13033958424
I1001 15:23:01.384675 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13033966616
I1001 15:23:01.389835 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13033974808
I1001 15:23:01.395127 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13050752024
I1001 15:23:01.400421 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13067529240
I1001 15:23:01.405657 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13067537432
I1001 15:23:01.410821 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13067545624
I1001 15:23:01.416038 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13084322840
I1001 15:23:01.421213 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13101100056
I1001 15:23:01.426471 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13101108248
I1001 15:23:01.431748 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13101116440
I1001 15:23:01.436890 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13117893656
I1001 15:23:01.442154 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13134670872
I1001 15:23:01.447353 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13134679064
I1001 15:23:01.453194 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13134687256
I1001 15:23:01.458317 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13134695448
I1001 15:23:01.463594 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13134703640
I1001 15:23:01.468768 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13134711832
I1001 15:23:01.474011 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13134720024
I1001 15:23:01.479181 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13134752792
I1001 15:23:01.484411 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13134785560
I1001 15:23:01.489523 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13201894424
I1001 15:23:01.494818 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13269003288
I1001 15:23:01.500139 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13269011480
I1001 15:23:01.505359 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13269019672
I1001 15:23:01.510620 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13336128536
I1001 15:23:01.515787 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13403237400
I1001 15:23:01.521021 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13403245592
I1001 15:23:01.526169 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13403253784
I1001 15:23:01.531458 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13403261976
I1001 15:23:01.536589 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13403270168
I1001 15:23:01.540644 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13403270680
I1001 15:23:01.544516 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13403271192
I1001 15:23:01.549549 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13420048408
I1001 15:23:01.554851 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13436825624
I1001 15:23:01.560123 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13436833816
I1001 15:23:01.565277 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13436842008
I1001 15:23:01.571056 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13453619224
I1001 15:23:01.576196 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13470396440
I1001 15:23:01.581436 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13470404632
I1001 15:23:01.586592 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13470412824
I1001 15:23:01.591847 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13487190040
I1001 15:23:01.596997 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13503967256
I1001 15:23:01.602310 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13503975448
I1001 15:23:01.607583 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13503983640
I1001 15:23:01.612742 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13520760856
I1001 15:23:01.617998 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13537538072
I1001 15:23:01.623189 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13537546264
I1001 15:23:01.628425 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13537554456
I1001 15:23:01.633557 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13537562648
I1001 15:23:01.638845 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13537570840
I1001 15:23:01.643980 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13537579032
I1001 15:23:01.649225 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13537587224
I1001 15:23:01.654352 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13537619992
I1001 15:23:01.659625 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13537652760
I1001 15:23:01.664757 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13604761624
I1001 15:23:01.670004 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13671870488
I1001 15:23:01.675270 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13671878680
I1001 15:23:01.680422 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13671886872
I1001 15:23:01.686173 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13738995736
I1001 15:23:01.691363 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13806104600
I1001 15:23:01.696602 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13806112792
I1001 15:23:01.701817 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13806120984
I1001 15:23:01.707078 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13806129176
I1001 15:23:01.712219 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13806137368
I1001 15:23:01.716278 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13806137880
I1001 15:23:01.720142 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13806138392
I1001 15:23:01.725203 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13822915608
I1001 15:23:01.730466 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13839692824
I1001 15:23:01.735732 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13839701016
I1001 15:23:01.740866 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13839709208
I1001 15:23:01.746141 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13856486424
I1001 15:23:01.751312 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13873263640
I1001 15:23:01.756578 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13873271832
I1001 15:23:01.761716 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13873280024
I1001 15:23:01.766995 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13890057240
I1001 15:23:01.772145 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13906834456
I1001 15:23:01.777396 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13906842648
I1001 15:23:01.782676 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13906850840
I1001 15:23:01.787816 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13923628056
I1001 15:23:01.793076 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13940405272
I1001 15:23:01.798268 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13940413464
I1001 15:23:01.804012 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13940421656
I1001 15:23:01.809135 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13940429848
I1001 15:23:01.814387 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13940438040
I1001 15:23:01.819561 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13940446232
I1001 15:23:01.824792 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13940454424
I1001 15:23:01.829933 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13940487192
I1001 15:23:01.835193 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13940519960
I1001 15:23:01.840342 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14007628824
I1001 15:23:01.845566 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14074737688
I1001 15:23:01.850823 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14074745880
I1001 15:23:01.856008 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14074754072
I1001 15:23:01.861263 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14141862936
I1001 15:23:01.866427 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14208971800
I1001 15:23:01.871706 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14208979992
I1001 15:23:01.876875 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14208988184
I1001 15:23:01.882100 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14208996376
I1001 15:23:01.887282 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14209004568
I1001 15:23:01.891338 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14209005080
I1001 15:23:01.895237 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14209005592
I1001 15:23:01.900341 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14225782808
I1001 15:23:01.905806 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14242560024
I1001 15:23:01.911103 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14242568216
I1001 15:23:01.916261 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14242576408
I1001 15:23:01.922004 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14259353624
I1001 15:23:01.927218 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14276130840
I1001 15:23:01.932533 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14276139032
I1001 15:23:01.937713 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14276147224
I1001 15:23:01.942988 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14292924440
I1001 15:23:01.948147 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14309701656
I1001 15:23:01.953398 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14309709848
I1001 15:23:01.958695 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14309718040
I1001 15:23:01.963850 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14326495256
I1001 15:23:01.969129 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14343272472
I1001 15:23:01.974300 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14343280664
I1001 15:23:01.979638 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14343288856
I1001 15:23:01.984771 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14343297048
I1001 15:23:01.990024 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14343305240
I1001 15:23:01.995199 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14343313432
I1001 15:23:02.000454 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14343321624
I1001 15:23:02.005616 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14343354392
I1001 15:23:02.010915 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14343387160
I1001 15:23:02.016077 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14410496024
I1001 15:23:02.021333 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14477604888
I1001 15:23:02.026596 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14477613080
I1001 15:23:02.031737 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14477621272
I1001 15:23:02.037520 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14544730136
I1001 15:23:02.042720 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14611839000
I1001 15:23:02.047998 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14611847192
I1001 15:23:02.053181 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14611855384
I1001 15:23:02.058419 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14611863576
I1001 15:23:02.063638 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14611871768
I1001 15:23:02.067699 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14611872280
I1001 15:23:02.071577 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14611872792
I1001 15:23:02.076659 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14628650008
I1001 15:23:02.081921 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14645427224
I1001 15:23:02.087281 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14645435416
I1001 15:23:02.092436 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14645443608
I1001 15:23:02.097699 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14662220824
I1001 15:23:02.103054 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14678998040
I1001 15:23:02.108306 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14679006232
I1001 15:23:02.113460 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14679014424
I1001 15:23:02.118709 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14695791640
I1001 15:23:02.123877 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14712568856
I1001 15:23:02.129165 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14712577048
I1001 15:23:02.134433 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14712585240
I1001 15:23:02.139643 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14729362456
I1001 15:23:02.144914 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14746139672
I1001 15:23:02.150076 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14746147864
I1001 15:23:02.155829 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14746156056
I1001 15:23:02.160964 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14746164248
I1001 15:23:02.166328 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14746172440
I1001 15:23:02.171510 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14746180632
I1001 15:23:02.176768 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14746188824
I1001 15:23:02.181937 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14746221592
I1001 15:23:02.187221 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14746254360
I1001 15:23:02.192557 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14813363224
I1001 15:23:02.197839 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14880472088
I1001 15:23:02.203191 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14880480280
I1001 15:23:02.208350 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14880488472
I1001 15:23:02.213624 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14947597336
I1001 15:23:02.218853 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15014706200
I1001 15:23:02.224114 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15014714392
I1001 15:23:02.229287 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15014722584
I1001 15:23:02.234527 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15014730776
I1001 15:23:02.239746 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15014738968
I1001 15:23:02.243888 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15014739480
I1001 15:23:02.247801 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15014739992
I1001 15:23:02.252869 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15031517208
I1001 15:23:02.258153 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15048294424
I1001 15:23:02.263458 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15048302616
I1001 15:23:02.268608 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15048310808
I1001 15:23:02.274407 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15065088024
I1001 15:23:02.279666 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15081865240
I1001 15:23:02.284938 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15081873432
I1001 15:23:02.290117 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15081881624
I1001 15:23:02.295395 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15098658840
I1001 15:23:02.300705 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15115436056
I1001 15:23:02.305955 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15115444248
I1001 15:23:02.311324 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15115452440
I1001 15:23:02.316496 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15132229656
I1001 15:23:02.321751 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15149006872
I1001 15:23:02.326951 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15149015064
I1001 15:23:02.332205 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15149023256
I1001 15:23:02.337341 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15149031448
I1001 15:23:02.342683 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15149039640
I1001 15:23:02.347866 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15149047832
I1001 15:23:02.353168 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15149056024
I1001 15:23:02.358309 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15149088792
I1001 15:23:02.363588 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15149121560
I1001 15:23:02.368745 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15216230424
I1001 15:23:02.374024 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15283339288
I1001 15:23:02.379324 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15283347480
I1001 15:23:02.384496 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15283355672
I1001 15:23:02.390231 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15350464536
I1001 15:23:02.395438 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15417573400
I1001 15:23:02.400739 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15417581592
I1001 15:23:02.405990 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15417589784
I1001 15:23:02.411281 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15417597976
I1001 15:23:02.416461 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15417606168
I1001 15:23:02.420533 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15417606680
I1001 15:23:02.424426 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15417607192
I1001 15:23:02.429476 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15434384408
I1001 15:23:02.434790 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15451161624
I1001 15:23:02.440081 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15451169816
I1001 15:23:02.445267 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15451178008
I1001 15:23:02.450517 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15467955224
I1001 15:23:02.455730 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15484732440
I1001 15:23:02.461000 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15484740632
I1001 15:23:02.466149 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15484748824
I1001 15:23:02.471427 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15501526040
I1001 15:23:02.476598 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15518303256
I1001 15:23:02.481874 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15518311448
I1001 15:23:02.487225 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15518319640
I1001 15:23:02.492411 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15535096856
I1001 15:23:02.497682 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15551874072
I1001 15:23:02.502982 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15551882264
I1001 15:23:02.508702 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15551890456
I1001 15:23:02.513888 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15551898648
I1001 15:23:02.519177 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15551906840
I1001 15:23:02.524365 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15551915032
I1001 15:23:02.529643 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15551923224
I1001 15:23:02.534838 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15551955992
I1001 15:23:02.540130 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15551988760
I1001 15:23:02.545321 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15619097624
I1001 15:23:02.550656 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15686206488
I1001 15:23:02.555925 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15686214680
I1001 15:23:02.561079 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15686222872
I1001 15:23:02.566359 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15753331736
I1001 15:23:02.571556 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15820440600
I1001 15:23:02.576815 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15820448792
I1001 15:23:02.582015 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15820456984
I1001 15:23:02.587298 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15820465176
I1001 15:23:02.592468 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15820473368
I1001 15:23:02.596549 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15820473880
I1001 15:23:02.600434 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15820474392
I1001 15:23:02.605524 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15837251608
I1001 15:23:02.610848 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15854028824
I1001 15:23:02.616142 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15854037016
I1001 15:23:02.621354 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15854045208
I1001 15:23:02.627138 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15870822424
I1001 15:23:02.632333 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15887599640
I1001 15:23:02.637602 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15887607832
I1001 15:23:02.642827 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15887616024
I1001 15:23:02.648134 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15904393240
I1001 15:23:02.653291 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15921170456
I1001 15:23:02.658612 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15921178648
I1001 15:23:02.663881 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15921186840
I1001 15:23:02.669059 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15937964056
I1001 15:23:02.674366 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15954741272
I1001 15:23:02.679578 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15954749464
I1001 15:23:02.684859 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15954757656
I1001 15:23:02.690011 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15954765848
I1001 15:23:02.695368 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15954774040
I1001 15:23:02.700658 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15954782232
I1001 15:23:02.705945 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15954790424
I1001 15:23:02.711141 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15954823192
I1001 15:23:02.716422 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15954855960
I1001 15:23:02.721614 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16021964824
I1001 15:23:02.726932 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16089073688
I1001 15:23:02.732240 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16089081880
I1001 15:23:02.737402 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16089090072
I1001 15:23:02.743155 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16156198936
I1001 15:23:02.748325 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16223307800
I1001 15:23:02.753613 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16223315992
I1001 15:23:02.758818 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16223324184
I1001 15:23:02.764113 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16223332376
I1001 15:23:02.769294 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16223340568
I1001 15:23:02.773411 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16223341080
I1001 15:23:02.777347 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16223341592
I1001 15:23:02.782486 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16240118808
I1001 15:23:02.787840 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16256896024
I1001 15:23:02.793141 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16256904216
I1001 15:23:02.798315 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16256912408
I1001 15:23:02.803709 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16273689624
I1001 15:23:02.808899 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16290466840
I1001 15:23:02.814190 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16290475032
I1001 15:23:02.819411 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16290483224
I1001 15:23:02.824676 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16307260440
I1001 15:23:02.829893 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16324037656
I1001 15:23:02.835198 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16324045848
I1001 15:23:02.840497 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16324054040
I1001 15:23:02.845664 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16340831256
I1001 15:23:02.851005 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16357608472
I1001 15:23:02.856210 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16357616664
I1001 15:23:02.862018 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16357624856
I1001 15:23:02.867225 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16357633048
I1001 15:23:02.872530 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16357641240
I1001 15:23:02.877722 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16357649432
I1001 15:23:02.883029 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16357657624
I1001 15:23:02.888218 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16357690392
I1001 15:23:02.893498 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16357723160
I1001 15:23:02.898720 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16424832024
I1001 15:23:02.904280 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16491940888
I1001 15:23:02.909554 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16491949080
I1001 15:23:02.914771 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16491957272
I1001 15:23:02.920089 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16559066136
I1001 15:23:02.925263 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16626175000
I1001 15:23:02.930596 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16626183192
I1001 15:23:02.935775 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16626191384
I1001 15:23:02.941075 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16626199576
I1001 15:23:02.946262 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16626207768
I1001 15:23:02.950380 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16626208280
I1001 15:23:02.954274 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16626208792
I1001 15:23:02.959415 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16642986008
I1001 15:23:02.964705 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16659763224
I1001 15:23:02.970013 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16659771416
I1001 15:23:02.975225 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16659779608
I1001 15:23:02.980966 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16676556824
I1001 15:23:02.986144 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16693334040
I1001 15:23:02.991493 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16693342232
I1001 15:23:02.996689 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16693350424
I1001 15:23:03.001966 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16710127640
I1001 15:23:03.007194 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16726904856
I1001 15:23:03.012474 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16726913048
I1001 15:23:03.017737 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16726921240
I1001 15:23:03.022972 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16743698456
I1001 15:23:03.028276 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16760475672
I1001 15:23:03.033453 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16760483864
I1001 15:23:03.038770 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16760492056
I1001 15:23:03.043945 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16760500248
I1001 15:23:03.049225 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16760508440
I1001 15:23:03.054421 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16760516632
I1001 15:23:03.059741 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16760524824
I1001 15:23:03.064915 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16760557592
I1001 15:23:03.070212 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16760590360
I1001 15:23:03.075437 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16827699224
I1001 15:23:03.080779 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16894808088
I1001 15:23:03.086051 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16894816280
I1001 15:23:03.091277 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16894824472
I1001 15:23:03.097026 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16961933336
I1001 15:23:03.102301 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17029042200
I1001 15:23:03.107608 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17029050392
I1001 15:23:03.112816 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17029058584
I1001 15:23:03.118089 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17029066776
I1001 15:23:03.123335 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17029074968
I1001 15:23:03.127443 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17029075480
I1001 15:23:03.131375 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17029075992
I1001 15:23:03.136476 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17045853208
I1001 15:23:03.141788 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17062630424
I1001 15:23:03.147109 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17062638616
I1001 15:23:03.152481 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17062646808
I1001 15:23:03.157791 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17079424024
I1001 15:23:03.163024 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17096201240
I1001 15:23:03.168304 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17096209432
I1001 15:23:03.173508 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17096217624
I1001 15:23:03.178855 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17112994840
I1001 15:23:03.184062 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17129772056
I1001 15:23:03.189349 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17129780248
I1001 15:23:03.194740 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17129788440
I1001 15:23:03.199954 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17146565656
I1001 15:23:03.205260 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17163342872
I1001 15:23:03.210481 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17163351064
I1001 15:23:03.216256 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17163359256
I1001 15:23:03.221434 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17163367448
I1001 15:23:03.226781 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17163375640
I1001 15:23:03.232051 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17163383832
I1001 15:23:03.237328 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17163392024
I1001 15:23:03.242532 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17163424792
I1001 15:23:03.247870 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17163457560
I1001 15:23:03.253088 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17230566424
I1001 15:23:03.258393 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17297675288
I1001 15:23:03.263733 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17297683480
I1001 15:23:03.268922 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17297691672
I1001 15:23:03.274234 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17364800536
I1001 15:23:03.279473 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17431909400
I1001 15:23:03.284773 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17431917592
I1001 15:23:03.289966 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17431925784
I1001 15:23:03.295288 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17431933976
I1001 15:23:03.300529 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17431942168
I1001 15:23:03.304676 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17431942680
I1001 15:23:03.308595 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17431943192
I1001 15:23:03.313712 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17448720408
I1001 15:23:03.319056 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17465497624
I1001 15:23:03.324394 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17465505816
I1001 15:23:03.329581 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17465514008
I1001 15:23:03.335395 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17482291224
I1001 15:23:03.340614 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17499068440
I1001 15:23:03.345917 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17499076632
I1001 15:23:03.351157 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17499084824
I1001 15:23:03.356487 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17515862040
I1001 15:23:03.361714 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17532639256
I1001 15:23:03.367148 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17532647448
I1001 15:23:03.372432 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17532655640
I1001 15:23:03.377667 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17549432856
I1001 15:23:03.382985 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17566210072
I1001 15:23:03.388209 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17566218264
I1001 15:23:03.393621 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17566226456
I1001 15:23:03.398849 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17566234648
I1001 15:23:03.404200 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17566242840
I1001 15:23:03.409425 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17566251032
I1001 15:23:03.414756 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17566259224
I1001 15:23:03.419957 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17566291992
I1001 15:23:03.425271 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17566324760
I1001 15:23:03.430489 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17633433624
I1001 15:23:03.435827 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17700542488
I1001 15:23:03.441129 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17700550680
I1001 15:23:03.446417 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17700558872
I1001 15:23:03.452223 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17767667736
I1001 15:23:03.457439 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17834776600
I1001 15:23:03.462759 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17834784792
I1001 15:23:03.467981 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17834792984
I1001 15:23:03.473272 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17834801176
I1001 15:23:03.478468 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17834809368
I1001 15:23:03.482643 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17834809880
I1001 15:23:03.486585 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17834810392
I1001 15:23:03.491700 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17851587608
I1001 15:23:03.497019 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17868364824
I1001 15:23:03.502432 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17868373016
I1001 15:23:03.507665 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17868381208
I1001 15:23:03.512988 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17885158424
I1001 15:23:03.518218 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17901935640
I1001 15:23:03.523541 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17901943832
I1001 15:23:03.528738 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17901952024
I1001 15:23:03.534034 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17918729240
I1001 15:23:03.539309 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17935506456
I1001 15:23:03.544638 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17935514648
I1001 15:23:03.549919 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17935522840
I1001 15:23:03.555182 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17952300056
I1001 15:23:03.560479 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17969077272
I1001 15:23:03.565712 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17969085464
I1001 15:23:03.571532 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17969093656
I1001 15:23:03.576704 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17969101848
I1001 15:23:03.582051 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17969110040
I1001 15:23:03.587281 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17969118232
I1001 15:23:03.592596 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17969126424
I1001 15:23:03.597799 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17969159192
I1001 15:23:03.603130 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17969191960
I1001 15:23:03.608372 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18036300824
I1001 15:23:03.613703 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18103409688
I1001 15:23:03.619037 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18103417880
I1001 15:23:03.624253 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18103426072
I1001 15:23:03.629559 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18170534936
I1001 15:23:03.634834 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18237643800
I1001 15:23:03.640131 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18237651992
I1001 15:23:03.645372 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18237660184
I1001 15:23:03.650710 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18237668376
I1001 15:23:03.655926 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18237676568
I1001 15:23:03.660135 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18237677080
I1001 15:23:03.664087 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18237677592
I1001 15:23:03.669218 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18254454808
I1001 15:23:03.674558 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18271232024
I1001 15:23:03.679886 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18271240216
I1001 15:23:03.685097 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18271248408
I1001 15:23:03.690892 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18288025624
I1001 15:23:03.696123 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18304802840
I1001 15:23:04.242038 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18304811032
I1001 15:23:04.247680 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18304819224
I1001 15:23:04.253011 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18321596440
I1001 15:23:04.258313 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18338373656
I1001 15:23:04.263608 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18338381848
I1001 15:23:04.268921 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18338390040
I1001 15:23:04.274160 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18355167256
I1001 15:23:04.279546 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18371944472
I1001 15:23:04.284765 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18371952664
I1001 15:23:04.290111 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18371960856
I1001 15:23:04.295360 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18371969048
I1001 15:23:04.300741 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18371977240
I1001 15:23:04.306050 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18371985432
I1001 15:23:04.311429 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18371993624
I1001 15:23:04.316763 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18372026392
I1001 15:23:04.321976 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18372059160
I1001 15:23:04.327332 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18439168024
I1001 15:23:04.332556 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18506276888
I1001 15:23:04.337890 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18506285080
I1001 15:23:04.343132 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18506293272
I1001 15:23:04.348474 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18573402136
I1001 15:23:04.353716 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18640511000
I1001 15:23:04.359615 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18640519192
I1001 15:23:04.364836 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18640527384
I1001 15:23:04.370171 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18640535576
I1001 15:23:04.375544 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18640543768
I1001 15:23:04.379602 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18640544280
I1001 15:23:04.383561 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18640544792
I1001 15:23:04.388892 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18657322008
I1001 15:23:04.394143 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18674099224
I1001 15:23:04.399499 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18674107416
I1001 15:23:04.404757 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18674115608
I1001 15:23:04.410094 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18690892824
I1001 15:23:04.415461 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18707670040
I1001 15:23:04.420670 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18707678232
I1001 15:23:04.426005 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18707686424
I1001 15:23:04.431237 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18724463640
I1001 15:23:04.436599 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18741240856
I1001 15:23:04.441835 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18741249048
I1001 15:23:04.447204 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18741257240
I1001 15:23:04.452447 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18758034456
I1001 15:23:04.457792 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18774811672
I1001 15:23:04.463038 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18774819864
I1001 15:23:04.468389 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18774828056
I1001 15:23:04.474188 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18774836248
I1001 15:23:04.479473 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18774844440
I1001 15:23:04.484798 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18774852632
I1001 15:23:04.490030 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18774860824
I1001 15:23:04.495386 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18774893592
I1001 15:23:04.500644 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18774926360
I1001 15:23:04.506050 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18842035224
I1001 15:23:04.511303 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18909144088
I1001 15:23:04.516645 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18909152280
I1001 15:23:04.521877 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18909160472
I1001 15:23:04.527213 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18976269336
I1001 15:23:04.532433 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19043378200
I1001 15:23:04.537755 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19043386392
I1001 15:23:04.543105 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19043394584
I1001 15:23:04.548356 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19043402776
I1001 15:23:04.553700 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19043410968
I1001 15:23:04.557766 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19043411480
I1001 15:23:04.561706 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19043411992
I1001 15:23:04.566969 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19060189208
I1001 15:23:04.572217 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19076966424
I1001 15:23:04.577538 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19076974616
I1001 15:23:04.582863 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19076982808
I1001 15:23:04.588089 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19093760024
I1001 15:23:04.593945 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19110537240
I1001 15:23:04.599199 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19110545432
I1001 15:23:04.604531 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19110553624
I1001 15:23:04.609798 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19127330840
I1001 15:23:04.615183 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19144108056
I1001 15:23:04.620447 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19144116248
I1001 15:23:04.625767 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19144124440
I1001 15:23:04.631036 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19160901656
I1001 15:23:04.636372 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19177678872
I1001 15:23:04.641707 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19177687064
I1001 15:23:04.646981 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19177695256
I1001 15:23:04.652304 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19177703448
I1001 15:23:04.657579 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19177711640
I1001 15:23:04.662976 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19177719832
I1001 15:23:04.668259 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19177728024
I1001 15:23:04.673602 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19177760792
I1001 15:23:04.678872 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19177793560
I1001 15:23:04.684313 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19244902424
I1001 15:23:04.689562 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19312011288
I1001 15:23:04.694949 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19312019480
I1001 15:23:04.700293 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19312027672
I1001 15:23:04.705657 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19379136536
I1001 15:23:04.711559 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19446245400
I1001 15:23:04.716797 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19446253592
I1001 15:23:04.722146 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19446261784
I1001 15:23:04.727426 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19446269976
I1001 15:23:04.732758 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19446278168
I1001 15:23:04.736868 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19446278680
I1001 15:23:04.740871 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19446279192
I1001 15:23:04.746212 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19463056408
I1001 15:23:04.751584 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19479833624
I1001 15:23:04.756819 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19479841816
I1001 15:23:04.762152 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19479850008
I1001 15:23:04.767434 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19496627224
I1001 15:23:04.772787 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19513404440
I1001 15:23:04.778030 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19513412632
I1001 15:23:04.783408 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19513420824
I1001 15:23:04.788637 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19530198040
I1001 15:23:04.794008 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19546975256
I1001 15:23:04.799269 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19546983448
I1001 15:23:04.804609 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19546991640
I1001 15:23:04.809952 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19563768856
I1001 15:23:04.815243 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19580546072
I1001 15:23:04.820621 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19580554264
I1001 15:23:04.825888 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19580562456
I1001 15:23:04.831768 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19580570648
I1001 15:23:04.837056 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19580578840
I1001 15:23:04.842401 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19580587032
I1001 15:23:04.847692 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19580595224
I1001 15:23:04.853032 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19580627992
I1001 15:23:04.858323 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19580660760
I1001 15:23:04.863691 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19647769624
I1001 15:23:04.868958 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19714878488
I1001 15:23:04.874380 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19714886680
I1001 15:23:04.879767 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19714894872
I1001 15:23:04.885001 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19782003736
I1001 15:23:04.890398 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19849112600
I1001 15:23:04.895674 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19849120792
I1001 15:23:04.901133 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19849128984
I1001 15:23:04.906519 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19849137176
I1001 15:23:04.911906 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19849145368
I1001 15:23:04.915965 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19849145880
I1001 15:23:04.919955 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19849146392
I1001 15:23:04.925273 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19865923608
I1001 15:23:04.930664 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19882700824
I1001 15:23:04.935929 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19882709016
I1001 15:23:04.941294 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19882717208
I1001 15:23:04.946569 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19899494424
I1001 15:23:04.952474 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19916271640
I1001 15:23:04.957701 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19916279832
I1001 15:23:04.963113 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19916288024
I1001 15:23:04.968343 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19933065240
I1001 15:23:04.973700 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19949842456
I1001 15:23:04.979006 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19949850648
I1001 15:23:04.984342 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19949858840
I1001 15:23:04.989687 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19966636056
I1001 15:23:04.994976 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19983413272
I1001 15:23:05.000323 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19983421464
I1001 15:23:05.005564 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19983429656
I1001 15:23:05.010916 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19983437848
I1001 15:23:05.016217 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19983446040
I1001 15:23:05.021593 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19983454232
I1001 15:23:05.026847 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19983462424
I1001 15:23:05.032212 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19983495192
I1001 15:23:05.037473 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19983527960
I1001 15:23:05.042824 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20050636824
I1001 15:23:05.048077 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20117745688
I1001 15:23:05.053426 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20117753880
I1001 15:23:05.058829 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20117762072
I1001 15:23:05.064090 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20184870936
I1001 15:23:05.069910 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20251979800
I1001 15:23:05.075190 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20251987992
I1001 15:23:05.080548 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20251996184
I1001 15:23:05.085764 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20252004376
I1001 15:23:05.091190 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20252012568
I1001 15:23:05.095279 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20252013080
I1001 15:23:05.099242 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20252013592
I1001 15:23:05.104614 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20268790808
I1001 15:23:05.110023 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20285568024
I1001 15:23:05.115384 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20285576216
I1001 15:23:05.120729 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20285584408
I1001 15:23:05.125984 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20302361624
I1001 15:23:05.131375 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20319138840
I1001 15:23:05.136614 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20319147032
I1001 15:23:05.141976 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20319155224
I1001 15:23:05.147248 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20335932440
I1001 15:23:05.152615 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20352709656
I1001 15:23:05.157871 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20352717848
I1001 15:23:05.163230 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20352726040
I1001 15:23:05.168643 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20369503256
I1001 15:23:05.173908 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386280472
I1001 15:23:05.179317 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386288664
I1001 15:23:05.184555 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386296856
I1001 15:23:05.190341 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386305048
I1001 15:23:05.195748 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386313240
I1001 15:23:05.201146 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386321432
I1001 15:23:05.206422 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386329624
I1001 15:23:05.211825 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386337624
I1001 15:23:05.217081 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386345624
I1001 15:23:05.222498 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386353624
I1001 15:23:05.227796 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386361624
I1001 15:23:05.233146 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386369624
I1001 15:23:05.238504 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386377624
I1001 15:23:05.243803 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386385624
I1001 15:23:05.249185 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386393624
I1001 15:23:05.254423 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386401624
I1001 15:23:05.259805 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386409624
I1001 15:23:05.265048 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386417624
I1001 15:23:05.270397 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386425624
I1001 15:23:05.275677 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386433624
I1001 15:23:05.281059 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386441624
I1001 15:23:05.286330 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386449624
I1001 15:23:05.291729 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386457624
I1001 15:23:05.296982 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386465624
I1001 15:23:05.302397 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386473624
I1001 15:23:05.308223 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386481624
I1001 15:23:05.313507 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386489624
I1001 15:23:05.318912 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386497624
I1001 15:23:05.324157 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386505624
I1001 15:23:05.329527 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386513624
I1001 15:23:05.334863 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386521624
I1001 15:23:05.340213 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386529624
I1001 15:23:05.345484 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386537624
I1001 15:23:05.350866 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386545624
I1001 15:23:05.356137 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386553624
I1001 15:23:05.361490 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386561624
I1001 15:23:05.366786 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386569624
I1001 15:23:05.372144 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386577624
I1001 15:23:05.377502 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386585624
I1001 15:23:05.382798 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20402969624
I1001 15:23:05.388172 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20419353624
I1001 15:23:05.393506 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20435737624
I1001 15:23:05.398877 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20452121624
I1001 15:23:05.404168 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20468505624
I1001 15:23:05.409562 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20484889624
I1001 15:23:05.414868 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20501273624
I1001 15:23:05.420255 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20517657624
I1001 15:23:05.425535 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20534041624
I1001 15:23:05.431413 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20550425624
I1001 15:23:05.436789 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20566809624
I1001 15:23:05.442078 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20583193624
I1001 15:23:05.447496 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20599577624
I1001 15:23:05.452782 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20615961624
I1001 15:23:05.458131 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20632345624
I1001 15:23:05.463463 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20648729624
I1001 15:23:05.468846 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20665113624
I1001 15:23:05.474161 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20681497624
I1001 15:23:05.479562 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20697881624
I1001 15:23:05.484832 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20714265624
I1001 15:23:05.490193 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20730649624
I1001 15:23:05.495597 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20747033624
I1001 15:23:05.500893 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20763417624
I1001 15:23:05.506360 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20779801624
I1001 15:23:05.511666 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20796185624
I1001 15:23:05.517051 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20812569624
I1001 15:23:05.522317 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20828953624
I1001 15:23:05.527724 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20845337624
I1001 15:23:05.533000 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20861721624
I1001 15:23:05.538385 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20878105624
I1001 15:23:05.543684 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20894489624
I1001 15:23:05.549540 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20910873624
I1001 15:23:06.295808 139818304546624 cluster.py:515] Place variable total_nan_gradients/var on /job:local/replica:0/task:0/device:CPU:0 20910873632
I1001 15:23:06.297938 139818304546624 py_utils.py:1389] Creating var total_nan_gradients/var:0 shape=() on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:08.975022 139818304546624 py_utils.py:1474] MODEL ANALYSIS: 
I1001 15:23:08.975181 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.emb.src_token_emb.wm                               (32000, 2048)          65536000 1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var
I1001 15:23:08.975250 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[0].b                  (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var
I1001 15:23:08.975306 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[0].w                  (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var
I1001 15:23:08.975358 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[1].b                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var
I1001 15:23:08.975407 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[1].w                  (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var
I1001 15:23:08.975455 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.fflayer.layer_norm.bias                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var
I1001 15:23:08.975503 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.fflayer.layer_norm.scale                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var
I1001 15:23:08.975551 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.self_atten.atten.atten.per_dim_scale     (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1001 15:23:08.975610 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.self_atten.atten.ctx_post_proj           (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1001 15:23:08.975661 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.self_atten.atten.ctx_post_proj_b         (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1001 15:23:08.975708 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.self_atten.atten.ctx_proj                (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var
I1001 15:23:08.975756 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.self_atten.atten.ctx_proj_b              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1001 15:23:08.975803 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.self_atten.atten.query_proj              (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var
I1001 15:23:08.975850 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.self_atten.atten.query_proj_b            (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var
I1001 15:23:08.975897 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.self_atten.atten.source_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var
I1001 15:23:08.975944 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.self_atten.atten.source_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var
I1001 15:23:08.975991 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.self_atten.layer_norm.bias               (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var
I1001 15:23:08.976038 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.self_atten.layer_norm.scale              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var
I1001 15:23:08.976085 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[0].b                  (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var
I1001 15:23:08.976132 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[0].w                  (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var
I1001 15:23:08.976179 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[1].b                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var
I1001 15:23:08.976226 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[1].w                  (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var
I1001 15:23:08.976272 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.fflayer.layer_norm.bias                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var
I1001 15:23:08.976319 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.fflayer.layer_norm.scale                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var
I1001 15:23:08.976369 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.self_atten.atten.atten.per_dim_scale     (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1001 15:23:08.976417 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.self_atten.atten.ctx_post_proj           (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1001 15:23:08.976464 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.self_atten.atten.ctx_post_proj_b         (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1001 15:23:08.976510 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.self_atten.atten.ctx_proj                (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var
I1001 15:23:08.976556 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.self_atten.atten.ctx_proj_b              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1001 15:23:08.976603 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.self_atten.atten.query_proj              (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var
I1001 15:23:08.976649 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.self_atten.atten.query_proj_b            (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var
I1001 15:23:08.976695 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.self_atten.atten.source_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var
I1001 15:23:08.976741 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.self_atten.atten.source_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var
I1001 15:23:08.976787 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.self_atten.layer_norm.bias               (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var
I1001 15:23:08.976834 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.self_atten.layer_norm.scale              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var
I1001 15:23:08.976880 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[0].b                  (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var
I1001 15:23:08.976926 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[0].w                  (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var
I1001 15:23:08.976972 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[1].b                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var
I1001 15:23:08.977017 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[1].w                  (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var
I1001 15:23:08.977066 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.fflayer.layer_norm.bias                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var
I1001 15:23:08.977113 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.fflayer.layer_norm.scale                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var
I1001 15:23:08.977159 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.self_atten.atten.atten.per_dim_scale     (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1001 15:23:08.977205 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.self_atten.atten.ctx_post_proj           (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1001 15:23:08.977252 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.self_atten.atten.ctx_post_proj_b         (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1001 15:23:08.977298 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.self_atten.atten.ctx_proj                (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var
I1001 15:23:08.977344 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.self_atten.atten.ctx_proj_b              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1001 15:23:08.977390 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.self_atten.atten.query_proj              (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var
I1001 15:23:08.977437 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.self_atten.atten.query_proj_b            (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var
I1001 15:23:08.977483 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.self_atten.atten.source_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var
I1001 15:23:08.977529 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.self_atten.atten.source_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var
I1001 15:23:08.977575 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.self_atten.layer_norm.bias               (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var
I1001 15:23:08.977621 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.self_atten.layer_norm.scale              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var
I1001 15:23:08.977667 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[0].b                  (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var
I1001 15:23:08.977712 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[0].w                  (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var
I1001 15:23:08.977761 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[1].b                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var
I1001 15:23:08.977808 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[1].w                  (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var
I1001 15:23:08.977854 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.fflayer.layer_norm.bias                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var
I1001 15:23:08.977900 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.fflayer.layer_norm.scale                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var
I1001 15:23:08.977945 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.self_atten.atten.atten.per_dim_scale     (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1001 15:23:08.977991 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.self_atten.atten.ctx_post_proj           (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1001 15:23:08.978037 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.self_atten.atten.ctx_post_proj_b         (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1001 15:23:08.978083 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.self_atten.atten.ctx_proj                (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var
I1001 15:23:08.978128 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.self_atten.atten.ctx_proj_b              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1001 15:23:08.978174 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.self_atten.atten.query_proj              (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var
I1001 15:23:08.978220 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.self_atten.atten.query_proj_b            (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var
I1001 15:23:08.978265 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.self_atten.atten.source_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var
I1001 15:23:08.978311 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.self_atten.atten.source_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var
I1001 15:23:08.978356 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.self_atten.layer_norm.bias               (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var
I1001 15:23:08.978402 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.self_atten.layer_norm.scale              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var
I1001 15:23:08.978451 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[0].b                  (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var
I1001 15:23:08.978503 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[0].w                  (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var
I1001 15:23:08.978574 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[1].b                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var
I1001 15:23:08.978626 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[1].w                  (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var
I1001 15:23:08.978672 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.fflayer.layer_norm.bias                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var
I1001 15:23:08.978718 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.fflayer.layer_norm.scale                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var
I1001 15:23:08.978764 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.self_atten.atten.atten.per_dim_scale     (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1001 15:23:08.978810 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.self_atten.atten.ctx_post_proj           (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1001 15:23:08.978857 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.self_atten.atten.ctx_post_proj_b         (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1001 15:23:08.978903 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.self_atten.atten.ctx_proj                (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var
I1001 15:23:08.978949 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.self_atten.atten.ctx_proj_b              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1001 15:23:08.978994 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.self_atten.atten.query_proj              (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var
I1001 15:23:08.979040 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.self_atten.atten.query_proj_b            (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var
I1001 15:23:08.979086 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.self_atten.atten.source_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var
I1001 15:23:08.979131 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.self_atten.atten.source_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var
I1001 15:23:08.979177 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.self_atten.layer_norm.bias               (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var
I1001 15:23:08.979229 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.self_atten.layer_norm.scale              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var
I1001 15:23:08.979276 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[0].b                  (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var
I1001 15:23:08.979322 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[0].w                  (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var
I1001 15:23:08.979367 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[1].b                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var
I1001 15:23:08.979413 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[1].w                  (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var
I1001 15:23:08.979459 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.fflayer.layer_norm.bias                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var
I1001 15:23:08.979505 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.fflayer.layer_norm.scale                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var
I1001 15:23:08.979550 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.self_atten.atten.atten.per_dim_scale     (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1001 15:23:08.979596 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.self_atten.atten.ctx_post_proj           (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1001 15:23:08.979642 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.self_atten.atten.ctx_post_proj_b         (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1001 15:23:08.979688 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.self_atten.atten.ctx_proj                (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var
I1001 15:23:08.979734 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.self_atten.atten.ctx_proj_b              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1001 15:23:08.979780 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.self_atten.atten.query_proj              (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var
I1001 15:23:08.979827 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.self_atten.atten.query_proj_b            (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var
I1001 15:23:08.979872 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.self_atten.atten.source_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var
I1001 15:23:08.979923 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.self_atten.atten.source_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var
I1001 15:23:08.979970 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.self_atten.layer_norm.bias               (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var
I1001 15:23:08.980017 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.self_atten.layer_norm.scale              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var
I1001 15:23:08.980063 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[0].b                  (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var
I1001 15:23:08.980109 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[0].w                  (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var
I1001 15:23:08.980155 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[1].b                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var
I1001 15:23:08.980200 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[1].w                  (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var
I1001 15:23:08.980245 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.fflayer.layer_norm.bias                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var
I1001 15:23:08.980290 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.fflayer.layer_norm.scale                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var
I1001 15:23:08.980336 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.self_atten.atten.atten.per_dim_scale     (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1001 15:23:08.980382 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.self_atten.atten.ctx_post_proj           (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1001 15:23:08.980428 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.self_atten.atten.ctx_post_proj_b         (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1001 15:23:08.980474 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.self_atten.atten.ctx_proj                (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var
I1001 15:23:08.980520 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.self_atten.atten.ctx_proj_b              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1001 15:23:08.980566 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.self_atten.atten.query_proj              (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var
I1001 15:23:08.980612 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.self_atten.atten.query_proj_b            (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var
I1001 15:23:08.980662 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.self_atten.atten.source_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var
I1001 15:23:08.980709 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.self_atten.atten.source_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var
I1001 15:23:08.980756 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.self_atten.layer_norm.bias               (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var
I1001 15:23:08.980802 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.self_atten.layer_norm.scale              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var
I1001 15:23:08.980848 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[0].b                  (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var
I1001 15:23:08.980895 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[0].w                  (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var
I1001 15:23:08.980941 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[1].b                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var
I1001 15:23:08.980987 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[1].w                  (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var
I1001 15:23:08.981032 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.fflayer.layer_norm.bias                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var
I1001 15:23:08.981077 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.fflayer.layer_norm.scale                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var
I1001 15:23:08.981123 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.self_atten.atten.atten.per_dim_scale     (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1001 15:23:08.981169 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.self_atten.atten.ctx_post_proj           (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1001 15:23:08.981215 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.self_atten.atten.ctx_post_proj_b         (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1001 15:23:08.981261 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.self_atten.atten.ctx_proj                (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var
I1001 15:23:08.981307 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.self_atten.atten.ctx_proj_b              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1001 15:23:08.981360 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.self_atten.atten.query_proj              (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var
I1001 15:23:08.981407 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.self_atten.atten.query_proj_b            (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var
I1001 15:23:08.981453 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.self_atten.atten.source_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var
I1001 15:23:08.981499 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.self_atten.atten.source_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var
I1001 15:23:08.981545 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.self_atten.layer_norm.bias               (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var
I1001 15:23:08.981592 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.self_atten.layer_norm.scale              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var
I1001 15:23:08.981638 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var
I1001 15:23:08.981684 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var
I1001 15:23:08.981730 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var
I1001 15:23:08.981776 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var
I1001 15:23:08.981822 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var
I1001 15:23:08.981868 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var
I1001 15:23:08.981914 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1001 15:23:08.981960 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1001 15:23:08.982006 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1001 15:23:08.982052 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var
I1001 15:23:08.982103 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1001 15:23:08.982150 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var
I1001 15:23:08.982197 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var
I1001 15:23:08.982243 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var
I1001 15:23:08.982289 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var
I1001 15:23:08.982336 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var
I1001 15:23:08.982382 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var
I1001 15:23:08.982428 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var
I1001 15:23:08.982474 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var
I1001 15:23:08.982520 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var
I1001 15:23:08.982585 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var
I1001 15:23:08.982634 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var
I1001 15:23:08.982680 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var
I1001 15:23:08.982726 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1001 15:23:08.982772 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1001 15:23:08.982823 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1001 15:23:08.982870 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var
I1001 15:23:08.982916 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1001 15:23:08.982962 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var
I1001 15:23:08.983008 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var
I1001 15:23:08.983054 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var
I1001 15:23:08.983100 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var
I1001 15:23:08.983145 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var
I1001 15:23:08.983191 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var
I1001 15:23:08.983237 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var
I1001 15:23:08.983283 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var
I1001 15:23:08.983328 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var
I1001 15:23:08.983374 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var
I1001 15:23:08.983419 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var
I1001 15:23:08.983464 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var
I1001 15:23:08.983509 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1001 15:23:08.983559 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1001 15:23:08.983606 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1001 15:23:08.983652 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var
I1001 15:23:08.983697 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1001 15:23:08.983743 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var
I1001 15:23:08.983788 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var
I1001 15:23:08.983834 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var
I1001 15:23:08.983880 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var
I1001 15:23:08.983926 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var
I1001 15:23:08.983972 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var
I1001 15:23:08.984019 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var
I1001 15:23:08.984064 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var
I1001 15:23:08.984109 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var
I1001 15:23:08.984155 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var
I1001 15:23:08.984201 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var
I1001 15:23:08.984251 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var
I1001 15:23:08.984297 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1001 15:23:08.984344 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1001 15:23:08.984390 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1001 15:23:08.984436 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var
I1001 15:23:08.984482 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1001 15:23:08.984528 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var
I1001 15:23:08.984573 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var
I1001 15:23:08.984619 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var
I1001 15:23:08.984665 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var
I1001 15:23:08.984711 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var
I1001 15:23:08.984757 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var
I1001 15:23:08.984803 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var
I1001 15:23:08.984849 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var
I1001 15:23:08.984894 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var
I1001 15:23:08.984944 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var
I1001 15:23:08.984990 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var
I1001 15:23:08.985036 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var
I1001 15:23:08.985081 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1001 15:23:08.985127 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1001 15:23:08.985173 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1001 15:23:08.985218 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var
I1001 15:23:08.985264 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1001 15:23:08.985310 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var
I1001 15:23:08.985355 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var
I1001 15:23:08.985401 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var
I1001 15:23:08.985447 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var
I1001 15:23:08.985492 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var
I1001 15:23:08.985539 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var
I1001 15:23:08.985585 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var
I1001 15:23:08.985631 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var
I1001 15:23:08.985682 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var
I1001 15:23:08.985728 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var
I1001 15:23:08.985774 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var
I1001 15:23:08.985820 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var
I1001 15:23:08.985865 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1001 15:23:08.985911 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1001 15:23:08.985956 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1001 15:23:08.986002 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var
I1001 15:23:08.986048 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1001 15:23:08.986094 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var
I1001 15:23:08.986140 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var
I1001 15:23:08.986186 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var
I1001 15:23:08.986232 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var
I1001 15:23:08.986278 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var
I1001 15:23:08.986324 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var
I1001 15:23:08.986374 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[0].b                  (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var
I1001 15:23:08.986421 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[0].w                  (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var
I1001 15:23:08.986467 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[1].b                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var
I1001 15:23:08.986513 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[1].w                  (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var
I1001 15:23:08.986575 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.fflayer.layer_norm.bias                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var
I1001 15:23:08.986624 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.fflayer.layer_norm.scale                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var
I1001 15:23:08.986670 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.self_atten.atten.atten.per_dim_scale     (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1001 15:23:08.986716 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.self_atten.atten.ctx_post_proj           (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1001 15:23:08.986762 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.self_atten.atten.ctx_post_proj_b         (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1001 15:23:08.986809 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.self_atten.atten.ctx_proj                (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var
I1001 15:23:08.986855 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.self_atten.atten.ctx_proj_b              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1001 15:23:08.986901 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.self_atten.atten.query_proj              (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var
I1001 15:23:08.986946 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.self_atten.atten.query_proj_b            (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var
I1001 15:23:08.986992 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.self_atten.atten.source_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var
I1001 15:23:08.987038 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.self_atten.atten.source_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var
I1001 15:23:08.987089 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.self_atten.layer_norm.bias               (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var
I1001 15:23:08.987137 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.self_atten.layer_norm.scale              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var
I1001 15:23:08.987183 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[0].b                  (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var
I1001 15:23:08.987229 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[0].w                  (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var
I1001 15:23:08.987274 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[1].b                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var
I1001 15:23:08.987319 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[1].w                  (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var
I1001 15:23:08.987364 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.fflayer.layer_norm.bias                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var
I1001 15:23:08.987410 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.fflayer.layer_norm.scale                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var
I1001 15:23:08.987456 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.self_atten.atten.atten.per_dim_scale     (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1001 15:23:08.987502 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.self_atten.atten.ctx_post_proj           (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1001 15:23:08.987549 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.self_atten.atten.ctx_post_proj_b         (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1001 15:23:08.987596 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.self_atten.atten.ctx_proj                (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var
I1001 15:23:08.987642 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.self_atten.atten.ctx_proj_b              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1001 15:23:08.987689 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.self_atten.atten.query_proj              (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var
I1001 15:23:08.987735 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.self_atten.atten.query_proj_b            (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var
I1001 15:23:08.987780 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.self_atten.atten.source_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var
I1001 15:23:08.987831 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.self_atten.atten.source_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var
I1001 15:23:08.987878 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.self_atten.layer_norm.bias               (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var
I1001 15:23:08.987924 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.self_atten.layer_norm.scale              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var
I1001 15:23:08.987971 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var
I1001 15:23:08.988017 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var
I1001 15:23:08.988062 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var
I1001 15:23:08.988108 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var
I1001 15:23:08.988153 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var
I1001 15:23:08.988199 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var
I1001 15:23:08.988244 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1001 15:23:08.988290 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1001 15:23:08.988336 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1001 15:23:08.988381 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var
I1001 15:23:08.988427 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1001 15:23:08.988473 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var
I1001 15:23:08.988527 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var
I1001 15:23:08.988574 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var
I1001 15:23:08.988621 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var
I1001 15:23:08.988667 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var
I1001 15:23:08.988713 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var
I1001 15:23:08.988760 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var
I1001 15:23:08.988806 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var
I1001 15:23:08.988852 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var
I1001 15:23:08.988898 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var
I1001 15:23:08.988943 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var
I1001 15:23:08.988988 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var
I1001 15:23:08.989033 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1001 15:23:08.989079 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1001 15:23:08.989125 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1001 15:23:08.989171 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var
I1001 15:23:08.989217 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1001 15:23:08.989268 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var
I1001 15:23:08.989314 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var
I1001 15:23:08.989360 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var
I1001 15:23:08.989411 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var
I1001 15:23:08.989459 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var
I1001 15:23:08.989505 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var
I1001 15:23:08.989552 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var
I1001 15:23:08.989598 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var
I1001 15:23:08.989644 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var
I1001 15:23:08.989690 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var
I1001 15:23:08.989735 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var
I1001 15:23:08.989781 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var
I1001 15:23:08.989827 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1001 15:23:08.989872 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1001 15:23:08.989918 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1001 15:23:08.989968 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var
I1001 15:23:08.990015 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1001 15:23:08.990062 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var
I1001 15:23:08.990108 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var
I1001 15:23:08.990154 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var
I1001 15:23:08.990200 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var
I1001 15:23:08.990247 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var
I1001 15:23:08.990293 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var
I1001 15:23:08.990339 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var
I1001 15:23:08.990386 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var
I1001 15:23:08.990431 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var
I1001 15:23:08.990477 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var
I1001 15:23:08.990523 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var
I1001 15:23:08.990591 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var
I1001 15:23:08.990639 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1001 15:23:08.990691 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1001 15:23:08.990739 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1001 15:23:08.990786 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var
I1001 15:23:08.990832 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1001 15:23:08.990879 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var
I1001 15:23:08.990926 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var
I1001 15:23:08.990972 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var
I1001 15:23:08.991018 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var
I1001 15:23:08.991064 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var
I1001 15:23:08.991110 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var
I1001 15:23:08.991157 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var
I1001 15:23:08.991202 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var
I1001 15:23:08.991248 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var
I1001 15:23:08.991294 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var
I1001 15:23:08.991339 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var
I1001 15:23:08.991385 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var
I1001 15:23:08.991434 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1001 15:23:08.991482 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1001 15:23:08.991528 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1001 15:23:08.991573 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var
I1001 15:23:08.991619 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1001 15:23:08.991665 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var
I1001 15:23:08.991710 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var
I1001 15:23:08.991756 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var
I1001 15:23:08.991801 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var
I1001 15:23:08.991847 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var
I1001 15:23:08.991893 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var
I1001 15:23:08.991940 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var
I1001 15:23:08.991986 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var
I1001 15:23:08.992031 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var
I1001 15:23:08.992077 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var
I1001 15:23:08.992127 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var
I1001 15:23:08.992174 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var
I1001 15:23:08.992220 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1001 15:23:08.992266 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1001 15:23:08.992312 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1001 15:23:08.992358 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var
I1001 15:23:08.992404 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1001 15:23:08.992450 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var
I1001 15:23:08.992496 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var
I1001 15:23:08.992542 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var
I1001 15:23:08.992587 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var
I1001 15:23:08.992633 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var
I1001 15:23:08.992678 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var
I1001 15:23:08.992724 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var
I1001 15:23:08.992769 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var
I1001 15:23:08.992819 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var
I1001 15:23:08.992865 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var
I1001 15:23:08.992910 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var
I1001 15:23:08.992956 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var
I1001 15:23:08.993001 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1001 15:23:08.993046 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1001 15:23:08.993092 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1001 15:23:08.993138 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var
I1001 15:23:08.993184 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1001 15:23:08.993230 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var
I1001 15:23:08.993276 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var
I1001 15:23:08.993322 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var
I1001 15:23:08.993368 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var
I1001 15:23:08.993414 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var
I1001 15:23:08.993459 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var
I1001 15:23:08.993505 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var
I1001 15:23:08.993555 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var
I1001 15:23:08.993602 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var
I1001 15:23:08.993648 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var
I1001 15:23:08.993694 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var
I1001 15:23:08.993739 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var
I1001 15:23:08.993785 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1001 15:23:08.993831 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1001 15:23:08.993878 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1001 15:23:08.993924 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var
I1001 15:23:08.993970 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1001 15:23:08.994016 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var
I1001 15:23:08.994062 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var
I1001 15:23:08.994108 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var
I1001 15:23:08.994153 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var
I1001 15:23:08.994199 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var
I1001 15:23:08.994249 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var
I1001 15:23:08.994296 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var
I1001 15:23:08.994342 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var
I1001 15:23:08.994388 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var
I1001 15:23:08.994434 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var
I1001 15:23:08.994480 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var
I1001 15:23:08.994526 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var
I1001 15:23:08.994590 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1001 15:23:08.994639 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1001 15:23:08.994685 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1001 15:23:08.994731 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var
I1001 15:23:08.994777 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1001 15:23:08.994822 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var
I1001 15:23:08.994868 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var
I1001 15:23:08.994913 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var
I1001 15:23:08.994963 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var
I1001 15:23:08.995010 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var
I1001 15:23:08.995056 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var
I1001 15:23:08.995102 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var
I1001 15:23:08.995148 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var
I1001 15:23:08.995194 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var
I1001 15:23:08.995239 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var
I1001 15:23:08.995285 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var
I1001 15:23:08.995330 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var
I1001 15:23:08.995375 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1001 15:23:08.995421 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1001 15:23:08.995467 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1001 15:23:08.995513 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var
I1001 15:23:08.995559 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1001 15:23:08.995605 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var
I1001 15:23:08.995651 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var
I1001 15:23:08.995702 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var
I1001 15:23:08.995749 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var
I1001 15:23:08.995796 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var
I1001 15:23:08.995842 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var
I1001 15:23:08.995888 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var
I1001 15:23:08.995934 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var
I1001 15:23:08.995980 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var
I1001 15:23:08.996025 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var
I1001 15:23:08.996070 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var
I1001 15:23:08.996116 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var
I1001 15:23:08.996162 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1001 15:23:08.996208 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1001 15:23:08.996254 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1001 15:23:08.996300 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var
I1001 15:23:08.996346 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1001 15:23:08.996396 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var
I1001 15:23:08.996443 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var
I1001 15:23:08.996490 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var
I1001 15:23:08.996536 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var
I1001 15:23:08.996582 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var
I1001 15:23:08.996629 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var
I1001 15:23:08.996675 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var
I1001 15:23:08.996721 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var
I1001 15:23:08.996765 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var
I1001 15:23:08.996811 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var
I1001 15:23:08.996856 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var
I1001 15:23:08.996901 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var
I1001 15:23:08.996946 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1001 15:23:08.996991 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1001 15:23:08.997036 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1001 15:23:08.997082 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var
I1001 15:23:08.997132 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1001 15:23:08.997178 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var
I1001 15:23:08.997225 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var
I1001 15:23:08.997271 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var
I1001 15:23:08.997316 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var
I1001 15:23:08.997362 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var
I1001 15:23:08.997408 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var
I1001 15:23:08.997454 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var
I1001 15:23:08.997499 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var
I1001 15:23:08.997545 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var
I1001 15:23:08.997590 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var
I1001 15:23:08.997636 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var
I1001 15:23:08.997681 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var
I1001 15:23:08.997726 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1001 15:23:08.997772 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1001 15:23:08.997822 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1001 15:23:08.997868 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var
I1001 15:23:08.997914 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1001 15:23:08.997959 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var
I1001 15:23:08.998005 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var
I1001 15:23:08.998050 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var
I1001 15:23:08.998096 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var
I1001 15:23:08.998141 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var
I1001 15:23:08.998187 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var
I1001 15:23:08.998234 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var
I1001 15:23:08.998279 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var
I1001 15:23:08.998325 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var
I1001 15:23:08.998370 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var
I1001 15:23:08.998416 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var
I1001 15:23:08.998461 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var
I1001 15:23:08.998510 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1001 15:23:08.998574 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1001 15:23:08.998624 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1001 15:23:08.998671 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var
I1001 15:23:08.998717 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1001 15:23:08.998763 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var
I1001 15:23:08.998808 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var
I1001 15:23:08.998854 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var
I1001 15:23:08.998899 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var
I1001 15:23:08.998945 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var
I1001 15:23:08.998991 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var
I1001 15:23:08.999036 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var
I1001 15:23:08.999082 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var
I1001 15:23:08.999128 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var
I1001 15:23:08.999173 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var
I1001 15:23:08.999219 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var
I1001 15:23:08.999269 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var
I1001 15:23:08.999315 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1001 15:23:08.999361 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1001 15:23:08.999407 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1001 15:23:08.999452 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var
I1001 15:23:08.999498 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1001 15:23:08.999543 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var
I1001 15:23:08.999588 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var
I1001 15:23:08.999634 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var
I1001 15:23:08.999680 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var
I1001 15:23:08.999725 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var
I1001 15:23:08.999771 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var
I1001 15:23:08.999817 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var
I1001 15:23:08.999863 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var
I1001 15:23:08.999909 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var
I1001 15:23:08.999958 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var
I1001 15:23:09.000006 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var
I1001 15:23:09.000052 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var
I1001 15:23:09.000098 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1001 15:23:09.000144 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1001 15:23:09.000191 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1001 15:23:09.000238 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var
I1001 15:23:09.000284 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1001 15:23:09.000331 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var
I1001 15:23:09.000377 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var
I1001 15:23:09.000423 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var
I1001 15:23:09.000468 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var
I1001 15:23:09.000514 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var
I1001 15:23:09.000561 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var
I1001 15:23:09.000607 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.bias_0                                     (2000,)                    2000 1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var
I1001 15:23:09.000660 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.bias_1                                     (2000,)                    2000 1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var
I1001 15:23:09.000707 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.bias_10                                    (2000,)                    2000 1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var
I1001 15:23:09.000753 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.bias_11                                    (2000,)                    2000 1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var
I1001 15:23:09.000799 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.bias_12                                    (2000,)                    2000 1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var
I1001 15:23:09.000846 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.bias_13                                    (2000,)                    2000 1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var
I1001 15:23:09.000891 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.bias_14                                    (2000,)                    2000 1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var
I1001 15:23:09.000936 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.bias_15                                    (2000,)                    2000 1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var
I1001 15:23:09.000981 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.bias_2                                     (2000,)                    2000 1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var
I1001 15:23:09.001027 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.bias_3                                     (2000,)                    2000 1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var
I1001 15:23:09.001072 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.bias_4                                     (2000,)                    2000 1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var
I1001 15:23:09.001116 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.bias_5                                     (2000,)                    2000 1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var
I1001 15:23:09.001162 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.bias_6                                     (2000,)                    2000 1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var
I1001 15:23:09.001207 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.bias_7                                     (2000,)                    2000 1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var
I1001 15:23:09.001252 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.bias_8                                     (2000,)                    2000 1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var
I1001 15:23:09.001297 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.bias_9                                     (2000,)                    2000 1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var
I1001 15:23:09.001343 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.weight_0                                   (2048, 2000)            4096000 1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var
I1001 15:23:09.001388 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.weight_1                                   (2048, 2000)            4096000 1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var
I1001 15:23:09.001434 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.weight_10                                  (2048, 2000)            4096000 1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var
I1001 15:23:09.001484 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.weight_11                                  (2048, 2000)            4096000 1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var
I1001 15:23:09.001531 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.weight_12                                  (2048, 2000)            4096000 1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var
I1001 15:23:09.001576 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.weight_13                                  (2048, 2000)            4096000 1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var
I1001 15:23:09.001622 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.weight_14                                  (2048, 2000)            4096000 1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var
I1001 15:23:09.001668 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.weight_15                                  (2048, 2000)            4096000 1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var
I1001 15:23:09.001714 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.weight_2                                   (2048, 2000)            4096000 1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var
I1001 15:23:09.001760 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.weight_3                                   (2048, 2000)            4096000 1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var
I1001 15:23:09.001805 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.weight_4                                   (2048, 2000)            4096000 1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var
I1001 15:23:09.001851 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.weight_5                                   (2048, 2000)            4096000 1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var
I1001 15:23:09.001897 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.weight_6                                   (2048, 2000)            4096000 1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var
I1001 15:23:09.001942 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.weight_7                                   (2048, 2000)            4096000 1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var
I1001 15:23:09.001987 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.weight_8                                   (2048, 2000)            4096000 1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var
I1001 15:23:09.002033 139818304546624 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.weight_9                                   (2048, 2000)            4096000 1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var
I1001 15:23:09.002079 139818304546624 py_utils.py:1474] MODEL ANALYSIS: ====================================================================================================
I1001 15:23:09.002124 139818304546624 py_utils.py:1474] MODEL ANALYSIS: total #params: 1742572800
I1001 15:23:09.002171 139818304546624 py_utils.py:1474] MODEL ANALYSIS: 
I1001 15:23:28.294823 139818304546624 trainer.py:1515] Job trainer_client start
I1001 15:23:28.307823 139818304546624 base_runner.py:57] ============================================================
I1001 15:23:28.313372 139818304546624 base_runner.py:59] allow_implicit_capture : NoneType
I1001 15:23:28.313467 139818304546624 base_runner.py:59] cls : type/lingvo.core.base_model/SingleTaskModel
I1001 15:23:28.313534 139818304546624 base_runner.py:59] cluster.add_summary : NoneType
I1001 15:23:28.313594 139818304546624 base_runner.py:59] cluster.cls : type/lingvo.core.cluster/_Cluster
I1001 15:23:28.313649 139818304546624 base_runner.py:59] cluster.controller.cpus_per_replica : 1
I1001 15:23:28.313703 139818304546624 base_runner.py:59] cluster.controller.devices_per_split : 1
I1001 15:23:28.313769 139818304546624 base_runner.py:59] cluster.controller.gpus_per_replica : 0
I1001 15:23:28.313823 139818304546624 base_runner.py:59] cluster.controller.name : '/job:local'
I1001 15:23:28.313876 139818304546624 base_runner.py:59] cluster.controller.num_tpu_hosts : 0
I1001 15:23:28.313927 139818304546624 base_runner.py:59] cluster.controller.replicas : 1
I1001 15:23:28.313979 139818304546624 base_runner.py:59] cluster.controller.targets : ''
I1001 15:23:28.314030 139818304546624 base_runner.py:59] cluster.controller.tpus_per_replica : 0
I1001 15:23:28.314082 139818304546624 base_runner.py:59] cluster.decoder.cpus_per_replica : 1
I1001 15:23:28.314133 139818304546624 base_runner.py:59] cluster.decoder.devices_per_split : 1
I1001 15:23:28.314184 139818304546624 base_runner.py:59] cluster.decoder.gpus_per_replica : 1
I1001 15:23:28.314235 139818304546624 base_runner.py:59] cluster.decoder.name : '/job:local'
I1001 15:23:28.314286 139818304546624 base_runner.py:59] cluster.decoder.num_tpu_hosts : 0
I1001 15:23:28.314337 139818304546624 base_runner.py:59] cluster.decoder.replicas : 1
I1001 15:23:28.314388 139818304546624 base_runner.py:59] cluster.decoder.targets : ''
I1001 15:23:28.314447 139818304546624 base_runner.py:59] cluster.decoder.tpus_per_replica : 0
I1001 15:23:28.314530 139818304546624 base_runner.py:59] cluster.evaler.cpus_per_replica : 1
I1001 15:23:28.314634 139818304546624 base_runner.py:59] cluster.evaler.devices_per_split : 1
I1001 15:23:28.314698 139818304546624 base_runner.py:59] cluster.evaler.gpus_per_replica : 1
I1001 15:23:28.314749 139818304546624 base_runner.py:59] cluster.evaler.name : '/job:local'
I1001 15:23:28.314801 139818304546624 base_runner.py:59] cluster.evaler.num_tpu_hosts : 0
I1001 15:23:28.314851 139818304546624 base_runner.py:59] cluster.evaler.replicas : 1
I1001 15:23:28.314902 139818304546624 base_runner.py:59] cluster.evaler.targets : ''
I1001 15:23:28.314952 139818304546624 base_runner.py:59] cluster.evaler.tpus_per_replica : 0
I1001 15:23:28.315002 139818304546624 base_runner.py:59] cluster.input.cpus_per_replica : 1
I1001 15:23:28.315053 139818304546624 base_runner.py:59] cluster.input.devices_per_split : 1
I1001 15:23:28.315103 139818304546624 base_runner.py:59] cluster.input.gpus_per_replica : 0
I1001 15:23:28.315153 139818304546624 base_runner.py:59] cluster.input.name : '/job:local'
I1001 15:23:28.315202 139818304546624 base_runner.py:59] cluster.input.num_tpu_hosts : 0
I1001 15:23:28.315252 139818304546624 base_runner.py:59] cluster.input.replicas : 0
I1001 15:23:28.315301 139818304546624 base_runner.py:59] cluster.input.targets : ''
I1001 15:23:28.315351 139818304546624 base_runner.py:59] cluster.input.tpus_per_replica : 0
I1001 15:23:28.315402 139818304546624 base_runner.py:59] cluster.job : 'trainer_client'
I1001 15:23:28.315452 139818304546624 base_runner.py:59] cluster.logdir : ''
I1001 15:23:28.315505 139818304546624 base_runner.py:59] cluster.mode : 'sync'
I1001 15:23:28.315556 139818304546624 base_runner.py:59] cluster.ps.cpus_per_replica : 1
I1001 15:23:28.315606 139818304546624 base_runner.py:59] cluster.ps.devices_per_split : 1
I1001 15:23:28.315656 139818304546624 base_runner.py:59] cluster.ps.gpus_per_replica : 0
I1001 15:23:28.315706 139818304546624 base_runner.py:59] cluster.ps.name : '/job:local'
I1001 15:23:28.315756 139818304546624 base_runner.py:59] cluster.ps.num_tpu_hosts : 0
I1001 15:23:28.315805 139818304546624 base_runner.py:59] cluster.ps.replicas : 1
I1001 15:23:28.315855 139818304546624 base_runner.py:59] cluster.ps.targets : ''
I1001 15:23:28.315905 139818304546624 base_runner.py:59] cluster.ps.tpus_per_replica : 0
I1001 15:23:28.315955 139818304546624 base_runner.py:59] cluster.task : 0
I1001 15:23:28.316005 139818304546624 base_runner.py:59] cluster.worker.cpus_per_replica : 1
I1001 15:23:28.316054 139818304546624 base_runner.py:59] cluster.worker.devices_per_split : 1
I1001 15:23:28.316103 139818304546624 base_runner.py:59] cluster.worker.gpus_per_replica : 1
I1001 15:23:28.316159 139818304546624 base_runner.py:59] cluster.worker.name : '/job:local'
I1001 15:23:28.316211 139818304546624 base_runner.py:59] cluster.worker.num_tpu_hosts : 0
I1001 15:23:28.316261 139818304546624 base_runner.py:59] cluster.worker.replicas : 1
I1001 15:23:28.316310 139818304546624 base_runner.py:59] cluster.worker.targets : ''
I1001 15:23:28.316360 139818304546624 base_runner.py:59] cluster.worker.tpus_per_replica : 0
I1001 15:23:28.316410 139818304546624 base_runner.py:59] dtype : float32
I1001 15:23:28.316459 139818304546624 base_runner.py:59] fprop_dtype : NoneType
I1001 15:23:28.316509 139818304546624 base_runner.py:59] inference_driver_name : NoneType
I1001 15:23:28.316558 139818304546624 base_runner.py:59] input.allow_implicit_capture : NoneType
I1001 15:23:28.316608 139818304546624 base_runner.py:59] input.bucket_adjust_every_n : 0
I1001 15:23:28.316657 139818304546624 base_runner.py:59] input.bucket_batch_limit : [32]
I1001 15:23:28.316707 139818304546624 base_runner.py:59] input.bucket_upper_bound : [1024]
I1001 15:23:28.316757 139818304546624 base_runner.py:59] input.cls : type/lingvo.tasks.lm.input_generator/LmInput
I1001 15:23:28.316807 139818304546624 base_runner.py:59] input.dtype : float32
I1001 15:23:28.316857 139818304546624 base_runner.py:59] input.file_buffer_size : 10000000
I1001 15:23:28.316907 139818304546624 base_runner.py:59] input.file_datasource : NoneType
I1001 15:23:28.316957 139818304546624 base_runner.py:59] input.file_parallelism : 10
I1001 15:23:28.317007 139818304546624 base_runner.py:59] input.file_pattern : 'text:/tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en*'
I1001 15:23:28.317058 139818304546624 base_runner.py:59] input.file_random_seed : 301
I1001 15:23:28.317109 139818304546624 base_runner.py:59] input.fixed_input_shape : True
I1001 15:23:28.317158 139818304546624 base_runner.py:59] input.flush_every_n : 0
I1001 15:23:28.317208 139818304546624 base_runner.py:59] input.fprop_dtype : NoneType
I1001 15:23:28.317257 139818304546624 base_runner.py:59] input.inference_driver_name : NoneType
I1001 15:23:28.317307 139818304546624 base_runner.py:59] input.is_eval : NoneType
I1001 15:23:28.317357 139818304546624 base_runner.py:59] input.is_inference : NoneType
I1001 15:23:28.317407 139818304546624 base_runner.py:59] input.name : '1bwds_train_set'
I1001 15:23:28.317456 139818304546624 base_runner.py:59] input.num_batcher_threads : 16
I1001 15:23:28.317507 139818304546624 base_runner.py:59] input.num_samples : 0
I1001 15:23:28.317557 139818304546624 base_runner.py:59] input.pad_to_max_seq_length : False
I1001 15:23:28.317607 139818304546624 base_runner.py:59] input.params_init.method : 'xavier'
I1001 15:23:28.317657 139818304546624 base_runner.py:59] input.params_init.scale : 1.000001
I1001 15:23:28.317707 139818304546624 base_runner.py:59] input.params_init.seed : NoneType
I1001 15:23:28.317757 139818304546624 base_runner.py:59] input.random_seed : NoneType
I1001 15:23:28.317807 139818304546624 base_runner.py:59] input.remote.max_inflights_per_target : 32
I1001 15:23:28.317856 139818304546624 base_runner.py:59] input.remote.shardable_batch : False
I1001 15:23:28.317906 139818304546624 base_runner.py:59] input.require_sequential_order : False
I1001 15:23:28.317956 139818304546624 base_runner.py:59] input.skip_lp_regularization : NoneType
I1001 15:23:28.318005 139818304546624 base_runner.py:59] input.source_max_length : NoneType
I1001 15:23:28.318055 139818304546624 base_runner.py:59] input.target_max_length : 1024
I1001 15:23:28.318104 139818304546624 base_runner.py:59] input.tokenizer.allow_implicit_capture : NoneType
I1001 15:23:28.318154 139818304546624 base_runner.py:59] input.tokenizer.append_eos : True
I1001 15:23:28.318203 139818304546624 base_runner.py:59] input.tokenizer.cls : type/lingvo.core.tokenizers/AsciiTokenizer
I1001 15:23:28.318253 139818304546624 base_runner.py:59] input.tokenizer.dtype : float32
I1001 15:23:28.318304 139818304546624 base_runner.py:59] input.tokenizer.fprop_dtype : NoneType
I1001 15:23:28.318358 139818304546624 base_runner.py:59] input.tokenizer.inference_driver_name : NoneType
I1001 15:23:28.318409 139818304546624 base_runner.py:59] input.tokenizer.is_eval : NoneType
I1001 15:23:28.318460 139818304546624 base_runner.py:59] input.tokenizer.is_inference : NoneType
I1001 15:23:28.318510 139818304546624 base_runner.py:59] input.tokenizer.name : 'tokenizer'
I1001 15:23:28.318580 139818304546624 base_runner.py:59] input.tokenizer.pad_to_max_length : True
I1001 15:23:28.318634 139818304546624 base_runner.py:59] input.tokenizer.params_init.method : 'xavier'
I1001 15:23:28.318685 139818304546624 base_runner.py:59] input.tokenizer.params_init.scale : 1.000001
I1001 15:23:28.318735 139818304546624 base_runner.py:59] input.tokenizer.params_init.seed : NoneType
I1001 15:23:28.318785 139818304546624 base_runner.py:59] input.tokenizer.random_seed : NoneType
I1001 15:23:28.318836 139818304546624 base_runner.py:59] input.tokenizer.skip_lp_regularization : NoneType
I1001 15:23:28.318886 139818304546624 base_runner.py:59] input.tokenizer.target_eos_id : 2
I1001 15:23:28.318936 139818304546624 base_runner.py:59] input.tokenizer.target_sos_id : 1
I1001 15:23:28.318986 139818304546624 base_runner.py:59] input.tokenizer.target_unk_id : 0
I1001 15:23:28.319036 139818304546624 base_runner.py:59] input.tokenizer.vn.global_vn : False
I1001 15:23:28.319086 139818304546624 base_runner.py:59] input.tokenizer.vn.per_step_vn : False
I1001 15:23:28.319137 139818304546624 base_runner.py:59] input.tokenizer.vn.scale : NoneType
I1001 15:23:28.319188 139818304546624 base_runner.py:59] input.tokenizer.vn.seed : NoneType
I1001 15:23:28.319238 139818304546624 base_runner.py:59] input.tokenizer.vocab_size : 32000
I1001 15:23:28.319288 139818304546624 base_runner.py:59] input.tokenizer_dict : {}
I1001 15:23:28.319338 139818304546624 base_runner.py:59] input.tpu_infeed_parallelism : 1
I1001 15:23:28.319388 139818304546624 base_runner.py:59] input.use_chaining : False
I1001 15:23:28.319437 139818304546624 base_runner.py:59] input.use_per_host_infeed : False
I1001 15:23:28.319487 139818304546624 base_runner.py:59] input.use_within_batch_mixing : False
I1001 15:23:28.319537 139818304546624 base_runner.py:59] input.vn.global_vn : False
I1001 15:23:28.319587 139818304546624 base_runner.py:59] input.vn.per_step_vn : False
I1001 15:23:28.319636 139818304546624 base_runner.py:59] input.vn.scale : NoneType
I1001 15:23:28.319686 139818304546624 base_runner.py:59] input.vn.seed : NoneType
I1001 15:23:28.319735 139818304546624 base_runner.py:59] is_eval : NoneType
I1001 15:23:28.319785 139818304546624 base_runner.py:59] is_inference : NoneType
I1001 15:23:28.319834 139818304546624 base_runner.py:59] model : 'lm.one_billion_wds.OneBWdsGPipeTransformerWPM@/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/tasks/lm/params/one_billion_wds.py:188'
I1001 15:23:28.319885 139818304546624 base_runner.py:59] name : ''
I1001 15:23:28.319935 139818304546624 base_runner.py:59] params_init.method : 'xavier'
I1001 15:23:28.319984 139818304546624 base_runner.py:59] params_init.scale : 1.000001
I1001 15:23:28.320033 139818304546624 base_runner.py:59] params_init.seed : NoneType
I1001 15:23:28.320082 139818304546624 base_runner.py:59] random_seed : NoneType
I1001 15:23:28.320132 139818304546624 base_runner.py:59] skip_lp_regularization : NoneType
I1001 15:23:28.320183 139818304546624 base_runner.py:59] task.allow_implicit_capture : NoneType
I1001 15:23:28.320237 139818304546624 base_runner.py:59] task.cls : type/lingvo.tasks.lm.model/FixedShapeInputLanguageModel
I1001 15:23:28.320287 139818304546624 base_runner.py:59] task.decoder : NoneType
I1001 15:23:28.320336 139818304546624 base_runner.py:59] task.dtype : float32
I1001 15:23:28.320385 139818304546624 base_runner.py:59] task.encoder : NoneType
I1001 15:23:28.320435 139818304546624 base_runner.py:59] task.eval.decoder_samples_per_summary : 0
I1001 15:23:28.320483 139818304546624 base_runner.py:59] task.eval.load_checkpoint_from : NoneType
I1001 15:23:28.320533 139818304546624 base_runner.py:59] task.eval.samples_per_summary : 0
I1001 15:23:28.320587 139818304546624 base_runner.py:59] task.eval.start_decoder_after : 0
I1001 15:23:28.320638 139818304546624 base_runner.py:59] task.eval.start_eval_after : 0
I1001 15:23:28.320688 139818304546624 base_runner.py:59] task.fprop_dtype : NoneType
I1001 15:23:28.320737 139818304546624 base_runner.py:59] task.inference_driver_name : NoneType
I1001 15:23:28.320787 139818304546624 base_runner.py:59] task.input : NoneType
I1001 15:23:28.320836 139818304546624 base_runner.py:59] task.is_eval : NoneType
I1001 15:23:28.320886 139818304546624 base_runner.py:59] task.is_inference : NoneType
I1001 15:23:28.320935 139818304546624 base_runner.py:59] task.lm.allow_implicit_capture : NoneType
I1001 15:23:28.320985 139818304546624 base_runner.py:59] task.lm.cls : type/lingvo.tasks.lm.layers/GPipeTransformerLm
I1001 15:23:28.321035 139818304546624 base_runner.py:59] task.lm.dtype : float32
I1001 15:23:28.321085 139818304546624 base_runner.py:59] task.lm.fprop_dtype : NoneType
I1001 15:23:28.321134 139818304546624 base_runner.py:59] task.lm.inference_driver_name : NoneType
I1001 15:23:28.321183 139818304546624 base_runner.py:59] task.lm.is_eval : NoneType
I1001 15:23:28.321233 139818304546624 base_runner.py:59] task.lm.is_inference : NoneType
I1001 15:23:28.321282 139818304546624 base_runner.py:59] task.lm.name : 'transformerlm'
I1001 15:23:28.321332 139818304546624 base_runner.py:59] task.lm.params_init.method : 'xavier'
I1001 15:23:28.321382 139818304546624 base_runner.py:59] task.lm.params_init.scale : 1.000001
I1001 15:23:28.321432 139818304546624 base_runner.py:59] task.lm.params_init.seed : NoneType
I1001 15:23:28.321481 139818304546624 base_runner.py:59] task.lm.random_seed : NoneType
I1001 15:23:28.321531 139818304546624 base_runner.py:59] task.lm.skip_lp_regularization : NoneType
I1001 15:23:28.321581 139818304546624 base_runner.py:59] task.lm.stack.allow_implicit_capture : NoneType
I1001 15:23:28.321630 139818304546624 base_runner.py:59] task.lm.stack.batch_dim : 1
I1001 15:23:28.321680 139818304546624 base_runner.py:59] task.lm.stack.cls : type/lingvo.core.layers_with_gpipe/GPipeTransformerStack
I1001 15:23:28.321730 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.allow_implicit_capture : NoneType
I1001 15:23:28.321780 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.cls : type/lingvo.core.layers_with_gpipe/GPipeTransformerLayer
I1001 15:23:28.321830 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.dtype : float32
I1001 15:23:28.321880 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.final_enc_layer : False
I1001 15:23:28.321929 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.fprop_dtype : NoneType
I1001 15:23:28.321979 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.has_aux_atten : True
I1001 15:23:28.322029 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.inference_driver_name : NoneType
I1001 15:23:28.322078 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.is_decoder : False
I1001 15:23:28.322128 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.is_eval : NoneType
I1001 15:23:28.322178 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.is_inference : NoneType
I1001 15:23:28.322227 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.is_transparent : False
I1001 15:23:28.322277 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.allow_implicit_capture : NoneType
I1001 15:23:28.322327 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.cls : type/lingvo.core.layers/LayerNorm
I1001 15:23:28.322376 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.dtype : float32
I1001 15:23:28.322426 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.epsilon : 1e-06
I1001 15:23:28.322476 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.fprop_dtype : NoneType
I1001 15:23:28.322525 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.inference_driver_name : NoneType
I1001 15:23:28.322605 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.input_dim : 0
I1001 15:23:28.322658 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.is_eval : NoneType
I1001 15:23:28.322709 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.is_inference : NoneType
I1001 15:23:28.322760 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.name : ''
I1001 15:23:28.322811 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.params_init.method : 'xavier'
I1001 15:23:28.322861 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.params_init.scale : 1.000001
I1001 15:23:28.322912 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.params_init.seed : NoneType
I1001 15:23:28.322962 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.random_seed : NoneType
I1001 15:23:28.323013 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.skip_lp_regularization : NoneType
I1001 15:23:28.323063 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.vn.global_vn : False
I1001 15:23:28.323113 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.vn.per_step_vn : False
I1001 15:23:28.323164 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.vn.scale : NoneType
I1001 15:23:28.323214 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.vn.seed : NoneType
I1001 15:23:28.323265 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.mask_self_atten : True
I1001 15:23:28.323314 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.name : ''
I1001 15:23:28.323365 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.normalize_output : False
I1001 15:23:28.323415 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.output_dim : 0
I1001 15:23:28.323465 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.packed_input : False
I1001 15:23:28.323515 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.params_init.method : 'xavier'
I1001 15:23:28.323565 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.params_init.scale : 1.000001
I1001 15:23:28.323615 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.params_init.seed : NoneType
I1001 15:23:28.323665 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.random_seed : NoneType
I1001 15:23:28.323715 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.skip_lp_regularization : NoneType
I1001 15:23:28.323765 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.source_dim : 0
I1001 15:23:28.323814 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.add_unnormalized_input : False
I1001 15:23:28.323864 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.allow_implicit_capture : NoneType
I1001 15:23:28.323915 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_dropout_prob : 0.0
I1001 15:23:28.323965 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_hidden_dim : 0
I1001 15:23:28.324015 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.allow_implicit_capture : NoneType
I1001 15:23:28.324065 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.atten_dropout_deterministic : False
I1001 15:23:28.324115 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.atten_dropout_prob : 0.0
I1001 15:23:28.324166 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.cls : type/lingvo.core.attention/MultiHeadedAttention
I1001 15:23:28.324216 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.context_dim : 0
I1001 15:23:28.324266 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.ctx_post_proj_dim : 0
I1001 15:23:28.324317 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.dtype : float32
I1001 15:23:28.324371 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.enable_ctx_post_proj : True
I1001 15:23:28.324422 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.enable_ctx_pre_proj : False
I1001 15:23:28.324473 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.enable_query_proj : True
I1001 15:23:28.324523 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.enable_source_proj : True
I1001 15:23:28.324573 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.fprop_dtype : NoneType
I1001 15:23:28.324624 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.hidden_dim : 0
I1001 15:23:28.324674 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inference_driver_name : NoneType
I1001 15:23:28.324724 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.allow_implicit_capture : NoneType
I1001 15:23:28.324774 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.atten_dropout_deterministic : False
I1001 15:23:28.324824 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.atten_dropout_prob : 0.0
I1001 15:23:28.324874 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.cls : type/lingvo.core.attention/DotProductAttention
I1001 15:23:28.324924 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.dtype : float32
I1001 15:23:28.324974 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.fprop_dtype : NoneType
I1001 15:23:28.325024 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.hidden_dim : 0
I1001 15:23:28.325073 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.inference_driver_name : NoneType
I1001 15:23:28.325123 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.is_eval : NoneType
I1001 15:23:28.325172 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.is_inference : NoneType
I1001 15:23:28.325222 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.name : ''
I1001 15:23:28.325271 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.packed_input : False
I1001 15:23:28.325320 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.params_init.method : 'xavier'
I1001 15:23:28.325370 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.params_init.scale : 1.000001
I1001 15:23:28.325420 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.params_init.seed : NoneType
I1001 15:23:28.325469 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.qdomain.default : NoneType
I1001 15:23:28.325519 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.qdomain.fullyconnected : NoneType
I1001 15:23:28.325568 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.qdomain.softmax : NoneType
I1001 15:23:28.325618 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.query_dim : 0
I1001 15:23:28.325667 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.random_seed : NoneType
I1001 15:23:28.325717 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.skip_lp_regularization : NoneType
I1001 15:23:28.325771 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.source_dim : 0
I1001 15:23:28.325822 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.global_vn : False
I1001 15:23:28.325872 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.per_step_vn : False
I1001 15:23:28.325921 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.scale : NoneType
I1001 15:23:28.325971 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.seed : NoneType
I1001 15:23:28.326020 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.is_eval : NoneType
I1001 15:23:28.326070 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.is_inference : NoneType
I1001 15:23:28.326120 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.name : ''
I1001 15:23:28.326169 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.num_attention_heads : 2
I1001 15:23:28.326219 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.packed_input : False
I1001 15:23:28.326268 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.params_init.method : 'xavier'
I1001 15:23:28.326318 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.params_init.scale : 1.0
I1001 15:23:28.326368 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.params_init.seed : NoneType
I1001 15:23:28.326418 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.qdomain.atten_context : NoneType
I1001 15:23:28.326467 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.qdomain.default : NoneType
I1001 15:23:28.326517 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.qdomain.fullyconnected : NoneType
I1001 15:23:28.326585 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.qdomain.softmax : NoneType
I1001 15:23:28.326637 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.query_dim : 0
I1001 15:23:28.326686 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.random_seed : NoneType
I1001 15:23:28.326736 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.skip_lp_regularization : NoneType
I1001 15:23:28.326786 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.source_dim : 0
I1001 15:23:28.326836 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.use_source_vec_as_attention_value : False
I1001 15:23:28.326885 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.vn.global_vn : False
I1001 15:23:28.326935 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.vn.per_step_vn : False
I1001 15:23:28.326985 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.vn.scale : NoneType
I1001 15:23:28.327035 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.vn.seed : NoneType
I1001 15:23:28.327084 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.cls : type/lingvo.core.layers_with_attention/TransformerAttentionLayer
I1001 15:23:28.327135 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.context_dim : 0
I1001 15:23:28.327184 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.dtype : float32
I1001 15:23:28.327234 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.fprop_dtype : NoneType
I1001 15:23:28.327283 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.inference_driver_name : NoneType
I1001 15:23:28.327337 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.is_eval : NoneType
I1001 15:23:28.327388 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.is_inference : NoneType
I1001 15:23:28.327438 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.is_masked : False
I1001 15:23:28.327487 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.allow_implicit_capture : NoneType
I1001 15:23:28.327538 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.cls : type/lingvo.core.layers/LayerNorm
I1001 15:23:28.327587 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.dtype : float32
I1001 15:23:28.327637 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.epsilon : 1e-06
I1001 15:23:28.327687 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.fprop_dtype : NoneType
I1001 15:23:28.327737 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.inference_driver_name : NoneType
I1001 15:23:28.327786 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.input_dim : 0
I1001 15:23:28.327836 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.is_eval : NoneType
I1001 15:23:28.327885 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.is_inference : NoneType
I1001 15:23:28.327934 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.name : ''
I1001 15:23:28.327983 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.params_init.method : 'xavier'
I1001 15:23:28.328033 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.params_init.scale : 1.000001
I1001 15:23:28.328083 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.params_init.seed : NoneType
I1001 15:23:28.328133 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.random_seed : NoneType
I1001 15:23:28.328182 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.skip_lp_regularization : NoneType
I1001 15:23:28.328232 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.vn.global_vn : False
I1001 15:23:28.328282 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.vn.per_step_vn : False
I1001 15:23:28.328331 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.vn.scale : NoneType
I1001 15:23:28.328380 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.vn.seed : NoneType
I1001 15:23:28.328429 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.mask_type : 'future'
I1001 15:23:28.328478 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.name : ''
I1001 15:23:28.328527 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.num_attention_heads : 8
I1001 15:23:28.328576 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.packed_input : False
I1001 15:23:28.328625 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.params_init.method : 'xavier'
I1001 15:23:28.328675 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.params_init.scale : 1.000001
I1001 15:23:28.328724 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.params_init.seed : NoneType
I1001 15:23:28.328773 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.random_seed : NoneType
I1001 15:23:28.328822 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_prob : 0.0
I1001 15:23:28.328871 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.allow_implicit_capture : NoneType
I1001 15:23:28.328920 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.cls : type/lingvo.core.layers/DropoutLayer
I1001 15:23:28.328974 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.dropout_at_eval : False
I1001 15:23:28.329024 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.dtype : float32
I1001 15:23:28.329074 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.fprop_dtype : NoneType
I1001 15:23:28.329123 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.inference_driver_name : NoneType
I1001 15:23:28.329173 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.is_eval : NoneType
I1001 15:23:28.329222 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.is_inference : NoneType
I1001 15:23:28.329272 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.keep_prob : 1.0
I1001 15:23:28.329321 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.name : ''
I1001 15:23:28.329371 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.noise_shape : NoneType
I1001 15:23:28.329421 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.noise_shape_broadcast_dims : NoneType
I1001 15:23:28.329470 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.params_init.method : 'xavier'
I1001 15:23:28.329520 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.params_init.scale : 1.000001
I1001 15:23:28.329569 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.params_init.seed : NoneType
I1001 15:23:28.329618 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.random_seed : NoneType
I1001 15:23:28.329668 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.skip_lp_regularization : NoneType
I1001 15:23:28.329716 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.global_vn : False
I1001 15:23:28.329766 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.per_step_vn : False
I1001 15:23:28.329815 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.scale : NoneType
I1001 15:23:28.329864 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.seed : NoneType
I1001 15:23:28.329913 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.skip_lp_regularization : NoneType
I1001 15:23:28.329962 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.source_dim : 0
I1001 15:23:28.330011 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.vn.global_vn : False
I1001 15:23:28.330060 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.vn.per_step_vn : False
I1001 15:23:28.330109 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.vn.scale : NoneType
I1001 15:23:28.330159 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.vn.seed : NoneType
I1001 15:23:28.330209 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_aux_atten_tpl : NoneType
I1001 15:23:28.330261 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.activation : 'RELU'
I1001 15:23:28.330312 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.allow_implicit_capture : NoneType
I1001 15:23:28.330362 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.cls : type/lingvo.core.layers_with_attention/TransformerFeedForwardLayer
I1001 15:23:28.330411 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.dtype : float32
I1001 15:23:28.330461 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.activation : ['RELU', 'NONE']
I1001 15:23:28.330514 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.allow_implicit_capture : NoneType
I1001 15:23:28.330583 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.batch_norm : False
I1001 15:23:28.330636 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.bn_fold_weights : NoneType
I1001 15:23:28.330686 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.cls : type/lingvo.core.layers/FeedForwardNet
I1001 15:23:28.330736 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.allow_implicit_capture : NoneType
I1001 15:23:28.330786 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.cls : type/lingvo.core.layers/DropoutLayer
I1001 15:23:28.330835 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.dropout_at_eval : False
I1001 15:23:28.330885 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.dtype : float32
I1001 15:23:28.330935 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.fprop_dtype : NoneType
I1001 15:23:28.330984 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.inference_driver_name : NoneType
I1001 15:23:28.331034 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.is_eval : NoneType
I1001 15:23:28.331084 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.is_inference : NoneType
I1001 15:23:28.331134 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.keep_prob : 1.0
I1001 15:23:28.331184 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.name : ''
I1001 15:23:28.331234 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.noise_shape : NoneType
I1001 15:23:28.331284 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.noise_shape_broadcast_dims : NoneType
I1001 15:23:28.331333 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.params_init.method : 'xavier'
I1001 15:23:28.331382 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.params_init.scale : 1.000001
I1001 15:23:28.331432 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.params_init.seed : NoneType
I1001 15:23:28.331481 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.random_seed : NoneType
I1001 15:23:28.331530 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.skip_lp_regularization : NoneType
I1001 15:23:28.331579 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.global_vn : False
I1001 15:23:28.331629 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.per_step_vn : False
I1001 15:23:28.331678 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.scale : NoneType
I1001 15:23:28.331728 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.seed : NoneType
I1001 15:23:28.331778 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dtype : float32
I1001 15:23:28.331829 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.fprop_dtype : NoneType
I1001 15:23:28.331879 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.inference_driver_name : NoneType
I1001 15:23:28.331933 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.input_dim : 0
I1001 15:23:28.331984 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.is_eval : NoneType
I1001 15:23:28.332034 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.is_inference : NoneType
I1001 15:23:28.332083 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.name : ''
I1001 15:23:28.332134 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.params_init.method : 'xavier'
I1001 15:23:28.332183 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.params_init.scale : 1.000001
I1001 15:23:28.332233 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.params_init.seed : NoneType
I1001 15:23:28.332282 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.activation : 'RELU'
I1001 15:23:28.332332 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.affine_last : False
I1001 15:23:28.332381 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.allow_implicit_capture : NoneType
I1001 15:23:28.332431 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.batch_norm : True
I1001 15:23:28.332481 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.bias_init : 0.0
I1001 15:23:28.332530 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.bn_fold_weights : NoneType
I1001 15:23:28.332580 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.cls : type/lingvo.core.layers/ProjectionLayer
I1001 15:23:28.332629 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.dtype : float32
I1001 15:23:28.332679 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.fprop_dtype : NoneType
I1001 15:23:28.332728 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.has_bias : False
I1001 15:23:28.332778 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.inference_driver_name : NoneType
I1001 15:23:28.332827 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.input_dim : 0
I1001 15:23:28.332877 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.is_eval : NoneType
I1001 15:23:28.332926 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.is_inference : NoneType
I1001 15:23:28.332980 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.name : ''
I1001 15:23:28.333032 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.output_dim : 0
I1001 15:23:28.333083 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.params_init.method : 'xavier'
I1001 15:23:28.333132 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.params_init.scale : 1.000001
I1001 15:23:28.333183 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.params_init.seed : NoneType
I1001 15:23:28.333232 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.qdomain.default : NoneType
I1001 15:23:28.333282 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.random_seed : NoneType
I1001 15:23:28.333332 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.skip_lp_regularization : NoneType
I1001 15:23:28.333386 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.global_vn : False
I1001 15:23:28.333437 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.per_step_vn : False
I1001 15:23:28.333487 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.scale : NoneType
I1001 15:23:28.333537 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.seed : NoneType
I1001 15:23:28.333587 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.weight_norm : False
I1001 15:23:28.333637 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.qdomain.default : NoneType
I1001 15:23:28.333686 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.random_seed : NoneType
I1001 15:23:28.333736 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.skip_connections : NoneType
I1001 15:23:28.333786 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.skip_lp_regularization : NoneType
I1001 15:23:28.333836 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.global_vn : False
I1001 15:23:28.333886 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.per_step_vn : False
I1001 15:23:28.333936 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.scale : NoneType
I1001 15:23:28.333986 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.seed : NoneType
I1001 15:23:28.334037 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.weight_norm : False
I1001 15:23:28.334086 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fprop_dtype : NoneType
I1001 15:23:28.334135 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.hidden_dim : 2048
I1001 15:23:28.334185 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.inference_driver_name : NoneType
I1001 15:23:28.334235 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.input_dim : 0
I1001 15:23:28.334285 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.is_eval : NoneType
I1001 15:23:28.334335 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.is_inference : NoneType
I1001 15:23:28.334384 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.allow_implicit_capture : NoneType
I1001 15:23:28.334434 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.cls : type/lingvo.core.layers/LayerNorm
I1001 15:23:28.334483 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.dtype : float32
I1001 15:23:28.334548 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.epsilon : 1e-06
I1001 15:23:28.334607 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.fprop_dtype : NoneType
I1001 15:23:28.334659 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.inference_driver_name : NoneType
I1001 15:23:28.334709 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.input_dim : 0
I1001 15:23:28.334759 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.is_eval : NoneType
I1001 15:23:28.334810 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.is_inference : NoneType
I1001 15:23:28.334862 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.name : ''
I1001 15:23:28.334913 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.params_init.method : 'xavier'
I1001 15:23:28.334963 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.params_init.scale : 1.000001
I1001 15:23:28.335019 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.params_init.seed : NoneType
I1001 15:23:28.335072 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.random_seed : NoneType
I1001 15:23:28.335123 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.skip_lp_regularization : NoneType
I1001 15:23:28.335173 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.vn.global_vn : False
I1001 15:23:28.335224 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.vn.per_step_vn : False
I1001 15:23:28.335275 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.vn.scale : NoneType
I1001 15:23:28.335326 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.vn.seed : NoneType
I1001 15:23:28.335376 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.name : ''
I1001 15:23:28.335426 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.output_dim : 0
I1001 15:23:28.335476 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.params_init.method : 'xavier'
I1001 15:23:28.335527 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.params_init.scale : 1.000001
I1001 15:23:28.335578 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.params_init.seed : NoneType
I1001 15:23:28.335628 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.random_seed : NoneType
I1001 15:23:28.335678 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.relu_dropout_prob : 0.0
I1001 15:23:28.335728 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.activation : 'RELU'
I1001 15:23:28.335779 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.affine_last : False
I1001 15:23:28.335829 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.allow_implicit_capture : NoneType
I1001 15:23:28.335880 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.batch_norm : True
I1001 15:23:28.335931 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.bias_init : 0.0
I1001 15:23:28.335982 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.bn_fold_weights : NoneType
I1001 15:23:28.336032 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.cls : type/lingvo.core.layers/ProjectionLayer
I1001 15:23:28.336082 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.dtype : float32
I1001 15:23:28.336132 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.fprop_dtype : NoneType
I1001 15:23:28.336182 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.has_bias : False
I1001 15:23:28.336232 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.inference_driver_name : NoneType
I1001 15:23:28.336283 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.input_dim : 0
I1001 15:23:28.336333 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.is_eval : NoneType
I1001 15:23:28.336384 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.is_inference : NoneType
I1001 15:23:28.336434 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.name : ''
I1001 15:23:28.336484 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.output_dim : 0
I1001 15:23:28.336534 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.params_init.method : 'xavier'
I1001 15:23:28.336588 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.params_init.scale : 1.000001
I1001 15:23:28.336639 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.params_init.seed : NoneType
I1001 15:23:28.336690 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.qdomain.default : NoneType
I1001 15:23:28.336740 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.random_seed : NoneType
I1001 15:23:28.336791 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.skip_lp_regularization : NoneType
I1001 15:23:28.336842 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.global_vn : False
I1001 15:23:28.336892 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.per_step_vn : False
I1001 15:23:28.336942 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.scale : NoneType
I1001 15:23:28.336992 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.seed : NoneType
I1001 15:23:28.337042 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.weight_norm : False
I1001 15:23:28.337092 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_prob : 0.0
I1001 15:23:28.337143 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.allow_implicit_capture : NoneType
I1001 15:23:28.337193 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.cls : type/lingvo.core.layers/DropoutLayer
I1001 15:23:28.337244 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.dropout_at_eval : False
I1001 15:23:28.337294 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.dtype : float32
I1001 15:23:28.337344 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.fprop_dtype : NoneType
I1001 15:23:28.337394 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.inference_driver_name : NoneType
I1001 15:23:28.337444 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.is_eval : NoneType
I1001 15:23:28.337494 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.is_inference : NoneType
I1001 15:23:28.337543 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.keep_prob : 1.0
I1001 15:23:28.337593 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.name : ''
I1001 15:23:28.337642 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.noise_shape : NoneType
I1001 15:23:28.337692 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.noise_shape_broadcast_dims : NoneType
I1001 15:23:28.337742 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.params_init.method : 'xavier'
I1001 15:23:28.337791 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.params_init.scale : 1.000001
I1001 15:23:28.337841 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.params_init.seed : NoneType
I1001 15:23:28.337891 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.random_seed : NoneType
I1001 15:23:28.337941 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.skip_lp_regularization : NoneType
I1001 15:23:28.337991 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.global_vn : False
I1001 15:23:28.338047 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.per_step_vn : False
I1001 15:23:28.338098 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.scale : NoneType
I1001 15:23:28.338148 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.seed : NoneType
I1001 15:23:28.338198 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.skip_lp_regularization : NoneType
I1001 15:23:28.338248 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.vn.global_vn : False
I1001 15:23:28.338297 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.vn.per_step_vn : False
I1001 15:23:28.338347 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.vn.scale : NoneType
I1001 15:23:28.338397 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.vn.seed : NoneType
I1001 15:23:28.338448 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.transparent_merger_tpl : NoneType
I1001 15:23:28.338498 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.vn.global_vn : False
I1001 15:23:28.338561 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.vn.per_step_vn : False
I1001 15:23:28.338618 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.vn.scale : NoneType
I1001 15:23:28.338668 139818304546624 base_runner.py:59] task.lm.stack.decoder_tpl.vn.seed : NoneType
I1001 15:23:28.338718 139818304546624 base_runner.py:59] task.lm.stack.dtype : float32
I1001 15:23:28.338769 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.add_tgt_embedding_layer : False
I1001 15:23:28.338819 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.allow_implicit_capture : NoneType
I1001 15:23:28.338870 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.batch_dim : 1
I1001 15:23:28.338920 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.cls : type/lingvo.core.layers_with_gpipe/GPipeTransformerEmbeddingLayer
I1001 15:23:28.338969 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.dec_task_emb : NoneType
I1001 15:23:28.339019 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.allow_implicit_capture : NoneType
I1001 15:23:28.339068 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.cls : type/lingvo.core.layers/DropoutLayer
I1001 15:23:28.339118 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.dropout_at_eval : False
I1001 15:23:28.339168 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.dtype : float32
I1001 15:23:28.339218 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.fprop_dtype : NoneType
I1001 15:23:28.339267 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.inference_driver_name : NoneType
I1001 15:23:28.339317 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.is_eval : NoneType
I1001 15:23:28.339365 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.is_inference : NoneType
I1001 15:23:28.339415 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.keep_prob : 1.0
I1001 15:23:28.339464 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.name : ''
I1001 15:23:28.339514 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.noise_shape : NoneType
I1001 15:23:28.339564 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.noise_shape_broadcast_dims : NoneType
I1001 15:23:28.339614 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.params_init.method : 'xavier'
I1001 15:23:28.339664 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.params_init.scale : 1.000001
I1001 15:23:28.339714 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.params_init.seed : NoneType
I1001 15:23:28.339764 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.random_seed : NoneType
I1001 15:23:28.339823 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.skip_lp_regularization : NoneType
I1001 15:23:28.339874 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.vn.global_vn : False
I1001 15:23:28.339925 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.vn.per_step_vn : False
I1001 15:23:28.339975 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.vn.scale : NoneType
I1001 15:23:28.340025 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.vn.seed : NoneType
I1001 15:23:28.340075 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.dtype : float32
I1001 15:23:28.340123 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.enc_task_emb : NoneType
I1001 15:23:28.340173 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.fprop_dtype : NoneType
I1001 15:23:28.340223 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.inference_driver_name : NoneType
I1001 15:23:28.340272 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.input_dropout_prob : 0.0
I1001 15:23:28.340326 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.is_eval : NoneType
I1001 15:23:28.340376 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.is_inference : NoneType
I1001 15:23:28.340425 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.is_transparent : False
I1001 15:23:28.340474 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.max_seq_len : 300
I1001 15:23:28.340524 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.name : ''
I1001 15:23:28.340574 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.packed_input : False
I1001 15:23:28.340623 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.params_init.method : 'xavier'
I1001 15:23:28.340672 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.params_init.scale : 1.000001
I1001 15:23:28.340722 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.params_init.seed : NoneType
I1001 15:23:28.340771 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.allow_implicit_capture : NoneType
I1001 15:23:28.340821 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.cls : type/lingvo.core.layers/PositionalEmbeddingLayer
I1001 15:23:28.340870 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.dtype : float32
I1001 15:23:28.340920 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.embedding_dim : 2048
I1001 15:23:28.340970 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.fprop_dtype : NoneType
I1001 15:23:28.341019 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.inference_driver_name : NoneType
I1001 15:23:28.341069 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.is_eval : NoneType
I1001 15:23:28.341118 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.is_inference : NoneType
I1001 15:23:28.341168 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.max_timescale : 10000
I1001 15:23:28.341218 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.min_timescale : 1
I1001 15:23:28.341268 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.name : ''
I1001 15:23:28.341317 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.params_init.method : 'xavier'
I1001 15:23:28.341367 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.params_init.scale : 1.000001
I1001 15:23:28.341416 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.params_init.seed : NoneType
I1001 15:23:28.341465 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.random_seed : NoneType
I1001 15:23:28.341515 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.skip_lp_regularization : NoneType
I1001 15:23:28.341564 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.trainable_scaling : False
I1001 15:23:28.341614 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.trainable_scaling_init : 1.0
I1001 15:23:28.341668 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.vn.global_vn : False
I1001 15:23:28.341718 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.vn.per_step_vn : False
I1001 15:23:28.341768 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.vn.scale : NoneType
I1001 15:23:28.341817 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.vn.seed : NoneType
I1001 15:23:28.341867 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.random_seed : NoneType
I1001 15:23:28.341916 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.skip_lp_regularization : NoneType
I1001 15:23:28.341966 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.allow_implicit_capture : NoneType
I1001 15:23:28.342015 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.apply_pruning : False
I1001 15:23:28.342065 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.cls : type/lingvo.core.layers/SimpleEmbeddingLayer
I1001 15:23:28.342114 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.dtype : float32
I1001 15:23:28.342164 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.embedding_dim : 2048
I1001 15:23:28.342213 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.fprop_dtype : NoneType
I1001 15:23:28.342262 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.fprop_mode : NoneType
I1001 15:23:28.342312 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.inference_driver_name : NoneType
I1001 15:23:28.342361 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.is_eval : NoneType
I1001 15:23:28.342411 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.is_inference : NoneType
I1001 15:23:28.342461 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.name : ''
I1001 15:23:28.342510 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.params_init.method : 'gaussian'
I1001 15:23:28.342577 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.params_init.scale : 0.022097086912079608
I1001 15:23:28.342631 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.params_init.seed : NoneType
I1001 15:23:28.342682 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.qdomain.default : NoneType
I1001 15:23:28.342732 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.random_seed : NoneType
I1001 15:23:28.342782 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.skip_lp_regularization : NoneType
I1001 15:23:28.342832 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.use_3d_weight_tensor : False
I1001 15:23:28.342882 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.use_matmul : False
I1001 15:23:28.342931 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.vn.global_vn : False
I1001 15:23:28.342981 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.vn.per_step_vn : False
I1001 15:23:28.343031 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.vn.scale : NoneType
I1001 15:23:28.343080 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.vn.seed : NoneType
I1001 15:23:28.343131 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.vocab_size : 32000
I1001 15:23:28.343180 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.vn.global_vn : False
I1001 15:23:28.343230 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.vn.per_step_vn : False
I1001 15:23:28.343279 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.vn.scale : NoneType
I1001 15:23:28.343329 139818304546624 base_runner.py:59] task.lm.stack.emb_tpl.vn.seed : NoneType
I1001 15:23:28.343379 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.allow_implicit_capture : NoneType
I1001 15:23:28.343428 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.cls : type/lingvo.core.layers_with_gpipe/GPipeTransformerLayer
I1001 15:23:28.343482 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.dtype : float32
I1001 15:23:28.343533 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.final_enc_layer : False
I1001 15:23:28.343582 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.fprop_dtype : NoneType
I1001 15:23:28.343631 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.has_aux_atten : False
I1001 15:23:28.343681 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.inference_driver_name : NoneType
I1001 15:23:28.343731 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.is_decoder : False
I1001 15:23:28.343780 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.is_eval : NoneType
I1001 15:23:28.343830 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.is_inference : NoneType
I1001 15:23:28.343879 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.is_transparent : False
I1001 15:23:28.343928 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.allow_implicit_capture : NoneType
I1001 15:23:28.343977 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.cls : type/lingvo.core.layers/LayerNorm
I1001 15:23:28.344027 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.dtype : float32
I1001 15:23:28.344076 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.epsilon : 1e-06
I1001 15:23:28.344126 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.fprop_dtype : NoneType
I1001 15:23:28.344176 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.inference_driver_name : NoneType
I1001 15:23:28.344225 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.input_dim : 0
I1001 15:23:28.344274 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.is_eval : NoneType
I1001 15:23:28.344324 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.is_inference : NoneType
I1001 15:23:28.344373 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.name : ''
I1001 15:23:28.344423 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.params_init.method : 'xavier'
I1001 15:23:28.344473 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.params_init.scale : 1.000001
I1001 15:23:28.344522 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.params_init.seed : NoneType
I1001 15:23:28.344572 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.random_seed : NoneType
I1001 15:23:28.344621 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.skip_lp_regularization : NoneType
I1001 15:23:28.344670 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.vn.global_vn : False
I1001 15:23:28.344720 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.vn.per_step_vn : False
I1001 15:23:28.344769 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.vn.scale : NoneType
I1001 15:23:28.344818 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.vn.seed : NoneType
I1001 15:23:28.344868 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.mask_self_atten : True
I1001 15:23:28.344918 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.name : ''
I1001 15:23:28.344968 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.normalize_output : False
I1001 15:23:28.345018 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.output_dim : 0
I1001 15:23:28.345068 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.packed_input : False
I1001 15:23:28.345118 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.params_init.method : 'xavier'
I1001 15:23:28.345167 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.params_init.scale : 1.000001
I1001 15:23:28.345216 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.params_init.seed : NoneType
I1001 15:23:28.345267 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.random_seed : NoneType
I1001 15:23:28.345319 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.skip_lp_regularization : NoneType
I1001 15:23:28.345370 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.source_dim : 2048
I1001 15:23:28.345420 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.add_unnormalized_input : False
I1001 15:23:28.345469 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.allow_implicit_capture : NoneType
I1001 15:23:28.345519 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_dropout_prob : 0.0
I1001 15:23:28.345569 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_hidden_dim : 0
I1001 15:23:28.345618 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.allow_implicit_capture : NoneType
I1001 15:23:28.345668 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.atten_dropout_deterministic : False
I1001 15:23:28.345717 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.atten_dropout_prob : 0.0
I1001 15:23:28.345767 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.cls : type/lingvo.core.attention/MultiHeadedAttention
I1001 15:23:28.345816 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.context_dim : 0
I1001 15:23:28.345865 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.ctx_post_proj_dim : 0
I1001 15:23:28.345915 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.dtype : float32
I1001 15:23:28.345964 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.enable_ctx_post_proj : True
I1001 15:23:28.346014 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.enable_ctx_pre_proj : True
I1001 15:23:28.346063 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.enable_query_proj : True
I1001 15:23:28.346113 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.enable_source_proj : True
I1001 15:23:28.346163 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.fprop_dtype : NoneType
I1001 15:23:28.346213 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.hidden_dim : 0
I1001 15:23:28.346262 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inference_driver_name : NoneType
I1001 15:23:28.346312 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.allow_implicit_capture : NoneType
I1001 15:23:28.346361 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.atten_dropout_deterministic : False
I1001 15:23:28.346410 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.atten_dropout_prob : 0.0
I1001 15:23:28.346460 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.cls : type/lingvo.core.attention/DotProductAttention
I1001 15:23:28.346510 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.dtype : float32
I1001 15:23:28.346580 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.fprop_dtype : NoneType
I1001 15:23:28.346633 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.hidden_dim : 0
I1001 15:23:28.346683 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.inference_driver_name : NoneType
I1001 15:23:28.346734 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.is_eval : NoneType
I1001 15:23:28.346784 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.is_inference : NoneType
I1001 15:23:28.346838 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.name : ''
I1001 15:23:28.346889 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.packed_input : False
I1001 15:23:28.346940 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.params_init.method : 'xavier'
I1001 15:23:28.346990 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.params_init.scale : 1.000001
I1001 15:23:28.347039 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.params_init.seed : NoneType
I1001 15:23:28.347089 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.qdomain.default : NoneType
I1001 15:23:28.347138 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.qdomain.fullyconnected : NoneType
I1001 15:23:28.347187 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.qdomain.softmax : NoneType
I1001 15:23:28.347237 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.query_dim : 0
I1001 15:23:28.347286 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.random_seed : NoneType
I1001 15:23:28.347336 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.skip_lp_regularization : NoneType
I1001 15:23:28.347386 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.source_dim : 0
I1001 15:23:28.347435 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.global_vn : False
I1001 15:23:28.347485 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.per_step_vn : False
I1001 15:23:28.347535 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.scale : NoneType
I1001 15:23:28.347584 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.seed : NoneType
I1001 15:23:28.347635 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.is_eval : NoneType
I1001 15:23:28.347685 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.is_inference : NoneType
I1001 15:23:28.347734 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.name : ''
I1001 15:23:28.347784 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.num_attention_heads : 2
I1001 15:23:28.347834 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.packed_input : False
I1001 15:23:28.347884 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.params_init.method : 'xavier'
I1001 15:23:28.347933 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.params_init.scale : 1.0
I1001 15:23:28.347982 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.params_init.seed : NoneType
I1001 15:23:28.348031 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.qdomain.atten_context : NoneType
I1001 15:23:28.348081 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.qdomain.default : NoneType
I1001 15:23:28.348130 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.qdomain.fullyconnected : NoneType
I1001 15:23:28.348180 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.qdomain.softmax : NoneType
I1001 15:23:28.348229 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.query_dim : 0
I1001 15:23:28.348278 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.random_seed : NoneType
I1001 15:23:28.348332 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.skip_lp_regularization : NoneType
I1001 15:23:28.348383 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.source_dim : 0
I1001 15:23:28.348433 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.use_source_vec_as_attention_value : False
I1001 15:23:28.348483 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.vn.global_vn : False
I1001 15:23:28.348532 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.vn.per_step_vn : False
I1001 15:23:28.348582 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.vn.scale : NoneType
I1001 15:23:28.348632 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.vn.seed : NoneType
I1001 15:23:28.348682 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.cls : type/lingvo.core.layers_with_attention/TransformerAttentionLayer
I1001 15:23:28.348732 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.context_dim : 0
I1001 15:23:28.348781 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.dtype : float32
I1001 15:23:28.348831 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.fprop_dtype : NoneType
I1001 15:23:28.348880 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.inference_driver_name : NoneType
I1001 15:23:28.348930 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.is_eval : NoneType
I1001 15:23:28.348980 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.is_inference : NoneType
I1001 15:23:28.349029 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.is_masked : True
I1001 15:23:28.349078 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.allow_implicit_capture : NoneType
I1001 15:23:28.349128 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.cls : type/lingvo.core.layers/LayerNorm
I1001 15:23:28.349177 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.dtype : float32
I1001 15:23:28.349226 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.epsilon : 1e-06
I1001 15:23:28.349275 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.fprop_dtype : NoneType
I1001 15:23:28.349325 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.inference_driver_name : NoneType
I1001 15:23:28.349375 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.input_dim : 0
I1001 15:23:28.349423 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.is_eval : NoneType
I1001 15:23:28.349472 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.is_inference : NoneType
I1001 15:23:28.349521 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.name : ''
I1001 15:23:28.349570 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.params_init.method : 'xavier'
I1001 15:23:28.349620 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.params_init.scale : 1.000001
I1001 15:23:28.349669 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.params_init.seed : NoneType
I1001 15:23:28.349718 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.random_seed : NoneType
I1001 15:23:28.349767 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.skip_lp_regularization : NoneType
I1001 15:23:28.349816 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.vn.global_vn : False
I1001 15:23:28.349865 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.vn.per_step_vn : False
I1001 15:23:28.349920 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.vn.scale : NoneType
I1001 15:23:28.349971 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.vn.seed : NoneType
I1001 15:23:28.350021 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.mask_type : 'future'
I1001 15:23:28.350070 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.name : ''
I1001 15:23:28.350120 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.num_attention_heads : 16
I1001 15:23:28.350169 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.packed_input : False
I1001 15:23:28.350219 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.params_init.method : 'xavier'
I1001 15:23:28.350269 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.params_init.scale : 1.000001
I1001 15:23:28.350318 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.params_init.seed : NoneType
I1001 15:23:28.350371 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.random_seed : NoneType
I1001 15:23:28.350421 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_prob : 0.0
I1001 15:23:28.350471 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.allow_implicit_capture : NoneType
I1001 15:23:28.350519 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.cls : type/lingvo.core.layers/DropoutLayer
I1001 15:23:28.350586 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.dropout_at_eval : False
I1001 15:23:28.350637 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.dtype : float32
I1001 15:23:28.350687 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.fprop_dtype : NoneType
I1001 15:23:28.350736 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.inference_driver_name : NoneType
I1001 15:23:28.350785 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.is_eval : NoneType
I1001 15:23:28.350835 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.is_inference : NoneType
I1001 15:23:28.350884 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.keep_prob : 1.0
I1001 15:23:28.350934 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.name : ''
I1001 15:23:28.350983 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.noise_shape : NoneType
I1001 15:23:28.351033 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.noise_shape_broadcast_dims : NoneType
I1001 15:23:28.351083 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.params_init.method : 'xavier'
I1001 15:23:28.351132 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.params_init.scale : 1.000001
I1001 15:23:28.351181 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.params_init.seed : NoneType
I1001 15:23:28.351231 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.random_seed : NoneType
I1001 15:23:28.351280 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.skip_lp_regularization : NoneType
I1001 15:23:28.351329 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.global_vn : False
I1001 15:23:28.351378 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.per_step_vn : False
I1001 15:23:28.351431 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.scale : NoneType
I1001 15:23:28.351482 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.seed : NoneType
I1001 15:23:28.351531 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.skip_lp_regularization : NoneType
I1001 15:23:28.351581 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.source_dim : 0
I1001 15:23:28.351631 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.vn.global_vn : False
I1001 15:23:28.351681 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.vn.per_step_vn : False
I1001 15:23:28.351731 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.vn.scale : NoneType
I1001 15:23:28.351780 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.vn.seed : NoneType
I1001 15:23:28.351830 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_aux_atten_tpl : NoneType
I1001 15:23:28.351879 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.activation : 'RELU'
I1001 15:23:28.351929 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.allow_implicit_capture : NoneType
I1001 15:23:28.351979 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.cls : type/lingvo.core.layers_with_attention/TransformerFeedForwardLayer
I1001 15:23:28.352029 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.dtype : float32
I1001 15:23:28.352079 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.activation : ['RELU', 'NONE']
I1001 15:23:28.352128 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.allow_implicit_capture : NoneType
I1001 15:23:28.352178 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.batch_norm : False
I1001 15:23:28.352228 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.bn_fold_weights : NoneType
I1001 15:23:28.352277 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.cls : type/lingvo.core.layers/FeedForwardNet
I1001 15:23:28.352327 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.allow_implicit_capture : NoneType
I1001 15:23:28.352376 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.cls : type/lingvo.core.layers/DropoutLayer
I1001 15:23:28.352426 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.dropout_at_eval : False
I1001 15:23:28.352475 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.dtype : float32
I1001 15:23:28.352524 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.fprop_dtype : NoneType
I1001 15:23:28.352574 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.inference_driver_name : NoneType
I1001 15:23:28.352624 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.is_eval : NoneType
I1001 15:23:28.352674 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.is_inference : NoneType
I1001 15:23:28.352723 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.keep_prob : 1.0
I1001 15:23:28.352773 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.name : ''
I1001 15:23:28.352823 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.noise_shape : NoneType
I1001 15:23:28.352873 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.noise_shape_broadcast_dims : NoneType
I1001 15:23:28.352924 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.params_init.method : 'xavier'
I1001 15:23:28.352978 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.params_init.scale : 1.000001
I1001 15:23:28.353028 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.params_init.seed : NoneType
I1001 15:23:28.353077 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.random_seed : NoneType
I1001 15:23:28.353127 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.skip_lp_regularization : NoneType
I1001 15:23:28.353176 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.global_vn : False
I1001 15:23:28.353226 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.per_step_vn : False
I1001 15:23:28.353276 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.scale : NoneType
I1001 15:23:28.353325 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.seed : NoneType
I1001 15:23:28.353375 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dtype : float32
I1001 15:23:28.353424 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.fprop_dtype : NoneType
I1001 15:23:28.353474 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.inference_driver_name : NoneType
I1001 15:23:28.353523 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.input_dim : 0
I1001 15:23:28.353573 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.is_eval : NoneType
I1001 15:23:28.353623 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.is_inference : NoneType
I1001 15:23:28.353672 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.name : ''
I1001 15:23:28.353722 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.params_init.method : 'xavier'
I1001 15:23:28.353771 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.params_init.scale : 1.000001
I1001 15:23:28.353819 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.params_init.seed : NoneType
I1001 15:23:28.353868 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.activation : 'RELU'
I1001 15:23:28.353918 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.affine_last : False
I1001 15:23:28.353967 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.allow_implicit_capture : NoneType
I1001 15:23:28.354016 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.batch_norm : True
I1001 15:23:28.354065 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.bias_init : 0.0
I1001 15:23:28.354115 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.bn_fold_weights : NoneType
I1001 15:23:28.354165 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.cls : type/lingvo.core.layers/ProjectionLayer
I1001 15:23:28.354214 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.dtype : float32
I1001 15:23:28.354264 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.fprop_dtype : NoneType
I1001 15:23:28.354313 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.has_bias : False
I1001 15:23:28.354362 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.inference_driver_name : NoneType
I1001 15:23:28.354416 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.input_dim : 0
I1001 15:23:28.354467 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.is_eval : NoneType
I1001 15:23:28.354516 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.is_inference : NoneType
I1001 15:23:28.354583 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.name : ''
I1001 15:23:28.354635 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.output_dim : 0
I1001 15:23:28.354685 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.params_init.method : 'xavier'
I1001 15:23:28.354734 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.params_init.scale : 1.000001
I1001 15:23:28.354784 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.params_init.seed : NoneType
I1001 15:23:28.354833 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.qdomain.default : NoneType
I1001 15:23:28.354882 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.random_seed : NoneType
I1001 15:23:28.354932 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.skip_lp_regularization : NoneType
I1001 15:23:28.354981 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.global_vn : False
I1001 15:23:28.355029 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.per_step_vn : False
I1001 15:23:28.355078 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.scale : NoneType
I1001 15:23:28.355127 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.seed : NoneType
I1001 15:23:28.355176 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.weight_norm : False
I1001 15:23:28.355225 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.qdomain.default : NoneType
I1001 15:23:28.355274 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.random_seed : NoneType
I1001 15:23:28.355323 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.skip_connections : NoneType
I1001 15:23:28.355372 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.skip_lp_regularization : NoneType
I1001 15:23:28.355421 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.global_vn : False
I1001 15:23:28.355470 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.per_step_vn : False
I1001 15:23:28.355519 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.scale : NoneType
I1001 15:23:28.355568 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.seed : NoneType
I1001 15:23:28.355618 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.weight_norm : False
I1001 15:23:28.355666 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fprop_dtype : NoneType
I1001 15:23:28.355716 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.hidden_dim : 8192
I1001 15:23:28.355765 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.inference_driver_name : NoneType
I1001 15:23:28.355814 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.input_dim : 0
I1001 15:23:28.355864 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.is_eval : NoneType
I1001 15:23:28.355918 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.is_inference : NoneType
I1001 15:23:28.355969 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.allow_implicit_capture : NoneType
I1001 15:23:28.356018 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.cls : type/lingvo.core.layers/LayerNorm
I1001 15:23:28.356067 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.dtype : float32
I1001 15:23:28.356117 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.epsilon : 1e-06
I1001 15:23:28.356166 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.fprop_dtype : NoneType
I1001 15:23:28.356215 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.inference_driver_name : NoneType
I1001 15:23:28.356265 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.input_dim : 0
I1001 15:23:28.356315 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.is_eval : NoneType
I1001 15:23:28.356365 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.is_inference : NoneType
I1001 15:23:28.356415 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.name : ''
I1001 15:23:28.356465 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.params_init.method : 'xavier'
I1001 15:23:28.356514 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.params_init.scale : 1.000001
I1001 15:23:28.356564 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.params_init.seed : NoneType
I1001 15:23:28.356613 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.random_seed : NoneType
I1001 15:23:28.356663 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.skip_lp_regularization : NoneType
I1001 15:23:28.356713 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.vn.global_vn : False
I1001 15:23:28.356763 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.vn.per_step_vn : False
I1001 15:23:28.356812 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.vn.scale : NoneType
I1001 15:23:28.356862 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.vn.seed : NoneType
I1001 15:23:28.356912 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.name : ''
I1001 15:23:28.356962 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.output_dim : 0
I1001 15:23:28.357011 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.params_init.method : 'xavier'
I1001 15:23:28.357061 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.params_init.scale : 1.000001
I1001 15:23:28.357110 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.params_init.seed : NoneType
I1001 15:23:28.357159 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.random_seed : NoneType
I1001 15:23:28.357209 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.relu_dropout_prob : 0.0
I1001 15:23:28.357258 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.activation : 'RELU'
I1001 15:23:28.357308 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.affine_last : False
I1001 15:23:28.357358 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.allow_implicit_capture : NoneType
I1001 15:23:28.357407 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.batch_norm : True
I1001 15:23:28.357457 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.bias_init : 0.0
I1001 15:23:28.357510 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.bn_fold_weights : NoneType
I1001 15:23:28.357561 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.cls : type/lingvo.core.layers/ProjectionLayer
I1001 15:23:28.357611 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.dtype : float32
I1001 15:23:28.357661 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.fprop_dtype : NoneType
I1001 15:23:28.357711 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.has_bias : False
I1001 15:23:28.357761 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.inference_driver_name : NoneType
I1001 15:23:28.357811 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.input_dim : 0
I1001 15:23:28.357860 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.is_eval : NoneType
I1001 15:23:28.357910 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.is_inference : NoneType
I1001 15:23:28.357960 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.name : ''
I1001 15:23:28.358009 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.output_dim : 0
I1001 15:23:28.358059 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.params_init.method : 'xavier'
I1001 15:23:28.358109 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.params_init.scale : 1.000001
I1001 15:23:28.358159 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.params_init.seed : NoneType
I1001 15:23:28.358209 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.qdomain.default : NoneType
I1001 15:23:28.358258 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.random_seed : NoneType
I1001 15:23:28.358308 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.skip_lp_regularization : NoneType
I1001 15:23:28.358357 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.global_vn : False
I1001 15:23:28.358407 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.per_step_vn : False
I1001 15:23:28.358457 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.scale : NoneType
I1001 15:23:28.358506 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.seed : NoneType
I1001 15:23:28.358575 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.weight_norm : False
I1001 15:23:28.358629 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_prob : 0.0
I1001 15:23:28.358680 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.allow_implicit_capture : NoneType
I1001 15:23:28.358730 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.cls : type/lingvo.core.layers/DropoutLayer
I1001 15:23:28.358779 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.dropout_at_eval : False
I1001 15:23:28.358829 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.dtype : float32
I1001 15:23:28.358879 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.fprop_dtype : NoneType
I1001 15:23:28.358928 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.inference_driver_name : NoneType
I1001 15:23:28.358978 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.is_eval : NoneType
I1001 15:23:28.359033 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.is_inference : NoneType
I1001 15:23:28.359084 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.keep_prob : 1.0
I1001 15:23:28.359134 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.name : ''
I1001 15:23:28.359184 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.noise_shape : NoneType
I1001 15:23:28.359234 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.noise_shape_broadcast_dims : NoneType
I1001 15:23:28.359283 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.params_init.method : 'xavier'
I1001 15:23:28.359333 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.params_init.scale : 1.000001
I1001 15:23:28.359383 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.params_init.seed : NoneType
I1001 15:23:28.359432 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.random_seed : NoneType
I1001 15:23:28.359483 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.skip_lp_regularization : NoneType
I1001 15:23:28.359533 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.global_vn : False
I1001 15:23:28.359582 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.per_step_vn : False
I1001 15:23:28.359631 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.scale : NoneType
I1001 15:23:28.359680 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.seed : NoneType
I1001 15:23:28.359730 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.skip_lp_regularization : NoneType
I1001 15:23:28.359779 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.vn.global_vn : False
I1001 15:23:28.359829 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.vn.per_step_vn : False
I1001 15:23:28.359879 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.vn.scale : NoneType
I1001 15:23:28.359928 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.vn.seed : NoneType
I1001 15:23:28.359978 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.transparent_merger_tpl : NoneType
I1001 15:23:28.360027 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.vn.global_vn : False
I1001 15:23:28.360077 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.vn.per_step_vn : False
I1001 15:23:28.360126 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.vn.scale : NoneType
I1001 15:23:28.360176 139818304546624 base_runner.py:59] task.lm.stack.encoder_tpl.vn.seed : NoneType
I1001 15:23:28.360226 139818304546624 base_runner.py:59] task.lm.stack.fprop_dtype : NoneType
I1001 15:23:28.360275 139818304546624 base_runner.py:59] task.lm.stack.inference_driver_name : NoneType
I1001 15:23:28.360325 139818304546624 base_runner.py:59] task.lm.stack.is_eval : NoneType
I1001 15:23:28.360374 139818304546624 base_runner.py:59] task.lm.stack.is_inference : NoneType
I1001 15:23:28.360427 139818304546624 base_runner.py:59] task.lm.stack.is_transparent : False
I1001 15:23:28.360477 139818304546624 base_runner.py:59] task.lm.stack.label_smoothing : NoneType
I1001 15:23:28.360527 139818304546624 base_runner.py:59] task.lm.stack.model_dim : 2048
I1001 15:23:28.360576 139818304546624 base_runner.py:59] task.lm.stack.name : ''
I1001 15:23:28.360625 139818304546624 base_runner.py:59] task.lm.stack.normalize_encoder : False
I1001 15:23:28.360674 139818304546624 base_runner.py:59] task.lm.stack.num_decoder_layers : 0
I1001 15:23:28.360729 139818304546624 base_runner.py:59] task.lm.stack.num_encoder_layers : 32
I1001 15:23:28.360780 139818304546624 base_runner.py:59] task.lm.stack.num_micro_batches : 32
I1001 15:23:28.360829 139818304546624 base_runner.py:59] task.lm.stack.packed_input : False
I1001 15:23:28.360878 139818304546624 base_runner.py:59] task.lm.stack.params_init.method : 'xavier'
I1001 15:23:28.360928 139818304546624 base_runner.py:59] task.lm.stack.params_init.scale : 1.000001
I1001 15:23:28.360978 139818304546624 base_runner.py:59] task.lm.stack.params_init.seed : NoneType
I1001 15:23:28.361028 139818304546624 base_runner.py:59] task.lm.stack.random_seed : NoneType
I1001 15:23:28.361077 139818304546624 base_runner.py:59] task.lm.stack.skip_lp_regularization : NoneType
I1001 15:23:28.361127 139818304546624 base_runner.py:59] task.lm.stack.softmax_tpl.allow_implicit_capture : NoneType
I1001 15:23:28.361177 139818304546624 base_runner.py:59] task.lm.stack.softmax_tpl.apply_pruning : False
I1001 15:23:28.361226 139818304546624 base_runner.py:59] task.lm.stack.softmax_tpl.chunk_size : 4194
I1001 15:23:28.361275 139818304546624 base_runner.py:59] task.lm.stack.softmax_tpl.cls : type/lingvo.core.layers_with_gpipe/GPipeTransformerSoftmaxLayer
I1001 15:23:28.361325 139818304546624 base_runner.py:59] task.lm.stack.softmax_tpl.dtype : float32
I1001 15:23:28.361374 139818304546624 base_runner.py:59] task.lm.stack.softmax_tpl.fprop_dtype : NoneType
I1001 15:23:28.361423 139818304546624 base_runner.py:59] task.lm.stack.softmax_tpl.inference_driver_name : NoneType
I1001 15:23:28.361473 139818304546624 base_runner.py:59] task.lm.stack.softmax_tpl.input_dim : 2048
I1001 15:23:28.361522 139818304546624 base_runner.py:59] task.lm.stack.softmax_tpl.inputs_from_decoder : False
I1001 15:23:28.361571 139818304546624 base_runner.py:59] task.lm.stack.softmax_tpl.is_eval : NoneType
I1001 15:23:28.361621 139818304546624 base_runner.py:59] task.lm.stack.softmax_tpl.is_inference : NoneType
I1001 15:23:28.361670 139818304546624 base_runner.py:59] task.lm.stack.softmax_tpl.logits_abs_max : NoneType
I1001 15:23:28.361720 139818304546624 base_runner.py:59] task.lm.stack.softmax_tpl.name : ''
I1001 15:23:28.361770 139818304546624 base_runner.py:59] task.lm.stack.softmax_tpl.num_classes : 32000
I1001 15:23:28.361818 139818304546624 base_runner.py:59] task.lm.stack.softmax_tpl.num_sampled : 0
I1001 15:23:28.361867 139818304546624 base_runner.py:59] task.lm.stack.softmax_tpl.num_shards : 16
I1001 15:23:28.361917 139818304546624 base_runner.py:59] task.lm.stack.softmax_tpl.params_init.method : 'xavier'
I1001 15:23:28.361966 139818304546624 base_runner.py:59] task.lm.stack.softmax_tpl.params_init.scale : 1.000001
I1001 15:23:28.362015 139818304546624 base_runner.py:59] task.lm.stack.softmax_tpl.params_init.seed : NoneType
I1001 15:23:28.362064 139818304546624 base_runner.py:59] task.lm.stack.softmax_tpl.qdomain.default : NoneType
I1001 15:23:28.362114 139818304546624 base_runner.py:59] task.lm.stack.softmax_tpl.random_seed : NoneType
I1001 15:23:28.362163 139818304546624 base_runner.py:59] task.lm.stack.softmax_tpl.skip_lp_regularization : NoneType
I1001 15:23:28.362213 139818304546624 base_runner.py:59] task.lm.stack.softmax_tpl.vn.global_vn : False
I1001 15:23:28.362262 139818304546624 base_runner.py:59] task.lm.stack.softmax_tpl.vn.per_step_vn : False
I1001 15:23:28.362312 139818304546624 base_runner.py:59] task.lm.stack.softmax_tpl.vn.scale : NoneType
I1001 15:23:28.362361 139818304546624 base_runner.py:59] task.lm.stack.softmax_tpl.vn.seed : NoneType
I1001 15:23:28.362411 139818304546624 base_runner.py:59] task.lm.stack.splits : [8, 16, 24, 32]
I1001 15:23:28.362459 139818304546624 base_runner.py:59] task.lm.stack.state_dtype : float32
I1001 15:23:28.362509 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_dropout_prob : 0.1
I1001 15:23:28.362575 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.allow_implicit_capture : NoneType
I1001 15:23:28.362628 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.cls : type/lingvo.core.layers_with_gpipe/DeterministicWeightsLayer
I1001 15:23:28.362683 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.allow_implicit_capture : NoneType
I1001 15:23:28.362734 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.cls : type/lingvo.core.layers/DeterministicDropoutLayer
I1001 15:23:28.362784 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.dropout_at_eval : False
I1001 15:23:28.362833 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.dtype : float32
I1001 15:23:28.362884 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.fprop_dtype : NoneType
I1001 15:23:28.362934 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.inference_driver_name : NoneType
I1001 15:23:28.362983 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.is_eval : NoneType
I1001 15:23:28.363037 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.is_inference : NoneType
I1001 15:23:28.363088 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.keep_prob : 1.0
I1001 15:23:28.363138 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.name : ''
I1001 15:23:28.363189 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.noise_shape : NoneType
I1001 15:23:28.363239 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.noise_shape_broadcast_dims : NoneType
I1001 15:23:28.363289 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.params_init.method : 'xavier'
I1001 15:23:28.363338 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.params_init.scale : 1.000001
I1001 15:23:28.363389 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.params_init.seed : NoneType
I1001 15:23:28.363438 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.random_seed : NoneType
I1001 15:23:28.363488 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.skip_lp_regularization : NoneType
I1001 15:23:28.363537 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.vn.global_vn : False
I1001 15:23:28.363587 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.vn.per_step_vn : False
I1001 15:23:28.363636 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.vn.scale : NoneType
I1001 15:23:28.363686 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.vn.seed : NoneType
I1001 15:23:28.363736 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dtype : float32
I1001 15:23:28.363785 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.fprop_dtype : NoneType
I1001 15:23:28.363835 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.global_weight_scale : 1.0
I1001 15:23:28.363884 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.inference_driver_name : NoneType
I1001 15:23:28.363933 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.is_eval : NoneType
I1001 15:23:28.363983 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.is_inference : NoneType
I1001 15:23:28.364032 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.minimal_prob : 0.0
I1001 15:23:28.364083 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.name : ''
I1001 15:23:28.364133 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.num_sources : 0
I1001 15:23:28.364182 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.params_init.method : 'xavier'
I1001 15:23:28.364232 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.params_init.scale : 1.000001
I1001 15:23:28.364286 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.params_init.seed : NoneType
I1001 15:23:28.364336 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.random_seed : NoneType
I1001 15:23:28.364386 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.skip_lp_regularization : NoneType
I1001 15:23:28.364435 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.vn.global_vn : False
I1001 15:23:28.364485 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.vn.per_step_vn : False
I1001 15:23:28.364534 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.vn.scale : NoneType
I1001 15:23:28.364583 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.vn.seed : NoneType
I1001 15:23:28.364633 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.weighted_merger_dropout_prob : 0.0
I1001 15:23:28.364683 139818304546624 base_runner.py:59] task.lm.stack.transparent_merger_tpl.weighted_merger_softmax : True
I1001 15:23:28.364733 139818304546624 base_runner.py:59] task.lm.stack.use_pipelined_embeddings : True
I1001 15:23:28.364783 139818304546624 base_runner.py:59] task.lm.stack.vn.global_vn : False
I1001 15:23:28.364833 139818304546624 base_runner.py:59] task.lm.stack.vn.per_step_vn : False
I1001 15:23:28.364882 139818304546624 base_runner.py:59] task.lm.stack.vn.scale : NoneType
I1001 15:23:28.364932 139818304546624 base_runner.py:59] task.lm.stack.vn.seed : NoneType
I1001 15:23:28.364981 139818304546624 base_runner.py:59] task.lm.vn.global_vn : False
I1001 15:23:28.365031 139818304546624 base_runner.py:59] task.lm.vn.per_step_vn : False
I1001 15:23:28.365082 139818304546624 base_runner.py:59] task.lm.vn.scale : NoneType
I1001 15:23:28.365131 139818304546624 base_runner.py:59] task.lm.vn.seed : NoneType
I1001 15:23:28.365181 139818304546624 base_runner.py:59] task.lm.vocab_size : 32000
I1001 15:23:28.365230 139818304546624 base_runner.py:59] task.name : '1bwds_wpm_level_lm'
I1001 15:23:28.365281 139818304546624 base_runner.py:59] task.online_encoder : NoneType
I1001 15:23:28.365330 139818304546624 base_runner.py:59] task.params_init.method : 'xavier'
I1001 15:23:28.365380 139818304546624 base_runner.py:59] task.params_init.scale : 1.000001
I1001 15:23:28.365430 139818304546624 base_runner.py:59] task.params_init.seed : NoneType
I1001 15:23:28.365480 139818304546624 base_runner.py:59] task.random_seed : NoneType
I1001 15:23:28.365529 139818304546624 base_runner.py:59] task.skip_lp_regularization : NoneType
I1001 15:23:28.365579 139818304546624 base_runner.py:59] task.train.bprop_variable_exclusion : NoneType
I1001 15:23:28.365629 139818304546624 base_runner.py:59] task.train.bprop_variable_filter : NoneType
I1001 15:23:28.365679 139818304546624 base_runner.py:59] task.train.clip_gradient_norm_to_value : 0.0
I1001 15:23:28.365729 139818304546624 base_runner.py:59] task.train.clip_gradient_single_norm_to_value : 0.0
I1001 15:23:28.365778 139818304546624 base_runner.py:59] task.train.colocate_gradients_with_ops : True
I1001 15:23:28.365828 139818304546624 base_runner.py:59] task.train.early_stop.metric_history.jobname : 'eval_dev'
I1001 15:23:28.365878 139818304546624 base_runner.py:59] task.train.early_stop.metric_history.local_filesystem : False
I1001 15:23:28.365927 139818304546624 base_runner.py:59] task.train.early_stop.metric_history.logdir : ''
I1001 15:23:28.365977 139818304546624 base_runner.py:59] task.train.early_stop.metric_history.metric : 'log_pplx'
I1001 15:23:28.366028 139818304546624 base_runner.py:59] task.train.early_stop.metric_history.minimize : True
I1001 15:23:28.366078 139818304546624 base_runner.py:59] task.train.early_stop.metric_history.name : 'MetricHistory'
I1001 15:23:28.366128 139818304546624 base_runner.py:59] task.train.early_stop.metric_history.tfevent_file : False
I1001 15:23:28.366178 139818304546624 base_runner.py:59] task.train.early_stop.min_steps : 0
I1001 15:23:28.366231 139818304546624 base_runner.py:59] task.train.early_stop.name : 'EarlyStop'
I1001 15:23:28.366281 139818304546624 base_runner.py:59] task.train.early_stop.tolerance : 0.0
I1001 15:23:28.366331 139818304546624 base_runner.py:59] task.train.early_stop.verbose : True
I1001 15:23:28.366380 139818304546624 base_runner.py:59] task.train.early_stop.window : 0
I1001 15:23:28.366430 139818304546624 base_runner.py:59] task.train.ema_decay : 0.0
I1001 15:23:28.366479 139818304546624 base_runner.py:59] task.train.enqueue_max_steps : -1
I1001 15:23:28.366529 139818304546624 base_runner.py:59] task.train.gate_gradients : False
I1001 15:23:28.366597 139818304546624 base_runner.py:59] task.train.grad_aggregation_method : 1
I1001 15:23:28.366648 139818304546624 base_runner.py:59] task.train.grad_norm_to_clip_to_zero : 0.0
I1001 15:23:28.366698 139818304546624 base_runner.py:59] task.train.grad_norm_tracker : NoneType
I1001 15:23:28.366747 139818304546624 base_runner.py:59] task.train.init_from_checkpoint_rules : {}
I1001 15:23:28.366797 139818304546624 base_runner.py:59] task.train.l1_regularizer_weight : NoneType
I1001 15:23:28.366846 139818304546624 base_runner.py:59] task.train.l2_regularizer_weight : 1e-06
I1001 15:23:28.366896 139818304546624 base_runner.py:59] task.train.learner : NoneType
I1001 15:23:28.366945 139818304546624 base_runner.py:59] task.train.learning_rate : 0.5
I1001 15:23:28.366994 139818304546624 base_runner.py:59] task.train.lr_schedule.allow_implicit_capture : NoneType
I1001 15:23:28.367044 139818304546624 base_runner.py:59] task.train.lr_schedule.cls : type/lingvo.core.schedule/TransformerLearningRateSchedule
I1001 15:23:28.367093 139818304546624 base_runner.py:59] task.train.lr_schedule.decay_end : NoneType
I1001 15:23:28.367142 139818304546624 base_runner.py:59] task.train.lr_schedule.dtype : float32
I1001 15:23:28.367191 139818304546624 base_runner.py:59] task.train.lr_schedule.fprop_dtype : NoneType
I1001 15:23:28.367240 139818304546624 base_runner.py:59] task.train.lr_schedule.inference_driver_name : NoneType
I1001 15:23:28.367290 139818304546624 base_runner.py:59] task.train.lr_schedule.is_eval : NoneType
I1001 15:23:28.367339 139818304546624 base_runner.py:59] task.train.lr_schedule.is_inference : NoneType
I1001 15:23:28.367388 139818304546624 base_runner.py:59] task.train.lr_schedule.model_dim : 2048
I1001 15:23:28.367437 139818304546624 base_runner.py:59] task.train.lr_schedule.name : 'LRSched'
I1001 15:23:28.367486 139818304546624 base_runner.py:59] task.train.lr_schedule.params_init.method : 'xavier'
I1001 15:23:28.367535 139818304546624 base_runner.py:59] task.train.lr_schedule.params_init.scale : 1.000001
I1001 15:23:28.367584 139818304546624 base_runner.py:59] task.train.lr_schedule.params_init.seed : NoneType
I1001 15:23:28.367633 139818304546624 base_runner.py:59] task.train.lr_schedule.random_seed : NoneType
I1001 15:23:28.367682 139818304546624 base_runner.py:59] task.train.lr_schedule.skip_lp_regularization : NoneType
I1001 15:23:28.367732 139818304546624 base_runner.py:59] task.train.lr_schedule.vn.global_vn : False
I1001 15:23:28.367781 139818304546624 base_runner.py:59] task.train.lr_schedule.vn.per_step_vn : False
I1001 15:23:28.367830 139818304546624 base_runner.py:59] task.train.lr_schedule.vn.scale : NoneType
I1001 15:23:28.367879 139818304546624 base_runner.py:59] task.train.lr_schedule.vn.seed : NoneType
I1001 15:23:28.367928 139818304546624 base_runner.py:59] task.train.lr_schedule.warmup_steps : 40000
I1001 15:23:28.367977 139818304546624 base_runner.py:59] task.train.lr_schedule.worker_replicas : 1
I1001 15:23:28.368026 139818304546624 base_runner.py:59] task.train.max_lstm_gradient_norm : 0.0
I1001 15:23:28.368075 139818304546624 base_runner.py:59] task.train.max_steps : 4000000
I1001 15:23:28.368124 139818304546624 base_runner.py:59] task.train.optimizer.allow_implicit_capture : NoneType
I1001 15:23:28.368173 139818304546624 base_runner.py:59] task.train.optimizer.beta1 : 0.9
I1001 15:23:28.368222 139818304546624 base_runner.py:59] task.train.optimizer.beta2 : 0.997
I1001 15:23:28.368276 139818304546624 base_runner.py:59] task.train.optimizer.cls : type/lingvo.core.optimizer/Adam
I1001 15:23:28.368326 139818304546624 base_runner.py:59] task.train.optimizer.dtype : float32
I1001 15:23:28.368376 139818304546624 base_runner.py:59] task.train.optimizer.epsilon : 1e-09
I1001 15:23:28.368426 139818304546624 base_runner.py:59] task.train.optimizer.fprop_dtype : NoneType
I1001 15:23:28.368476 139818304546624 base_runner.py:59] task.train.optimizer.inference_driver_name : NoneType
I1001 15:23:28.368526 139818304546624 base_runner.py:59] task.train.optimizer.is_eval : NoneType
I1001 15:23:28.368576 139818304546624 base_runner.py:59] task.train.optimizer.is_inference : NoneType
I1001 15:23:28.368625 139818304546624 base_runner.py:59] task.train.optimizer.name : 'Adam'
I1001 15:23:28.368675 139818304546624 base_runner.py:59] task.train.optimizer.params_init.method : 'xavier'
I1001 15:23:28.368725 139818304546624 base_runner.py:59] task.train.optimizer.params_init.scale : 1.000001
I1001 15:23:28.368775 139818304546624 base_runner.py:59] task.train.optimizer.params_init.seed : NoneType
I1001 15:23:28.368824 139818304546624 base_runner.py:59] task.train.optimizer.random_seed : NoneType
I1001 15:23:28.368874 139818304546624 base_runner.py:59] task.train.optimizer.skip_lp_regularization : NoneType
I1001 15:23:28.368924 139818304546624 base_runner.py:59] task.train.optimizer.vn.global_vn : False
I1001 15:23:28.368973 139818304546624 base_runner.py:59] task.train.optimizer.vn.per_step_vn : False
I1001 15:23:28.369023 139818304546624 base_runner.py:59] task.train.optimizer.vn.scale : NoneType
I1001 15:23:28.369073 139818304546624 base_runner.py:59] task.train.optimizer.vn.seed : NoneType
I1001 15:23:28.369122 139818304546624 base_runner.py:59] task.train.pruning_hparams_dict : NoneType
I1001 15:23:28.369172 139818304546624 base_runner.py:59] task.train.save_interval_seconds : 600
I1001 15:23:28.369222 139818304546624 base_runner.py:59] task.train.save_keep_checkpoint_every_n_hours : 0.5
I1001 15:23:28.369271 139818304546624 base_runner.py:59] task.train.save_max_to_keep : 100
I1001 15:23:28.369321 139818304546624 base_runner.py:59] task.train.start_up_delay_steps : 200
I1001 15:23:28.369371 139818304546624 base_runner.py:59] task.train.sum_loss_across_tokens_in_batch : False
I1001 15:23:28.369421 139818304546624 base_runner.py:59] task.train.summary_interval_steps : 100
I1001 15:23:28.369471 139818304546624 base_runner.py:59] task.train.tpu_steps_per_loop : 100
I1001 15:23:28.369520 139818304546624 base_runner.py:59] task.train.vn_start_step : 20000
I1001 15:23:28.369569 139818304546624 base_runner.py:59] task.train.vn_std : 0.0
I1001 15:23:28.369619 139818304546624 base_runner.py:59] task.vn.global_vn : False
I1001 15:23:28.369668 139818304546624 base_runner.py:59] task.vn.per_step_vn : False
I1001 15:23:28.369718 139818304546624 base_runner.py:59] task.vn.scale : NoneType
I1001 15:23:28.369768 139818304546624 base_runner.py:59] task.vn.seed : NoneType
I1001 15:23:28.369817 139818304546624 base_runner.py:59] train.early_stop.metric_history.jobname : 'eval_dev'
I1001 15:23:28.369867 139818304546624 base_runner.py:59] train.early_stop.metric_history.local_filesystem : False
I1001 15:23:28.369918 139818304546624 base_runner.py:59] train.early_stop.metric_history.logdir : ''
I1001 15:23:28.369967 139818304546624 base_runner.py:59] train.early_stop.metric_history.metric : 'log_pplx'
I1001 15:23:28.370017 139818304546624 base_runner.py:59] train.early_stop.metric_history.minimize : True
I1001 15:23:28.370067 139818304546624 base_runner.py:59] train.early_stop.metric_history.name : 'MetricHistory'
I1001 15:23:28.370117 139818304546624 base_runner.py:59] train.early_stop.metric_history.tfevent_file : False
I1001 15:23:28.370166 139818304546624 base_runner.py:59] train.early_stop.min_steps : 0
I1001 15:23:28.370216 139818304546624 base_runner.py:59] train.early_stop.name : 'EarlyStop'
I1001 15:23:28.370265 139818304546624 base_runner.py:59] train.early_stop.tolerance : 0.0
I1001 15:23:28.370319 139818304546624 base_runner.py:59] train.early_stop.verbose : True
I1001 15:23:28.370370 139818304546624 base_runner.py:59] train.early_stop.window : 0
I1001 15:23:28.370421 139818304546624 base_runner.py:59] train.ema_decay : 0.0
I1001 15:23:28.370475 139818304546624 base_runner.py:59] train.enqueue_max_steps : -1
I1001 15:23:28.370525 139818304546624 base_runner.py:59] train.init_from_checkpoint_rules : {}
I1001 15:23:28.370599 139818304546624 base_runner.py:59] train.max_steps : 4000000
I1001 15:23:28.370651 139818304546624 base_runner.py:59] train.save_interval_seconds : 600
I1001 15:23:28.370702 139818304546624 base_runner.py:59] train.save_keep_checkpoint_every_n_hours : 0.5
I1001 15:23:28.370753 139818304546624 base_runner.py:59] train.save_max_to_keep : 100
I1001 15:23:28.370802 139818304546624 base_runner.py:59] train.start_up_delay_steps : 200
I1001 15:23:28.370852 139818304546624 base_runner.py:59] train.summary_interval_steps : 100
I1001 15:23:28.370902 139818304546624 base_runner.py:59] train.tpu_steps_per_loop : 100
I1001 15:23:28.370952 139818304546624 base_runner.py:59] vn.global_vn : False
I1001 15:23:28.371002 139818304546624 base_runner.py:59] vn.per_step_vn : False
I1001 15:23:28.371052 139818304546624 base_runner.py:59] vn.scale : NoneType
I1001 15:23:28.371102 139818304546624 base_runner.py:59] vn.seed : NoneType
I1001 15:23:28.371152 139818304546624 base_runner.py:59] 
I1001 15:23:28.371224 139818304546624 base_runner.py:60] ============================================================
I1001 15:23:28.373151 139818304546624 base_runner.py:106] Starting ...
I1001 15:23:28.373348 139818304546624 cluster.py:497] _LeastLoadedPlacer : ['/job:local/replica:0/task:0/device:CPU:0']
I1001 15:23:28.382128 139818304546624 cluster.py:515] Place variable global_step on /job:local/replica:0/task:0/device:CPU:0 8
I1001 15:23:28.395572 139818304546624 base_model.py:1093] Training parameters for <class 'lingvo.core.base_model.SingleTaskModel'>: {
  early_stop: {
    metric_history: {
"eval_dev"
      local_filesystem: False
"/tmp/mnist/log"
"log_pplx"
      minimize: True
"MetricHistory"
      tfevent_file: False
    }
    min_steps: 0
"EarlyStop"
    tolerance: 0.0
    verbose: True
    window: 0
  }
  ema_decay: 0.0
  enqueue_max_steps: -1
  init_from_checkpoint_rules: {}
  max_steps: 4000000
  save_interval_seconds: 600
  save_keep_checkpoint_every_n_hours: 0.5
  save_max_to_keep: 100
  start_up_delay_steps: 200
  summary_interval_steps: 100
  tpu_steps_per_loop: 100
}
I1001 15:23:28.411836 139818304546624 base_model.py:301] input_params: {
  allow_implicit_capture: None
  bucket_adjust_every_n: 0
  bucket_batch_limit: [32]
  bucket_upper_bound: [1024]
  cls: <class 'lingvo.tasks.lm.input_generator.LmInput'>
  dtype: <dtype: 'float32'>
  file_buffer_size: 10000000
  file_datasource: None
  file_parallelism: 10
"text:/tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en*"
  file_random_seed: 301
  fixed_input_shape: True
  flush_every_n: 0
  fprop_dtype: None
  inference_driver_name: None
  is_eval: None
  is_inference: None
"1bwds_train_set"
  num_batcher_threads: 16
  num_samples: 0
  pad_to_max_seq_length: False
  params_init: {
"xavier"
    scale: 1.000001
    seed: None
  }
  random_seed: None
  remote: {
    max_inflights_per_target: 32
    shardable_batch: False
  }
  require_sequential_order: False
  skip_lp_regularization: None
  source_max_length: None
  target_max_length: 1024
  tokenizer: {
    allow_implicit_capture: None
    append_eos: True
    cls: <class 'lingvo.core.tokenizers.AsciiTokenizer'>
    dtype: <dtype: 'float32'>
    fprop_dtype: None
    inference_driver_name: None
    is_eval: None
    is_inference: None
"tokenizer"
    pad_to_max_length: True
    params_init: {
"xavier"
      scale: 1.000001
      seed: None
    }
    random_seed: None
    skip_lp_regularization: None
    target_eos_id: 2
    target_sos_id: 1
    target_unk_id: 0
    vn: {
      global_vn: False
      per_step_vn: False
      scale: None
      seed: None
    }
    vocab_size: 32000
  }
  tokenizer_dict: {}
  tpu_infeed_parallelism: 1
  use_chaining: False
  use_per_host_infeed: False
  use_within_batch_mixing: False
  vn: {
    global_vn: False
    per_step_vn: False
    scale: None
    seed: None
  }
}
I1001 15:23:28.415222 139818304546624 base_input_generator.py:624] bucket_batch_limit [32]
I1001 15:23:28.464666 139818304546624 learner.py:351] Ignoring legacy param start_up_delay_steps=200 for optimization program
I1001 15:23:28.464772 139818304546624 learner.py:351] Ignoring legacy param max_steps=4000000 for optimization program
I1001 15:23:28.464840 139818304546624 learner.py:351] Ignoring legacy param tpu_steps_per_loop=100 for optimization program
I1001 15:23:28.464899 139818304546624 learner.py:351] Ignoring legacy param vn_start_step=20000 for optimization program
I1001 15:23:28.464954 139818304546624 learner.py:351] Ignoring legacy param vn_std=0.0 for optimization program
I1001 15:23:28.465008 139818304546624 learner.py:351] Ignoring legacy param early_stop={
  metric_history: {
"eval_dev"
    local_filesystem: False
"/tmp/mnist/log"
"log_pplx"
    minimize: True
"MetricHistory"
    tfevent_file: False
  }
  min_steps: 0
"EarlyStop"
  tolerance: 0.0
  verbose: True
  window: 0
} for optimization program
I1001 15:23:28.465120 139818304546624 learner.py:351] Ignoring legacy param ema_decay=0.0 for optimization program
I1001 15:23:28.465178 139818304546624 learner.py:351] Ignoring legacy param init_from_checkpoint_rules={} for optimization program
I1001 15:23:28.465232 139818304546624 learner.py:351] Ignoring legacy param pruning_hparams_dict=None for optimization program
I1001 15:23:28.465283 139818304546624 learner.py:351] Ignoring legacy param enqueue_max_steps=-1 for optimization program
I1001 15:23:28.465333 139818304546624 learner.py:351] Ignoring legacy param save_interval_seconds=600 for optimization program
I1001 15:23:28.465383 139818304546624 learner.py:351] Ignoring legacy param save_max_to_keep=100 for optimization program
I1001 15:23:28.465432 139818304546624 learner.py:351] Ignoring legacy param save_keep_checkpoint_every_n_hours=0.5 for optimization program
I1001 15:23:28.465484 139818304546624 learner.py:351] Ignoring legacy param summary_interval_steps=100 for optimization program
I1001 15:23:28.465533 139818304546624 learner.py:351] Ignoring legacy param learner=None for optimization program
I1001 15:23:28.465615 139818304546624 learner.py:351] Ignoring legacy param max_lstm_gradient_norm=0.0 for optimization program
I1001 15:23:28.465669 139818304546624 learner.py:351] Ignoring legacy param sum_loss_across_tokens_in_batch=False for optimization program
I1001 15:23:28.466100 139818304546624 learner.py:356] Learner params: allow_implicit_capture : NoneType
I1001 15:23:28.466181 139818304546624 learner.py:356] Learner params: bprop_variable_exclusion : NoneType
I1001 15:23:28.466243 139818304546624 learner.py:356] Learner params: bprop_variable_filter : NoneType
I1001 15:23:28.466299 139818304546624 learner.py:356] Learner params: clip_gradient_norm_to_value : 0.0
I1001 15:23:28.466353 139818304546624 learner.py:356] Learner params: clip_gradient_single_norm_to_value : 0.0
I1001 15:23:28.466407 139818304546624 learner.py:356] Learner params: cls : type/lingvo.core.learner/Learner
I1001 15:23:28.466459 139818304546624 learner.py:356] Learner params: colocate_gradients_with_ops : True
I1001 15:23:28.466511 139818304546624 learner.py:356] Learner params: dtype : float32
I1001 15:23:28.466593 139818304546624 learner.py:356] Learner params: fprop_dtype : NoneType
I1001 15:23:28.466650 139818304546624 learner.py:356] Learner params: gate_gradients : False
I1001 15:23:28.466702 139818304546624 learner.py:356] Learner params: grad_aggregation_method : 1
I1001 15:23:28.466754 139818304546624 learner.py:356] Learner params: grad_norm_to_clip_to_zero : 0.0
I1001 15:23:28.466805 139818304546624 learner.py:356] Learner params: grad_norm_tracker : NoneType
I1001 15:23:28.466857 139818304546624 learner.py:356] Learner params: inference_driver_name : NoneType
I1001 15:23:28.466916 139818304546624 learner.py:356] Learner params: is_eval : NoneType
I1001 15:23:28.466969 139818304546624 learner.py:356] Learner params: is_inference : NoneType
I1001 15:23:28.467020 139818304546624 learner.py:356] Learner params: l1_regularizer_weight : NoneType
I1001 15:23:28.467072 139818304546624 learner.py:356] Learner params: l2_regularizer_weight : 1e-06
I1001 15:23:28.467123 139818304546624 learner.py:356] Learner params: learning_rate : 0.5
I1001 15:23:28.467173 139818304546624 learner.py:356] Learner params: lr_schedule.allow_implicit_capture : NoneType
I1001 15:23:28.467225 139818304546624 learner.py:356] Learner params: lr_schedule.cls : type/lingvo.core.schedule/TransformerLearningRateSchedule
I1001 15:23:28.467275 139818304546624 learner.py:356] Learner params: lr_schedule.decay_end : NoneType
I1001 15:23:28.467325 139818304546624 learner.py:356] Learner params: lr_schedule.dtype : float32
I1001 15:23:28.467375 139818304546624 learner.py:356] Learner params: lr_schedule.fprop_dtype : NoneType
I1001 15:23:28.467426 139818304546624 learner.py:356] Learner params: lr_schedule.inference_driver_name : NoneType
I1001 15:23:28.467477 139818304546624 learner.py:356] Learner params: lr_schedule.is_eval : NoneType
I1001 15:23:28.467527 139818304546624 learner.py:356] Learner params: lr_schedule.is_inference : NoneType
I1001 15:23:28.467578 139818304546624 learner.py:356] Learner params: lr_schedule.model_dim : 2048
I1001 15:23:28.467629 139818304546624 learner.py:356] Learner params: lr_schedule.name : 'LRSched'
I1001 15:23:28.467680 139818304546624 learner.py:356] Learner params: lr_schedule.params_init.method : 'xavier'
I1001 15:23:28.467731 139818304546624 learner.py:356] Learner params: lr_schedule.params_init.scale : 1.000001
I1001 15:23:28.467783 139818304546624 learner.py:356] Learner params: lr_schedule.params_init.seed : NoneType
I1001 15:23:28.467834 139818304546624 learner.py:356] Learner params: lr_schedule.random_seed : NoneType
I1001 15:23:28.467885 139818304546624 learner.py:356] Learner params: lr_schedule.skip_lp_regularization : NoneType
I1001 15:23:28.467936 139818304546624 learner.py:356] Learner params: lr_schedule.vn.global_vn : False
I1001 15:23:28.467987 139818304546624 learner.py:356] Learner params: lr_schedule.vn.per_step_vn : False
I1001 15:23:28.468038 139818304546624 learner.py:356] Learner params: lr_schedule.vn.scale : NoneType
I1001 15:23:28.468089 139818304546624 learner.py:356] Learner params: lr_schedule.vn.seed : NoneType
I1001 15:23:28.468139 139818304546624 learner.py:356] Learner params: lr_schedule.warmup_steps : 40000
I1001 15:23:28.468191 139818304546624 learner.py:356] Learner params: lr_schedule.worker_replicas : 1
I1001 15:23:28.468242 139818304546624 learner.py:356] Learner params: name : 'loss'
I1001 15:23:28.468293 139818304546624 learner.py:356] Learner params: optimizer.allow_implicit_capture : NoneType
I1001 15:23:28.468345 139818304546624 learner.py:356] Learner params: optimizer.beta1 : 0.9
I1001 15:23:28.468396 139818304546624 learner.py:356] Learner params: optimizer.beta2 : 0.997
I1001 15:23:28.468447 139818304546624 learner.py:356] Learner params: optimizer.cls : type/lingvo.core.optimizer/Adam
I1001 15:23:28.468499 139818304546624 learner.py:356] Learner params: optimizer.dtype : float32
I1001 15:23:28.468550 139818304546624 learner.py:356] Learner params: optimizer.epsilon : 1e-09
I1001 15:23:28.468601 139818304546624 learner.py:356] Learner params: optimizer.fprop_dtype : NoneType
I1001 15:23:28.468652 139818304546624 learner.py:356] Learner params: optimizer.inference_driver_name : NoneType
I1001 15:23:28.468703 139818304546624 learner.py:356] Learner params: optimizer.is_eval : NoneType
I1001 15:23:28.468755 139818304546624 learner.py:356] Learner params: optimizer.is_inference : NoneType
I1001 15:23:28.468806 139818304546624 learner.py:356] Learner params: optimizer.name : 'Adam'
I1001 15:23:28.468857 139818304546624 learner.py:356] Learner params: optimizer.params_init.method : 'xavier'
I1001 15:23:28.468908 139818304546624 learner.py:356] Learner params: optimizer.params_init.scale : 1.000001
I1001 15:23:28.468965 139818304546624 learner.py:356] Learner params: optimizer.params_init.seed : NoneType
I1001 15:23:28.469018 139818304546624 learner.py:356] Learner params: optimizer.random_seed : NoneType
I1001 15:23:28.469069 139818304546624 learner.py:356] Learner params: optimizer.skip_lp_regularization : NoneType
I1001 15:23:28.469120 139818304546624 learner.py:356] Learner params: optimizer.vn.global_vn : False
I1001 15:23:28.469171 139818304546624 learner.py:356] Learner params: optimizer.vn.per_step_vn : False
I1001 15:23:28.469221 139818304546624 learner.py:356] Learner params: optimizer.vn.scale : NoneType
I1001 15:23:28.469273 139818304546624 learner.py:356] Learner params: optimizer.vn.seed : NoneType
I1001 15:23:28.469324 139818304546624 learner.py:356] Learner params: params_init.method : 'xavier'
I1001 15:23:28.469374 139818304546624 learner.py:356] Learner params: params_init.scale : 1.000001
I1001 15:23:28.469425 139818304546624 learner.py:356] Learner params: params_init.seed : NoneType
I1001 15:23:28.469476 139818304546624 learner.py:356] Learner params: random_seed : NoneType
I1001 15:23:28.469527 139818304546624 learner.py:356] Learner params: skip_lp_regularization : NoneType
I1001 15:23:28.469579 139818304546624 learner.py:356] Learner params: vn.global_vn : False
I1001 15:23:28.469630 139818304546624 learner.py:356] Learner params: vn.per_step_vn : False
I1001 15:23:28.469680 139818304546624 learner.py:356] Learner params: vn.scale : NoneType
I1001 15:23:28.469732 139818304546624 learner.py:356] Learner params: vn.seed : NoneType
I1001 15:23:28.469782 139818304546624 learner.py:356] Learner params: 
I1001 15:23:28.712210 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var on /job:local/replica:0/task:0/device:CPU:0 262144008
I1001 15:23:28.714141 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var:0 shape=(32000, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:28.733418 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 278921224
I1001 15:23:28.735398 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:28.737990 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 278929416
I1001 15:23:28.739756 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:28.746780 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 295706632
I1001 15:23:28.748748 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:28.751459 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 295714824
I1001 15:23:28.753110 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:28.760170 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 312492040
I1001 15:23:28.762212 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:28.764847 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 312500232
I1001 15:23:28.766483 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:28.773571 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 329277448
I1001 15:23:28.775559 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:28.778176 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 329285640
I1001 15:23:28.779949 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:28.783812 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 329286152
I1001 15:23:28.785460 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:28.789378 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 329294344
I1001 15:23:28.791069 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:28.794247 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 329302536
I1001 15:23:28.795922 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:23:28.805554 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:23:28.811879 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 396411400
I1001 15:23:28.813919 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:28.816579 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 396444168
I1001 15:23:28.818228 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:23:28.820213 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:23:28.826564 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 463553032
I1001 15:23:28.828515 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:28.831249 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 463561224
I1001 15:23:28.832945 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:28.837919 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 463569416
I1001 15:23:28.839602 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:28.842330 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 463577608
I1001 15:23:28.844010 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:28.864470 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 480354824
I1001 15:23:28.866417 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:28.869038 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 480363016
I1001 15:23:28.870817 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:28.877873 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 497140232
I1001 15:23:28.879832 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:28.882522 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 497148424
I1001 15:23:28.884210 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:28.891299 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 513925640
I1001 15:23:28.893311 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:28.895947 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 513933832
I1001 15:23:28.897600 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:28.905218 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 530711048
I1001 15:23:28.907187 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:28.909831 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 530719240
I1001 15:23:28.911624 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:28.915419 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 530719752
I1001 15:23:28.917080 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:28.921015 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 530727944
I1001 15:23:28.922693 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:28.925405 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 530736136
I1001 15:23:28.927089 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:23:28.936723 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:23:28.943070 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 597845000
I1001 15:23:28.945093 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:28.947731 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 597877768
I1001 15:23:28.949385 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:23:28.951388 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:23:28.958214 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 664986632
I1001 15:23:28.960211 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:28.962917 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 664994824
I1001 15:23:28.964597 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:28.969324 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 665003016
I1001 15:23:28.971005 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:28.973759 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 665011208
I1001 15:23:28.975427 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:28.995521 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 681788424
I1001 15:23:28.997472 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.000111 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 681796616
I1001 15:23:29.001921 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.009518 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 698573832
I1001 15:23:29.011496 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.014201 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 698582024
I1001 15:23:29.015885 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.023080 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 715359240
I1001 15:23:29.025097 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.027729 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 715367432
I1001 15:23:29.029389 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.036526 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 732144648
I1001 15:23:29.038490 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.041153 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 732152840
I1001 15:23:29.042948 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.046712 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 732153352
I1001 15:23:29.048388 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.052319 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 732161544
I1001 15:23:29.053992 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.056746 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 732169736
I1001 15:23:29.058399 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:23:29.068576 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:23:29.074943 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 799278600
I1001 15:23:29.076968 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.079623 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 799311368
I1001 15:23:29.081289 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:23:29.083295 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:23:29.089644 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 866420232
I1001 15:23:29.091631 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.094331 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 866428424
I1001 15:23:29.096023 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.100764 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 866436616
I1001 15:23:29.102431 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.105214 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 866444808
I1001 15:23:29.106910 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.127650 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 883222024
I1001 15:23:29.129622 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.132249 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 883230216
I1001 15:23:29.134021 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.141133 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 900007432
I1001 15:23:29.143116 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.145863 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 900015624
I1001 15:23:29.147555 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.154631 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 916792840
I1001 15:23:29.156657 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.159299 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 916801032
I1001 15:23:29.160959 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.168127 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 933578248
I1001 15:23:29.170097 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.172763 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 933586440
I1001 15:23:29.174567 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.178304 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 933586952
I1001 15:23:29.180000 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.183962 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 933595144
I1001 15:23:29.185635 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.188837 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 933603336
I1001 15:23:29.190526 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:23:29.200334 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:23:29.206701 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 1000712200
I1001 15:23:29.208736 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.211377 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 1000744968
I1001 15:23:29.213050 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:23:29.215089 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:23:29.221390 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 1067853832
I1001 15:23:29.223394 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.226094 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 1067862024
I1001 15:23:29.227798 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.232611 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1067870216
I1001 15:23:29.234275 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.237061 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1067878408
I1001 15:23:29.238768 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.259319 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 1084655624
I1001 15:23:29.261279 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.263920 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1084663816
I1001 15:23:29.265721 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.272826 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 1101441032
I1001 15:23:29.274807 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.277533 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1101449224
I1001 15:23:29.279252 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.286295 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 1118226440
I1001 15:23:29.288377 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.291035 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1118234632
I1001 15:23:29.292712 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.300276 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 1135011848
I1001 15:23:29.302233 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.304903 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1135020040
I1001 15:23:29.306763 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.310484 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 1135020552
I1001 15:23:29.312202 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.316248 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1135028744
I1001 15:23:29.317921 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.320675 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1135036936
I1001 15:23:29.322354 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:23:29.332084 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:23:29.338457 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 1202145800
I1001 15:23:29.340526 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.343189 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 1202178568
I1001 15:23:29.344865 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:23:29.346908 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:23:29.353805 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 1269287432
I1001 15:23:29.355869 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.358629 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 1269295624
I1001 15:23:29.360323 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.365089 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1269303816
I1001 15:23:29.366795 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.369557 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1269312008
I1001 15:23:29.371263 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.391475 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 1286089224
I1001 15:23:29.393564 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.396218 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1286097416
I1001 15:23:29.398002 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.405693 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 1302874632
I1001 15:23:29.407701 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.410425 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1302882824
I1001 15:23:29.412140 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.419246 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 1319660040
I1001 15:23:29.421285 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.423938 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1319668232
I1001 15:23:29.425625 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.432798 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 1336445448
I1001 15:23:29.434809 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.437479 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1336453640
I1001 15:23:29.439297 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.443087 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 1336454152
I1001 15:23:29.444783 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.448807 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1336462344
I1001 15:23:29.450494 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.453265 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1336470536
I1001 15:23:29.454972 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:23:29.465247 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:23:29.471621 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 1403579400
I1001 15:23:29.473672 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.476336 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 1403612168
I1001 15:23:29.478039 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:23:29.480093 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:23:29.486460 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 1470721032
I1001 15:23:29.488478 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.491243 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 1470729224
I1001 15:23:29.492944 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.497764 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1470737416
I1001 15:23:29.499497 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.502263 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1470745608
I1001 15:23:29.503973 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.524821 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 1487522824
I1001 15:23:29.526830 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.529466 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1487531016
I1001 15:23:29.531300 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.538348 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 1504308232
I1001 15:23:29.540359 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.543130 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1504316424
I1001 15:23:29.544837 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.551912 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 1521093640
I1001 15:23:29.553972 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.556635 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1521101832
I1001 15:23:29.558330 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.565646 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 1537879048
I1001 15:23:29.567650 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.570315 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1537887240
I1001 15:23:29.572141 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.575948 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 1537887752
I1001 15:23:29.577650 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.581655 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1537895944
I1001 15:23:29.583362 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.586583 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1537904136
I1001 15:23:29.588284 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:23:29.598065 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:23:29.604443 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 1605013000
I1001 15:23:29.606502 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.609168 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 1605045768
I1001 15:23:29.610949 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:23:29.613043 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:23:29.619437 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 1672154632
I1001 15:23:29.621424 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.624187 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 1672162824
I1001 15:23:29.625909 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.630769 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1672171016
I1001 15:23:29.632468 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.635279 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1672179208
I1001 15:23:29.636975 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.657832 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 1688956424
I1001 15:23:29.659841 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.662464 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1688964616
I1001 15:23:29.664280 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.671372 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 1705741832
I1001 15:23:29.673371 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.676145 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1705750024
I1001 15:23:29.677839 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.684937 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 1722527240
I1001 15:23:29.687082 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.689741 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1722535432
I1001 15:23:29.691459 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.699046 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 1739312648
I1001 15:23:29.701039 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.703752 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1739320840
I1001 15:23:29.705558 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.709378 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 1739321352
I1001 15:23:29.711105 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.715169 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1739329544
I1001 15:23:29.716942 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.719728 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1739337736
I1001 15:23:29.721430 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:23:29.731271 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:23:29.737669 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 1806446600
I1001 15:23:29.739781 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.742427 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 1806479368
I1001 15:23:29.744159 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:23:29.746232 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:23:29.753159 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 1873588232
I1001 15:23:29.755198 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.757931 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 1873596424
I1001 15:23:29.759652 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.764508 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1873604616
I1001 15:23:29.766210 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.769013 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1873612808
I1001 15:23:29.770725 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.843515 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 1890390024
I1001 15:23:29.845510 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.848210 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1890398216
I1001 15:23:29.850019 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.857112 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 1907175432
I1001 15:23:29.859142 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.862284 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1907183624
I1001 15:23:29.864029 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.871151 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 1923960840
I1001 15:23:29.873212 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.875912 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1923969032
I1001 15:23:29.877618 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.884796 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 1940746248
I1001 15:23:29.886809 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.889506 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1940754440
I1001 15:23:29.891369 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.895200 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 1940754952
I1001 15:23:29.896916 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.900989 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1940763144
I1001 15:23:29.902708 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.905591 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1940771336
I1001 15:23:29.907337 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:23:29.917243 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:23:29.924172 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 2007880200
I1001 15:23:29.926242 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.928953 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 2007912968
I1001 15:23:29.930694 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:23:29.932763 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:23:29.939159 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 2075021832
I1001 15:23:29.941168 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.943955 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 2075030024
I1001 15:23:29.945677 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.950564 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2075038216
I1001 15:23:29.952286 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.955111 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2075046408
I1001 15:23:29.956830 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.977584 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 2091823624
I1001 15:23:29.979605 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.982249 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2091831816
I1001 15:23:29.984081 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.991193 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 2108609032
I1001 15:23:29.993220 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:29.995997 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2108617224
I1001 15:23:29.997708 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.004888 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 2125394440
I1001 15:23:30.006980 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.009652 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2125402632
I1001 15:23:30.011387 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.018557 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 2142179848
I1001 15:23:30.020606 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.023322 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2142188040
I1001 15:23:30.025129 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.028979 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 2142188552
I1001 15:23:30.030717 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.034792 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2142196744
I1001 15:23:30.036501 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.039299 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2142204936
I1001 15:23:30.041013 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:23:30.051316 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:23:30.057699 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 2209313800
I1001 15:23:30.059799 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.062457 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 2209346568
I1001 15:23:30.064194 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:23:30.066263 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:23:30.072746 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 2276455432
I1001 15:23:30.074785 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.077543 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 2276463624
I1001 15:23:30.079287 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.084182 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2276471816
I1001 15:23:30.085901 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.088730 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2276480008
I1001 15:23:30.090437 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.111300 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 2293257224
I1001 15:23:30.113306 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.116063 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2293265416
I1001 15:23:30.117880 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.124989 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 2310042632
I1001 15:23:30.127022 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.129804 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2310050824
I1001 15:23:30.131551 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.138670 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 2326828040
I1001 15:23:30.140763 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.143478 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2326836232
I1001 15:23:30.145200 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.152992 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 2343613448
I1001 15:23:30.155026 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.157757 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2343621640
I1001 15:23:30.159615 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.163517 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 2343622152
I1001 15:23:30.165236 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.169460 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2343630344
I1001 15:23:30.171206 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.173977 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2343638536
I1001 15:23:30.175720 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:23:30.185613 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:23:30.192160 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 2410747400
I1001 15:23:30.194243 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.196964 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 2410780168
I1001 15:23:30.198701 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:23:30.200801 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:23:30.207259 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 2477889032
I1001 15:23:30.209273 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.212628 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 2477897224
I1001 15:23:30.214363 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.219337 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2477905416
I1001 15:23:30.221147 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.223975 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2477913608
I1001 15:23:30.226024 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.246426 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 2494690824
I1001 15:23:30.248472 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.251163 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2494699016
I1001 15:23:30.252993 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.260138 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 2511476232
I1001 15:23:30.262230 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.265540 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2511484424
I1001 15:23:30.267289 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.274408 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 2528261640
I1001 15:23:30.276514 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.279210 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2528269832
I1001 15:23:30.280992 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.288220 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 2545047048
I1001 15:23:30.290259 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.292997 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2545055240
I1001 15:23:30.294836 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.298802 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 2545055752
I1001 15:23:30.300539 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.304673 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2545063944
I1001 15:23:30.306389 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.309202 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2545072136
I1001 15:23:30.310969 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:23:30.320942 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:23:30.327915 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 2612181000
I1001 15:23:30.330000 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.332773 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 2612213768
I1001 15:23:30.334515 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:23:30.336631 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:23:30.343059 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 2679322632
I1001 15:23:30.345087 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.347891 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 2679330824
I1001 15:23:30.349637 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.354622 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2679339016
I1001 15:23:30.356348 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.359188 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2679347208
I1001 15:23:30.360920 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.382035 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 2696124424
I1001 15:23:30.384089 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.386787 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2696132616
I1001 15:23:30.388622 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.395894 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 2712909832
I1001 15:23:30.397908 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.400724 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2712918024
I1001 15:23:30.402463 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.409665 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 2729695240
I1001 15:23:30.411788 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.414481 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2729703432
I1001 15:23:30.416232 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.423443 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 2746480648
I1001 15:23:30.425467 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.428222 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2746488840
I1001 15:23:30.430065 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.433965 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 2746489352
I1001 15:23:30.435723 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.439903 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2746497544
I1001 15:23:30.441637 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.444466 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2746505736
I1001 15:23:30.446205 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:23:30.456720 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:23:30.463219 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 2813614600
I1001 15:23:30.465323 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.468045 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 2813647368
I1001 15:23:30.469769 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:23:30.471908 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:23:30.478318 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 2880756232
I1001 15:23:30.480372 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.483210 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 2880764424
I1001 15:23:30.484948 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.489908 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2880772616
I1001 15:23:30.491664 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.494496 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2880780808
I1001 15:23:30.496263 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.517366 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 2897558024
I1001 15:23:30.519437 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.522187 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2897566216
I1001 15:23:30.524053 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.531227 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 2914343432
I1001 15:23:30.533259 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.536065 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2914351624
I1001 15:23:30.537804 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.544952 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 2931128840
I1001 15:23:30.547060 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.549756 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2931137032
I1001 15:23:30.551516 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.559264 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 2947914248
I1001 15:23:30.561287 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.564047 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2947922440
I1001 15:23:30.565885 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.569844 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 2947922952
I1001 15:23:30.571606 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.575885 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2947931144
I1001 15:23:30.577615 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.580464 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2947939336
I1001 15:23:30.582204 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:23:30.592196 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:23:30.598697 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 3015048200
I1001 15:23:30.600792 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.603557 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 3015080968
I1001 15:23:30.605305 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:23:30.607457 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:23:30.613877 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 3082189832
I1001 15:23:30.615953 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.619306 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 3082198024
I1001 15:23:30.621047 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.626049 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3082206216
I1001 15:23:30.627821 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.630678 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3082214408
I1001 15:23:30.632426 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.653025 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 3098991624
I1001 15:23:30.655166 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.657929 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3098999816
I1001 15:23:30.659800 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.667048 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 3115777032
I1001 15:23:30.669101 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.672685 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3115785224
I1001 15:23:30.674435 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.681615 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 3132562440
I1001 15:23:30.683748 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.686478 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3132570632
I1001 15:23:30.688248 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.695481 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 3149347848
I1001 15:23:30.697532 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.700300 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3149356040
I1001 15:23:30.702156 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.706119 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 3149356552
I1001 15:23:30.707896 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.712127 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3149364744
I1001 15:23:30.713872 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.716774 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3149372936
I1001 15:23:30.718528 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:23:30.728680 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:23:30.735738 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 3216481800
I1001 15:23:30.737867 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.740596 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 3216514568
I1001 15:23:30.742345 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:23:30.744517 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:23:30.750958 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 3283623432
I1001 15:23:30.753021 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.755850 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 3283631624
I1001 15:23:30.757603 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.762612 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3283639816
I1001 15:23:30.764355 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.767220 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3283648008
I1001 15:23:30.768984 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.790073 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 3300425224
I1001 15:23:30.792153 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.794899 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3300433416
I1001 15:23:30.796766 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.803962 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 3317210632
I1001 15:23:30.806006 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.808858 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3317218824
I1001 15:23:30.810621 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.817769 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 3333996040
I1001 15:23:30.819911 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.822652 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3334004232
I1001 15:23:30.824422 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.831690 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 3350781448
I1001 15:23:30.833732 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.836516 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3350789640
I1001 15:23:30.838376 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.842297 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 3350790152
I1001 15:23:30.844095 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.848300 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3350798344
I1001 15:23:30.850047 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.852889 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3350806536
I1001 15:23:30.854677 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:23:30.865197 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:23:30.871676 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 3417915400
I1001 15:23:30.873796 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.876545 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 3417948168
I1001 15:23:30.878286 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:23:30.880473 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:23:30.886952 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 3485057032
I1001 15:23:30.889021 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.891851 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 3485065224
I1001 15:23:30.893647 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.898671 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3485073416
I1001 15:23:30.900417 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.903301 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3485081608
I1001 15:23:30.905163 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.977589 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 3501858824
I1001 15:23:30.979686 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.982403 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3501867016
I1001 15:23:30.984314 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.991514 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 3518644232
I1001 15:23:30.993565 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:30.996391 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3518652424
I1001 15:23:30.998141 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.005400 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 3535429640
I1001 15:23:31.007471 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.010200 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3535437832
I1001 15:23:31.011990 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.019229 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 3552215048
I1001 15:23:31.021295 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.024116 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3552223240
I1001 15:23:31.026463 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.030418 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 3552223752
I1001 15:23:31.032209 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.036463 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3552231944
I1001 15:23:31.038219 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.041063 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3552240136
I1001 15:23:31.042847 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:23:31.052889 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:23:31.059376 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 3619349000
I1001 15:23:31.061496 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.064251 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 3619381768
I1001 15:23:31.066016 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:23:31.068211 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:23:31.074679 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 3686490632
I1001 15:23:31.076738 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.079579 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 3686498824
I1001 15:23:31.081341 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.086413 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3686507016
I1001 15:23:31.088211 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.091641 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3686515208
I1001 15:23:31.093397 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.114016 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 3703292424
I1001 15:23:31.116168 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.118950 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3703300616
I1001 15:23:31.120820 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.128016 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 3720077832
I1001 15:23:31.130080 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.132945 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3720086024
I1001 15:23:31.134729 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.142394 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 3736863240
I1001 15:23:31.144589 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.147343 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3736871432
I1001 15:23:31.149105 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.156386 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 3753648648
I1001 15:23:31.158468 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.161249 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3753656840
I1001 15:23:31.163132 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.167167 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 3753657352
I1001 15:23:31.168940 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.173306 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3753665544
I1001 15:23:31.175129 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.177970 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3753673736
I1001 15:23:31.179771 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:23:31.189880 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:23:31.196475 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 3820782600
I1001 15:23:31.199225 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.201960 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 3820815368
I1001 15:23:31.203747 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:23:31.205921 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:23:31.212393 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 3887924232
I1001 15:23:31.214458 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.217319 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 3887932424
I1001 15:23:31.219122 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.224153 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3887940616
I1001 15:23:31.225912 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.228824 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3887948808
I1001 15:23:31.230612 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.251846 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 3904726024
I1001 15:23:31.253909 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.256653 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3904734216
I1001 15:23:31.258526 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.265736 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 3921511432
I1001 15:23:31.267814 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.270756 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3921519624
I1001 15:23:31.272534 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.279746 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 3938296840
I1001 15:23:31.281861 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.284617 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3938305032
I1001 15:23:31.286496 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.293744 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 3955082248
I1001 15:23:31.295831 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.298627 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3955090440
I1001 15:23:31.300498 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.304481 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 3955090952
I1001 15:23:31.306266 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.310579 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3955099144
I1001 15:23:31.312346 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.315215 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3955107336
I1001 15:23:31.317079 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:23:31.327765 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:23:31.334233 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 4022216200
I1001 15:23:31.336406 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.339207 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 4022248968
I1001 15:23:31.340998 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:23:31.343192 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:23:31.349659 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 4089357832
I1001 15:23:31.351752 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.354597 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 4089366024
I1001 15:23:31.356426 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.361513 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4089374216
I1001 15:23:31.363306 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.366183 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4089382408
I1001 15:23:31.367972 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.389433 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 4106159624
I1001 15:23:31.391574 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.394326 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4106167816
I1001 15:23:31.396253 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.403450 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 4122945032
I1001 15:23:31.405567 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.408440 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4122953224
I1001 15:23:31.410199 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.417394 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 4139730440
I1001 15:23:31.419581 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.422322 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4139738632
I1001 15:23:31.424112 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.431395 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 4156515848
I1001 15:23:31.433473 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.436331 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4156524040
I1001 15:23:31.438735 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.442764 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 4156524552
I1001 15:23:31.444535 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.448858 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4156532744
I1001 15:23:31.450659 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.453497 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4156540936
I1001 15:23:31.455302 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:23:31.465558 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:23:31.472130 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 4223649800
I1001 15:23:31.474269 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.477046 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 4223682568
I1001 15:23:31.478851 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:23:31.481065 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:23:31.487570 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 4290791432
I1001 15:23:31.489659 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.492535 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 4290799624
I1001 15:23:31.494320 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.499477 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4290807816
I1001 15:23:31.501258 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.504649 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4290816008
I1001 15:23:31.506437 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.527276 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 4307593224
I1001 15:23:31.529344 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.532118 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4307601416
I1001 15:23:31.534017 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.541224 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 4324378632
I1001 15:23:31.543318 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.546175 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4324386824
I1001 15:23:31.547986 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.555878 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 4341164040
I1001 15:23:31.558023 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.560803 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4341172232
I1001 15:23:31.562621 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.569878 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 4357949448
I1001 15:23:31.571987 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.574794 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4357957640
I1001 15:23:31.576683 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.580680 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 4357958152
I1001 15:23:31.582455 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.586830 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4357966344
I1001 15:23:31.588600 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.591481 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4357974536
I1001 15:23:31.593275 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:23:31.603391 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:23:31.609893 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 4425083400
I1001 15:23:31.612637 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.615432 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 4425116168
I1001 15:23:31.617230 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:23:31.619473 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:23:31.625966 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 4492225032
I1001 15:23:31.628088 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.630940 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 4492233224
I1001 15:23:31.632744 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.637871 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4492241416
I1001 15:23:31.639689 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.642614 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4492249608
I1001 15:23:31.644435 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.665789 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 4509026824
I1001 15:23:31.667901 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.670686 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4509035016
I1001 15:23:31.672566 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.679789 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 4525812232
I1001 15:23:31.681882 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.684764 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4525820424
I1001 15:23:31.686561 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.693785 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 4542597640
I1001 15:23:31.695967 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.698753 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4542605832
I1001 15:23:31.700542 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.707793 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 4559383048
I1001 15:23:31.709888 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.712687 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4559391240
I1001 15:23:31.714591 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.718683 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 4559391752
I1001 15:23:31.720466 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.724797 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4559399944
I1001 15:23:31.726599 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.729487 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4559408136
I1001 15:23:31.731305 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:23:31.742021 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:23:31.748519 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 4626517000
I1001 15:23:31.750685 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.753443 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 4626549768
I1001 15:23:31.755270 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:23:31.757510 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:23:31.763994 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 4693658632
I1001 15:23:31.766084 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.768963 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 4693666824
I1001 15:23:31.770802 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.775951 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4693675016
I1001 15:23:31.777744 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.780687 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4693683208
I1001 15:23:31.782479 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.803783 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 4710460424
I1001 15:23:31.805866 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.808637 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4710468616
I1001 15:23:31.810523 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.817753 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 4727245832
I1001 15:23:31.819878 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.822767 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4727254024
I1001 15:23:31.824553 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.831793 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 4744031240
I1001 15:23:31.833937 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.836724 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4744039432
I1001 15:23:31.838547 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.845797 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 4760816648
I1001 15:23:31.847921 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.850749 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4760824840
I1001 15:23:31.853114 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.857137 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 4760825352
I1001 15:23:31.858960 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.863317 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4760833544
I1001 15:23:31.865111 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.868006 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4760841736
I1001 15:23:31.869812 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:23:31.879986 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:23:31.886498 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 4827950600
I1001 15:23:31.888693 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.891482 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 4827983368
I1001 15:23:31.893287 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:23:31.895581 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:23:31.902070 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 4895092232
I1001 15:23:31.904197 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.907209 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 4895100424
I1001 15:23:31.909010 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.914178 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4895108616
I1001 15:23:31.915990 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.919469 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4895116808
I1001 15:23:31.921279 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.942058 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 4911894024
I1001 15:23:31.944181 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.946967 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4911902216
I1001 15:23:31.948882 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.956102 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 4928679432
I1001 15:23:31.958206 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.961107 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4928687624
I1001 15:23:31.962932 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.970691 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 4945464840
I1001 15:23:31.972846 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.975628 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4945473032
I1001 15:23:31.977437 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.984790 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 4962250248
I1001 15:23:31.986918 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.989731 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4962258440
I1001 15:23:31.991661 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:31.995695 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 4962258952
I1001 15:23:31.997519 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.001910 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4962267144
I1001 15:23:32.003734 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.006631 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4962275336
I1001 15:23:32.008430 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:23:32.018600 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:23:32.025110 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 5029384200
I1001 15:23:32.027867 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.030674 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 5029416968
I1001 15:23:32.032480 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:23:32.034753 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:23:32.041322 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 5096525832
I1001 15:23:32.043451 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.047037 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 5096534024
I1001 15:23:32.048847 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.054029 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5096542216
I1001 15:23:32.055856 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.058851 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5096550408
I1001 15:23:32.060667 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.133429 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 5113327624
I1001 15:23:32.135578 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.138846 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5113335816
I1001 15:23:32.140767 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.147997 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 5130113032
I1001 15:23:32.150166 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.152951 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5130121224
I1001 15:23:32.154777 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.162038 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 5146898440
I1001 15:23:32.164184 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.166989 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5146906632
I1001 15:23:32.168892 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.176131 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 5163683848
I1001 15:23:32.178231 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.181155 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5163692040
I1001 15:23:32.182988 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.187015 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 5163692552
I1001 15:23:32.188820 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.193287 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5163700744
I1001 15:23:32.195126 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.198004 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5163708936
I1001 15:23:32.199834 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:23:32.210599 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:23:32.217118 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 5230817800
I1001 15:23:32.219331 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.222100 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 5230850568
I1001 15:23:32.223955 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:23:32.226217 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:23:32.232710 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 5297959432
I1001 15:23:32.234827 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.237710 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 5297967624
I1001 15:23:32.239552 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.244773 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5297975816
I1001 15:23:32.246606 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.249567 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5297984008
I1001 15:23:32.251469 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.272655 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 5314761224
I1001 15:23:32.274783 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.277563 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5314769416
I1001 15:23:32.279492 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.286735 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 5331546632
I1001 15:23:32.288840 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.291771 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5331554824
I1001 15:23:32.293573 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.300817 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 5348332040
I1001 15:23:32.303016 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.305799 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5348340232
I1001 15:23:32.307636 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.314974 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 5365117448
I1001 15:23:32.317149 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.320007 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5365125640
I1001 15:23:32.321928 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.326471 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 5365126152
I1001 15:23:32.328314 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.332733 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5365134344
I1001 15:23:32.334565 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.337463 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5365142536
I1001 15:23:32.339309 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:23:32.349589 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:23:32.356131 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 5432251400
I1001 15:23:32.358314 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.361201 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 5432284168
I1001 15:23:32.363061 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:23:32.365340 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:23:32.371850 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 5499393032
I1001 15:23:32.373951 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.376844 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 5499401224
I1001 15:23:32.378700 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.383918 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5499409416
I1001 15:23:32.385734 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.388688 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5499417608
I1001 15:23:32.390505 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.411916 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 5516194824
I1001 15:23:32.414036 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.416853 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5516203016
I1001 15:23:32.418790 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.426050 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 5532980232
I1001 15:23:32.428197 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.431121 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5532988424
I1001 15:23:32.432949 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.440342 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 5549765640
I1001 15:23:32.443059 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.445868 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5549773832
I1001 15:23:32.447700 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.454983 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 5566551048
I1001 15:23:32.457087 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.459939 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5566559240
I1001 15:23:32.461877 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.465952 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 5566559752
I1001 15:23:32.467800 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.472228 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5566567944
I1001 15:23:32.474048 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.476975 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5566576136
I1001 15:23:32.478806 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:23:32.489080 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:23:32.495641 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 5633685000
I1001 15:23:32.497827 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.500677 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 5633717768
I1001 15:23:32.502507 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:23:32.505340 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:23:32.511896 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 5700826632
I1001 15:23:32.514020 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.516948 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 5700834824
I1001 15:23:32.518841 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.524096 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5700843016
I1001 15:23:32.525920 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.528892 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5700851208
I1001 15:23:32.530740 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.551616 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 5717628424
I1001 15:23:32.553736 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.556600 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5717636616
I1001 15:23:32.559080 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.566348 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 5734413832
I1001 15:23:32.568521 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.571577 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5734422024
I1001 15:23:32.573414 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.580671 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 5751199240
I1001 15:23:32.582872 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.585666 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5751207432
I1001 15:23:32.587506 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.594817 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 5767984648
I1001 15:23:32.596938 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.599810 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5767992840
I1001 15:23:32.601748 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.605841 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 5767993352
I1001 15:23:32.607704 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.612144 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5768001544
I1001 15:23:32.613963 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.616892 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5768009736
I1001 15:23:32.618753 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:23:32.629579 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:23:32.636154 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 5835118600
I1001 15:23:32.638350 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.641189 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 5835151368
I1001 15:23:32.643060 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:23:32.645421 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:23:32.651964 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 5902260232
I1001 15:23:32.654106 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.657031 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 5902268424
I1001 15:23:32.658875 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.664155 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5902276616
I1001 15:23:32.665983 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.668979 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5902284808
I1001 15:23:32.670835 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.692256 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 5919062024
I1001 15:23:32.694388 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.697232 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5919070216
I1001 15:23:32.699189 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.706387 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 5935847432
I1001 15:23:32.708539 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.711469 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5935855624
I1001 15:23:32.713287 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.720595 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 5952632840
I1001 15:23:32.722802 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.725659 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5952641032
I1001 15:23:32.727521 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.734827 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 5969418248
I1001 15:23:32.736955 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.739829 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5969426440
I1001 15:23:32.741758 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.746324 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 5969426952
I1001 15:23:32.748185 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.752660 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5969435144
I1001 15:23:32.754497 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.757434 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5969443336
I1001 15:23:32.759294 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:23:32.769584 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:23:32.776165 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 6036552200
I1001 15:23:32.778368 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.781225 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 6036584968
I1001 15:23:32.783096 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:23:32.785411 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:23:32.791966 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 6103693832
I1001 15:23:32.794105 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.797032 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 6103702024
I1001 15:23:32.798949 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.804230 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 6103710216
I1001 15:23:32.806067 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.809036 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 6103718408
I1001 15:23:32.810920 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.832362 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 6120495624
I1001 15:23:32.834499 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.837319 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6120503816
I1001 15:23:32.839285 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.846604 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 6137281032
I1001 15:23:32.848735 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.851679 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6137289224
I1001 15:23:32.853496 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.860769 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 6154066440
I1001 15:23:32.863522 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.866356 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6154074632
I1001 15:23:32.868220 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.875551 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 6170851848
I1001 15:23:32.877726 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.880629 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6170860040
I1001 15:23:32.882578 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.886708 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 6170860552
I1001 15:23:32.888556 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.893060 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 6170868744
I1001 15:23:32.894940 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.897895 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 6170876936
I1001 15:23:32.899764 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:23:32.910314 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:23:32.916887 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 6237985800
I1001 15:23:32.919203 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.922021 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 6238018568
I1001 15:23:32.923904 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:23:32.926786 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:23:32.933372 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 6305127432
I1001 15:23:32.935554 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.938471 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 6305135624
I1001 15:23:32.940342 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.945659 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 6305143816
I1001 15:23:32.947527 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.950478 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 6305152008
I1001 15:23:32.952343 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.973469 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 6321929224
I1001 15:23:32.975638 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.978624 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6321937416
I1001 15:23:32.981099 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.988359 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 6338714632
I1001 15:23:32.990485 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:32.993456 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6338722824
I1001 15:23:32.995316 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:33.002605 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 6355500040
I1001 15:23:33.004828 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:33.007713 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6355508232
I1001 15:23:33.009561 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:33.016896 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 6372285448
I1001 15:23:33.019063 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:33.021931 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6372293640
I1001 15:23:33.023900 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:33.028042 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 6372294152
I1001 15:23:33.029889 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:33.034441 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 6372302344
I1001 15:23:33.036319 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:33.039278 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 6372310536
I1001 15:23:33.041134 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:23:33.711429 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:23:33.718782 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 6439419400
I1001 15:23:33.720994 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:33.723869 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 6439452168
I1001 15:23:33.725718 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:23:33.728138 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:23:33.734773 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 6506561032
I1001 15:23:33.737017 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:33.739899 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 6506569224
I1001 15:23:33.741763 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:33.747223 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 6506577416
I1001 15:23:33.749296 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:33.752154 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 6506585608
I1001 15:23:33.754021 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:33.776035 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 6523362824
I1001 15:23:33.778194 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:33.781152 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6523371016
I1001 15:23:33.783039 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:33.790321 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 6540148232
I1001 15:23:33.792567 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:33.795439 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6540156424
I1001 15:23:33.797312 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:33.804710 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 6556933640
I1001 15:23:33.806906 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:33.809798 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6556941832
I1001 15:23:33.811807 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:33.819157 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 6573719048
I1001 15:23:33.821339 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:33.824347 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6573727240
I1001 15:23:33.826201 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:33.830344 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 6573727752
I1001 15:23:33.832249 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:33.836830 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 6573735944
I1001 15:23:33.838712 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:33.842222 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 6573744136
I1001 15:23:33.844126 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:23:33.854631 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:23:33.861237 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 6640853000
I1001 15:23:33.863511 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:33.866341 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 6640885768
I1001 15:23:33.868289 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:23:33.870686 139818304546624 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 15:23:33.877259 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 6707994632
I1001 15:23:33.879462 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:33.882417 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 6708002824
I1001 15:23:33.884292 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:33.889686 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 6708011016
I1001 15:23:33.891597 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:33.894615 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 6708019208
I1001 15:23:33.896479 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:23:33.902055 139818304546624 py_utils.py:1229] WARNING!!! var weight_0 is using the default xavier initializer. Make sure this is intended.
I1001 15:23:33.908843 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var on /job:local/replica:0/task:0/device:CPU:0 6724403208
I1001 15:23:33.911038 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:23:33.911905 139818304546624 py_utils.py:1229] WARNING!!! var weight_1 is using the default xavier initializer. Make sure this is intended.
I1001 15:23:33.919046 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var on /job:local/replica:0/task:0/device:CPU:0 6740787208
I1001 15:23:33.921211 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:23:33.922065 139818304546624 py_utils.py:1229] WARNING!!! var weight_2 is using the default xavier initializer. Make sure this is intended.
I1001 15:23:33.928592 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var on /job:local/replica:0/task:0/device:CPU:0 6757171208
I1001 15:23:33.930788 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:23:33.931654 139818304546624 py_utils.py:1229] WARNING!!! var weight_3 is using the default xavier initializer. Make sure this is intended.
I1001 15:23:33.938180 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var on /job:local/replica:0/task:0/device:CPU:0 6773555208
I1001 15:23:33.940377 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:23:33.941241 139818304546624 py_utils.py:1229] WARNING!!! var weight_4 is using the default xavier initializer. Make sure this is intended.
I1001 15:23:33.947707 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var on /job:local/replica:0/task:0/device:CPU:0 6789939208
I1001 15:23:33.949859 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:23:33.950747 139818304546624 py_utils.py:1229] WARNING!!! var weight_5 is using the default xavier initializer. Make sure this is intended.
I1001 15:23:33.957239 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var on /job:local/replica:0/task:0/device:CPU:0 6806323208
I1001 15:23:33.959415 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:23:33.960286 139818304546624 py_utils.py:1229] WARNING!!! var weight_6 is using the default xavier initializer. Make sure this is intended.
I1001 15:23:33.966751 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var on /job:local/replica:0/task:0/device:CPU:0 6822707208
I1001 15:23:33.968913 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:23:33.969777 139818304546624 py_utils.py:1229] WARNING!!! var weight_7 is using the default xavier initializer. Make sure this is intended.
I1001 15:23:33.976284 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var on /job:local/replica:0/task:0/device:CPU:0 6839091208
I1001 15:23:33.978466 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:23:33.979358 139818304546624 py_utils.py:1229] WARNING!!! var weight_8 is using the default xavier initializer. Make sure this is intended.
I1001 15:23:33.985787 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var on /job:local/replica:0/task:0/device:CPU:0 6855475208
I1001 15:23:33.987969 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:23:33.988836 139818304546624 py_utils.py:1229] WARNING!!! var weight_9 is using the default xavier initializer. Make sure this is intended.
I1001 15:23:33.995942 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var on /job:local/replica:0/task:0/device:CPU:0 6871859208
I1001 15:23:33.998095 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:23:33.998982 139818304546624 py_utils.py:1229] WARNING!!! var weight_10 is using the default xavier initializer. Make sure this is intended.
I1001 15:23:34.005405 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var on /job:local/replica:0/task:0/device:CPU:0 6888243208
I1001 15:23:34.007599 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:23:34.008450 139818304546624 py_utils.py:1229] WARNING!!! var weight_11 is using the default xavier initializer. Make sure this is intended.
I1001 15:23:34.014960 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var on /job:local/replica:0/task:0/device:CPU:0 6904627208
I1001 15:23:34.017096 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:23:34.017955 139818304546624 py_utils.py:1229] WARNING!!! var weight_12 is using the default xavier initializer. Make sure this is intended.
I1001 15:23:34.024414 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var on /job:local/replica:0/task:0/device:CPU:0 6921011208
I1001 15:23:34.026561 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:23:34.027421 139818304546624 py_utils.py:1229] WARNING!!! var weight_13 is using the default xavier initializer. Make sure this is intended.
I1001 15:23:34.033907 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var on /job:local/replica:0/task:0/device:CPU:0 6937395208
I1001 15:23:34.036064 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:23:34.036924 139818304546624 py_utils.py:1229] WARNING!!! var weight_14 is using the default xavier initializer. Make sure this is intended.
I1001 15:23:34.043390 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var on /job:local/replica:0/task:0/device:CPU:0 6953779208
I1001 15:23:34.045524 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1001 15:23:34.046384 139818304546624 py_utils.py:1229] WARNING!!! var weight_15 is using the default xavier initializer. Make sure this is intended.
I1001 15:23:34.052917 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var on /job:local/replica:0/task:0/device:CPU:0 6970163208
I1001 15:23:34.055096 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:34.057988 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var on /job:local/replica:0/task:0/device:CPU:0 6970171208
I1001 15:23:34.059971 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:34.062828 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var on /job:local/replica:0/task:0/device:CPU:0 6970179208
I1001 15:23:34.064671 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:34.068137 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var on /job:local/replica:0/task:0/device:CPU:0 6970187208
I1001 15:23:34.069996 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:34.072841 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var on /job:local/replica:0/task:0/device:CPU:0 6970195208
I1001 15:23:34.074718 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:34.077650 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var on /job:local/replica:0/task:0/device:CPU:0 6970203208
I1001 15:23:34.079508 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:34.082338 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var on /job:local/replica:0/task:0/device:CPU:0 6970211208
I1001 15:23:34.084206 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:34.087141 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var on /job:local/replica:0/task:0/device:CPU:0 6970219208
I1001 15:23:34.088986 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:34.091840 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var on /job:local/replica:0/task:0/device:CPU:0 6970227208
I1001 15:23:34.093788 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:34.096753 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var on /job:local/replica:0/task:0/device:CPU:0 6970235208
I1001 15:23:34.098633 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:34.101592 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var on /job:local/replica:0/task:0/device:CPU:0 6970243208
I1001 15:23:34.103501 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:34.106334 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var on /job:local/replica:0/task:0/device:CPU:0 6970251208
I1001 15:23:34.108213 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:34.111163 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var on /job:local/replica:0/task:0/device:CPU:0 6970259208
I1001 15:23:34.113018 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:34.115867 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var on /job:local/replica:0/task:0/device:CPU:0 6970267208
I1001 15:23:34.117704 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:34.120780 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var on /job:local/replica:0/task:0/device:CPU:0 6970275208
I1001 15:23:34.122643 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:34.125466 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var on /job:local/replica:0/task:0/device:CPU:0 6970283208
I1001 15:23:34.127439 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:34.130272 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var on /job:local/replica:0/task:0/device:CPU:0 6970291208
I1001 15:23:34.132158 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:23:34.807799 139818304546624 py_utils.py:1484] === worker 0 ===
I1001 15:23:34.822448 139818304546624 py_utils.py:1474] worker 0: global_step                                                           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.822556 139818304546624 py_utils.py:1474] worker 0: input._tokenizer_default.global_step                                  /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.822625 139818304546624 py_utils.py:1474] worker 0: input.global_step                                                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.822681 139818304546624 py_utils.py:1474] worker 0: learners[0].global_step                                               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.822732 139818304546624 py_utils.py:1474] worker 0: learners[0].lr_schedule.global_step                                   /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.822782 139818304546624 py_utils.py:1474] worker 0: learners[0].optimizer.global_step                                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.822840 139818304546624 py_utils.py:1474] worker 0: lm.global_step                                                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.822890 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.emb.global_step                                       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.822938 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.emb.src_dropout.global_step                           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.822987 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.emb.src_pos_emb.global_step                           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.823034 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.emb.src_token_emb.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.823082 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.emb.src_token_emb.wm                                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.823129 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.823177 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.823225 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.823272 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.823320 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.823367 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.823414 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.823462 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.823509 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.823556 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.823603 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.823651 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.823698 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.823750 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.823799 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.global_step                                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.823846 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.823894 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.823941 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.823989 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.824041 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.824089 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.824136 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.824184 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.824231 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.824279 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.824326 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.824373 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.global_step                      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.824421 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.824476 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.824524 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.824572 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.824620 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.824671 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.824720 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.824767 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.824815 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.824862 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.824909 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.824956 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.825004 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.825051 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.825099 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.825147 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.825195 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.825242 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.825289 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.global_step                                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.825337 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.825385 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.825432 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.825480 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.825531 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.825580 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.825628 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.825676 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.825724 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.825772 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.825820 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.825868 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.global_step                      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.825915 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.825963 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.826011 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.826059 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.826106 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.826154 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.826201 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.826248 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.826296 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.826343 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.826391 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.826442 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.826490 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.826557 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.826613 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.826662 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.826710 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.826758 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.826805 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.global_step                                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.826852 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.826900 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.826946 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.826993 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.827040 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.827088 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.827135 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.827182 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.827230 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.827277 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.827329 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.827378 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.global_step                      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.827425 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.827472 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.827520 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.827568 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.827615 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.827662 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.827709 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.827757 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.827804 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.827852 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.827899 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.827947 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.827994 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.828042 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.828090 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.828138 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.828185 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.828236 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.828285 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.global_step                                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.828333 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.828381 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.828429 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.828477 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.828524 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.828572 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.828619 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.828666 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.828714 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.828761 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.828809 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.828855 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.global_step                      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.828902 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.828949 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.828997 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.829044 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.829095 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.829143 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.829191 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.829238 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.829286 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.829333 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.829380 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.829427 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.829473 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.829521 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.829567 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.829615 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.829661 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.829708 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.829755 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.global_step                                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.829802 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.829848 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.829895 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.829941 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.829991 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.830039 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.830085 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.830132 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.830178 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.830224 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.830271 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.830318 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.global_step                      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.830364 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.830411 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.830458 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.830505 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.830568 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.830619 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.830667 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.830714 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.830761 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.830809 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.830860 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.830909 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.830956 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.831003 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.831050 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.831097 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.831144 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.831191 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.831237 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.global_step                                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.831284 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.831331 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.831378 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.831425 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.831472 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.831519 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.831566 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.831612 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.831659 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.831706 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.831757 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.831805 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.global_step                      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.831852 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.831899 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.831947 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.831994 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.832042 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.832089 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.832136 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.832183 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.832230 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.832277 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.832324 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.832370 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.832417 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.832463 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.832510 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.832556 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.832607 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.832655 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.832702 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.global_step                                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.832749 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.832796 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.832843 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.832889 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.832937 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.832983 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.833030 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.833077 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.833124 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.833171 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.833218 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.833266 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.global_step                      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.833312 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.833359 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.833406 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.833453 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.833504 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.833553 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.833600 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.833647 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.833694 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.833742 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.833789 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.833836 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.833884 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.833930 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.833978 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.834024 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.834075 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.834123 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.834171 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.global_step                                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.834219 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.834266 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.834313 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.834364 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.834413 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.834460 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.834507 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.834569 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.834620 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.834668 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.834715 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.834763 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.global_step                      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.834809 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.834856 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.834903 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.834951 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.834997 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_0.global_step                                           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.835044 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.835090 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.835137 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.835184 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.835231 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.835281 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.835329 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.835375 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.835422 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.835469 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.835515 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.835561 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.835608 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.835655 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.835702 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.835749 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.835795 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.835842 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.835889 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.835937 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.835983 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.836029 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.836076 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.836126 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.836174 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.836221 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.836267 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.836314 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.836361 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.836408 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.836454 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.836501 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.836548 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.836594 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.836641 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.836687 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.836735 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.836782 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.836829 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.836876 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.836923 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.836970 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.837020 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.837068 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.837116 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.837162 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.837210 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.837257 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.837303 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.837350 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.837397 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.837443 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.837490 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.837536 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.837583 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.837629 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.837676 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.837722 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.837768 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.837815 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.837862 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.837912 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.837960 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.838007 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.838054 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.838100 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.838147 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.838194 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.838241 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.838287 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.838334 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.838382 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.838428 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.838475 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.838522 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.838590 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.838639 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.838687 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.838734 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.838785 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.838833 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.838881 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.838928 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.838974 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.839021 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.839068 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.839114 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.839160 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.839206 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.839252 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.839298 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.839345 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.839391 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.839437 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.839483 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.839529 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.839575 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.839621 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.839671 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.839718 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.839765 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.839812 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.839859 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.839905 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.839951 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.839998 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.840045 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.840092 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.840138 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.840185 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.840231 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.840277 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.840323 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.840369 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.840415 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.840460 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.840509 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.840557 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.840603 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.840649 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.840696 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.840742 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.840789 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.840834 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.840880 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.840926 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.840972 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.841018 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.841064 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.841111 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.841157 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.841203 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.841249 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.841295 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.841341 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.841391 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.841438 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.841485 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.841531 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.841578 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.841624 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.841670 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.841717 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.841762 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.841808 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.841855 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.841900 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.841946 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.841993 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.842039 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.842085 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.842132 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.842178 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.842230 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.842278 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.842325 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.842371 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.842418 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.842464 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.842511 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.842575 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.842625 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.842672 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.842719 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.842766 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.842813 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.842859 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.842905 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.842952 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.843000 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.843047 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.843094 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.843146 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.843194 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.843242 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.843289 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.843336 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.843383 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.843431 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.843478 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.843525 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.843572 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.843619 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.843667 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.843713 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.843760 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.843807 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.843854 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.843902 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.843948 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.844000 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.844049 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.844095 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.844146 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.844194 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.844241 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.844287 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.844334 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.844381 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.844428 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.844475 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.global_step                                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.844521 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.844568 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.844615 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.844662 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.844708 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.844755 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.844802 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.844849 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.844900 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.844947 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.844994 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.845041 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.global_step                      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.845087 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.845135 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.845181 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.845228 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.845275 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.845322 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.845368 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.845415 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.845462 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.845508 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.845554 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.845601 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.845647 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.845694 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.845744 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.845792 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.845839 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.845885 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.845932 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.global_step                                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.845978 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.846025 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.846071 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.846118 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.846164 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.846211 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.846257 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.846304 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.846350 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.846396 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.846443 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.846489 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.global_step                      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.846548 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.846601 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.846653 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.846701 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.846748 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_1.global_step                                           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.846795 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.846841 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.846887 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.846933 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.846980 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.847025 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.847072 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.847118 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.847165 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.847211 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.847258 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.847304 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.847350 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.847397 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.847444 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.847495 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.847543 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.847589 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.847636 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.847683 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.847730 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.847777 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.847823 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.847870 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.847917 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.847964 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.848010 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.848057 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.848104 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.848151 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.848198 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.848245 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.848292 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.848339 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.848391 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.848439 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.848486 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.848533 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.848580 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.848627 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.848674 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.848720 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.848767 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.848814 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.848860 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.848907 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.848954 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.849000 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.849047 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.849093 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.849140 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.849187 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.849237 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.849285 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.849331 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.849378 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.849425 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.849471 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.849518 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.849565 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.849611 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.849658 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.849704 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.849751 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.849798 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.849845 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.849892 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.849939 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.849986 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.850033 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.850080 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.850131 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.850178 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.850225 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.850272 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.850319 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.850366 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.850413 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.850460 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.850507 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.850574 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.850625 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.850673 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.850721 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.850767 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.850814 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.850861 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.850909 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.850956 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.851003 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.851055 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.851104 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.851151 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.851198 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.851246 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.851294 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.851341 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.851388 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.851435 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.851482 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.851530 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.851577 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.851624 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.851672 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.851719 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.851766 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.851813 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.851860 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.851910 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.851959 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.852006 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.852053 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.852100 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.852147 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.852194 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.852241 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.852288 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.852335 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.852382 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.852429 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.852476 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.852522 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.852569 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.852615 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.852662 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.852708 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.852754 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.852805 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.852853 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.852900 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.852946 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.852993 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.853040 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.853087 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.853134 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.853180 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.853227 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.853274 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.853320 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.853366 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.853413 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.853460 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.853507 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.853554 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.853600 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.853651 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.853699 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.853746 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.853792 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.853839 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.853886 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.853933 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.853979 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.854026 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.854072 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.854119 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.854165 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.854217 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.854264 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.854311 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.854357 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.854404 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.854450 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.854504 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.854572 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.854623 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.854670 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.854717 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.854763 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.854810 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.854857 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.854902 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.854949 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.854995 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.855042 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.855088 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.855134 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.855181 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.855227 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.855274 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.855320 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.855366 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.855417 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.855464 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.855511 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.855558 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.855605 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.855651 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.855698 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.855744 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.855790 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.855836 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.855883 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.855930 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.855976 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.856023 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.856069 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.856116 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.856163 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.856210 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.856256 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.856307 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.856355 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.856402 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.856448 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.856495 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.856542 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.856589 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.856635 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.856681 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.856728 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.856774 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.856821 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.856867 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.856913 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.856960 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.857007 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.857054 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.857100 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.857151 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.857199 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.857245 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.857291 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.857338 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.857385 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.857431 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.857477 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.857524 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.857571 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.857617 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.857664 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.857711 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.857758 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.857805 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.857852 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.857898 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.857945 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.857992 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.858042 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.858090 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.858137 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.858184 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.858231 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.858277 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.858324 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.858371 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.858418 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.858465 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.858512 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_2.global_step                                           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.858575 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.858626 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.858673 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.858721 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.858768 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.858815 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.858863 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.858915 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.858963 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.859011 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.859057 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.859105 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.859152 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.859199 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.859245 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.859292 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.859339 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.859386 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.859433 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.859481 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.859528 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.859575 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.859622 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.859668 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.859715 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.859761 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.859811 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.859859 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.859906 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.859953 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.859999 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.860046 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.860093 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.860139 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.860186 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.860233 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.860280 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.860327 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.860373 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.860420 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.860467 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.860514 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.860560 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.860607 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.860657 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.860705 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.860752 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.860799 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.860846 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.860893 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.860940 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.860987 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.861034 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.861080 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.861127 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.861174 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.861220 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.861267 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.861314 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.861360 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.861407 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.861453 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.861500 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.861552 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.861599 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.861646 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.861693 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.861739 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.861786 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.861832 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.861879 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.861925 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.861971 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.862018 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.862064 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.862110 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.862156 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.862203 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.862249 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.862295 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.862342 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.862393 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.862441 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.862487 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.862550 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.862604 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.862652 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.862700 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.862747 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.862794 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.862841 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.862888 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.862935 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.862982 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.863029 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.863076 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.863124 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.863171 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.863218 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.863265 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.863318 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.863366 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.863414 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.863461 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.863507 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.863554 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.863601 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.863649 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.863696 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.863743 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.863789 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.863835 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.863882 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.863929 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.863976 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.864022 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.864069 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.864116 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.864163 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.864215 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.864267 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.864314 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.864361 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.864408 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.864455 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.864502 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.864549 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.864596 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.864643 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.864689 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.864736 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.864783 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.864830 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.864876 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.864923 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.864970 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.865017 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.865068 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.865116 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.865163 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.865210 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.865257 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.865303 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.865350 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.865396 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.865443 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.865490 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.865536 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.865583 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.865629 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.865676 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.865722 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.865769 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.865816 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.865863 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.865909 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.865960 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.866008 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.866055 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.866101 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.866147 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.866194 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.866241 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.866287 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.866334 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.866381 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.866428 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.866475 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.866521 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.866586 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.866634 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.866681 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.866728 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.866774 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.866825 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.866873 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.866919 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.866966 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.867012 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.867059 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.867105 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.867152 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.867198 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.867244 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.867291 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.867337 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.867383 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.867429 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.867476 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.867522 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.867569 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.867615 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.867662 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.867712 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.867759 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.867806 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.867853 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.867899 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.867946 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.867992 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.868039 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.868086 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.868132 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.868179 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.868226 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.868273 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.868320 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.868367 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.868414 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.868461 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.868507 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.868559 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.868607 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.868655 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.868701 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.868747 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.868794 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.868841 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.868888 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.868935 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.868981 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.869028 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.869075 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.869121 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.869168 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.869214 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.869261 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.869307 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.869354 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.869400 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.869451 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.869499 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.869546 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.869593 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.869639 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.869686 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.869733 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.869780 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.869826 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.869873 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.869919 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.869965 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.870012 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.870058 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.870104 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.870150 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.870197 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.870243 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.870294 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.global_step                                           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.870341 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_0                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.870388 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_1                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.870434 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_10                                       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.870481 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_11                                       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.870527 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_12                                       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.870590 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_13                                       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.870639 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_14                                       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.870686 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_15                                       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.870732 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_2                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.870778 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_3                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.870825 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_4                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.870872 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_5                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.870918 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_6                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.870964 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_7                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.871010 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_8                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.871056 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_9                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.871103 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.global_step                                   /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.871149 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_0                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.871200 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_1                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.871248 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_10                                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.871295 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_11                                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.871342 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_12                                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.871389 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_13                                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.871436 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_14                                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.871483 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_15                                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.871530 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_2                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.871577 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_3                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.871623 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_4                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.871670 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_5                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.871717 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_6                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.871763 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_7                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.871810 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_8                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.871857 139818304546624 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_9                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.871904 139818304546624 py_utils.py:1474] worker 0: lm.stack.global_step                                                  /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 15:23:34.871989 139818304546624 py_utils.py:1490] ==========
I1001 15:23:37.145354 139818304546624 gpipe.py:457] cell 0 input [<tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/GatherV2_1:0' shape=(1024, 1) dtype=int32>, <tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/GatherV2_2:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None, None, None]
I1001 15:23:39.358137 139818304546624 gpipe.py:457] cell 1 input [<tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/encoder_7/add:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/GatherV2_2:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I1001 15:23:41.521646 139818304546624 gpipe.py:457] cell 2 input [<tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/encoder_15/add:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/GatherV2_2:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I1001 15:23:43.684351 139818304546624 gpipe.py:457] cell 3 input [<tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/encoder_23/add:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/GatherV2_2:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I1001 15:23:45.988131 139818304546624 gpipe.py:457] cell 0 input [<tf.Tensor 'arg259:0' shape=(1024, 1) dtype=int32>, <tf.Tensor 'arg260:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None, None, None]
W1001 15:23:48.707660 139818304546624 recurrent.py:886] cell_fn contains stateful ops: [('emb/Assert/Assert', 'Assert'), ('emb/Assert_1/Assert', 'Assert'), ('encoder_0/fflayer_0/encoder_0/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_0/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_0/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_1/fflayer_0/encoder_1/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_1/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_1/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_2/fflayer_0/encoder_2/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_2/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_2/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_3/fflayer_0/encoder_3/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_3/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_3/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_4/fflayer_0/encoder_4/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_4/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_4/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_5/fflayer_0/encoder_5/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_5/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_5/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_6/fflayer_0/encoder_6/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_6/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_6/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_7/fflayer_0/encoder_7/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_7/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_7/fflayer_1/Assert/AssertGuard/Assert', 'Assert')]
I1001 15:23:49.134574 139818304546624 gpipe.py:457] cell 1 input [<tf.Tensor 'arg254:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'arg255:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
W1001 15:23:51.026853 139818304546624 recurrent.py:886] cell_fn contains stateful ops: [('encoder_8/fflayer_0/encoder_8/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_8/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_8/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_9/fflayer_0/encoder_9/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_9/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_9/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_10/fflayer_0/encoder_10/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_10/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_10/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_11/fflayer_0/encoder_11/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_11/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_11/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_12/fflayer_0/encoder_12/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_12/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_12/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_13/fflayer_0/encoder_13/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_13/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_13/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_14/fflayer_0/encoder_14/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_14/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_14/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_15/fflayer_0/encoder_15/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_15/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_15/fflayer_1/Assert/AssertGuard/Assert', 'Assert')]
I1001 15:23:51.478815 139818304546624 gpipe.py:457] cell 2 input [<tf.Tensor 'arg254:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'arg255:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
W1001 15:23:53.355763 139818304546624 recurrent.py:886] cell_fn contains stateful ops: [('encoder_16/fflayer_0/encoder_16/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_16/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_16/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_17/fflayer_0/encoder_17/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_17/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_17/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_18/fflayer_0/encoder_18/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_18/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_18/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_19/fflayer_0/encoder_19/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_19/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_19/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_20/fflayer_0/encoder_20/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_20/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_20/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_21/fflayer_0/encoder_21/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_21/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_21/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_22/fflayer_0/encoder_22/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_22/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_22/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_23/fflayer_0/encoder_23/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_23/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_23/fflayer_1/Assert/AssertGuard/Assert', 'Assert')]
I1001 15:23:53.842602 139818304546624 gpipe.py:457] cell 3 input [<tf.Tensor 'arg286:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'arg287:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
W1001 15:23:55.734823 139818304546624 recurrent.py:886] cell_fn contains stateful ops: [('encoder_24/fflayer_0/encoder_24/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_24/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_24/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_25/fflayer_0/encoder_25/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_25/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_25/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_26/fflayer_0/encoder_26/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_26/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_26/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_27/fflayer_0/encoder_27/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_27/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_27/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_28/fflayer_0/encoder_28/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_28/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_28/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_29/fflayer_0/encoder_29/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_29/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_29/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_30/fflayer_0/encoder_30/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_30/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_30/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_31/fflayer_0/encoder_31/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_31/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_31/fflayer_1/Assert/AssertGuard/Assert', 'Assert')]
I1001 15:23:56.618506 139818304546624 gpipe.py:457] cell 0 input [<tf.Tensor 'arg259:0' shape=(1024, 1) dtype=int32>, <tf.Tensor 'arg260:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None, None, None]
I1001 15:23:59.197536 139818304546624 gpipe.py:457] cell 1 input [<tf.Tensor 'Recv_1:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'Recv_2:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I1001 15:24:01.721778 139818304546624 gpipe.py:457] cell 2 input [<tf.Tensor 'Recv_1:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'Recv_2:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I1001 15:24:05.603569 139818304546624 gpipe.py:457] cell 3 input [<tf.Tensor 'Recv_1:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'Recv_2:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I1001 15:24:07.659468 139818304546624 gpipe.py:548] pipeline output = [<tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/Reshape_2:0' shape=(1024, 32, 32000) dtype=float32>]
I1001 15:24:07.663865 139818304546624 layers.py:2786] Using sparse_softmax_cross_entropy_with_logits() in SimpleFullSoftmax::_FProp2D logits_shape=[32768, 32000]
I1001 15:24:07.755107 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/total_samples/var on /job:local/replica:0/task:0/device:CPU:0 6970291216
I1001 15:24:07.756956 139818304546624 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/total_samples/var:0 shape=() on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:24:07.764785 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var:0
I1001 15:24:07.764881 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 15:24:07.764950 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 15:24:07.765010 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 15:24:07.765066 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 15:24:07.765119 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var:0
I1001 15:24:07.765171 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var:0
I1001 15:24:07.765222 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 15:24:07.765275 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 15:24:07.765327 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 15:24:07.765379 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 15:24:07.765429 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 15:24:07.765480 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 15:24:07.765541 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 15:24:07.765594 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 15:24:07.765645 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 15:24:07.765695 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var:0
I1001 15:24:07.765745 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var:0
I1001 15:24:07.765796 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 15:24:07.765852 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 15:24:07.765902 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 15:24:07.765953 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 15:24:07.766004 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var:0
I1001 15:24:07.766054 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var:0
I1001 15:24:07.766105 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 15:24:07.766155 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 15:24:07.766206 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 15:24:07.766258 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 15:24:07.766309 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 15:24:07.766359 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 15:24:07.766410 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 15:24:07.766460 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 15:24:07.766511 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 15:24:07.766588 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var:0
I1001 15:24:07.766643 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var:0
I1001 15:24:07.766694 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 15:24:07.766745 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 15:24:07.766796 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 15:24:07.766851 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 15:24:07.766903 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var:0
I1001 15:24:07.766953 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var:0
I1001 15:24:07.767004 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 15:24:07.767055 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 15:24:07.767106 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 15:24:07.767157 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 15:24:07.767208 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 15:24:07.767259 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 15:24:07.767309 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 15:24:07.767360 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 15:24:07.767411 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 15:24:07.767461 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var:0
I1001 15:24:07.767511 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var:0
I1001 15:24:07.767562 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 15:24:07.767612 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 15:24:07.767663 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 15:24:07.767713 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 15:24:07.767762 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var:0
I1001 15:24:07.767812 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var:0
I1001 15:24:07.767863 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 15:24:07.767914 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 15:24:07.767964 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 15:24:07.768014 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 15:24:07.768074 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 15:24:07.768127 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 15:24:07.768178 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 15:24:07.768229 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 15:24:07.768279 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 15:24:07.768330 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var:0
I1001 15:24:07.768380 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var:0
I1001 15:24:07.768429 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 15:24:07.768479 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 15:24:07.768529 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 15:24:07.768579 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 15:24:07.768628 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var:0
I1001 15:24:07.768678 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var:0
I1001 15:24:07.768728 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 15:24:07.768779 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 15:24:07.768830 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 15:24:07.768880 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 15:24:07.768931 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 15:24:07.768981 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 15:24:07.769032 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 15:24:07.769083 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 15:24:07.769133 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 15:24:07.769183 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var:0
I1001 15:24:07.769234 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var:0
I1001 15:24:07.769284 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 15:24:07.769338 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 15:24:07.769389 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 15:24:07.769440 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 15:24:07.769491 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var:0
I1001 15:24:07.769542 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var:0
I1001 15:24:07.769593 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 15:24:07.769644 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 15:24:07.769695 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 15:24:07.769745 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 15:24:07.769796 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 15:24:07.769847 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 15:24:07.769897 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 15:24:07.769947 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 15:24:07.769997 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 15:24:07.770047 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var:0
I1001 15:24:07.770098 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var:0
I1001 15:24:07.770148 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 15:24:07.770198 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 15:24:07.770248 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 15:24:07.770297 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 15:24:07.770348 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var:0
I1001 15:24:07.770398 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var:0
I1001 15:24:07.770448 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 15:24:07.770499 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 15:24:07.770566 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 15:24:07.770628 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 15:24:07.770679 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 15:24:07.770730 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 15:24:07.770780 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 15:24:07.770830 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 15:24:07.770880 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 15:24:07.770930 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var:0
I1001 15:24:07.770981 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var:0
I1001 15:24:07.771032 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 15:24:07.771083 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 15:24:07.771133 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 15:24:07.771184 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 15:24:07.771235 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var:0
I1001 15:24:07.771286 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var:0
I1001 15:24:07.771337 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 15:24:07.771387 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 15:24:07.771438 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 15:24:07.771489 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 15:24:07.771539 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 15:24:07.771590 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 15:24:07.771640 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 15:24:07.771691 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 15:24:07.771742 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 15:24:07.771793 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var:0
I1001 15:24:07.771847 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var:0
I1001 15:24:07.771899 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 15:24:07.771951 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 15:24:07.772001 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 15:24:07.772052 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 15:24:07.772102 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var:0
I1001 15:24:07.772153 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var:0
I1001 15:24:07.772203 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 15:24:07.772253 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 15:24:07.772305 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 15:24:07.772356 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 15:24:07.772406 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 15:24:07.772456 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 15:24:07.772507 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 15:24:07.772557 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 15:24:07.772608 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 15:24:07.772659 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var:0
I1001 15:24:07.772709 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var:0
I1001 15:24:07.772760 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 15:24:07.772811 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 15:24:07.772862 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 15:24:07.772913 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 15:24:07.772963 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var:0
I1001 15:24:07.773014 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var:0
I1001 15:24:07.773065 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 15:24:07.773119 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 15:24:07.773171 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 15:24:07.773221 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 15:24:07.773272 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 15:24:07.773323 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 15:24:07.773373 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 15:24:07.773424 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 15:24:07.773473 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 15:24:07.773524 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var:0
I1001 15:24:07.773573 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var:0
I1001 15:24:07.773622 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 15:24:07.773673 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 15:24:07.773722 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 15:24:07.773772 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 15:24:07.773822 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var:0
I1001 15:24:07.773872 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var:0
I1001 15:24:07.773923 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 15:24:07.773972 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 15:24:07.774023 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 15:24:07.774073 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 15:24:07.774124 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 15:24:07.774174 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 15:24:07.774225 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 15:24:07.774276 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 15:24:07.774331 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 15:24:07.774383 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var:0
I1001 15:24:07.774434 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var:0
I1001 15:24:07.774484 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 15:24:07.774546 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 15:24:07.774606 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 15:24:07.774658 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 15:24:07.774709 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var:0
I1001 15:24:07.774760 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var:0
I1001 15:24:07.774811 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 15:24:07.774862 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 15:24:07.774913 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 15:24:07.774963 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 15:24:07.775013 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 15:24:07.775064 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 15:24:07.775115 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 15:24:07.775166 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 15:24:07.775216 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 15:24:07.775267 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var:0
I1001 15:24:07.775318 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var:0
I1001 15:24:07.775368 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 15:24:07.775418 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 15:24:07.775469 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 15:24:07.775519 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 15:24:07.775569 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var:0
I1001 15:24:07.775624 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var:0
I1001 15:24:07.775675 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 15:24:07.775726 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 15:24:07.775777 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 15:24:07.775827 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 15:24:07.775881 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 15:24:07.775932 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 15:24:07.775983 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 15:24:07.776033 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 15:24:07.776083 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 15:24:07.776133 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var:0
I1001 15:24:07.776183 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var:0
I1001 15:24:07.776234 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 15:24:07.776283 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 15:24:07.776333 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 15:24:07.776382 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 15:24:07.776433 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var:0
I1001 15:24:07.776483 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var:0
I1001 15:24:07.776533 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 15:24:07.776583 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 15:24:07.776633 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 15:24:07.776683 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 15:24:07.776733 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 15:24:07.776784 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 15:24:07.776840 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 15:24:07.776891 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 15:24:07.776941 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 15:24:07.776991 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var:0
I1001 15:24:07.777041 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var:0
I1001 15:24:07.777091 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 15:24:07.777141 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 15:24:07.777192 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 15:24:07.777241 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 15:24:07.777291 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var:0
I1001 15:24:07.777342 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var:0
I1001 15:24:07.777391 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 15:24:07.777442 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 15:24:07.777492 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 15:24:07.777542 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 15:24:07.777592 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 15:24:07.777642 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 15:24:07.777693 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 15:24:07.777743 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 15:24:07.777792 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 15:24:07.777842 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var:0
I1001 15:24:07.777892 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var:0
I1001 15:24:07.777942 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 15:24:07.777992 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 15:24:07.778042 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 15:24:07.778095 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 15:24:07.778146 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var:0
I1001 15:24:07.778195 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var:0
I1001 15:24:07.778245 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 15:24:07.778295 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 15:24:07.778346 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 15:24:07.778396 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 15:24:07.778446 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 15:24:07.778496 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 15:24:07.778565 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 15:24:07.778622 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 15:24:07.778673 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 15:24:07.778724 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var:0
I1001 15:24:07.778774 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var:0
I1001 15:24:07.778825 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 15:24:07.778876 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 15:24:07.778927 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 15:24:07.778977 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 15:24:07.779027 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var:0
I1001 15:24:07.779077 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var:0
I1001 15:24:07.779127 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 15:24:07.779177 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 15:24:07.779227 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 15:24:07.779277 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 15:24:07.779332 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 15:24:07.779384 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 15:24:07.779434 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 15:24:07.779484 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 15:24:07.779534 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 15:24:07.779585 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var:0
I1001 15:24:07.779636 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var:0
I1001 15:24:07.779686 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 15:24:07.779736 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 15:24:07.779787 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 15:24:07.779837 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 15:24:07.779886 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var:0
I1001 15:24:07.779936 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var:0
I1001 15:24:07.779986 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 15:24:07.780036 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 15:24:07.780086 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 15:24:07.780137 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 15:24:07.780187 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 15:24:07.780236 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 15:24:07.780287 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 15:24:07.780337 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 15:24:07.780387 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 15:24:07.780437 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var:0
I1001 15:24:07.780487 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var:0
I1001 15:24:07.780541 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 15:24:07.780592 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 15:24:07.780642 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 15:24:07.780693 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 15:24:07.780743 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var:0
I1001 15:24:07.780793 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var:0
I1001 15:24:07.780844 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 15:24:07.780894 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 15:24:07.780944 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 15:24:07.780995 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 15:24:07.781046 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 15:24:07.781096 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 15:24:07.781147 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 15:24:07.781197 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 15:24:07.781248 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 15:24:07.781298 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var:0
I1001 15:24:07.781347 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var:0
I1001 15:24:07.781397 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 15:24:07.781448 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 15:24:07.781499 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 15:24:07.781549 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 15:24:07.781600 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var:0
I1001 15:24:07.781651 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var:0
I1001 15:24:07.781701 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 15:24:07.781751 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 15:24:07.781805 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 15:24:07.781857 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 15:24:07.781908 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 15:24:07.781958 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 15:24:07.782009 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 15:24:07.782058 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 15:24:07.782108 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 15:24:07.782158 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var:0
I1001 15:24:07.782208 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var:0
I1001 15:24:07.782258 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 15:24:07.782309 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 15:24:07.782359 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 15:24:07.782408 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 15:24:07.782459 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var:0
I1001 15:24:07.782509 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var:0
I1001 15:24:07.782575 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 15:24:07.782629 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 15:24:07.782680 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 15:24:07.782732 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 15:24:07.782783 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 15:24:07.782833 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 15:24:07.782883 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 15:24:07.782933 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 15:24:07.782984 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 15:24:07.783039 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var:0
I1001 15:24:07.783090 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var:0
I1001 15:24:07.783140 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 15:24:07.783189 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 15:24:07.783238 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 15:24:07.783289 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 15:24:07.783338 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var:0
I1001 15:24:07.783388 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var:0
I1001 15:24:07.783437 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 15:24:07.783488 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 15:24:07.783538 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 15:24:07.783588 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 15:24:07.783638 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 15:24:07.783687 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 15:24:07.783738 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 15:24:07.783788 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 15:24:07.783839 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 15:24:07.783889 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var:0
I1001 15:24:07.783940 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var:0
I1001 15:24:07.783990 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 15:24:07.784040 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 15:24:07.784090 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 15:24:07.784140 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 15:24:07.784190 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var:0
I1001 15:24:07.784241 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var:0
I1001 15:24:07.784295 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 15:24:07.784346 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 15:24:07.784396 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 15:24:07.784447 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 15:24:07.784497 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 15:24:07.784546 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 15:24:07.784596 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 15:24:07.784647 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 15:24:07.784697 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 15:24:07.784747 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var:0
I1001 15:24:07.784797 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var:0
I1001 15:24:07.784847 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 15:24:07.784897 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 15:24:07.784946 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 15:24:07.784996 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 15:24:07.785046 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var:0
I1001 15:24:07.785095 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var:0
I1001 15:24:07.785145 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 15:24:07.785195 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 15:24:07.785244 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 15:24:07.785294 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 15:24:07.785343 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 15:24:07.785394 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 15:24:07.785443 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 15:24:07.785498 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 15:24:07.785549 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 15:24:07.785600 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var:0
I1001 15:24:07.785649 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var:0
I1001 15:24:07.785699 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 15:24:07.785748 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 15:24:07.785799 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 15:24:07.785848 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 15:24:07.785898 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var:0
I1001 15:24:07.785953 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var:0
I1001 15:24:07.786003 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 15:24:07.786053 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 15:24:07.786103 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 15:24:07.786154 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 15:24:07.786210 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 15:24:07.786288 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 15:24:07.786374 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 15:24:07.786431 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 15:24:07.786483 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 15:24:07.786546 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var:0
I1001 15:24:07.786606 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var:0
I1001 15:24:07.786658 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 15:24:07.786708 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 15:24:07.786758 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 15:24:07.786809 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 15:24:07.786865 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var:0
I1001 15:24:07.786917 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var:0
I1001 15:24:07.786968 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 15:24:07.787019 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 15:24:07.787069 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 15:24:07.787119 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 15:24:07.787169 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 15:24:07.787220 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 15:24:07.787270 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 15:24:07.787320 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 15:24:07.787370 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 15:24:07.787420 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var:0
I1001 15:24:07.787470 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var:0
I1001 15:24:07.787520 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 15:24:07.787570 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 15:24:07.787620 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 15:24:07.787669 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 15:24:07.787719 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var:0
I1001 15:24:07.787770 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var:0
I1001 15:24:07.787821 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 15:24:07.787872 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 15:24:07.787922 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 15:24:07.787973 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 15:24:07.788022 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 15:24:07.788077 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 15:24:07.788128 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 15:24:07.788179 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 15:24:07.788229 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 15:24:07.788280 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var:0
I1001 15:24:07.788330 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var:0
I1001 15:24:07.788380 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 15:24:07.788430 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 15:24:07.788481 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 15:24:07.788530 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 15:24:07.788581 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var:0
I1001 15:24:07.788631 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var:0
I1001 15:24:07.788681 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 15:24:07.788731 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 15:24:07.788782 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 15:24:07.788833 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 15:24:07.788883 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 15:24:07.788932 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 15:24:07.788982 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 15:24:07.789032 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 15:24:07.789082 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 15:24:07.789132 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var:0
I1001 15:24:07.789183 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var:0
I1001 15:24:07.789233 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 15:24:07.789288 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 15:24:07.789340 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 15:24:07.789390 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 15:24:07.789440 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var:0
I1001 15:24:07.789490 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var:0
I1001 15:24:07.789541 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 15:24:07.789591 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 15:24:07.789642 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 15:24:07.789691 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 15:24:07.789741 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 15:24:07.789792 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 15:24:07.789842 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 15:24:07.789892 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 15:24:07.789942 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 15:24:07.789993 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var:0
I1001 15:24:07.790043 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var:0
I1001 15:24:07.790094 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 15:24:07.790145 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 15:24:07.790196 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 15:24:07.790246 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 15:24:07.790296 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var:0
I1001 15:24:07.790348 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var:0
I1001 15:24:07.790399 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 15:24:07.790449 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 15:24:07.790500 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 15:24:07.790573 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 15:24:07.790630 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 15:24:07.790683 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 15:24:07.790734 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 15:24:07.790785 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 15:24:07.790836 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 15:24:07.790886 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var:0
I1001 15:24:07.790936 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var:0
I1001 15:24:07.790986 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 15:24:07.791037 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 15:24:07.791087 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 15:24:07.791137 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 15:24:07.791187 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var:0
I1001 15:24:07.791237 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var:0
I1001 15:24:07.791288 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 15:24:07.791338 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 15:24:07.791389 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 15:24:07.791440 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 15:24:07.791491 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 15:24:07.791541 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 15:24:07.791591 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 15:24:07.791641 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 15:24:07.791691 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 15:24:07.791741 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var:0
I1001 15:24:07.791796 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var:0
I1001 15:24:07.791848 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 15:24:07.791898 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 15:24:07.791948 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 15:24:07.791998 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 15:24:07.792048 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var:0
I1001 15:24:07.792098 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var:0
I1001 15:24:07.792148 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 15:24:07.792198 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 15:24:07.792248 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 15:24:07.792299 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 15:24:07.792349 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 15:24:07.792400 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 15:24:07.792450 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 15:24:07.792500 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 15:24:07.792557 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 15:24:07.792608 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var:0
I1001 15:24:07.792658 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var:0
I1001 15:24:07.792708 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var:0
I1001 15:24:07.792758 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var:0
I1001 15:24:07.792809 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var:0
I1001 15:24:07.792859 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var:0
I1001 15:24:07.792909 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var:0
I1001 15:24:07.792959 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var:0
I1001 15:24:07.793009 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var:0
I1001 15:24:07.793059 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var:0
I1001 15:24:07.793109 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var:0
I1001 15:24:07.793164 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var:0
I1001 15:24:07.793215 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var:0
I1001 15:24:07.793265 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var:0
I1001 15:24:07.793315 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var:0
I1001 15:24:07.793365 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var:0
I1001 15:24:07.793415 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var:0
I1001 15:24:07.793464 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var:0
I1001 15:24:07.793514 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var:0
I1001 15:24:07.793565 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var:0
I1001 15:24:07.793615 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var:0
I1001 15:24:07.793665 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var:0
I1001 15:24:07.793715 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var:0
I1001 15:24:07.793764 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var:0
I1001 15:24:07.793814 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var:0
I1001 15:24:07.793865 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var:0
I1001 15:24:07.793914 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var:0
I1001 15:24:07.793964 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var:0
I1001 15:24:07.794014 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var:0
I1001 15:24:07.794064 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var:0
I1001 15:24:07.794114 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var:0
I1001 15:24:07.794163 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var:0
I1001 15:24:07.794213 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var:0
I1001 15:24:07.794263 139818304546624 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var:0
I1001 15:24:12.376617 139818304546624 gpipe.py:457] cell 3 input [<tf.Tensor 'arg287:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'arg288:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I1001 15:24:19.032639 139818304546624 gpipe.py:457] cell 2 input [<tf.Tensor 'arg255:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'arg256:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I1001 15:24:24.041433 139818304546624 gpipe.py:457] cell 1 input [<tf.Tensor 'arg255:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'arg256:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I1001 15:24:30.565430 139818304546624 gpipe.py:457] cell 0 input [<tf.Tensor 'arg259:0' shape=(1024, 1) dtype=int32>, <tf.Tensor 'arg260:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None, None, None]
I1001 15:24:40.619924 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.emb.src_token_emb.wm: <tf.Variable '1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var:0' shape=(32000, 2048) dtype=float32_ref>
I1001 15:24:40.620157 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 15:24:40.620271 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 15:24:40.620400 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.620493 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 15:24:40.620588 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.620677 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.620764 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 15:24:40.620851 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.620941 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.621026 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.621115 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.621200 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.621288 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.621372 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.621460 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.621551 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.621637 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.621722 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 15:24:40.621806 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 15:24:40.621894 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.621979 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 15:24:40.622067 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.622152 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.622237 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 15:24:40.622321 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.622411 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.622497 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.622607 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.622694 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.622789 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.622875 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.622965 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.623049 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.623132 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.623219 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 15:24:40.623305 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 15:24:40.623394 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.623479 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 15:24:40.623567 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.623652 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.623736 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 15:24:40.623821 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.623909 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.623995 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.624088 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.624175 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.624264 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.624349 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.624437 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.624522 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.624607 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.624692 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 15:24:40.624778 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 15:24:40.624866 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.624951 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 15:24:40.625039 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.625124 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.625208 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 15:24:40.625292 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.625383 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.625467 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.625553 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.625636 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.625722 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.625804 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.625890 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.625974 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.626055 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.626137 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 15:24:40.626224 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 15:24:40.626309 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.626391 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 15:24:40.626478 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.626573 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.626663 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 15:24:40.626745 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.626834 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.626915 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.627002 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.627084 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.627170 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.627252 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.627338 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.627421 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.627503 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.627587 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 15:24:40.627668 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 15:24:40.627753 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.627834 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 15:24:40.627927 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.628008 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.628092 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 15:24:40.628172 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.628257 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.628339 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.628424 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.628507 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.628593 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.628674 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.628760 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.628840 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.628922 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.629004 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 15:24:40.629093 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 15:24:40.629177 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.629260 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 15:24:40.629345 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.629427 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.629509 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 15:24:40.629590 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.629676 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.629757 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.629843 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.629924 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.630002 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.630064 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.630127 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.630187 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.630253 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.630314 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 15:24:40.630373 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 15:24:40.630436 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.630496 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 15:24:40.630579 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.630643 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.630703 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 15:24:40.630763 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.630827 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.630888 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.630952 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.631012 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.631075 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.631136 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.631206 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.631268 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.631329 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.631389 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 15:24:40.631449 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 15:24:40.631512 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.631573 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 15:24:40.631637 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.631696 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.631757 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 15:24:40.631816 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.631880 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.631941 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.632005 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.632069 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.632133 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.632194 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.632257 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.632318 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.632378 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.632438 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 15:24:40.632498 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 15:24:40.632561 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.632622 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 15:24:40.632685 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.632746 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.632807 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 15:24:40.632867 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.632930 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.632996 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.633061 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.633122 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.633185 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.633246 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.633309 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.633370 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.633431 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.633492 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 15:24:40.633552 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 15:24:40.633616 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.633676 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 15:24:40.633739 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.633799 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.633859 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 15:24:40.633922 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.633987 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.634047 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.634110 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.634171 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.634235 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.634296 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.634359 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.634420 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.634480 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.634558 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 15:24:40.634626 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 15:24:40.634691 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.634752 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 15:24:40.634820 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.634883 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.634942 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 15:24:40.635002 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.635065 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.635124 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.635187 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.635248 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.635311 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.635371 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.635433 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.635494 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.635553 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.635613 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 15:24:40.635672 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 15:24:40.635739 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.635800 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 15:24:40.635863 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.635924 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.635984 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 15:24:40.636043 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.636106 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.636166 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.636233 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.636294 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.636357 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.636417 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.636480 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.636540 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.636603 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.636663 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 15:24:40.636723 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 15:24:40.636786 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.636847 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 15:24:40.636910 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.636970 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.637029 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 15:24:40.637088 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.637151 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.637211 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.637274 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.637334 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.637397 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.637457 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.637525 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.637585 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.637645 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.637704 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 15:24:40.637763 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 15:24:40.637826 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.637887 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 15:24:40.637949 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.638009 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.638068 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 15:24:40.638128 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.638191 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.638252 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.638315 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.638375 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.638443 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.638504 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.638587 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.638650 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.638710 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.638770 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 15:24:40.638831 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 15:24:40.638894 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.638955 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 15:24:40.639019 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.639080 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.639140 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 15:24:40.639200 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.639264 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.639324 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.639397 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.639460 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.639523 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.639584 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.639647 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.639708 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.639769 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.639829 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 15:24:40.639889 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 15:24:40.639952 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.640013 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 15:24:40.640075 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.640136 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.640195 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 15:24:40.640266 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.640331 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.640392 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.640455 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.640516 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.640578 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.640638 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.640702 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.640762 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.640821 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.640882 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 15:24:40.640942 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 15:24:40.641005 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.641066 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 15:24:40.641129 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.641193 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.641254 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 15:24:40.641314 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.641377 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.641439 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.641502 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.641563 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.641626 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.641686 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.641750 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.641810 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.641870 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.641929 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 15:24:40.641989 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 15:24:40.642051 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.642116 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 15:24:40.642180 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.642241 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.642302 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 15:24:40.642361 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.642425 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.642487 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.642566 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.642632 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.642696 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.642756 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.642820 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.642879 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.642939 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.643003 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 15:24:40.643064 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 15:24:40.643127 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.643187 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 15:24:40.643250 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.643310 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.643369 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 15:24:40.643429 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.643491 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.643551 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.643615 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.643675 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.643738 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.643797 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.643859 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.643923 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.643984 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.644042 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 15:24:40.644103 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 15:24:40.644165 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.644225 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 15:24:40.644287 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.644347 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.644406 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 15:24:40.644465 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.644528 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.644588 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.644651 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.644711 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.644778 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.644839 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.644902 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.644962 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.645021 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.645080 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 15:24:40.645139 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 15:24:40.645201 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.645261 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 15:24:40.645323 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.645383 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.645442 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 15:24:40.645501 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.645565 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.645625 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.645691 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.645752 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.645815 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.645875 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.645937 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.645997 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.646058 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.646117 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 15:24:40.646177 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 15:24:40.646239 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.646303 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 15:24:40.646366 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.646427 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.646487 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 15:24:40.646565 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.646639 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.646702 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.646765 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.646826 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.646888 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.646949 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.647012 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.647072 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.647132 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.647192 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 15:24:40.647252 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 15:24:40.647315 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.647375 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 15:24:40.647437 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.647497 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.647561 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 15:24:40.647622 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.647686 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.647747 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.647810 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.647870 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.647932 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.647993 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.648056 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.648115 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.648175 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.648233 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 15:24:40.648293 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 15:24:40.648356 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.648420 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 15:24:40.648484 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.648544 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.648603 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 15:24:40.648663 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.648725 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.648785 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.648847 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.648906 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.648970 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.649029 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.649091 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.649151 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.649210 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.649269 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 15:24:40.649333 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 15:24:40.649396 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.649457 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 15:24:40.649520 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.649580 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.649638 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 15:24:40.649697 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.649761 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.649821 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.649885 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.649946 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.650009 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.650069 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.650132 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.650198 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.650258 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.650317 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 15:24:40.650377 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 15:24:40.650440 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.650501 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 15:24:40.650582 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.650645 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.650705 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 15:24:40.650764 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.650828 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.650889 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.650954 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.651015 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.651079 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.651144 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.651208 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.651269 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.651329 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.651388 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 15:24:40.651449 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 15:24:40.651512 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.651573 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 15:24:40.651635 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.651695 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.651754 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 15:24:40.651813 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.651877 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.651937 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.651999 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.652064 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.652128 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.652189 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.652252 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.652312 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.652372 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.652432 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 15:24:40.652491 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 15:24:40.652554 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.652615 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 15:24:40.652677 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.652738 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.652797 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 15:24:40.652858 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.652925 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.652986 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.653049 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.653110 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.653174 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.653236 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.653299 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.653359 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.653419 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.653479 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 15:24:40.653539 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 15:24:40.653602 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.653662 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 15:24:40.653726 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.653786 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.653850 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 15:24:40.653911 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.653975 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.654036 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.654098 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.654160 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.654222 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.654283 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.654345 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.654404 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.654464 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.654525 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 15:24:40.654604 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 15:24:40.654669 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.654730 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 15:24:40.654798 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.654860 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.654921 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 15:24:40.654980 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.655044 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.655104 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.655168 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.655228 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.655291 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.655352 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.655415 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.655476 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.655537 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.655597 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 15:24:40.655661 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 15:24:40.655725 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.655786 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 15:24:40.655848 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.655909 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.655969 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 15:24:40.656029 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.656093 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.656154 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.656217 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.656279 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.656346 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.656408 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 15:24:40.656472 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.656533 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.656598 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 15:24:40.656659 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_0: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var:0' shape=(2000,) dtype=float32_ref>
I1001 15:24:40.656720 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_1: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var:0' shape=(2000,) dtype=float32_ref>
I1001 15:24:40.656780 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_10: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var:0' shape=(2000,) dtype=float32_ref>
I1001 15:24:40.656840 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_11: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var:0' shape=(2000,) dtype=float32_ref>
I1001 15:24:40.656899 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_12: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var:0' shape=(2000,) dtype=float32_ref>
I1001 15:24:40.656958 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_13: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var:0' shape=(2000,) dtype=float32_ref>
I1001 15:24:40.657017 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_14: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var:0' shape=(2000,) dtype=float32_ref>
I1001 15:24:40.657076 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_15: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var:0' shape=(2000,) dtype=float32_ref>
I1001 15:24:40.657135 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_2: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var:0' shape=(2000,) dtype=float32_ref>
I1001 15:24:40.657194 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_3: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var:0' shape=(2000,) dtype=float32_ref>
I1001 15:24:40.657253 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_4: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var:0' shape=(2000,) dtype=float32_ref>
I1001 15:24:40.657312 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_5: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var:0' shape=(2000,) dtype=float32_ref>
I1001 15:24:40.657371 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_6: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var:0' shape=(2000,) dtype=float32_ref>
I1001 15:24:40.657430 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_7: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var:0' shape=(2000,) dtype=float32_ref>
I1001 15:24:40.657489 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_8: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var:0' shape=(2000,) dtype=float32_ref>
I1001 15:24:40.657547 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_9: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var:0' shape=(2000,) dtype=float32_ref>
I1001 15:24:40.657608 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_0: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var:0' shape=(2048, 2000) dtype=float32_ref>
I1001 15:24:40.657676 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_1: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var:0' shape=(2048, 2000) dtype=float32_ref>
I1001 15:24:40.657741 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_10: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var:0' shape=(2048, 2000) dtype=float32_ref>
I1001 15:24:40.657805 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_11: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var:0' shape=(2048, 2000) dtype=float32_ref>
I1001 15:24:40.657870 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_12: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var:0' shape=(2048, 2000) dtype=float32_ref>
I1001 15:24:40.657934 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_13: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var:0' shape=(2048, 2000) dtype=float32_ref>
I1001 15:24:40.657998 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_14: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var:0' shape=(2048, 2000) dtype=float32_ref>
I1001 15:24:40.658061 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_15: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var:0' shape=(2048, 2000) dtype=float32_ref>
I1001 15:24:40.658123 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_2: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var:0' shape=(2048, 2000) dtype=float32_ref>
I1001 15:24:40.658187 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_3: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var:0' shape=(2048, 2000) dtype=float32_ref>
I1001 15:24:40.658250 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_4: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var:0' shape=(2048, 2000) dtype=float32_ref>
I1001 15:24:40.658313 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_5: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var:0' shape=(2048, 2000) dtype=float32_ref>
I1001 15:24:40.658376 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_6: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var:0' shape=(2048, 2000) dtype=float32_ref>
I1001 15:24:40.658439 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_7: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var:0' shape=(2048, 2000) dtype=float32_ref>
I1001 15:24:40.658503 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_8: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var:0' shape=(2048, 2000) dtype=float32_ref>
I1001 15:24:40.658587 139818304546624 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_9: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var:0' shape=(2048, 2000) dtype=float32_ref>
I1001 15:24:50.201582 139818304546624 learner.py:279] gradient_adjuster=<bound method LanguageModel.AdjustGradients of <lingvo.tasks.lm.model.FixedShapeInputLanguageModel object at 0x7f288eb7f630>>
I1001 15:24:53.858697 139818304546624 cluster.py:515] Place variable beta1_power on /job:local/replica:0/task:0/device:CPU:0 6970291220
I1001 15:24:53.861739 139818304546624 cluster.py:515] Place variable beta2_power on /job:local/replica:0/task:0/device:CPU:0 6970291224
I1001 15:24:53.866508 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7232435224
I1001 15:24:53.871770 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7494579224
I1001 15:24:53.876922 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7494611992
I1001 15:24:53.881969 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7494644760
I1001 15:24:53.887133 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7561753624
I1001 15:24:53.892158 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7628862488
I1001 15:24:53.897321 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7628870680
I1001 15:24:53.902361 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7628878872
I1001 15:24:53.907517 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7695987736
I1001 15:24:53.912561 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7763096600
I1001 15:24:53.918412 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7763104792
I1001 15:24:53.923489 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7763112984
I1001 15:24:53.928638 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7763121176
I1001 15:24:53.933761 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7763129368
I1001 15:24:53.937615 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7763129880
I1001 15:24:53.941387 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7763130392
I1001 15:24:53.946448 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7779907608
I1001 15:24:53.951516 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7796684824
I1001 15:24:53.972592 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7796693016
I1001 15:24:53.977639 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7796701208
I1001 15:24:53.982818 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7813478424
I1001 15:24:53.987957 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7830255640
I1001 15:24:53.992999 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7830263832
I1001 15:24:53.998122 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7830272024
I1001 15:24:54.003172 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7847049240
I1001 15:24:54.008315 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7863826456
I1001 15:24:54.013361 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7863834648
I1001 15:24:54.018498 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7863842840
I1001 15:24:54.023561 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7880620056
I1001 15:24:54.028709 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7897397272
I1001 15:24:54.033741 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7897405464
I1001 15:24:54.038928 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7897413656
I1001 15:24:54.044509 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7897421848
I1001 15:24:54.049685 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7897430040
I1001 15:24:54.054829 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7897438232
I1001 15:24:54.059864 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7897446424
I1001 15:24:54.065002 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7897479192
I1001 15:24:54.070041 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7897511960
I1001 15:24:54.075208 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7964620824
I1001 15:24:54.080265 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8031729688
I1001 15:24:54.085408 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8031737880
I1001 15:24:54.090440 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8031746072
I1001 15:24:54.095565 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8098854936
I1001 15:24:54.100676 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8165963800
I1001 15:24:54.105795 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8165971992
I1001 15:24:54.110978 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8165980184
I1001 15:24:54.116010 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8165988376
I1001 15:24:54.121138 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8165996568
I1001 15:24:54.124989 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8165997080
I1001 15:24:54.128759 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8165997592
I1001 15:24:54.133800 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8182774808
I1001 15:24:54.138863 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8199552024
I1001 15:24:54.143982 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8199560216
I1001 15:24:54.149119 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8199568408
I1001 15:24:54.154197 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8216345624
I1001 15:24:54.159833 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8233122840
I1001 15:24:54.164942 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8233131032
I1001 15:24:54.170104 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8233139224
I1001 15:24:54.175184 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8249916440
I1001 15:24:54.180373 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8266693656
I1001 15:24:54.185444 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8266701848
I1001 15:24:54.190658 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8266710040
I1001 15:24:54.195744 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8283487256
I1001 15:24:54.200923 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8300264472
I1001 15:24:54.206098 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8300272664
I1001 15:24:54.211189 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8300280856
I1001 15:24:54.216355 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8300289048
I1001 15:24:54.221413 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8300297240
I1001 15:24:54.226588 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8300305432
I1001 15:24:54.231650 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8300313624
I1001 15:24:54.236797 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8300346392
I1001 15:24:54.241862 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8300379160
I1001 15:24:54.247071 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8367488024
I1001 15:24:54.252112 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8434596888
I1001 15:24:54.257299 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8434605080
I1001 15:24:54.262458 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8434613272
I1001 15:24:54.267551 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8501722136
I1001 15:24:54.273173 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8568831000
I1001 15:24:54.278232 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8568839192
I1001 15:24:54.283433 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8568847384
I1001 15:24:54.288520 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8568855576
I1001 15:24:54.293692 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8568863768
I1001 15:24:54.297579 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8568864280
I1001 15:24:54.301363 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8568864792
I1001 15:24:54.306436 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8585642008
I1001 15:24:54.311623 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8602419224
I1001 15:24:54.316763 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8602427416
I1001 15:24:54.321939 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8602435608
I1001 15:24:54.327044 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8619212824
I1001 15:24:54.332236 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8635990040
I1001 15:24:54.337298 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8635998232
I1001 15:24:54.342489 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8636006424
I1001 15:24:54.347608 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8652783640
I1001 15:24:54.352783 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8669560856
I1001 15:24:54.357840 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8669569048
I1001 15:24:54.363031 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8669577240
I1001 15:24:54.368313 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8686354456
I1001 15:24:54.373405 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8703131672
I1001 15:24:54.378627 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8703139864
I1001 15:24:54.383702 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8703148056
I1001 15:24:54.389374 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8703156248
I1001 15:24:54.394459 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8703164440
I1001 15:24:54.399632 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8703172632
I1001 15:24:54.404712 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8703180824
I1001 15:24:54.409894 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8703213592
I1001 15:24:54.415050 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8703246360
I1001 15:24:54.420269 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8770355224
I1001 15:24:54.425346 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8837464088
I1001 15:24:54.430485 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8837472280
I1001 15:24:54.435665 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8837480472
I1001 15:24:54.440716 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8904589336
I1001 15:24:54.445925 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8971698200
I1001 15:24:54.451012 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8971706392
I1001 15:24:54.456191 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8971714584
I1001 15:24:54.461297 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8971722776
I1001 15:24:54.466472 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8971730968
I1001 15:24:54.470399 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8971731480
I1001 15:24:54.474202 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8971731992
I1001 15:24:54.479345 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8988509208
I1001 15:24:54.484530 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9005286424
I1001 15:24:54.489595 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9005294616
I1001 15:24:54.494812 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9005302808
I1001 15:24:54.499902 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9022080024
I1001 15:24:54.505603 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9038857240
I1001 15:24:54.510704 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9038865432
I1001 15:24:54.515892 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9038873624
I1001 15:24:54.520959 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9055650840
I1001 15:24:54.526150 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9072428056
I1001 15:24:54.531247 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9072436248
I1001 15:24:54.536414 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9072444440
I1001 15:24:54.541588 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9089221656
I1001 15:24:54.546715 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9105998872
I1001 15:24:54.551924 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9106007064
I1001 15:24:54.557019 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9106015256
I1001 15:24:54.562153 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9106023448
I1001 15:24:54.567369 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9106031640
I1001 15:24:54.572528 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9106039832
I1001 15:24:54.577574 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9106048024
I1001 15:24:54.582787 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9106080792
I1001 15:24:54.587853 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9106113560
I1001 15:24:54.593009 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9173222424
I1001 15:24:54.598072 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9240331288
I1001 15:24:54.603288 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9240339480
I1001 15:24:54.608431 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9240347672
I1001 15:24:54.613502 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9307456536
I1001 15:24:54.619154 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9374565400
I1001 15:24:54.624217 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9374573592
I1001 15:24:54.629402 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9374581784
I1001 15:24:54.634480 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9374589976
I1001 15:24:54.639674 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9374598168
I1001 15:24:54.643558 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9374598680
I1001 15:24:54.647362 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9374599192
I1001 15:24:54.652481 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9391376408
I1001 15:24:54.657688 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9408153624
I1001 15:24:54.662786 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9408161816
I1001 15:24:54.667957 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9408170008
I1001 15:24:54.673054 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9424947224
I1001 15:24:54.678249 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9441724440
I1001 15:24:54.683432 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9441732632
I1001 15:24:54.688624 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9441740824
I1001 15:24:54.693689 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9458518040
I1001 15:24:54.698906 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9475295256
I1001 15:24:54.703988 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9475303448
I1001 15:24:54.709260 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9475311640
I1001 15:24:54.714472 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9492088856
I1001 15:24:54.719579 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9508866072
I1001 15:24:54.724788 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9508874264
I1001 15:24:54.729875 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9508882456
I1001 15:24:54.735568 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9508890648
I1001 15:24:54.740622 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9508898840
I1001 15:24:54.745803 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9508907032
I1001 15:24:54.750911 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9508915224
I1001 15:24:54.756099 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9508947992
I1001 15:24:54.761215 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9508980760
I1001 15:24:54.766460 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9576089624
I1001 15:24:54.771578 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9643198488
I1001 15:24:54.776757 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9643206680
I1001 15:24:54.781937 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9643214872
I1001 15:24:54.787081 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9710323736
I1001 15:24:54.792276 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9777432600
I1001 15:24:54.797365 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9777440792
I1001 15:24:54.802567 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9777448984
I1001 15:24:54.807648 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9777457176
I1001 15:24:54.812888 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9777465368
I1001 15:24:54.816788 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9777465880
I1001 15:24:54.820600 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9777466392
I1001 15:24:54.825696 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9794243608
I1001 15:24:54.830915 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9811020824
I1001 15:24:54.835991 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9811029016
I1001 15:24:54.841224 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9811037208
I1001 15:24:54.846325 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9827814424
I1001 15:24:54.852004 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9844591640
I1001 15:24:54.857103 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9844599832
I1001 15:24:54.862287 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9844608024
I1001 15:24:54.867409 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9861385240
I1001 15:24:54.872601 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9878162456
I1001 15:24:54.877670 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9878170648
I1001 15:24:54.882894 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9878178840
I1001 15:24:54.888094 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9894956056
I1001 15:24:54.893304 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9911733272
I1001 15:24:54.898481 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9911741464
I1001 15:24:54.903600 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9911749656
I1001 15:24:54.908760 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9911757848
I1001 15:24:54.913834 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9911766040
I1001 15:24:54.919266 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9911774232
I1001 15:24:54.924360 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9911782424
I1001 15:24:54.929575 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9911815192
I1001 15:24:54.934693 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9911847960
I1001 15:24:54.939882 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9978956824
I1001 15:24:54.944998 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10046065688
I1001 15:24:54.950191 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10046073880
I1001 15:24:54.955400 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10046082072
I1001 15:24:54.960476 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10113190936
I1001 15:24:54.966213 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10180299800
I1001 15:24:54.971414 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10180307992
I1001 15:24:54.976595 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10180316184
I1001 15:24:54.981700 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10180324376
I1001 15:24:54.986894 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10180332568
I1001 15:24:54.990793 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10180333080
I1001 15:24:54.994596 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10180333592
I1001 15:24:54.999731 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10197110808
I1001 15:24:55.004918 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10213888024
I1001 15:24:55.010019 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10213896216
I1001 15:24:55.015265 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10213904408
I1001 15:24:55.020345 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10230681624
I1001 15:24:55.025572 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10247458840
I1001 15:24:55.030698 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10247467032
I1001 15:24:55.035918 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10247475224
I1001 15:24:55.040999 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10264252440
I1001 15:24:55.046209 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10281029656
I1001 15:24:55.051392 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10281037848
I1001 15:24:55.056572 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10281046040
I1001 15:24:55.061779 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10297823256
I1001 15:24:55.066916 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10314600472
I1001 15:24:55.072111 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10314608664
I1001 15:24:55.077231 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10314616856
I1001 15:24:55.082887 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10314625048
I1001 15:24:55.087982 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10314633240
I1001 15:24:55.093175 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10314641432
I1001 15:24:55.098347 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10314649624
I1001 15:24:55.103600 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10314682392
I1001 15:24:55.108685 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10314715160
I1001 15:24:55.113878 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10381824024
I1001 15:24:55.118973 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10448932888
I1001 15:24:55.124154 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10448941080
I1001 15:24:55.129359 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10448949272
I1001 15:24:55.134437 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10516058136
I1001 15:24:55.139676 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10583167000
I1001 15:24:55.144755 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10583175192
I1001 15:24:55.149950 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10583183384
I1001 15:24:55.155108 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10583191576
I1001 15:24:55.160303 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10583199768
I1001 15:24:55.164214 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10583200280
I1001 15:24:55.168109 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10583200792
I1001 15:24:55.173210 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10599978008
I1001 15:24:55.178423 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10616755224
I1001 15:24:55.183557 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10616763416
I1001 15:24:55.188758 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10616771608
I1001 15:24:55.193936 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10633548824
I1001 15:24:55.199616 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10650326040
I1001 15:24:55.204731 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10650334232
I1001 15:24:55.209939 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10650342424
I1001 15:24:55.215082 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10667119640
I1001 15:24:55.220289 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10683896856
I1001 15:24:55.225403 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10683905048
I1001 15:24:55.230658 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10683913240
I1001 15:24:55.235882 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10700690456
I1001 15:24:55.241017 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10717467672
I1001 15:24:55.246248 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10717475864
I1001 15:24:55.251384 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10717484056
I1001 15:24:55.256581 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10717492248
I1001 15:24:55.261671 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10717500440
I1001 15:24:55.266878 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10717508632
I1001 15:24:55.271978 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10717516824
I1001 15:24:55.277174 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10717549592
I1001 15:24:55.282287 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10717582360
I1001 15:24:55.287505 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10784691224
I1001 15:24:55.292593 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10851800088
I1001 15:24:55.297798 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10851808280
I1001 15:24:55.303032 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10851816472
I1001 15:24:55.308217 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10918925336
I1001 15:24:55.313893 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10986034200
I1001 15:24:55.319026 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10986042392
I1001 15:24:55.324213 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10986050584
I1001 15:24:55.329312 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10986058776
I1001 15:24:55.334503 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10986066968
I1001 15:24:55.338452 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10986067480
I1001 15:24:55.342268 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10986067992
I1001 15:24:55.347438 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11002845208
I1001 15:24:55.352635 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11019622424
I1001 15:24:55.357734 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11019630616
I1001 15:24:55.363046 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11019638808
I1001 15:24:55.368241 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11036416024
I1001 15:24:55.373468 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11053193240
I1001 15:24:55.378585 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11053201432
I1001 15:24:55.383784 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11053209624
I1001 15:24:55.388935 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11069986840
I1001 15:24:55.394141 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11086764056
I1001 15:24:55.399353 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11086772248
I1001 15:24:55.404525 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11086780440
I1001 15:24:55.409753 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11103557656
I1001 15:24:55.414999 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11120334872
I1001 15:24:55.420214 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11120343064
I1001 15:24:55.425335 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11120351256
I1001 15:24:55.431030 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11120359448
I1001 15:24:55.436131 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11120367640
I1001 15:24:55.441371 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11120375832
I1001 15:24:55.446507 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11120384024
I1001 15:24:55.451750 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11120416792
I1001 15:24:55.456864 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11120449560
I1001 15:24:55.462064 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11187558424
I1001 15:24:55.467222 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11254667288
I1001 15:24:55.472450 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11254675480
I1001 15:24:55.477640 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11254683672
I1001 15:24:55.482776 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11321792536
I1001 15:24:55.487996 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11388901400
I1001 15:24:55.493115 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11388909592
I1001 15:24:55.498322 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11388917784
I1001 15:24:55.503468 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11388925976
I1001 15:24:55.508675 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11388934168
I1001 15:24:55.512609 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11388934680
I1001 15:24:55.516463 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11388935192
I1001 15:24:55.521608 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11405712408
I1001 15:24:55.526854 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11422489624
I1001 15:24:55.531974 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11422497816
I1001 15:24:55.537201 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11422506008
I1001 15:24:55.542339 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11439283224
I1001 15:24:55.548080 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11456060440
I1001 15:24:55.553197 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11456068632
I1001 15:24:55.558422 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11456076824
I1001 15:24:55.563567 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11472854040
I1001 15:24:55.568859 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11489631256
I1001 15:24:55.574050 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11489639448
I1001 15:24:55.579297 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11489647640
I1001 15:24:55.584522 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11506424856
I1001 15:24:55.589662 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11523202072
I1001 15:24:55.594917 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11523210264
I1001 15:24:55.600056 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11523218456
I1001 15:24:55.605283 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11523226648
I1001 15:24:55.610412 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11523234840
I1001 15:24:55.615651 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11523243032
I1001 15:24:55.620761 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11523251224
I1001 15:24:55.626089 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11523283992
I1001 15:24:55.631230 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11523316760
I1001 15:24:55.636427 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11590425624
I1001 15:24:55.641525 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11657534488
I1001 15:24:55.646830 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11657542680
I1001 15:24:55.652083 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11657550872
I1001 15:24:55.657185 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11724659736
I1001 15:24:55.662933 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11791768600
I1001 15:24:55.668058 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11791776792
I1001 15:24:55.673285 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11791784984
I1001 15:24:55.678399 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11791793176
I1001 15:24:55.683643 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11791801368
I1001 15:24:55.687587 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11791801880
I1001 15:24:55.691432 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11791802392
I1001 15:24:55.696571 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11808579608
I1001 15:24:55.701843 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11825356824
I1001 15:24:55.706988 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11825365016
I1001 15:24:55.712210 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11825373208
I1001 15:24:55.717321 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11842150424
I1001 15:24:55.722564 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11858927640
I1001 15:24:55.727744 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11858935832
I1001 15:24:55.732974 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11858944024
I1001 15:24:55.738093 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11875721240
I1001 15:24:55.743349 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11892498456
I1001 15:24:55.748482 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11892506648
I1001 15:24:55.753729 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11892514840
I1001 15:24:55.759000 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11909292056
I1001 15:24:55.764140 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11926069272
I1001 15:24:55.769460 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11926077464
I1001 15:24:55.774612 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11926085656
I1001 15:24:55.780278 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11926093848
I1001 15:24:55.785397 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11926102040
I1001 15:24:55.790647 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11926110232
I1001 15:24:55.795756 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11926118424
I1001 15:24:55.800985 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11926151192
I1001 15:24:55.806122 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11926183960
I1001 15:24:55.811378 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11993292824
I1001 15:24:55.816496 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12060401688
I1001 15:24:55.821711 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12060409880
I1001 15:24:55.826964 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12060418072
I1001 15:24:55.832083 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12127526936
I1001 15:24:55.837307 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12194635800
I1001 15:24:55.842435 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12194643992
I1001 15:24:55.847710 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12194652184
I1001 15:24:55.852806 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12194660376
I1001 15:24:55.858027 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12194668568
I1001 15:24:55.861959 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12194669080
I1001 15:24:55.865786 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12194669592
I1001 15:24:55.870951 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12211446808
I1001 15:24:55.876180 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12228224024
I1001 15:24:55.881321 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12228232216
I1001 15:24:55.886568 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12228240408
I1001 15:24:55.891700 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12245017624
I1001 15:24:55.897397 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12261794840
I1001 15:24:55.902504 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12261803032
I1001 15:24:55.907769 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12261811224
I1001 15:24:55.912895 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12278588440
I1001 15:24:55.918244 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12295365656
I1001 15:24:55.923417 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12295373848
I1001 15:24:55.928642 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12295382040
I1001 15:24:55.933862 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12312159256
I1001 15:24:55.939083 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12328936472
I1001 15:24:55.944296 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12328944664
I1001 15:24:55.949463 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12328952856
I1001 15:24:55.954711 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12328961048
I1001 15:24:55.959853 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12328969240
I1001 15:24:55.965153 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12328977432
I1001 15:24:55.970293 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12328985624
I1001 15:24:55.975558 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12329018392
I1001 15:24:55.980669 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12329051160
I1001 15:24:55.985893 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12396160024
I1001 15:24:55.991081 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12463268888
I1001 15:24:55.996320 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12463277080
I1001 15:24:56.001558 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12463285272
I1001 15:24:56.006735 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12530394136
I1001 15:24:56.012434 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12597503000
I1001 15:24:56.017545 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12597511192
I1001 15:24:56.022813 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12597519384
I1001 15:24:56.027948 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12597527576
I1001 15:24:56.033204 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12597535768
I1001 15:24:56.037152 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12597536280
I1001 15:24:56.040984 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12597536792
I1001 15:24:56.046161 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12614314008
I1001 15:24:56.051451 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12631091224
I1001 15:24:56.056570 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12631099416
I1001 15:24:56.061811 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12631107608
I1001 15:24:56.066955 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12647884824
I1001 15:24:56.072202 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12664662040
I1001 15:24:56.077343 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12664670232
I1001 15:24:56.082628 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12664678424
I1001 15:24:56.087781 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12681455640
I1001 15:24:56.093048 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12698232856
I1001 15:24:56.098287 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12698241048
I1001 15:24:56.103585 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12698249240
I1001 15:24:56.108859 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12715026456
I1001 15:24:56.114026 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12731803672
I1001 15:24:56.119396 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12731811864
I1001 15:24:56.124523 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12731820056
I1001 15:24:56.130278 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12731828248
I1001 15:24:56.135485 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12731836440
I1001 15:24:56.140727 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12731844632
I1001 15:24:56.145851 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12731852824
I1001 15:24:56.151153 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12731885592
I1001 15:24:56.156294 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12731918360
I1001 15:24:56.161538 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12799027224
I1001 15:24:56.166791 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12866136088
I1001 15:24:56.172048 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12866144280
I1001 15:24:56.177329 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12866152472
I1001 15:24:56.182478 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12933261336
I1001 15:24:56.187758 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13000370200
I1001 15:24:56.192983 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13000378392
I1001 15:24:56.198221 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13000386584
I1001 15:24:56.203511 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13000394776
I1001 15:24:56.208744 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13000402968
I1001 15:24:56.212709 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13000403480
I1001 15:24:56.216556 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13000403992
I1001 15:24:56.221733 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13017181208
I1001 15:24:56.227031 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13033958424
I1001 15:24:56.232172 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13033966616
I1001 15:24:56.237413 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13033974808
I1001 15:24:56.242600 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13050752024
I1001 15:24:56.248358 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13067529240
I1001 15:24:56.253516 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13067537432
I1001 15:24:56.258777 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13067545624
I1001 15:24:56.263942 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13084322840
I1001 15:24:56.269183 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13101100056
I1001 15:24:56.274351 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13101108248
I1001 15:24:56.279621 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13101116440
I1001 15:24:56.284898 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13117893656
I1001 15:24:56.290063 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13134670872
I1001 15:24:56.295326 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13134679064
I1001 15:24:56.300486 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13134687256
I1001 15:24:56.305733 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13134695448
I1001 15:24:56.310923 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13134703640
I1001 15:24:56.316184 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13134711832
I1001 15:24:56.321349 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13134720024
I1001 15:24:56.326619 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13134752792
I1001 15:24:56.331825 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13134785560
I1001 15:24:56.337031 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13201894424
I1001 15:24:56.342185 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13269003288
I1001 15:24:56.347448 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13269011480
I1001 15:24:56.352725 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13269019672
I1001 15:24:56.357875 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13336128536
I1001 15:24:56.363607 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13403237400
I1001 15:24:56.368835 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13403245592
I1001 15:24:56.374066 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13403253784
I1001 15:24:56.379246 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13403261976
I1001 15:24:56.384613 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13403270168
I1001 15:24:56.388614 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13403270680
I1001 15:24:56.392499 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13403271192
I1001 15:24:56.397693 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13420048408
I1001 15:24:56.403016 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13436825624
I1001 15:24:56.408170 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13436833816
I1001 15:24:56.413450 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13436842008
I1001 15:24:56.418684 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13453619224
I1001 15:24:56.423969 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13470396440
I1001 15:24:56.429134 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13470404632
I1001 15:24:56.434396 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13470412824
I1001 15:24:56.439582 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13487190040
I1001 15:24:56.444854 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13503967256
I1001 15:24:56.450010 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13503975448
I1001 15:24:56.455322 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13503983640
I1001 15:24:56.460562 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13520760856
I1001 15:24:56.465850 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13537538072
I1001 15:24:56.471138 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13537546264
I1001 15:24:56.476290 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13537554456
I1001 15:24:56.482021 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13537562648
I1001 15:24:56.487225 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13537570840
I1001 15:24:56.492521 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13537579032
I1001 15:24:56.497662 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13537587224
I1001 15:24:56.502961 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13537619992
I1001 15:24:56.508122 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13537652760
I1001 15:24:56.513384 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13604761624
I1001 15:24:56.518563 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13671870488
I1001 15:24:56.523846 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13671878680
I1001 15:24:56.529091 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13671886872
I1001 15:24:56.534229 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13738995736
I1001 15:24:56.539531 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13806104600
I1001 15:24:56.544686 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13806112792
I1001 15:24:56.549922 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13806120984
I1001 15:24:56.555122 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13806129176
I1001 15:24:56.560392 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13806137368
I1001 15:24:56.564367 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13806137880
I1001 15:24:56.568306 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13806138392
I1001 15:24:56.573542 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13822915608
I1001 15:24:56.578827 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13839692824
I1001 15:24:56.583997 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13839701016
I1001 15:24:56.589264 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13839709208
I1001 15:24:56.594434 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13856486424
I1001 15:24:56.600257 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13873263640
I1001 15:24:56.605416 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13873271832
I1001 15:24:56.610722 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13873280024
I1001 15:24:56.615869 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13890057240
I1001 15:24:56.621123 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13906834456
I1001 15:24:56.626299 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13906842648
I1001 15:24:56.631595 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13906850840
I1001 15:24:56.636838 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13923628056
I1001 15:24:56.642024 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13940405272
I1001 15:24:56.647314 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13940413464
I1001 15:24:56.652494 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13940421656
I1001 15:24:56.657757 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13940429848
I1001 15:24:56.662949 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13940438040
I1001 15:24:56.668228 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13940446232
I1001 15:24:56.673384 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13940454424
I1001 15:24:56.678677 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13940487192
I1001 15:24:56.683849 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13940519960
I1001 15:24:56.689097 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14007628824
I1001 15:24:56.694252 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14074737688
I1001 15:24:56.699550 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14074745880
I1001 15:24:56.704801 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14074754072
I1001 15:24:56.709954 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14141862936
I1001 15:24:56.715738 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14208971800
I1001 15:24:56.720886 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14208979992
I1001 15:24:56.726158 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14208988184
I1001 15:24:56.731366 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14208996376
I1001 15:24:56.736627 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14209004568
I1001 15:24:56.740619 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14209005080
I1001 15:24:56.744498 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14209005592
I1001 15:24:56.749680 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14225782808
I1001 15:24:56.755035 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14242560024
I1001 15:24:56.760219 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14242568216
I1001 15:24:56.765568 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14242576408
I1001 15:24:56.770787 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14259353624
I1001 15:24:56.776103 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14276130840
I1001 15:24:56.781284 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14276139032
I1001 15:24:56.786528 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14276147224
I1001 15:24:56.791739 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14292924440
I1001 15:24:56.797008 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14309701656
I1001 15:24:56.802194 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14309709848
I1001 15:24:56.807480 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14309718040
I1001 15:24:56.812788 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14326495256
I1001 15:24:56.817975 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14343272472
I1001 15:24:56.823268 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14343280664
I1001 15:24:56.828435 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14343288856
I1001 15:24:56.834267 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14343297048
I1001 15:24:56.839502 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14343305240
I1001 15:24:56.844767 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14343313432
I1001 15:24:56.849941 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14343321624
I1001 15:24:56.855265 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14343354392
I1001 15:24:56.860439 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14343387160
I1001 15:24:56.865711 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14410496024
I1001 15:24:56.870930 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14477604888
I1001 15:24:56.876207 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14477613080
I1001 15:24:56.881489 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14477621272
I1001 15:24:56.886682 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14544730136
I1001 15:24:56.891977 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14611839000
I1001 15:24:56.897151 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14611847192
I1001 15:24:56.902431 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14611855384
I1001 15:24:56.907638 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14611863576
I1001 15:24:56.912905 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14611871768
I1001 15:24:56.917011 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14611872280
I1001 15:24:56.920914 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14611872792
I1001 15:24:56.926093 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14628650008
I1001 15:24:56.931422 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14645427224
I1001 15:24:56.936607 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14645435416
I1001 15:24:56.941923 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14645443608
I1001 15:24:56.947246 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14662220824
I1001 15:24:56.953318 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14678998040
I1001 15:24:56.958758 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14679006232
I1001 15:24:56.964158 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14679014424
I1001 15:24:56.969482 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14695791640
I1001 15:24:56.974767 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14712568856
I1001 15:24:56.979973 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14712577048
I1001 15:24:56.985242 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14712585240
I1001 15:24:56.990511 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14729362456
I1001 15:24:56.995755 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14746139672
I1001 15:24:57.001021 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14746147864
I1001 15:24:57.006211 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14746156056
I1001 15:24:57.011521 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14746164248
I1001 15:24:57.016729 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14746172440
I1001 15:24:57.022065 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14746180632
I1001 15:24:57.027546 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14746188824
I1001 15:24:57.033060 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14746221592
I1001 15:24:57.038377 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14746254360
I1001 15:24:57.043676 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14813363224
I1001 15:24:57.048895 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14880472088
I1001 15:24:57.054174 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14880480280
I1001 15:24:57.059473 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14880488472
I1001 15:24:57.064652 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14947597336
I1001 15:24:57.070432 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15014706200
I1001 15:24:57.075695 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15014714392
I1001 15:24:57.080982 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15014722584
I1001 15:24:57.086126 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15014730776
I1001 15:24:57.091443 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15014738968
I1001 15:24:57.095436 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15014739480
I1001 15:24:57.099367 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15014739992
I1001 15:24:57.104946 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15031517208
I1001 15:24:57.110559 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15048294424
I1001 15:24:57.115766 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15048302616
I1001 15:24:57.121051 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15048310808
I1001 15:24:57.126252 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15065088024
I1001 15:24:57.131599 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15081865240
I1001 15:24:57.136786 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15081873432
I1001 15:24:57.142097 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15081881624
I1001 15:24:57.147296 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15098658840
I1001 15:24:57.152569 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15115436056
I1001 15:24:57.157747 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15115444248
I1001 15:24:57.163091 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15115452440
I1001 15:24:57.168460 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15132229656
I1001 15:24:57.173698 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15149006872
I1001 15:24:57.179318 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15149015064
I1001 15:24:57.184906 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15149023256
I1001 15:24:57.190909 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15149031448
I1001 15:24:57.196171 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15149039640
I1001 15:24:57.201489 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15149047832
I1001 15:24:57.206721 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15149056024
I1001 15:24:57.212091 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15149088792
I1001 15:24:57.217295 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15149121560
I1001 15:24:57.222576 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15216230424
I1001 15:24:57.227770 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15283339288
I1001 15:24:57.233077 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15283347480
I1001 15:24:57.238366 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15283355672
I1001 15:24:57.243642 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15350464536
I1001 15:24:57.249402 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15417573400
I1001 15:24:57.254946 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15417581592
I1001 15:24:57.260299 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15417589784
I1001 15:24:57.265513 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15417597976
I1001 15:24:57.270845 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15417606168
I1001 15:24:57.274857 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15417606680
I1001 15:24:57.278755 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15417607192
I1001 15:24:57.283960 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15434384408
I1001 15:24:57.289268 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15451161624
I1001 15:24:57.294445 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15451169816
I1001 15:24:57.299749 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15451178008
I1001 15:24:57.304941 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15467955224
I1001 15:24:57.310780 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15484732440
I1001 15:24:57.316334 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15484740632
I1001 15:24:57.321960 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15484748824
I1001 15:24:57.327382 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15501526040
I1001 15:24:57.332688 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15518303256
I1001 15:24:57.337875 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15518311448
I1001 15:24:57.343193 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15518319640
I1001 15:24:57.348500 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15535096856
I1001 15:24:57.353733 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15551874072
I1001 15:24:57.359027 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15551882264
I1001 15:24:57.364215 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15551890456
I1001 15:24:57.369556 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15551898648
I1001 15:24:57.374799 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15551906840
I1001 15:24:57.380094 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15551915032
I1001 15:24:57.385359 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15551923224
I1001 15:24:57.391055 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15551955992
I1001 15:24:57.396562 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15551988760
I1001 15:24:57.401894 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15619097624
I1001 15:24:57.407120 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15686206488
I1001 15:24:57.412402 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15686214680
I1001 15:24:57.417728 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15686222872
I1001 15:24:57.422956 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15753331736
I1001 15:24:57.428762 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15820440600
I1001 15:24:57.433980 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15820448792
I1001 15:24:57.439338 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15820456984
I1001 15:24:57.444552 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15820465176
I1001 15:24:57.449859 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15820473368
I1001 15:24:57.453910 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15820473880
I1001 15:24:57.457878 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15820474392
I1001 15:24:57.463284 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15837251608
I1001 15:24:57.468904 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15854028824
I1001 15:24:57.474364 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15854037016
I1001 15:24:57.479945 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15854045208
I1001 15:24:57.485236 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15870822424
I1001 15:24:57.490586 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15887599640
I1001 15:24:57.495827 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15887607832
I1001 15:24:57.501122 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15887616024
I1001 15:24:57.506313 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15904393240
I1001 15:24:57.511630 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15921170456
I1001 15:24:57.516831 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15921178648
I1001 15:24:57.522135 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15921186840
I1001 15:24:57.527447 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15937964056
I1001 15:24:57.532758 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15954741272
I1001 15:24:57.538038 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15954749464
I1001 15:24:57.543265 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15954757656
I1001 15:24:57.549104 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15954765848
I1001 15:24:57.554373 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15954774040
I1001 15:24:57.559758 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15954782232
I1001 15:24:57.564999 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15954790424
I1001 15:24:57.570396 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15954823192
I1001 15:24:57.575640 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15954855960
I1001 15:24:57.580922 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16021964824
I1001 15:24:57.586200 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16089073688
I1001 15:24:57.591541 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16089081880
I1001 15:24:57.596843 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16089090072
I1001 15:24:57.602042 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16156198936
I1001 15:24:57.607376 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16223307800
I1001 15:24:57.612583 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16223315992
I1001 15:24:57.617928 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16223324184
I1001 15:24:57.623157 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16223332376
I1001 15:24:57.628496 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16223340568
I1001 15:24:57.632530 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16223341080
I1001 15:24:57.636435 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16223341592
I1001 15:24:57.641657 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16240118808
I1001 15:24:57.647115 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16256896024
I1001 15:24:57.652339 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16256904216
I1001 15:24:57.657724 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16256912408
I1001 15:24:57.662970 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16273689624
I1001 15:24:57.668839 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16290466840
I1001 15:24:57.674041 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16290475032
I1001 15:24:57.679399 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16290483224
I1001 15:24:57.684620 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16307260440
I1001 15:24:57.689958 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16324037656
I1001 15:24:57.695271 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16324045848
I1001 15:24:57.700587 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16324054040
I1001 15:24:57.705916 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16340831256
I1001 15:24:57.711165 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16357608472
I1001 15:24:57.716558 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16357616664
I1001 15:24:57.721795 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16357624856
I1001 15:24:57.727143 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16357633048
I1001 15:24:57.732380 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16357641240
I1001 15:24:57.737690 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16357649432
I1001 15:24:57.742922 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16357657624
I1001 15:24:57.748229 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16357690392
I1001 15:24:57.753470 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16357723160
I1001 15:24:57.758820 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16424832024
I1001 15:24:57.764043 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16491940888
I1001 15:24:57.769509 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16491949080
I1001 15:24:57.774946 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16491957272
I1001 15:24:57.780178 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16559066136
I1001 15:24:57.786039 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16626175000
I1001 15:24:57.791273 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16626183192
I1001 15:24:57.796602 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16626191384
I1001 15:24:57.801877 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16626199576
I1001 15:24:57.807210 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16626207768
I1001 15:24:57.811231 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16626208280
I1001 15:24:57.815139 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16626208792
I1001 15:24:57.820380 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16642986008
I1001 15:24:57.825717 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16659763224
I1001 15:24:57.830974 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16659771416
I1001 15:24:57.836434 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16659779608
I1001 15:24:57.841713 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16676556824
I1001 15:24:57.847084 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16693334040
I1001 15:24:57.852326 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16693342232
I1001 15:24:57.857638 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16693350424
I1001 15:24:57.862862 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16710127640
I1001 15:24:57.868211 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16726904856
I1001 15:24:57.873404 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16726913048
I1001 15:24:57.878750 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16726921240
I1001 15:24:57.884043 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16743698456
I1001 15:24:57.889304 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16760475672
I1001 15:24:57.894649 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16760483864
I1001 15:24:57.899934 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16760492056
I1001 15:24:57.905876 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16760500248
I1001 15:24:57.911176 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16760508440
I1001 15:24:57.916608 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16760516632
I1001 15:24:57.921859 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16760524824
I1001 15:24:57.927205 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16760557592
I1001 15:24:57.932468 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16760590360
I1001 15:24:57.937769 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16827699224
I1001 15:24:57.943097 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16894808088
I1001 15:24:57.948390 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16894816280
I1001 15:24:57.953716 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16894824472
I1001 15:24:57.959037 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16961933336
I1001 15:24:57.964389 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17029042200
I1001 15:24:57.969700 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17029050392
I1001 15:24:57.975045 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17029058584
I1001 15:24:57.980281 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17029066776
I1001 15:24:57.985635 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17029074968
I1001 15:24:57.989656 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17029075480
I1001 15:24:57.993614 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17029075992
I1001 15:24:57.998873 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17045853208
I1001 15:24:58.004195 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17062630424
I1001 15:24:58.009396 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17062638616
I1001 15:24:58.014787 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17062646808
I1001 15:24:58.020039 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17079424024
I1001 15:24:58.025883 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17096201240
I1001 15:24:58.031174 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17096209432
I1001 15:24:58.036630 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17096217624
I1001 15:24:58.041956 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17112994840
I1001 15:24:58.047349 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17129772056
I1001 15:24:58.052567 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17129780248
I1001 15:24:58.057885 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17129788440
I1001 15:24:58.063225 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17146565656
I1001 15:24:58.068488 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17163342872
I1001 15:24:58.073820 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17163351064
I1001 15:24:58.079053 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17163359256
I1001 15:24:58.084384 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17163367448
I1001 15:24:58.089640 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17163375640
I1001 15:24:58.095009 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17163383832
I1001 15:24:58.100210 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17163392024
I1001 15:24:58.105649 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17163424792
I1001 15:24:58.111021 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17163457560
I1001 15:24:58.116428 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17230566424
I1001 15:24:58.121767 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17297675288
I1001 15:24:58.127131 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17297683480
I1001 15:24:58.132453 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17297691672
I1001 15:24:58.137677 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17364800536
I1001 15:24:58.143539 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17431909400
I1001 15:24:58.148808 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17431917592
I1001 15:24:58.154138 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17431925784
I1001 15:24:58.159402 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17431933976
I1001 15:24:58.164783 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17431942168
I1001 15:24:58.168946 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17431942680
I1001 15:24:58.172899 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17431943192
I1001 15:24:58.178190 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17448720408
I1001 15:24:58.183551 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17465497624
I1001 15:24:58.188767 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17465505816
I1001 15:24:58.194238 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17465514008
I1001 15:24:58.199494 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17482291224
I1001 15:24:58.204843 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17499068440
I1001 15:24:58.210053 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17499076632
I1001 15:24:58.215403 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17499084824
I1001 15:24:58.220657 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17515862040
I1001 15:24:58.225997 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17532639256
I1001 15:24:58.231304 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17532647448
I1001 15:24:58.236629 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17532655640
I1001 15:24:58.241969 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17549432856
I1001 15:24:58.247258 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17566210072
I1001 15:24:58.252643 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17566218264
I1001 15:24:58.257875 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17566226456
I1001 15:24:58.263728 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17566234648
I1001 15:24:58.268984 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17566242840
I1001 15:24:58.274316 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17566251032
I1001 15:24:58.279561 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17566259224
I1001 15:24:58.284886 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17566291992
I1001 15:24:58.290133 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17566324760
I1001 15:24:58.295468 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17633433624
I1001 15:24:58.300718 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17700542488
I1001 15:24:58.306014 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17700550680
I1001 15:24:58.311405 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17700558872
I1001 15:24:58.316641 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17767667736
I1001 15:24:58.321986 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17834776600
I1001 15:24:58.327225 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17834784792
I1001 15:24:58.332576 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17834792984
I1001 15:24:58.337787 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17834801176
I1001 15:24:58.343163 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17834809368
I1001 15:24:58.347206 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17834809880
I1001 15:24:58.351170 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17834810392
I1001 15:24:58.356387 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17851587608
I1001 15:24:58.361735 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17868364824
I1001 15:24:58.367043 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17868373016
I1001 15:24:58.372378 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17868381208
I1001 15:24:58.377615 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17885158424
I1001 15:24:58.383570 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17901935640
I1001 15:24:58.388807 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17901943832
I1001 15:24:58.394118 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17901952024
I1001 15:24:58.399378 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17918729240
I1001 15:24:58.404744 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17935506456
I1001 15:24:58.409973 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17935514648
I1001 15:24:58.415385 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17935522840
I1001 15:24:58.420696 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17952300056
I1001 15:24:58.425945 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17969077272
I1001 15:24:58.431306 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17969085464
I1001 15:24:58.436542 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17969093656
I1001 15:24:58.441979 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17969101848
I1001 15:24:58.447263 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17969110040
I1001 15:24:58.452605 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17969118232
I1001 15:24:58.457827 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17969126424
I1001 15:24:58.463209 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17969159192
I1001 15:24:58.468455 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17969191960
I1001 15:24:58.473770 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18036300824
I1001 15:24:58.479055 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18103409688
I1001 15:24:58.484393 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18103417880
I1001 15:24:58.489691 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18103426072
I1001 15:24:58.494980 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18170534936
I1001 15:24:58.500786 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18237643800
I1001 15:24:58.506016 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18237651992
I1001 15:24:58.511410 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18237660184
I1001 15:24:58.516648 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18237668376
I1001 15:24:58.522001 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18237676568
I1001 15:24:58.526055 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18237677080
I1001 15:24:58.530015 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18237677592
I1001 15:24:58.535280 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18254454808
I1001 15:24:58.540631 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18271232024
I1001 15:24:58.545856 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18271240216
I1001 15:24:58.551233 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18271248408
I1001 15:24:58.556484 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18288025624
I1001 15:24:58.561825 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18304802840
I1001 15:24:58.567187 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18304811032
I1001 15:24:58.572508 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18304819224
I1001 15:24:58.577760 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18321596440
I1001 15:24:58.583142 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18338373656
I1001 15:24:58.588377 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18338381848
I1001 15:24:58.593728 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18338390040
I1001 15:24:58.599063 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18355167256
I1001 15:24:58.604385 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18371944472
I1001 15:24:58.609746 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18371952664
I1001 15:24:58.615051 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18371960856
I1001 15:24:58.620857 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18371969048
I1001 15:24:58.626112 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18371977240
I1001 15:24:58.631507 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18371985432
I1001 15:24:58.636762 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18371993624
I1001 15:24:58.642106 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18372026392
I1001 15:24:58.647415 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18372059160
I1001 15:24:58.652751 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18439168024
I1001 15:24:58.658010 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18506276888
I1001 15:24:58.663402 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18506285080
I1001 15:24:58.668730 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18506293272
I1001 15:24:58.673958 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18573402136
I1001 15:24:58.679365 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18640511000
I1001 15:24:58.684607 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18640519192
I1001 15:24:58.689961 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18640527384
I1001 15:24:58.695228 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18640535576
I1001 15:24:58.700566 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18640543768
I1001 15:24:58.704628 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18640544280
I1001 15:24:58.708618 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18640544792
I1001 15:24:58.713954 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18657322008
I1001 15:24:58.719336 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18674099224
I1001 15:24:58.724564 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18674107416
I1001 15:24:58.729901 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18674115608
I1001 15:24:58.735165 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18690892824
I1001 15:24:58.741086 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18707670040
I1001 15:24:58.746339 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18707678232
I1001 15:24:58.751695 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18707686424
I1001 15:24:58.756955 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18724463640
I1001 15:24:58.762303 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18741240856
I1001 15:24:58.767686 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18741249048
I1001 15:24:58.773027 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18741257240
I1001 15:24:58.778371 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18758034456
I1001 15:24:58.783675 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18774811672
I1001 15:24:58.789037 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18774819864
I1001 15:24:58.794284 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18774828056
I1001 15:24:58.799644 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18774836248
I1001 15:24:58.804885 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18774844440
I1001 15:24:58.810217 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18774852632
I1001 15:24:58.815518 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18774860824
I1001 15:24:58.820864 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18774893592
I1001 15:24:58.826115 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18774926360
I1001 15:24:58.831506 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18842035224
I1001 15:24:58.836746 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18909144088
I1001 15:24:58.842074 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18909152280
I1001 15:24:58.847512 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18909160472
I1001 15:24:58.852789 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18976269336
I1001 15:24:58.858596 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19043378200
I1001 15:24:58.863836 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19043386392
I1001 15:24:58.869189 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19043394584
I1001 15:24:58.874463 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19043402776
I1001 15:24:58.879841 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19043410968
I1001 15:24:58.883927 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19043411480
I1001 15:24:58.887917 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19043411992
I1001 15:24:58.893177 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19060189208
I1001 15:24:58.898532 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19076966424
I1001 15:24:58.903809 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19076974616
I1001 15:24:58.909153 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19076982808
I1001 15:24:58.914405 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19093760024
I1001 15:24:58.919973 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19110537240
I1001 15:24:58.925250 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19110545432
I1001 15:24:58.930622 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19110553624
I1001 15:24:58.935883 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19127330840
I1001 15:24:58.941215 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19144108056
I1001 15:24:58.946463 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19144116248
I1001 15:24:58.951845 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19144124440
I1001 15:24:58.957193 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19160901656
I1001 15:24:58.962479 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19177678872
I1001 15:24:58.967915 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19177687064
I1001 15:24:58.973180 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19177695256
I1001 15:24:58.979096 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19177703448
I1001 15:24:58.984402 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19177711640
I1001 15:24:58.989763 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19177719832
I1001 15:24:58.995059 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19177728024
I1001 15:24:59.000394 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19177760792
I1001 15:24:59.005672 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19177793560
I1001 15:24:59.011029 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19244902424
I1001 15:24:59.016315 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19312011288
I1001 15:24:59.021669 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19312019480
I1001 15:24:59.027055 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19312027672
I1001 15:24:59.032311 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19379136536
I1001 15:24:59.037723 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19446245400
I1001 15:24:59.042992 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19446253592
I1001 15:24:59.048359 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19446261784
I1001 15:24:59.053601 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19446269976
I1001 15:24:59.058984 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19446278168
I1001 15:24:59.063080 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19446278680
I1001 15:24:59.067068 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19446279192
I1001 15:24:59.072335 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19463056408
I1001 15:24:59.077701 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19479833624
I1001 15:24:59.082967 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19479841816
I1001 15:24:59.088334 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19479850008
I1001 15:24:59.093618 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19496627224
I1001 15:24:59.099452 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19513404440
I1001 15:24:59.104700 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19513412632
I1001 15:24:59.110088 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19513420824
I1001 15:24:59.115386 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19530198040
I1001 15:24:59.120730 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19546975256
I1001 15:24:59.125975 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19546983448
I1001 15:24:59.131387 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19546991640
I1001 15:24:59.136737 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19563768856
I1001 15:24:59.142068 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19580546072
I1001 15:24:59.147453 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19580554264
I1001 15:24:59.152706 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19580562456
I1001 15:24:59.158067 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19580570648
I1001 15:24:59.163368 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19580578840
I1001 15:24:59.168790 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19580587032
I1001 15:24:59.174061 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19580595224
I1001 15:24:59.179467 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19580627992
I1001 15:24:59.184780 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19580660760
I1001 15:24:59.190137 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19647769624
I1001 15:24:59.195516 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19714878488
I1001 15:24:59.200887 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19714886680
I1001 15:24:59.206236 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19714894872
I1001 15:24:59.211546 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19782003736
I1001 15:24:59.217374 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19849112600
I1001 15:24:59.222688 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19849120792
I1001 15:24:59.228083 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19849128984
I1001 15:24:59.233345 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19849137176
I1001 15:24:59.238737 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19849145368
I1001 15:24:59.242819 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19849145880
I1001 15:24:59.246823 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19849146392
I1001 15:24:59.252144 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19865923608
I1001 15:24:59.257522 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19882700824
I1001 15:24:59.262835 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19882709016
I1001 15:24:59.268193 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19882717208
I1001 15:24:59.273471 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19899494424
I1001 15:24:59.278872 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19916271640
I1001 15:24:59.284167 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19916279832
I1001 15:24:59.289522 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19916288024
I1001 15:24:59.294832 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19933065240
I1001 15:24:59.300203 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19949842456
I1001 15:24:59.305545 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19949850648
I1001 15:24:59.310949 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19949858840
I1001 15:24:59.316308 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19966636056
I1001 15:24:59.321590 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19983413272
I1001 15:24:59.326990 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19983421464
I1001 15:24:59.332255 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19983429656
I1001 15:24:59.338098 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19983437848
I1001 15:24:59.343389 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19983446040
I1001 15:24:59.348777 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19983454232
I1001 15:24:59.354038 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19983462424
I1001 15:24:59.359490 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19983495192
I1001 15:24:59.364776 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19983527960
I1001 15:24:59.370206 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20050636824
I1001 15:24:59.375496 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20117745688
I1001 15:24:59.380856 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20117753880
I1001 15:24:59.386212 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20117762072
I1001 15:24:59.391535 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20184870936
I1001 15:24:59.396933 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20251979800
I1001 15:24:59.402200 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20251987992
I1001 15:24:59.407617 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20251996184
I1001 15:24:59.412878 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20252004376
I1001 15:24:59.418325 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20252012568
I1001 15:24:59.422390 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20252013080
I1001 15:24:59.426414 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20252013592
I1001 15:24:59.431756 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20268790808
I1001 15:24:59.437137 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20285568024
I1001 15:24:59.442409 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20285576216
I1001 15:24:59.447810 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20285584408
I1001 15:24:59.453098 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20302361624
I1001 15:24:59.458951 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20319138840
I1001 15:24:59.464229 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20319147032
I1001 15:24:59.469617 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20319155224
I1001 15:24:59.474913 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20335932440
I1001 15:24:59.480282 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20352709656
I1001 15:24:59.485569 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20352717848
I1001 15:24:59.490991 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20352726040
I1001 15:24:59.496350 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20369503256
I1001 15:24:59.501645 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386280472
I1001 15:24:59.507051 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386288664
I1001 15:24:59.512308 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386296856
I1001 15:24:59.517700 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386305048
I1001 15:24:59.523027 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386313240
I1001 15:24:59.528417 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386321432
I1001 15:24:59.533704 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386329624
I1001 15:24:59.539111 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386337624
I1001 15:24:59.544408 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386345624
I1001 15:24:59.549773 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386353624
I1001 15:24:59.555076 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386361624
I1001 15:24:59.560453 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386369624
I1001 15:24:59.565845 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386377624
I1001 15:24:59.571227 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386385624
I1001 15:24:59.577101 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386393624
I1001 15:24:59.582371 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386401624
I1001 15:24:59.587814 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386409624
I1001 15:24:59.593095 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386417624
I1001 15:24:59.598456 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386425624
I1001 15:24:59.603824 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386433624
I1001 15:24:59.609185 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386441624
I1001 15:24:59.614459 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386449624
I1001 15:24:59.619842 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386457624
I1001 15:24:59.625109 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386465624
I1001 15:24:59.630579 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386473624
I1001 15:24:59.635977 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386481624
I1001 15:24:59.641260 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386489624
I1001 15:24:59.646651 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386497624
I1001 15:24:59.651937 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386505624
I1001 15:24:59.657320 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386513624
I1001 15:24:59.662602 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386521624
I1001 15:24:59.667990 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386529624
I1001 15:24:59.673261 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386537624
I1001 15:24:59.678682 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386545624
I1001 15:24:59.683952 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386553624
I1001 15:24:59.689325 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386561624
I1001 15:24:59.694626 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386569624
I1001 15:24:59.700473 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386577624
I1001 15:24:59.705856 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386585624
I1001 15:24:59.711168 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20402969624
I1001 15:24:59.716575 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20419353624
I1001 15:24:59.721850 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20435737624
I1001 15:24:59.727297 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20452121624
I1001 15:24:59.732570 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20468505624
I1001 15:24:59.737935 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20484889624
I1001 15:24:59.743303 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20501273624
I1001 15:24:59.748686 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20517657624
I1001 15:24:59.753966 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20534041624
I1001 15:24:59.759391 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20550425624
I1001 15:24:59.764785 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20566809624
I1001 15:24:59.770200 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20583193624
I1001 15:24:59.775642 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20599577624
I1001 15:24:59.780932 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20615961624
I1001 15:24:59.786331 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20632345624
I1001 15:24:59.791630 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20648729624
I1001 15:24:59.797013 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20665113624
I1001 15:24:59.802311 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20681497624
I1001 15:24:59.807734 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20697881624
I1001 15:24:59.813041 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20714265624
I1001 15:24:59.818933 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20730649624
I1001 15:24:59.824347 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20747033624
I1001 15:24:59.829641 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20763417624
I1001 15:24:59.835074 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20779801624
I1001 15:24:59.840360 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20796185624
I1001 15:24:59.845757 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20812569624
I1001 15:24:59.851090 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20828953624
I1001 15:24:59.856571 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20845337624
I1001 15:24:59.861842 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20861721624
I1001 15:24:59.867242 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20878105624
I1001 15:24:59.872525 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20894489624
I1001 15:24:59.877919 139818304546624 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20910873624
I1001 15:25:00.635889 139818304546624 cluster.py:515] Place variable total_nan_gradients/var on /job:local/replica:0/task:0/device:CPU:0 20910873632
I1001 15:25:00.638026 139818304546624 py_utils.py:1389] Creating var total_nan_gradients/var:0 shape=() on device /job:local/replica:0/task:0/device:CPU:0
I1001 15:25:00.839079 139818304546624 trainer.py:401] Trainer number of enqueue ops: 0
I1001 15:25:00.839255 139818304546624 trainer.py:410] AttributeError. Expected for single task models.
I1001 15:25:20.658163 139818304546624 trainer.py:1590] Starting runners
I1001 15:25:20.658562 139811663546112 base_runner.py:167] controller started.
I1001 15:25:20.658857 139811655153408 base_runner.py:167] trainer started.
I1001 15:25:20.659100 139818304546624 trainer.py:1609] Waiting for runners to finish...
I1001 15:25:39.691632 139811663546112 checkpointer.py:133] Uninitialized var list: [b'global_step', b'1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var', b'1bwds_wpm_level_lm/total_samples/var', b'beta1_power', b'beta2_power', b'1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var/Adam', b'1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var/Adam_1', b'total_nan_gradients/var'] 
I1001 15:25:39.692888 139811663546112 checkpointer.py:140] Initialize ALL variables: [b'global_step', b'1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var', b'1bwds_wpm_level_lm/total_samples/var', b'beta1_power', b'beta2_power', b'1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var/Adam', b'1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var/Adam_1', b'total_nan_gradients/var']
I1001 15:26:51.412964 139811655153408 base_runner.py:106] step:     0
I1001 15:26:57.583874 139811663546112 checkpointer.py:142] Initialize variables done.
I1001 15:27:04.001525 139811663546112 trainer.py:380] Steps/second: 0.000000, Examples/second: 0.000000
I1001 15:27:04.002082 139811663546112 checkpointer.py:111] Save checkpoint
2019-10-01 15:29:45.691369: I ./lingvo/core/ops/input_common.h:71] Create RecordProcessor
2019-10-01 15:29:45.692435: I lingvo/core/ops/input_common.cc:33] Input source weights are empty, fall back to legacy behavior.
2019-10-01 15:29:45.692752: I lingvo/core/ops/record_yielder.cc:324] 0x7f1ad499c830 Record yielder start
2019-10-01 15:29:45.692780: I ./lingvo/core/ops/input_common.h:76] Create batcher
2019-10-01 15:29:45.692802: I lingvo/core/ops/record_yielder.cc:383] Epoch 1 /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en*
2019-10-01 15:30:11.032271: W tensorflow/core/common_runtime/bfc_allocator.cc:419] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.0KiB (rounded to 8192).  Current allocation summary follows.
2019-10-01 15:30:11.032355: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (256): 	Total Chunks: 769, Chunks in use: 769. 192.2KiB allocated for chunks. 192.2KiB in use in bin. 3.3KiB client-requested in use in bin.
2019-10-01 15:30:11.032368: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (512): 	Total Chunks: 55, Chunks in use: 55. 27.5KiB allocated for chunks. 27.5KiB in use in bin. 27.5KiB client-requested in use in bin.
2019-10-01 15:30:11.032376: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (1024): 	Total Chunks: 1, Chunks in use: 1. 1.2KiB allocated for chunks. 1.2KiB in use in bin. 1.0KiB client-requested in use in bin.
2019-10-01 15:30:11.032383: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (2048): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-10-01 15:30:11.032390: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (4096): 	Total Chunks: 1, Chunks in use: 1. 4.0KiB allocated for chunks. 4.0KiB in use in bin. 4.0KiB client-requested in use in bin.
2019-10-01 15:30:11.032397: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (8192): 	Total Chunks: 517, Chunks in use: 517. 4.05MiB allocated for chunks. 4.05MiB in use in bin. 4.03MiB client-requested in use in bin.
2019-10-01 15:30:11.032404: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (16384): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-10-01 15:30:11.032410: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (32768): 	Total Chunks: 51, Chunks in use: 51. 1.59MiB allocated for chunks. 1.59MiB in use in bin. 1.59MiB client-requested in use in bin.
2019-10-01 15:30:11.032417: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (65536): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-10-01 15:30:11.032424: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (131072): 	Total Chunks: 4, Chunks in use: 4. 512.2KiB allocated for chunks. 512.2KiB in use in bin. 512.0KiB client-requested in use in bin.
2019-10-01 15:30:11.032437: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (262144): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-10-01 15:30:11.032443: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (524288): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-10-01 15:30:11.032449: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (1048576): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-10-01 15:30:11.032455: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (2097152): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-10-01 15:30:11.032461: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (4194304): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-10-01 15:30:11.032469: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (8388608): 	Total Chunks: 33, Chunks in use: 33. 508.00MiB allocated for chunks. 508.00MiB in use in bin. 508.00MiB client-requested in use in bin.
2019-10-01 15:30:11.032475: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (16777216): 	Total Chunks: 213, Chunks in use: 213. 3.33GiB allocated for chunks. 3.33GiB in use in bin. 3.33GiB client-requested in use in bin.
2019-10-01 15:30:11.032481: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (33554432): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-10-01 15:30:11.032487: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (67108864): 	Total Chunks: 103, Chunks in use: 103. 6.48GiB allocated for chunks. 6.48GiB in use in bin. 6.44GiB client-requested in use in bin.
2019-10-01 15:30:11.032494: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (134217728): 	Total Chunks: 1, Chunks in use: 1. 250.00MiB allocated for chunks. 250.00MiB in use in bin. 250.00MiB client-requested in use in bin.
2019-10-01 15:30:11.032500: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (268435456): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-10-01 15:30:11.032506: I tensorflow/core/common_runtime/bfc_allocator.cc:885] Bin for 8.0KiB was 8.0KiB, Chunk State: 
2019-10-01 15:30:11.032513: I tensorflow/core/common_runtime/bfc_allocator.cc:898] Next region of size 11330115840
2019-10-01 15:30:11.032523: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b60000 next 1 of size 1280
2019-10-01 15:30:11.032528: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b60500 next 2 of size 256
2019-10-01 15:30:11.032534: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b60600 next 3 of size 256
2019-10-01 15:30:11.032539: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b60700 next 4 of size 256
2019-10-01 15:30:11.032545: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b60800 next 5 of size 256
2019-10-01 15:30:11.032550: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b60900 next 6 of size 256
2019-10-01 15:30:11.032556: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b60a00 next 7 of size 256
2019-10-01 15:30:11.032561: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b60b00 next 8 of size 256
2019-10-01 15:30:11.032566: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b60c00 next 9 of size 256
2019-10-01 15:30:11.032571: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b60d00 next 10 of size 256
2019-10-01 15:30:11.032576: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b60e00 next 11 of size 256
2019-10-01 15:30:11.032581: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b60f00 next 12 of size 256
2019-10-01 15:30:11.032590: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b61000 next 13 of size 256
2019-10-01 15:30:11.032596: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b61100 next 14 of size 256
2019-10-01 15:30:11.032601: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b61200 next 15 of size 256
2019-10-01 15:30:11.032606: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b61300 next 16 of size 256
2019-10-01 15:30:11.032611: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b61400 next 17 of size 256
2019-10-01 15:30:11.032617: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b61500 next 18 of size 256
2019-10-01 15:30:11.032622: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b61600 next 19 of size 256
2019-10-01 15:30:11.032627: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b61700 next 20 of size 256
2019-10-01 15:30:11.032632: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b61800 next 21 of size 256
2019-10-01 15:30:11.032638: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b61900 next 22 of size 256
2019-10-01 15:30:11.032643: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b61a00 next 23 of size 256
2019-10-01 15:30:11.032648: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b61b00 next 24 of size 256
2019-10-01 15:30:11.032653: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b61c00 next 25 of size 256
2019-10-01 15:30:11.032659: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b61d00 next 26 of size 256
2019-10-01 15:30:11.032664: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b61e00 next 27 of size 256
2019-10-01 15:30:11.032669: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b61f00 next 28 of size 256
2019-10-01 15:30:11.032674: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b62000 next 29 of size 256
2019-10-01 15:30:11.032679: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b62100 next 30 of size 256
2019-10-01 15:30:11.032684: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b62200 next 31 of size 256
2019-10-01 15:30:11.032689: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b62300 next 32 of size 256
2019-10-01 15:30:11.032694: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b62400 next 33 of size 256
2019-10-01 15:30:11.032699: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b62500 next 34 of size 256
2019-10-01 15:30:11.032704: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b62600 next 35 of size 256
2019-10-01 15:30:11.032709: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b62700 next 36 of size 256
2019-10-01 15:30:11.032715: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b62800 next 37 of size 256
2019-10-01 15:30:11.032720: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b62900 next 38 of size 256
2019-10-01 15:30:11.032726: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b62a00 next 39 of size 256
2019-10-01 15:30:11.032731: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b62b00 next 40 of size 256
2019-10-01 15:30:11.032736: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b62c00 next 41 of size 256
2019-10-01 15:30:11.032741: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b62d00 next 42 of size 256
2019-10-01 15:30:11.032746: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b62e00 next 43 of size 256
2019-10-01 15:30:11.032752: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b62f00 next 44 of size 67108864
2019-10-01 15:30:11.032760: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1207b62f00 next 45 of size 8192
2019-10-01 15:30:11.032765: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1207b64f00 next 46 of size 8192
2019-10-01 15:30:11.032771: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1207b66f00 next 47 of size 16384000
2019-10-01 15:30:11.032777: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1208b06f00 next 48 of size 8192
2019-10-01 15:30:11.032782: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1208b08f00 next 49 of size 16777216
2019-10-01 15:30:11.032787: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1209b08f00 next 50 of size 512
2019-10-01 15:30:11.032793: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1209b09100 next 51 of size 8192
2019-10-01 15:30:11.032798: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1209b0b100 next 52 of size 8192
2019-10-01 15:30:11.032805: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1209b0d100 next 53 of size 32768
2019-10-01 15:30:11.032810: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1209b15100 next 54 of size 8192
2019-10-01 15:30:11.032815: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1209b17100 next 55 of size 8192
2019-10-01 15:30:11.032821: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1209b19100 next 56 of size 8192
2019-10-01 15:30:11.032826: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1209b1b100 next 57 of size 8192
2019-10-01 15:30:11.032831: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1209b1d100 next 58 of size 8192
2019-10-01 15:30:11.032836: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1209b1f100 next 59 of size 16777216
2019-10-01 15:30:11.032842: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x120ab1f100 next 60 of size 67108864
2019-10-01 15:30:11.032847: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x120eb1f100 next 61 of size 16777216
2019-10-01 15:30:11.032853: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x120fb1f100 next 62 of size 8192
2019-10-01 15:30:11.032858: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x120fb21100 next 63 of size 8192
2019-10-01 15:30:11.032863: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x120fb23100 next 64 of size 16777216
2019-10-01 15:30:11.032868: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1210b23100 next 65 of size 8192
2019-10-01 15:30:11.032874: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1210b25100 next 66 of size 8192
2019-10-01 15:30:11.032879: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1210b27100 next 67 of size 67108864
2019-10-01 15:30:11.032884: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1214b27100 next 68 of size 16777216
2019-10-01 15:30:11.032889: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1215b27100 next 69 of size 8192
2019-10-01 15:30:11.032894: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1215b29100 next 70 of size 8192
2019-10-01 15:30:11.032899: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1215b2b100 next 71 of size 8192
2019-10-01 15:30:11.032905: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1215b2d100 next 72 of size 8192
2019-10-01 15:30:11.032911: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1215b2f100 next 73 of size 16777216
2019-10-01 15:30:11.032916: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1216b2f100 next 74 of size 512
2019-10-01 15:30:11.032921: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1216b2f300 next 75 of size 512
2019-10-01 15:30:11.032926: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1216b2f500 next 76 of size 8192
2019-10-01 15:30:11.032932: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1216b31500 next 77 of size 32768
2019-10-01 15:30:11.032941: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1216b39500 next 78 of size 67108864
2019-10-01 15:30:11.032946: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x121ab39500 next 79 of size 67108864
2019-10-01 15:30:11.032952: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x121eb39500 next 80 of size 16384000
2019-10-01 15:30:11.032957: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x121fad9500 next 81 of size 8192
2019-10-01 15:30:11.032962: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x121fadb500 next 82 of size 8192
2019-10-01 15:30:11.032968: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x121fadd500 next 83 of size 262144000
2019-10-01 15:30:11.032973: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x122f4dd500 next 84 of size 8192
2019-10-01 15:30:11.032979: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x122f4df500 next 85 of size 16777216
2019-10-01 15:30:11.032984: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12304df500 next 86 of size 67108864
2019-10-01 15:30:11.032989: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12344df500 next 87 of size 67108864
2019-10-01 15:30:11.032995: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12384df500 next 88 of size 8192
2019-10-01 15:30:11.033000: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12384e1500 next 89 of size 16777216
2019-10-01 15:30:11.033005: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12394e1500 next 90 of size 8192
2019-10-01 15:30:11.033010: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12394e3500 next 91 of size 512
2019-10-01 15:30:11.033015: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12394e3700 next 92 of size 512
2019-10-01 15:30:11.033020: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12394e3900 next 93 of size 8192
2019-10-01 15:30:11.033025: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12394e5900 next 94 of size 32768
2019-10-01 15:30:11.033031: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12394ed900 next 95 of size 8192
2019-10-01 15:30:11.033036: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12394ef900 next 96 of size 16777216
2019-10-01 15:30:11.033042: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x123a4ef900 next 97 of size 67108864
2019-10-01 15:30:11.033047: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x123e4ef900 next 98 of size 32768
2019-10-01 15:30:11.033052: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x123e4f7900 next 99 of size 8192
2019-10-01 15:30:11.033057: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x123e4f9900 next 100 of size 16777216
2019-10-01 15:30:11.033063: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x123f4f9900 next 101 of size 8192
2019-10-01 15:30:11.033068: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x123f4fb900 next 102 of size 16777216
2019-10-01 15:30:11.033073: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12404fb900 next 103 of size 16777216
2019-10-01 15:30:11.033078: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12414fb900 next 104 of size 32768
2019-10-01 15:30:11.033084: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1241503900 next 105 of size 8192
2019-10-01 15:30:11.033089: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1241505900 next 106 of size 16777216
2019-10-01 15:30:11.033094: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1242505900 next 107 of size 16777216
2019-10-01 15:30:11.033099: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1243505900 next 108 of size 67108864
2019-10-01 15:30:11.033107: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1247505900 next 109 of size 8192
2019-10-01 15:30:11.033112: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1247507900 next 110 of size 8192
2019-10-01 15:30:11.033117: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1247509900 next 111 of size 8192
2019-10-01 15:30:11.033123: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x124750b900 next 112 of size 8192
2019-10-01 15:30:11.033128: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x124750d900 next 113 of size 16777216
2019-10-01 15:30:11.033134: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x124850d900 next 114 of size 8192
2019-10-01 15:30:11.033139: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x124850f900 next 115 of size 16384000
2019-10-01 15:30:11.033144: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12494af900 next 116 of size 16384000
2019-10-01 15:30:11.033150: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x124a44f900 next 117 of size 8192
2019-10-01 15:30:11.033155: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x124a451900 next 118 of size 256
2019-10-01 15:30:11.033160: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x124a451a00 next 119 of size 8192
2019-10-01 15:30:11.033166: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x124a453a00 next 120 of size 256
2019-10-01 15:30:11.033171: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x124a453b00 next 121 of size 8192
2019-10-01 15:30:11.033176: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x124a455b00 next 122 of size 16777216
2019-10-01 15:30:11.033181: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x124b455b00 next 123 of size 256
2019-10-01 15:30:11.033186: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x124b455c00 next 124 of size 67108864
2019-10-01 15:30:11.033192: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x124f455c00 next 125 of size 8192
2019-10-01 15:30:11.033197: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x124f457c00 next 126 of size 8192
2019-10-01 15:30:11.033202: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x124f459c00 next 127 of size 16777216
2019-10-01 15:30:11.033207: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1250459c00 next 128 of size 8192
2019-10-01 15:30:11.033213: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x125045bc00 next 129 of size 16777216
2019-10-01 15:30:11.033218: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x125145bc00 next 130 of size 16777216
2019-10-01 15:30:11.033223: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x125245bc00 next 131 of size 16777216
2019-10-01 15:30:11.033228: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x125345bc00 next 160 of size 256
2019-10-01 15:30:11.033233: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x125345bd00 next 169 of size 512
2019-10-01 15:30:11.033238: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x125345bf00 next 186 of size 512
2019-10-01 15:30:11.033244: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x125345c100 next 193 of size 512
2019-10-01 15:30:11.033249: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x125345c300 next 195 of size 512
2019-10-01 15:30:11.033254: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x125345c500 next 232 of size 512
2019-10-01 15:30:11.033259: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x125345c700 next 288 of size 512
2019-10-01 15:30:11.033264: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x125345c900 next 293 of size 512
2019-10-01 15:30:11.033270: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x125345cb00 next 295 of size 512
2019-10-01 15:30:11.033275: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x125345cd00 next 326 of size 512
2019-10-01 15:30:11.033282: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x125345cf00 next 336 of size 512
2019-10-01 15:30:11.033288: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x125345d100 next 342 of size 512
2019-10-01 15:30:11.033293: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x125345d300 next 361 of size 512
2019-10-01 15:30:11.033299: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x125345d500 next 364 of size 512
2019-10-01 15:30:11.033304: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x125345d700 next 372 of size 512
2019-10-01 15:30:11.033309: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x125345d900 next 162 of size 512
2019-10-01 15:30:11.033315: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x125345db00 next 163 of size 8192
2019-10-01 15:30:11.033320: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x125345fb00 next 164 of size 8192
2019-10-01 15:30:11.033325: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1253461b00 next 165 of size 16777216
2019-10-01 15:30:11.033330: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1254461b00 next 166 of size 67108864
2019-10-01 15:30:11.033335: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1258461b00 next 161 of size 8192
2019-10-01 15:30:11.033341: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1258463b00 next 157 of size 67108864
2019-10-01 15:30:11.033346: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x125c463b00 next 158 of size 8192
2019-10-01 15:30:11.033356: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x125c465b00 next 152 of size 8192
2019-10-01 15:30:11.033365: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x125c467b00 next 153 of size 8192
2019-10-01 15:30:11.033373: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x125c469b00 next 159 of size 8192
2019-10-01 15:30:11.033382: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x125c46bb00 next 156 of size 67108864
2019-10-01 15:30:11.033391: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x126046bb00 next 151 of size 8192
2019-10-01 15:30:11.033396: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x126046db00 next 146 of size 8192
2019-10-01 15:30:11.033402: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x126046fb00 next 147 of size 8192
2019-10-01 15:30:11.033407: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1260471b00 next 155 of size 32768
2019-10-01 15:30:11.033413: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1260479b00 next 150 of size 16777216
2019-10-01 15:30:11.033418: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1261479b00 next 145 of size 8192
2019-10-01 15:30:11.033423: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x126147bb00 next 141 of size 16777216
2019-10-01 15:30:11.033429: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x126247bb00 next 142 of size 16777216
2019-10-01 15:30:11.033434: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x126347bb00 next 154 of size 8192
2019-10-01 15:30:11.033440: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x126347db00 next 149 of size 16384000
2019-10-01 15:30:11.033445: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x126441db00 next 144 of size 16384000
2019-10-01 15:30:11.033450: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12653bdb00 next 148 of size 8192
2019-10-01 15:30:11.033455: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12653bfb00 next 143 of size 8192
2019-10-01 15:30:11.033460: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12653c1b00 next 139 of size 8192
2019-10-01 15:30:11.033466: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12653c3b00 next 140 of size 32768
2019-10-01 15:30:11.033475: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12653cbb00 next 138 of size 16777216
2019-10-01 15:30:11.033480: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12663cbb00 next 135 of size 8192
2019-10-01 15:30:11.033486: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12663cdb00 next 136 of size 16777216
2019-10-01 15:30:11.033491: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12673cdb00 next 137 of size 16777216
2019-10-01 15:30:11.033496: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12683cdb00 next 134 of size 8192
2019-10-01 15:30:11.033501: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12683cfb00 next 132 of size 8192
2019-10-01 15:30:11.033506: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12683d1b00 next 133 of size 8192
2019-10-01 15:30:11.033512: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12683d3b00 next 167 of size 8192
2019-10-01 15:30:11.033517: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12683d5b00 next 168 of size 8192
2019-10-01 15:30:11.033522: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12683d7b00 next 170 of size 67108864
2019-10-01 15:30:11.033527: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x126c3d7b00 next 171 of size 8192
2019-10-01 15:30:11.033532: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x126c3d9b00 next 172 of size 8192
2019-10-01 15:30:11.033538: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x126c3dbb00 next 173 of size 16777216
2019-10-01 15:30:11.033543: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x126d3dbb00 next 174 of size 32768
2019-10-01 15:30:11.033549: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x126d3e3b00 next 175 of size 8192
2019-10-01 15:30:11.033554: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x126d3e5b00 next 176 of size 67108864
2019-10-01 15:30:11.033559: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12713e5b00 next 177 of size 67108864
2019-10-01 15:30:11.033564: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12753e5b00 next 178 of size 16777216
2019-10-01 15:30:11.033570: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12763e5b00 next 179 of size 16777216
2019-10-01 15:30:11.033575: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12773e5b00 next 180 of size 67108864
2019-10-01 15:30:11.033580: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x127b3e5b00 next 181 of size 67108864
2019-10-01 15:30:11.033585: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x127f3e5b00 next 182 of size 8192
2019-10-01 15:30:11.033591: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x127f3e7b00 next 183 of size 67108864
2019-10-01 15:30:11.033596: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12833e7b00 next 184 of size 8192
2019-10-01 15:30:11.033601: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12833e9b00 next 185 of size 8192
2019-10-01 15:30:11.033606: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12833ebb00 next 187 of size 16384000
2019-10-01 15:30:11.033611: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x128438bb00 next 188 of size 16384000
2019-10-01 15:30:11.033616: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x128532bb00 next 189 of size 8192
2019-10-01 15:30:11.033622: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x128532db00 next 190 of size 8192
2019-10-01 15:30:11.033627: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x128532fb00 next 191 of size 8192
2019-10-01 15:30:11.033632: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1285331b00 next 192 of size 8192
2019-10-01 15:30:11.033641: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1285333b00 next 194 of size 8192
2019-10-01 15:30:11.033646: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1285335b00 next 196 of size 8192
2019-10-01 15:30:11.033651: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1285337b00 next 197 of size 32768
2019-10-01 15:30:11.033657: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x128533fb00 next 198 of size 67108864
2019-10-01 15:30:11.033662: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x128933fb00 next 199 of size 8192
2019-10-01 15:30:11.033667: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1289341b00 next 200 of size 8192
2019-10-01 15:30:11.033672: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1289343b00 next 201 of size 8192
2019-10-01 15:30:11.033678: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1289345b00 next 202 of size 67108864
2019-10-01 15:30:11.033683: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x128d345b00 next 203 of size 16777216
2019-10-01 15:30:11.033688: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x128e345b00 next 204 of size 32768
2019-10-01 15:30:11.033693: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x128e34db00 next 205 of size 16777216
2019-10-01 15:30:11.033699: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x128f34db00 next 206 of size 8192
2019-10-01 15:30:11.033704: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x128f34fb00 next 207 of size 8192
2019-10-01 15:30:11.033709: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x128f351b00 next 208 of size 67108864
2019-10-01 15:30:11.033714: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1293351b00 next 209 of size 67108864
2019-10-01 15:30:11.033719: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1297351b00 next 210 of size 16777216
2019-10-01 15:30:11.033724: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1298351b00 next 211 of size 8192
2019-10-01 15:30:11.033730: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1298353b00 next 212 of size 8192
2019-10-01 15:30:11.033735: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1298355b00 next 213 of size 32768
2019-10-01 15:30:11.033740: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x129835db00 next 214 of size 8192
2019-10-01 15:30:11.033745: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x129835fb00 next 215 of size 32768
2019-10-01 15:30:11.033751: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1298367b00 next 216 of size 16777216
2019-10-01 15:30:11.033756: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1299367b00 next 217 of size 8192
2019-10-01 15:30:11.033761: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1299369b00 next 218 of size 16777216
2019-10-01 15:30:11.033766: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x129a369b00 next 219 of size 8192
2019-10-01 15:30:11.033771: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x129a36bb00 next 220 of size 16777216
2019-10-01 15:30:11.033777: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x129b36bb00 next 221 of size 16384000
2019-10-01 15:30:11.033782: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x129c30bb00 next 222 of size 8192
2019-10-01 15:30:11.033787: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x129c30db00 next 223 of size 8192
2019-10-01 15:30:11.033792: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x129c30fb00 next 224 of size 16777216
2019-10-01 15:30:11.033797: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x129d30fb00 next 225 of size 8192
2019-10-01 15:30:11.033803: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x129d311b00 next 226 of size 8192
2019-10-01 15:30:11.033811: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x129d313b00 next 227 of size 8192
2019-10-01 15:30:11.033816: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x129d315b00 next 228 of size 8192
2019-10-01 15:30:11.033822: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x129d317b00 next 229 of size 8192
2019-10-01 15:30:11.033827: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x129d319b00 next 230 of size 8192
2019-10-01 15:30:11.033833: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x129d31bb00 next 231 of size 16777216
2019-10-01 15:30:11.033838: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x129e31bb00 next 233 of size 8192
2019-10-01 15:30:11.033843: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x129e31db00 next 234 of size 16777216
2019-10-01 15:30:11.033848: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x129f31db00 next 235 of size 67108864
2019-10-01 15:30:11.033853: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12a331db00 next 236 of size 8192
2019-10-01 15:30:11.033858: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12a331fb00 next 237 of size 8192
2019-10-01 15:30:11.033864: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12a3321b00 next 238 of size 16777216
2019-10-01 15:30:11.033869: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12a4321b00 next 239 of size 8192
2019-10-01 15:30:11.033875: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12a4323b00 next 240 of size 8192
2019-10-01 15:30:11.033880: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12a4325b00 next 241 of size 16777216
2019-10-01 15:30:11.033885: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12a5325b00 next 242 of size 8192
2019-10-01 15:30:11.033890: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12a5327b00 next 243 of size 67108864
2019-10-01 15:30:11.033895: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12a9327b00 next 244 of size 16777216
2019-10-01 15:30:11.033900: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12aa327b00 next 245 of size 8192
2019-10-01 15:30:11.033905: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12aa329b00 next 246 of size 67108864
2019-10-01 15:30:11.033911: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12ae329b00 next 247 of size 8192
2019-10-01 15:30:11.033916: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12ae32bb00 next 248 of size 8192
2019-10-01 15:30:11.033921: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12ae32db00 next 249 of size 8192
2019-10-01 15:30:11.033926: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12ae32fb00 next 250 of size 16384000
2019-10-01 15:30:11.033932: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12af2cfb00 next 251 of size 16384000
2019-10-01 15:30:11.033937: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12b026fb00 next 252 of size 8192
2019-10-01 15:30:11.033942: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12b0271b00 next 253 of size 8192
2019-10-01 15:30:11.033947: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12b0273b00 next 254 of size 32768
2019-10-01 15:30:11.033952: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12b027bb00 next 255 of size 16777216
2019-10-01 15:30:11.033957: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12b127bb00 next 256 of size 8192
2019-10-01 15:30:11.033962: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12b127db00 next 257 of size 16777216
2019-10-01 15:30:11.033968: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12b227db00 next 258 of size 8192
2019-10-01 15:30:11.033973: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12b227fb00 next 259 of size 8192
2019-10-01 15:30:11.033980: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12b2281b00 next 260 of size 8192
2019-10-01 15:30:11.033986: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12b2283b00 next 261 of size 67108864
2019-10-01 15:30:11.033991: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12b6283b00 next 262 of size 67108864
2019-10-01 15:30:11.033996: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12ba283b00 next 263 of size 8192
2019-10-01 15:30:11.034001: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12ba285b00 next 264 of size 8192
2019-10-01 15:30:11.034007: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12ba287b00 next 265 of size 8192
2019-10-01 15:30:11.034012: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12ba289b00 next 266 of size 8192
2019-10-01 15:30:11.034017: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12ba28bb00 next 267 of size 8192
2019-10-01 15:30:11.034023: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12ba28db00 next 268 of size 8192
2019-10-01 15:30:11.034028: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12ba28fb00 next 269 of size 16777216
2019-10-01 15:30:11.034033: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12bb28fb00 next 270 of size 8192
2019-10-01 15:30:11.034038: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12bb291b00 next 271 of size 8192
2019-10-01 15:30:11.034044: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12bb293b00 next 272 of size 32768
2019-10-01 15:30:11.034049: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12bb29bb00 next 273 of size 32768
2019-10-01 15:30:11.034054: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12bb2a3b00 next 274 of size 8192
2019-10-01 15:30:11.034059: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12bb2a5b00 next 275 of size 8192
2019-10-01 15:30:11.034064: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12bb2a7b00 next 276 of size 8192
2019-10-01 15:30:11.034069: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12bb2a9b00 next 277 of size 16777216
2019-10-01 15:30:11.034074: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12bc2a9b00 next 278 of size 67108864
2019-10-01 15:30:11.034080: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12c02a9b00 next 279 of size 67108864
2019-10-01 15:30:11.034085: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12c42a9b00 next 280 of size 16384000
2019-10-01 15:30:11.034090: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12c5249b00 next 281 of size 16384000
2019-10-01 15:30:11.034096: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12c61e9b00 next 282 of size 67108864
2019-10-01 15:30:11.034101: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12ca1e9b00 next 283 of size 8192
2019-10-01 15:30:11.034106: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12ca1ebb00 next 284 of size 16777216
2019-10-01 15:30:11.034111: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12cb1ebb00 next 285 of size 8192
2019-10-01 15:30:11.034116: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12cb1edb00 next 286 of size 8192
2019-10-01 15:30:11.034121: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12cb1efb00 next 287 of size 8192
2019-10-01 15:30:11.034126: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12cb1f1b00 next 289 of size 16777216
2019-10-01 15:30:11.034132: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12cc1f1b00 next 290 of size 16777216
2019-10-01 15:30:11.034137: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12cd1f1b00 next 291 of size 16777216
2019-10-01 15:30:11.034142: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12ce1f1b00 next 292 of size 8192
2019-10-01 15:30:11.034149: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12ce1f3b00 next 294 of size 8192
2019-10-01 15:30:11.034155: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12ce1f5b00 next 296 of size 8192
2019-10-01 15:30:11.034160: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12ce1f7b00 next 297 of size 8192
2019-10-01 15:30:11.034165: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12ce1f9b00 next 298 of size 8192
2019-10-01 15:30:11.034171: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12ce1fbb00 next 299 of size 8192
2019-10-01 15:30:11.034176: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12ce1fdb00 next 300 of size 67108864
2019-10-01 15:30:11.034181: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12d21fdb00 next 301 of size 16777216
2019-10-01 15:30:11.034186: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12d31fdb00 next 302 of size 8192
2019-10-01 15:30:11.034191: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12d31ffb00 next 303 of size 16777216
2019-10-01 15:30:11.034196: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12d41ffb00 next 304 of size 67108864
2019-10-01 15:30:11.034202: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12d81ffb00 next 305 of size 8192
2019-10-01 15:30:11.034207: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12d8201b00 next 306 of size 32768
2019-10-01 15:30:11.034212: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12d8209b00 next 307 of size 16384000
2019-10-01 15:30:11.034217: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12d91a9b00 next 308 of size 16777216
2019-10-01 15:30:11.034222: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12da1a9b00 next 309 of size 67108864
2019-10-01 15:30:11.034227: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12de1a9b00 next 310 of size 16777216
2019-10-01 15:30:11.034232: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12df1a9b00 next 311 of size 67108864
2019-10-01 15:30:11.034238: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12e31a9b00 next 312 of size 16777216
2019-10-01 15:30:11.034243: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12e41a9b00 next 313 of size 8192
2019-10-01 15:30:11.034248: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12e41abb00 next 314 of size 8192
2019-10-01 15:30:11.034253: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12e41adb00 next 315 of size 8192
2019-10-01 15:30:11.034258: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12e41afb00 next 316 of size 8192
2019-10-01 15:30:11.034263: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12e41b1b00 next 317 of size 8192
2019-10-01 15:30:11.034268: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12e41b3b00 next 318 of size 32768
2019-10-01 15:30:11.034273: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12e41bbb00 next 319 of size 8192
2019-10-01 15:30:11.034279: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12e41bdb00 next 320 of size 8192
2019-10-01 15:30:11.034284: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12e41bfb00 next 321 of size 16777216
2019-10-01 15:30:11.034289: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12e51bfb00 next 322 of size 8192
2019-10-01 15:30:11.034294: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12e51c1b00 next 323 of size 8192
2019-10-01 15:30:11.034299: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12e51c3b00 next 324 of size 8192
2019-10-01 15:30:11.034304: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12e51c5b00 next 325 of size 16777216
2019-10-01 15:30:11.034309: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12e61c5b00 next 327 of size 8192
2019-10-01 15:30:11.034317: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12e61c7b00 next 328 of size 8192
2019-10-01 15:30:11.034322: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12e61c9b00 next 329 of size 16777216
2019-10-01 15:30:11.034327: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12e71c9b00 next 330 of size 67108864
2019-10-01 15:30:11.034333: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12eb1c9b00 next 331 of size 16384000
2019-10-01 15:30:11.034338: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12ec169b00 next 332 of size 32768
2019-10-01 15:30:11.034343: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12ec171b00 next 333 of size 8192
2019-10-01 15:30:11.034348: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12ec173b00 next 334 of size 16777216
2019-10-01 15:30:11.034353: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12ed173b00 next 335 of size 8192
2019-10-01 15:30:11.034359: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12ed175b00 next 337 of size 8192
2019-10-01 15:30:11.034364: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12ed177b00 next 338 of size 16777216
2019-10-01 15:30:11.034369: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12ee177b00 next 339 of size 67108864
2019-10-01 15:30:11.034375: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12f2177b00 next 340 of size 8192
2019-10-01 15:30:11.034380: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12f2179b00 next 341 of size 8192
2019-10-01 15:30:11.034385: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12f217bb00 next 343 of size 16777216
2019-10-01 15:30:11.034390: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12f317bb00 next 344 of size 16777216
2019-10-01 15:30:11.034395: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12f417bb00 next 345 of size 8192
2019-10-01 15:30:11.034400: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12f417db00 next 346 of size 16777216
2019-10-01 15:30:11.034405: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12f517db00 next 347 of size 8192
2019-10-01 15:30:11.034411: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12f517fb00 next 348 of size 8192
2019-10-01 15:30:11.034416: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12f5181b00 next 349 of size 16777216
2019-10-01 15:30:11.034421: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12f6181b00 next 350 of size 67108864
2019-10-01 15:30:11.034426: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12fa181b00 next 351 of size 16777216
2019-10-01 15:30:11.034431: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12fb181b00 next 352 of size 8192
2019-10-01 15:30:11.034436: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12fb183b00 next 353 of size 8192
2019-10-01 15:30:11.034441: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12fb185b00 next 354 of size 16777216
2019-10-01 15:30:11.034446: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12fc185b00 next 355 of size 8192
2019-10-01 15:30:11.034451: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12fc187b00 next 356 of size 16777216
2019-10-01 15:30:11.034456: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12fd187b00 next 357 of size 8192
2019-10-01 15:30:11.034461: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12fd189b00 next 358 of size 8192
2019-10-01 15:30:11.034466: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12fd18bb00 next 359 of size 8192
2019-10-01 15:30:11.034472: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12fd18db00 next 360 of size 8192
2019-10-01 15:30:11.034477: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12fd18fb00 next 362 of size 8192
2019-10-01 15:30:11.034485: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12fd191b00 next 363 of size 8192
2019-10-01 15:30:11.034490: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12fd193b00 next 365 of size 32768
2019-10-01 15:30:11.034495: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12fd19bb00 next 366 of size 8192
2019-10-01 15:30:11.034501: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12fd19db00 next 367 of size 8192
2019-10-01 15:30:11.034506: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12fd19fb00 next 368 of size 8192
2019-10-01 15:30:11.034511: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12fd1a1b00 next 369 of size 67108864
2019-10-01 15:30:11.034517: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13011a1b00 next 370 of size 8192
2019-10-01 15:30:11.034522: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13011a3b00 next 371 of size 16777216
2019-10-01 15:30:11.034527: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13021a3b00 next 373 of size 8192
2019-10-01 15:30:11.034532: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13021a5b00 next 374 of size 16777216
2019-10-01 15:30:11.034582: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13031a5b00 next 375 of size 8192
2019-10-01 15:30:11.034592: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13031a7b00 next 376 of size 16777216
2019-10-01 15:30:11.034600: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13041a7b00 next 377 of size 67108864
2019-10-01 15:30:11.034605: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13081a7b00 next 378 of size 16777216
2019-10-01 15:30:11.034611: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13091a7b00 next 379 of size 8192
2019-10-01 15:30:11.034618: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13091a9b00 next 380 of size 16777216
2019-10-01 15:30:11.034623: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x130a1a9b00 next 381 of size 67108864
2019-10-01 15:30:11.034629: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x130e1a9b00 next 382 of size 8192
2019-10-01 15:30:11.034634: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x130e1abb00 next 383 of size 8192
2019-10-01 15:30:11.034639: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x130e1adb00 next 384 of size 8192
2019-10-01 15:30:11.034644: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x130e1afb00 next 385 of size 8192
2019-10-01 15:30:11.034649: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x130e1b1b00 next 386 of size 8192
2019-10-01 15:30:11.034654: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x130e1b3b00 next 387 of size 8192
2019-10-01 15:30:11.034659: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x130e1b5b00 next 388 of size 8192
2019-10-01 15:30:11.034664: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x130e1b7b00 next 389 of size 8192
2019-10-01 15:30:11.034669: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x130e1b9b00 next 390 of size 67108864
2019-10-01 15:30:11.034674: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13121b9b00 next 391 of size 16777216
2019-10-01 15:30:11.034679: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13131b9b00 next 392 of size 32768
2019-10-01 15:30:11.034684: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13131c1b00 next 393 of size 8192
2019-10-01 15:30:11.034689: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13131c3b00 next 394 of size 16777216
2019-10-01 15:30:11.034694: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13141c3b00 next 395 of size 8192
2019-10-01 15:30:11.034703: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13141c5b00 next 396 of size 67108864
2019-10-01 15:30:11.034709: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181c5b00 next 439 of size 256
2019-10-01 15:30:11.034714: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181c5c00 next 437 of size 8192
2019-10-01 15:30:11.034719: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181c7c00 next 436 of size 8192
2019-10-01 15:30:11.034725: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181c9c00 next 399 of size 256
2019-10-01 15:30:11.034730: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181c9d00 next 434 of size 256
2019-10-01 15:30:11.034735: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181c9e00 next 413 of size 256
2019-10-01 15:30:11.034740: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181c9f00 next 432 of size 256
2019-10-01 15:30:11.034745: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181ca000 next 431 of size 8192
2019-10-01 15:30:11.034750: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181cc000 next 430 of size 8192
2019-10-01 15:30:11.034755: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181ce000 next 429 of size 8192
2019-10-01 15:30:11.034760: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181d0000 next 428 of size 8192
2019-10-01 15:30:11.034765: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181d2000 next 425 of size 512
2019-10-01 15:30:11.034770: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181d2200 next 397 of size 8192
2019-10-01 15:30:11.034775: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181d4200 next 433 of size 8192
2019-10-01 15:30:11.034780: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181d6200 next 423 of size 8192
2019-10-01 15:30:11.034787: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181d8200 next 422 of size 8192
2019-10-01 15:30:11.034792: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181da200 next 421 of size 8192
2019-10-01 15:30:11.034797: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181dc200 next 420 of size 8192
2019-10-01 15:30:11.034802: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181de200 next 419 of size 32768
2019-10-01 15:30:11.034807: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181e6200 next 418 of size 32768
2019-10-01 15:30:11.034812: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181ee200 next 411 of size 256
2019-10-01 15:30:11.034817: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181ee300 next 410 of size 256
2019-10-01 15:30:11.034823: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181ee400 next 409 of size 256
2019-10-01 15:30:11.034828: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181ee500 next 440 of size 256
2019-10-01 15:30:11.034833: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181ee600 next 441 of size 256
2019-10-01 15:30:11.034838: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181ee700 next 442 of size 256
2019-10-01 15:30:11.034843: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181ee800 next 445 of size 256
2019-10-01 15:30:11.034848: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181ee900 next 447 of size 256
2019-10-01 15:30:11.034853: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181eea00 next 448 of size 256
2019-10-01 15:30:11.034858: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181eeb00 next 449 of size 256
2019-10-01 15:30:11.034863: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181eec00 next 450 of size 256
2019-10-01 15:30:11.034868: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181eed00 next 451 of size 8192
2019-10-01 15:30:11.034876: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f0d00 next 452 of size 8192
2019-10-01 15:30:11.034882: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f2d00 next 454 of size 256
2019-10-01 15:30:11.034887: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f2e00 next 456 of size 4096
2019-10-01 15:30:11.034892: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f3e00 next 457 of size 256
2019-10-01 15:30:11.034898: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f3f00 next 458 of size 256
2019-10-01 15:30:11.034903: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f4000 next 459 of size 256
2019-10-01 15:30:11.034908: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f4100 next 460 of size 256
2019-10-01 15:30:11.034913: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f4200 next 461 of size 256
2019-10-01 15:30:11.034918: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f4300 next 462 of size 256
2019-10-01 15:30:11.034923: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f4400 next 463 of size 256
2019-10-01 15:30:11.034928: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f4500 next 464 of size 256
2019-10-01 15:30:11.034933: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f4600 next 465 of size 256
2019-10-01 15:30:11.034938: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f4700 next 466 of size 256
2019-10-01 15:30:11.034943: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f4800 next 467 of size 256
2019-10-01 15:30:11.034948: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f4900 next 468 of size 256
2019-10-01 15:30:11.034954: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f4a00 next 469 of size 256
2019-10-01 15:30:11.034960: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f4b00 next 470 of size 256
2019-10-01 15:30:11.034965: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f4c00 next 471 of size 256
2019-10-01 15:30:11.034970: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f4d00 next 472 of size 256
2019-10-01 15:30:11.034975: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f4e00 next 473 of size 256
2019-10-01 15:30:11.034980: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f4f00 next 474 of size 256
2019-10-01 15:30:11.034985: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f5000 next 475 of size 256
2019-10-01 15:30:11.034990: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f5100 next 476 of size 256
2019-10-01 15:30:11.034995: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f5200 next 477 of size 256
2019-10-01 15:30:11.035000: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f5300 next 478 of size 256
2019-10-01 15:30:11.035005: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f5400 next 479 of size 256
2019-10-01 15:30:11.035010: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f5500 next 480 of size 256
2019-10-01 15:30:11.035015: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f5600 next 481 of size 256
2019-10-01 15:30:11.035020: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f5700 next 482 of size 256
2019-10-01 15:30:11.035025: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f5800 next 483 of size 256
2019-10-01 15:30:11.035030: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f5900 next 484 of size 256
2019-10-01 15:30:11.035035: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f5a00 next 485 of size 256
2019-10-01 15:30:11.035043: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f5b00 next 486 of size 256
2019-10-01 15:30:11.035048: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f5c00 next 487 of size 256
2019-10-01 15:30:11.035053: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f5d00 next 488 of size 256
2019-10-01 15:30:11.035058: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f5e00 next 489 of size 256
2019-10-01 15:30:11.035064: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f5f00 next 490 of size 256
2019-10-01 15:30:11.035069: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f6000 next 491 of size 256
2019-10-01 15:30:11.035074: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f6100 next 492 of size 256
2019-10-01 15:30:11.035079: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f6200 next 493 of size 256
2019-10-01 15:30:11.035084: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f6300 next 494 of size 256
2019-10-01 15:30:11.035089: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f6400 next 495 of size 256
2019-10-01 15:30:11.035098: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f6500 next 496 of size 256
2019-10-01 15:30:11.035106: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f6600 next 497 of size 256
2019-10-01 15:30:11.035114: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f6700 next 498 of size 256
2019-10-01 15:30:11.035123: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f6800 next 499 of size 256
2019-10-01 15:30:11.035131: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f6900 next 500 of size 256
2019-10-01 15:30:11.035138: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f6a00 next 501 of size 256
2019-10-01 15:30:11.035143: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f6b00 next 502 of size 256
2019-10-01 15:30:11.035148: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f6c00 next 503 of size 256
2019-10-01 15:30:11.035153: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f6d00 next 504 of size 256
2019-10-01 15:30:11.035158: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f6e00 next 505 of size 256
2019-10-01 15:30:11.035163: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f6f00 next 506 of size 256
2019-10-01 15:30:11.035168: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f7000 next 507 of size 256
2019-10-01 15:30:11.035173: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f7100 next 508 of size 256
2019-10-01 15:30:11.035178: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f7200 next 509 of size 256
2019-10-01 15:30:11.035183: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f7300 next 510 of size 256
2019-10-01 15:30:11.035188: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f7400 next 511 of size 256
2019-10-01 15:30:11.035193: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f7500 next 512 of size 256
2019-10-01 15:30:11.035198: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f7600 next 513 of size 256
2019-10-01 15:30:11.035203: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f7700 next 514 of size 256
2019-10-01 15:30:11.035208: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f7800 next 515 of size 256
2019-10-01 15:30:11.035213: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f7900 next 516 of size 256
2019-10-01 15:30:11.035218: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f7a00 next 517 of size 256
2019-10-01 15:30:11.035223: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f7b00 next 518 of size 256
2019-10-01 15:30:11.035232: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f7c00 next 519 of size 256
2019-10-01 15:30:11.035238: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f7d00 next 520 of size 256
2019-10-01 15:30:11.035243: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f7e00 next 521 of size 256
2019-10-01 15:30:11.035248: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f7f00 next 522 of size 256
2019-10-01 15:30:11.035253: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f8000 next 523 of size 256
2019-10-01 15:30:11.035258: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f8100 next 524 of size 256
2019-10-01 15:30:11.035263: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f8200 next 525 of size 256
2019-10-01 15:30:11.035268: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f8300 next 526 of size 256
2019-10-01 15:30:11.035274: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f8400 next 527 of size 256
2019-10-01 15:30:11.035279: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f8500 next 528 of size 256
2019-10-01 15:30:11.035284: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f8600 next 529 of size 256
2019-10-01 15:30:11.035289: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f8700 next 530 of size 256
2019-10-01 15:30:11.035294: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f8800 next 531 of size 256
2019-10-01 15:30:11.035299: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f8900 next 532 of size 256
2019-10-01 15:30:11.035305: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f8a00 next 533 of size 256
2019-10-01 15:30:11.035310: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f8b00 next 534 of size 256
2019-10-01 15:30:11.035315: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f8c00 next 535 of size 256
2019-10-01 15:30:11.035320: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f8d00 next 536 of size 256
2019-10-01 15:30:11.035326: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f8e00 next 537 of size 256
2019-10-01 15:30:11.035331: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f8f00 next 538 of size 256
2019-10-01 15:30:11.035336: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f9000 next 539 of size 256
2019-10-01 15:30:11.035341: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f9100 next 540 of size 256
2019-10-01 15:30:11.035346: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f9200 next 541 of size 256
2019-10-01 15:30:11.035351: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f9300 next 542 of size 256
2019-10-01 15:30:11.035356: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f9400 next 543 of size 256
2019-10-01 15:30:11.035361: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f9500 next 544 of size 256
2019-10-01 15:30:11.035366: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f9600 next 545 of size 256
2019-10-01 15:30:11.035371: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f9700 next 546 of size 256
2019-10-01 15:30:11.035376: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f9800 next 547 of size 256
2019-10-01 15:30:11.035381: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f9900 next 548 of size 256
2019-10-01 15:30:11.035386: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f9a00 next 549 of size 256
2019-10-01 15:30:11.035391: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f9b00 next 550 of size 256
2019-10-01 15:30:11.035400: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f9c00 next 551 of size 256
2019-10-01 15:30:11.035405: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f9d00 next 552 of size 256
2019-10-01 15:30:11.035410: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f9e00 next 553 of size 256
2019-10-01 15:30:11.035415: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f9f00 next 554 of size 256
2019-10-01 15:30:11.035420: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181fa000 next 555 of size 256
2019-10-01 15:30:11.035425: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181fa100 next 556 of size 256
2019-10-01 15:30:11.035430: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181fa200 next 557 of size 256
2019-10-01 15:30:11.035435: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181fa300 next 558 of size 256
2019-10-01 15:30:11.035440: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181fa400 next 559 of size 256
2019-10-01 15:30:11.035445: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181fa500 next 560 of size 256
2019-10-01 15:30:11.035450: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181fa600 next 561 of size 256
2019-10-01 15:30:11.035456: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181fa700 next 562 of size 256
2019-10-01 15:30:11.035461: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181fa800 next 563 of size 256
2019-10-01 15:30:11.035466: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181fa900 next 564 of size 256
2019-10-01 15:30:11.035472: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181faa00 next 565 of size 256
2019-10-01 15:30:11.035477: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181fab00 next 566 of size 256
2019-10-01 15:30:11.035482: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181fac00 next 567 of size 256
2019-10-01 15:30:11.035487: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181fad00 next 568 of size 256
2019-10-01 15:30:11.035492: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181fae00 next 569 of size 256
2019-10-01 15:30:11.035497: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181faf00 next 570 of size 256
2019-10-01 15:30:11.035502: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181fb000 next 571 of size 256
2019-10-01 15:30:11.035507: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181fb100 next 572 of size 256
2019-10-01 15:30:11.035512: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181fb200 next 573 of size 256
2019-10-01 15:30:11.035517: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181fb300 next 574 of size 256
2019-10-01 15:30:11.035522: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181fb400 next 575 of size 256
2019-10-01 15:30:11.035527: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181fb500 next 576 of size 256
2019-10-01 15:30:11.035532: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181fb600 next 577 of size 256
2019-10-01 15:30:11.035537: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181fb700 next 578 of size 256
2019-10-01 15:30:11.035542: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181fb800 next 579 of size 256
2019-10-01 15:30:11.035547: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181fb900 next 580 of size 256
2019-10-01 15:30:11.035552: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181fba00 next 581 of size 256
2019-10-01 15:30:11.035558: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181fbb00 next 582 of size 256
2019-10-01 15:30:11.035563: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181fbc00 next 583 of size 256
2019-10-01 15:30:11.035570: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181fbd00 next 584 of size 256
2019-10-01 15:30:11.035576: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181fbe00 next 585 of size 256
2019-10-01 15:30:11.035581: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181fbf00 next 586 of size 256
2019-10-01 15:30:11.035586: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181fc000 next 587 of size 256
2019-10-01 15:30:11.035591: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181fc100 next 588 of size 256
2019-10-01 15:30:11.035596: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181fc200 next 589 of size 256
2019-10-01 15:30:11.035601: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181fc300 next 590 of size 256
2019-10-01 15:30:11.035607: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181fc400 next 591 of size 8192
2019-10-01 15:30:11.035612: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181fe400 next 592 of size 8192
2019-10-01 15:30:11.035617: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1318200400 next 594 of size 256
2019-10-01 15:30:11.035622: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1318200500 next 595 of size 256
2019-10-01 15:30:11.035627: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1318200600 next 597 of size 256
2019-10-01 15:30:11.035632: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1318200700 next 598 of size 256
2019-10-01 15:30:11.035638: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1318200800 next 599 of size 256
2019-10-01 15:30:11.035643: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1318200900 next 600 of size 8192
2019-10-01 15:30:11.035649: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1318202900 next 398 of size 12800
2019-10-01 15:30:11.035654: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1318205b00 next 400 of size 131328
2019-10-01 15:30:11.035659: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1318225c00 next 401 of size 256
2019-10-01 15:30:11.035664: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1318225d00 next 402 of size 131072
2019-10-01 15:30:11.035669: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1318245d00 next 403 of size 16777216
2019-10-01 15:30:11.035674: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1319245d00 next 404 of size 8192
2019-10-01 15:30:11.035679: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1319247d00 next 405 of size 32768
2019-10-01 15:30:11.035685: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x131924fd00 next 406 of size 16384000
2019-10-01 15:30:11.035690: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x131a1efd00 next 407 of size 16384000
2019-10-01 15:30:11.035695: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x131b18fd00 next 408 of size 16384000
2019-10-01 15:30:11.035700: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x131c12fd00 next 438 of size 131072
2019-10-01 15:30:11.035705: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x131c14fd00 next 435 of size 131072
2019-10-01 15:30:11.035710: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x131c16fd00 next 427 of size 16777216
2019-10-01 15:30:11.035715: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x131d16fd00 next 426 of size 16384000
2019-10-01 15:30:11.035720: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x131e10fd00 next 424 of size 16777216
2019-10-01 15:30:11.035725: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x131f10fd00 next 417 of size 16777216
2019-10-01 15:30:11.035731: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x132010fd00 next 416 of size 67108864
2019-10-01 15:30:11.035739: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x132410fd00 next 601 of size 8192
2019-10-01 15:30:11.035745: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1324111d00 next 602 of size 8192
2019-10-01 15:30:11.035750: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1324113d00 next 605 of size 256
2019-10-01 15:30:11.035755: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1324113e00 next 606 of size 256
2019-10-01 15:30:11.035760: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1324113f00 next 607 of size 256
2019-10-01 15:30:11.035765: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1324114000 next 608 of size 256
2019-10-01 15:30:11.035770: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1324114100 next 609 of size 256
2019-10-01 15:30:11.035775: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1324114200 next 610 of size 256
2019-10-01 15:30:11.035780: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1324114300 next 611 of size 256
2019-10-01 15:30:11.035785: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1324114400 next 612 of size 256
2019-10-01 15:30:11.035790: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1324114500 next 614 of size 256
2019-10-01 15:30:11.035795: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1324114600 next 615 of size 256
2019-10-01 15:30:11.035801: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1324114700 next 616 of size 8192
2019-10-01 15:30:11.035807: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1324116700 next 617 of size 8192
2019-10-01 15:30:11.035812: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1324118700 next 619 of size 256
2019-10-01 15:30:11.035817: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1324118800 next 621 of size 256
2019-10-01 15:30:11.035822: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1324118900 next 622 of size 256
2019-10-01 15:30:11.035827: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1324118a00 next 623 of size 8192
2019-10-01 15:30:11.035832: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x132411aa00 next 624 of size 8192
2019-10-01 15:30:11.035837: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x132411ca00 next 625 of size 8192
2019-10-01 15:30:11.035842: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x132411ea00 next 626 of size 8192
2019-10-01 15:30:11.035847: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1324120a00 next 627 of size 8192
2019-10-01 15:30:11.035852: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1324122a00 next 628 of size 8192
2019-10-01 15:30:11.035857: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1324124a00 next 631 of size 512
2019-10-01 15:30:11.035862: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1324124c00 next 632 of size 512
2019-10-01 15:30:11.035867: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1324124e00 next 634 of size 256
2019-10-01 15:30:11.035873: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1324124f00 next 635 of size 256
2019-10-01 15:30:11.035878: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1324125000 next 637 of size 8192
2019-10-01 15:30:11.035883: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1324127000 next 638 of size 8192
2019-10-01 15:30:11.035888: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1324129000 next 639 of size 8192
2019-10-01 15:30:11.035893: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x132412b000 next 640 of size 8192
2019-10-01 15:30:11.035898: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x132412d000 next 642 of size 256
2019-10-01 15:30:11.035910: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x132412d100 next 643 of size 256
2019-10-01 15:30:11.035919: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x132412d200 next 644 of size 256
2019-10-01 15:30:11.035928: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x132412d300 next 415 of size 10752
2019-10-01 15:30:11.035936: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x132412fd00 next 414 of size 67108864
2019-10-01 15:30:11.035944: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x132812fd00 next 412 of size 16777216
2019-10-01 15:30:11.035949: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x132912fd00 next 443 of size 67108864
2019-10-01 15:30:11.035954: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x132d12fd00 next 444 of size 16777216
2019-10-01 15:30:11.035959: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x132e12fd00 next 446 of size 16777216
2019-10-01 15:30:11.035965: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x132f12fd00 next 453 of size 67108864
2019-10-01 15:30:11.035970: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x133312fd00 next 455 of size 8388608
2019-10-01 15:30:11.035975: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x133392fd00 next 593 of size 16777216
2019-10-01 15:30:11.035980: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x133492fd00 next 596 of size 16777216
2019-10-01 15:30:11.035985: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x133592fd00 next 603 of size 16777216
2019-10-01 15:30:11.035992: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x133692fd00 next 604 of size 16777216
2019-10-01 15:30:11.035997: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x133792fd00 next 613 of size 67108864
2019-10-01 15:30:11.036002: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x133b92fd00 next 618 of size 16777216
2019-10-01 15:30:11.036007: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x133c92fd00 next 620 of size 16777216
2019-10-01 15:30:11.036012: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x133d92fd00 next 629 of size 16777216
2019-10-01 15:30:11.036017: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x133e92fd00 next 630 of size 16777216
2019-10-01 15:30:11.036023: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x133f92fd00 next 633 of size 16777216
2019-10-01 15:30:11.036028: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x134092fd00 next 636 of size 16777216
2019-10-01 15:30:11.036033: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x134192fd00 next 641 of size 67108864
2019-10-01 15:30:11.036038: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x134592fd00 next 645 of size 67108864
2019-10-01 15:30:11.036043: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x134992fd00 next 646 of size 8192
2019-10-01 15:30:11.036048: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1349931d00 next 647 of size 8192
2019-10-01 15:30:11.036053: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1349933d00 next 648 of size 8192
2019-10-01 15:30:11.036058: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1349935d00 next 649 of size 8192
2019-10-01 15:30:11.036063: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1349937d00 next 650 of size 8192
2019-10-01 15:30:11.036069: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1349939d00 next 651 of size 8192
2019-10-01 15:30:11.036074: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x134993bd00 next 652 of size 8192
2019-10-01 15:30:11.036079: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x134993dd00 next 653 of size 32768
2019-10-01 15:30:11.036084: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1349945d00 next 654 of size 32768
2019-10-01 15:30:11.036092: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x134994dd00 next 655 of size 16777216
2019-10-01 15:30:11.036097: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x134a94dd00 next 656 of size 256
2019-10-01 15:30:11.036102: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x134a94de00 next 657 of size 256
2019-10-01 15:30:11.036108: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x134a94df00 next 658 of size 256
2019-10-01 15:30:11.036113: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x134a94e000 next 659 of size 16777216
2019-10-01 15:30:11.036118: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x134b94e000 next 660 of size 8192
2019-10-01 15:30:11.036123: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x134b950000 next 661 of size 8192
2019-10-01 15:30:11.036128: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x134b952000 next 662 of size 16777216
2019-10-01 15:30:11.036133: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x134c952000 next 663 of size 256
2019-10-01 15:30:11.036138: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x134c952100 next 664 of size 256
2019-10-01 15:30:11.036143: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x134c952200 next 665 of size 256
2019-10-01 15:30:11.036148: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x134c952300 next 666 of size 16777216
2019-10-01 15:30:11.036153: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x134d952300 next 667 of size 8192
2019-10-01 15:30:11.036159: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x134d954300 next 668 of size 8192
2019-10-01 15:30:11.036164: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x134d956300 next 669 of size 67108864
2019-10-01 15:30:11.036169: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1351956300 next 670 of size 256
2019-10-01 15:30:11.036174: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1351956400 next 671 of size 8192
2019-10-01 15:30:11.036180: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1351958400 next 672 of size 8192
2019-10-01 15:30:11.036185: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x135195a400 next 673 of size 256
2019-10-01 15:30:11.036190: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x135195a500 next 674 of size 67108864
2019-10-01 15:30:11.036195: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x135595a500 next 675 of size 8192
2019-10-01 15:30:11.036200: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x135595c500 next 676 of size 8192
2019-10-01 15:30:11.036205: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x135595e500 next 677 of size 8192
2019-10-01 15:30:11.036209: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1355960500 next 678 of size 8192
2019-10-01 15:30:11.036214: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1355962500 next 679 of size 8192
2019-10-01 15:30:11.036220: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1355964500 next 680 of size 8192
2019-10-01 15:30:11.036225: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1355966500 next 681 of size 8192
2019-10-01 15:30:11.036230: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1355968500 next 682 of size 8192
2019-10-01 15:30:11.036235: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x135596a500 next 683 of size 67108864
2019-10-01 15:30:11.036240: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x135996a500 next 684 of size 256
2019-10-01 15:30:11.036245: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x135996a600 next 685 of size 256
2019-10-01 15:30:11.036250: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x135996a700 next 686 of size 256
2019-10-01 15:30:11.036257: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x135996a800 next 687 of size 256
2019-10-01 15:30:11.036263: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x135996a900 next 688 of size 256
2019-10-01 15:30:11.036268: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x135996aa00 next 689 of size 8192
2019-10-01 15:30:11.036273: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x135996ca00 next 690 of size 8192
2019-10-01 15:30:11.036278: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x135996ea00 next 691 of size 32768
2019-10-01 15:30:11.036283: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1359976a00 next 692 of size 32768
2019-10-01 15:30:11.036288: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x135997ea00 next 693 of size 256
2019-10-01 15:30:11.036293: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x135997eb00 next 694 of size 67108864
2019-10-01 15:30:11.036298: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x135d97eb00 next 695 of size 16777216
2019-10-01 15:30:11.036303: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x135e97eb00 next 696 of size 256
2019-10-01 15:30:11.036308: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x135e97ec00 next 697 of size 256
2019-10-01 15:30:11.036313: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x135e97ed00 next 698 of size 256
2019-10-01 15:30:11.036319: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x135e97ee00 next 699 of size 16777216
2019-10-01 15:30:11.036325: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x135f97ee00 next 700 of size 8192
2019-10-01 15:30:11.036330: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x135f980e00 next 701 of size 8192
2019-10-01 15:30:11.036335: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x135f982e00 next 702 of size 8192
2019-10-01 15:30:11.036340: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x135f984e00 next 703 of size 8192
2019-10-01 15:30:11.036345: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x135f986e00 next 704 of size 16777216
2019-10-01 15:30:11.036350: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1360986e00 next 705 of size 256
2019-10-01 15:30:11.036355: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1360986f00 next 706 of size 256
2019-10-01 15:30:11.036360: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1360987000 next 707 of size 256
2019-10-01 15:30:11.036365: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1360987100 next 708 of size 256
2019-10-01 15:30:11.036370: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1360987200 next 709 of size 256
2019-10-01 15:30:11.036375: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1360987300 next 710 of size 16777216
2019-10-01 15:30:11.036380: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1361987300 next 711 of size 67108864
2019-10-01 15:30:11.036385: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1365987300 next 712 of size 256
2019-10-01 15:30:11.036390: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1365987400 next 713 of size 256
2019-10-01 15:30:11.036395: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1365987500 next 714 of size 256
2019-10-01 15:30:11.036400: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1365987600 next 715 of size 67108864
2019-10-01 15:30:11.036406: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1369987600 next 716 of size 8192
2019-10-01 15:30:11.036411: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1369989600 next 717 of size 8192
2019-10-01 15:30:11.036416: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x136998b600 next 718 of size 32768
2019-10-01 15:30:11.036423: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1369993600 next 719 of size 32768
2019-10-01 15:30:11.036429: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x136999b600 next 720 of size 8192
2019-10-01 15:30:11.036434: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x136999d600 next 721 of size 8192
2019-10-01 15:30:11.036439: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x136999f600 next 722 of size 32768
2019-10-01 15:30:11.036444: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13699a7600 next 723 of size 32768
2019-10-01 15:30:11.036449: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13699af600 next 724 of size 67108864
2019-10-01 15:30:11.036454: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x136d9af600 next 725 of size 256
2019-10-01 15:30:11.036459: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x136d9af700 next 726 of size 256
2019-10-01 15:30:11.036464: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x136d9af800 next 727 of size 256
2019-10-01 15:30:11.036469: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x136d9af900 next 728 of size 256
2019-10-01 15:30:11.036474: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x136d9afa00 next 729 of size 256
2019-10-01 15:30:11.036479: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x136d9afb00 next 730 of size 256
2019-10-01 15:30:11.036484: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x136d9afc00 next 731 of size 256
2019-10-01 15:30:11.036490: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x136d9afd00 next 732 of size 256
2019-10-01 15:30:11.036496: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x136d9afe00 next 733 of size 256
2019-10-01 15:30:11.036500: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x136d9aff00 next 734 of size 256
2019-10-01 15:30:11.036506: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x136d9b0000 next 735 of size 256
2019-10-01 15:30:11.036511: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x136d9b0100 next 736 of size 256
2019-10-01 15:30:11.036516: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x136d9b0200 next 737 of size 256
2019-10-01 15:30:11.036521: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x136d9b0300 next 738 of size 256
2019-10-01 15:30:11.036526: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x136d9b0400 next 739 of size 8192
2019-10-01 15:30:11.036531: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x136d9b2400 next 740 of size 8192
2019-10-01 15:30:11.036536: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x136d9b4400 next 741 of size 67108864
2019-10-01 15:30:11.036541: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13719b4400 next 742 of size 16777216
2019-10-01 15:30:11.036546: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13729b4400 next 743 of size 256
2019-10-01 15:30:11.036551: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13729b4500 next 744 of size 256
2019-10-01 15:30:11.036556: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13729b4600 next 745 of size 256
2019-10-01 15:30:11.036561: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13729b4700 next 746 of size 16777216
2019-10-01 15:30:11.036566: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13739b4700 next 747 of size 8192
2019-10-01 15:30:11.036571: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13739b6700 next 748 of size 8192
2019-10-01 15:30:11.036576: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13739b8700 next 749 of size 8192
2019-10-01 15:30:11.036581: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13739ba700 next 750 of size 8192
2019-10-01 15:30:11.036591: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13739bc700 next 751 of size 8192
2019-10-01 15:30:11.036596: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13739be700 next 752 of size 8192
2019-10-01 15:30:11.036601: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13739c0700 next 753 of size 32768
2019-10-01 15:30:11.036607: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13739c8700 next 754 of size 32768
2019-10-01 15:30:11.036612: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13739d0700 next 755 of size 8192
2019-10-01 15:30:11.036617: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13739d2700 next 756 of size 8192
2019-10-01 15:30:11.036622: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13739d4700 next 757 of size 32768
2019-10-01 15:30:11.036627: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13739dc700 next 758 of size 32768
2019-10-01 15:30:11.036632: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13739e4700 next 759 of size 16777216
2019-10-01 15:30:11.036637: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13749e4700 next 760 of size 256
2019-10-01 15:30:11.036643: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13749e4800 next 761 of size 256
2019-10-01 15:30:11.036648: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13749e4900 next 762 of size 256
2019-10-01 15:30:11.036653: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13749e4a00 next 763 of size 256
2019-10-01 15:30:11.036659: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13749e4b00 next 764 of size 256
2019-10-01 15:30:11.036664: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13749e4c00 next 765 of size 8192
2019-10-01 15:30:11.036669: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13749e6c00 next 766 of size 8192
2019-10-01 15:30:11.036675: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13749e8c00 next 767 of size 16384000
2019-10-01 15:30:11.036680: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1375988c00 next 768 of size 16777216
2019-10-01 15:30:11.036685: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1376988c00 next 769 of size 16384000
2019-10-01 15:30:11.036690: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1377928c00 next 770 of size 8192
2019-10-01 15:30:11.036695: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137792ac00 next 771 of size 8192
2019-10-01 15:30:11.036700: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137792cc00 next 772 of size 8192
2019-10-01 15:30:11.036705: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137792ec00 next 773 of size 8192
2019-10-01 15:30:11.036710: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1377930c00 next 774 of size 8192
2019-10-01 15:30:11.036715: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1377932c00 next 775 of size 8192
2019-10-01 15:30:11.036720: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1377934c00 next 776 of size 8192
2019-10-01 15:30:11.036725: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1377936c00 next 777 of size 8192
2019-10-01 15:30:11.036730: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1377938c00 next 778 of size 8192
2019-10-01 15:30:11.036735: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137793ac00 next 779 of size 8192
2019-10-01 15:30:11.036740: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137793cc00 next 780 of size 16777216
2019-10-01 15:30:11.036745: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137893cc00 next 781 of size 256
2019-10-01 15:30:11.036750: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137893cd00 next 782 of size 16777216
2019-10-01 15:30:11.036758: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137993cd00 next 783 of size 67108864
2019-10-01 15:30:11.036763: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93cd00 next 784 of size 256
2019-10-01 15:30:11.036768: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93ce00 next 785 of size 256
2019-10-01 15:30:11.036773: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93cf00 next 786 of size 256
2019-10-01 15:30:11.036778: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93d000 next 787 of size 256
2019-10-01 15:30:11.036783: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93d100 next 788 of size 256
2019-10-01 15:30:11.036788: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93d200 next 789 of size 256
2019-10-01 15:30:11.036794: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93d300 next 790 of size 256
2019-10-01 15:30:11.036799: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93d400 next 791 of size 256
2019-10-01 15:30:11.036804: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93d500 next 792 of size 256
2019-10-01 15:30:11.036809: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93d600 next 793 of size 256
2019-10-01 15:30:11.036814: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93d700 next 794 of size 256
2019-10-01 15:30:11.036819: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93d800 next 795 of size 256
2019-10-01 15:30:11.036825: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93d900 next 796 of size 256
2019-10-01 15:30:11.036830: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93da00 next 797 of size 256
2019-10-01 15:30:11.036835: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93db00 next 798 of size 256
2019-10-01 15:30:11.036840: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93dc00 next 799 of size 256
2019-10-01 15:30:11.036845: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93dd00 next 800 of size 256
2019-10-01 15:30:11.036850: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93de00 next 801 of size 256
2019-10-01 15:30:11.036855: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93df00 next 802 of size 256
2019-10-01 15:30:11.036860: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93e000 next 803 of size 256
2019-10-01 15:30:11.036865: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93e100 next 804 of size 256
2019-10-01 15:30:11.036870: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93e200 next 805 of size 256
2019-10-01 15:30:11.036875: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93e300 next 806 of size 256
2019-10-01 15:30:11.036880: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93e400 next 807 of size 256
2019-10-01 15:30:11.036885: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93e500 next 808 of size 256
2019-10-01 15:30:11.036890: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93e600 next 809 of size 256
2019-10-01 15:30:11.036895: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93e700 next 810 of size 256
2019-10-01 15:30:11.036900: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93e800 next 811 of size 256
2019-10-01 15:30:11.036905: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93e900 next 812 of size 256
2019-10-01 15:30:11.036910: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93ea00 next 813 of size 256
2019-10-01 15:30:11.036915: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93eb00 next 814 of size 256
2019-10-01 15:30:11.036920: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93ec00 next 815 of size 256
2019-10-01 15:30:11.036928: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93ed00 next 816 of size 256
2019-10-01 15:30:11.036933: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93ee00 next 817 of size 256
2019-10-01 15:30:11.036938: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93ef00 next 818 of size 256
2019-10-01 15:30:11.036943: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93f000 next 819 of size 256
2019-10-01 15:30:11.036948: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93f100 next 820 of size 256
2019-10-01 15:30:11.036954: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93f200 next 821 of size 256
2019-10-01 15:30:11.036959: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93f300 next 822 of size 256
2019-10-01 15:30:11.036964: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93f400 next 823 of size 256
2019-10-01 15:30:11.036969: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93f500 next 824 of size 256
2019-10-01 15:30:11.036974: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93f600 next 825 of size 256
2019-10-01 15:30:11.036979: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93f700 next 826 of size 256
2019-10-01 15:30:11.036984: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93f800 next 827 of size 256
2019-10-01 15:30:11.036990: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93f900 next 828 of size 256
2019-10-01 15:30:11.036996: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93fa00 next 829 of size 256
2019-10-01 15:30:11.037001: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93fb00 next 830 of size 256
2019-10-01 15:30:11.037006: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93fc00 next 831 of size 256
2019-10-01 15:30:11.037011: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93fd00 next 832 of size 256
2019-10-01 15:30:11.037016: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93fe00 next 833 of size 256
2019-10-01 15:30:11.037021: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93ff00 next 834 of size 256
2019-10-01 15:30:11.037026: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d940000 next 835 of size 256
2019-10-01 15:30:11.037031: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d940100 next 836 of size 256
2019-10-01 15:30:11.037036: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d940200 next 837 of size 256
2019-10-01 15:30:11.037041: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d940300 next 838 of size 256
2019-10-01 15:30:11.037046: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d940400 next 839 of size 256
2019-10-01 15:30:11.037051: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d940500 next 840 of size 256
2019-10-01 15:30:11.037056: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d940600 next 841 of size 256
2019-10-01 15:30:11.037061: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d940700 next 842 of size 256
2019-10-01 15:30:11.037066: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d940800 next 843 of size 256
2019-10-01 15:30:11.037071: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d940900 next 844 of size 256
2019-10-01 15:30:11.037076: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d940a00 next 845 of size 256
2019-10-01 15:30:11.037081: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d940b00 next 846 of size 256
2019-10-01 15:30:11.037086: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d940c00 next 847 of size 256
2019-10-01 15:30:11.037093: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d940d00 next 848 of size 256
2019-10-01 15:30:11.037099: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d940e00 next 849 of size 256
2019-10-01 15:30:11.037104: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d940f00 next 850 of size 256
2019-10-01 15:30:11.037109: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d941000 next 851 of size 256
2019-10-01 15:30:11.037114: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d941100 next 852 of size 256
2019-10-01 15:30:11.037119: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d941200 next 853 of size 256
2019-10-01 15:30:11.037124: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d941300 next 854 of size 256
2019-10-01 15:30:11.037129: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d941400 next 855 of size 256
2019-10-01 15:30:11.037134: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d941500 next 856 of size 256
2019-10-01 15:30:11.037139: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d941600 next 857 of size 256
2019-10-01 15:30:11.037144: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d941700 next 858 of size 256
2019-10-01 15:30:11.037150: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d941800 next 859 of size 256
2019-10-01 15:30:11.037156: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d941900 next 860 of size 256
2019-10-01 15:30:11.037161: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d941a00 next 861 of size 256
2019-10-01 15:30:11.037166: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d941b00 next 862 of size 256
2019-10-01 15:30:11.037171: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d941c00 next 863 of size 256
2019-10-01 15:30:11.037176: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d941d00 next 864 of size 256
2019-10-01 15:30:11.037181: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d941e00 next 865 of size 256
2019-10-01 15:30:11.037186: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d941f00 next 866 of size 256
2019-10-01 15:30:11.037191: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d942000 next 867 of size 256
2019-10-01 15:30:11.037196: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d942100 next 868 of size 256
2019-10-01 15:30:11.037205: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d942200 next 869 of size 256
2019-10-01 15:30:11.037213: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d942300 next 870 of size 256
2019-10-01 15:30:11.037222: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d942400 next 871 of size 256
2019-10-01 15:30:11.037230: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d942500 next 872 of size 256
2019-10-01 15:30:11.037238: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d942600 next 873 of size 256
2019-10-01 15:30:11.037243: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d942700 next 874 of size 256
2019-10-01 15:30:11.037248: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d942800 next 875 of size 256
2019-10-01 15:30:11.037253: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d942900 next 876 of size 256
2019-10-01 15:30:11.037258: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d942a00 next 877 of size 256
2019-10-01 15:30:11.037264: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d942b00 next 878 of size 256
2019-10-01 15:30:11.037269: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d942c00 next 879 of size 256
2019-10-01 15:30:11.037274: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d942d00 next 880 of size 256
2019-10-01 15:30:11.037283: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d942e00 next 881 of size 256
2019-10-01 15:30:11.037288: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d942f00 next 882 of size 256
2019-10-01 15:30:11.037293: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d943000 next 883 of size 256
2019-10-01 15:30:11.037298: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d943100 next 884 of size 256
2019-10-01 15:30:11.037304: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d943200 next 885 of size 256
2019-10-01 15:30:11.037309: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d943300 next 886 of size 256
2019-10-01 15:30:11.037314: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d943400 next 887 of size 256
2019-10-01 15:30:11.037319: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d943500 next 888 of size 256
2019-10-01 15:30:11.037324: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d943600 next 889 of size 256
2019-10-01 15:30:11.037329: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d943700 next 890 of size 256
2019-10-01 15:30:11.037334: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d943800 next 891 of size 256
2019-10-01 15:30:11.037340: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d943900 next 892 of size 256
2019-10-01 15:30:11.037345: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d943a00 next 893 of size 256
2019-10-01 15:30:11.037350: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d943b00 next 894 of size 256
2019-10-01 15:30:11.037355: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d943c00 next 895 of size 256
2019-10-01 15:30:11.037361: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d943d00 next 896 of size 256
2019-10-01 15:30:11.037366: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d943e00 next 897 of size 256
2019-10-01 15:30:11.037371: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d943f00 next 898 of size 256
2019-10-01 15:30:11.037376: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d944000 next 899 of size 256
2019-10-01 15:30:11.037381: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d944100 next 900 of size 256
2019-10-01 15:30:11.037386: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d944200 next 901 of size 256
2019-10-01 15:30:11.037391: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d944300 next 902 of size 256
2019-10-01 15:30:11.037396: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d944400 next 903 of size 256
2019-10-01 15:30:11.037401: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d944500 next 904 of size 256
2019-10-01 15:30:11.037406: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d944600 next 905 of size 256
2019-10-01 15:30:11.037411: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d944700 next 906 of size 256
2019-10-01 15:30:11.037416: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d944800 next 907 of size 256
2019-10-01 15:30:11.037421: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d944900 next 908 of size 256
2019-10-01 15:30:11.037426: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d944a00 next 909 of size 256
2019-10-01 15:30:11.037431: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d944b00 next 910 of size 256
2019-10-01 15:30:11.037436: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d944c00 next 911 of size 256
2019-10-01 15:30:11.037441: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d944d00 next 912 of size 256
2019-10-01 15:30:11.037449: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d944e00 next 913 of size 256
2019-10-01 15:30:11.037455: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d944f00 next 914 of size 256
2019-10-01 15:30:11.037460: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d945000 next 915 of size 256
2019-10-01 15:30:11.037465: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d945100 next 916 of size 256
2019-10-01 15:30:11.037470: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d945200 next 917 of size 256
2019-10-01 15:30:11.037475: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d945300 next 918 of size 256
2019-10-01 15:30:11.037480: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d945400 next 919 of size 256
2019-10-01 15:30:11.037485: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d945500 next 920 of size 256
2019-10-01 15:30:11.037490: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d945600 next 921 of size 256
2019-10-01 15:30:11.037496: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d945700 next 922 of size 256
2019-10-01 15:30:11.037501: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d945800 next 923 of size 256
2019-10-01 15:30:11.037507: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d945900 next 924 of size 256
2019-10-01 15:30:11.037512: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d945a00 next 925 of size 256
2019-10-01 15:30:11.037517: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d945b00 next 926 of size 256
2019-10-01 15:30:11.037522: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d945c00 next 927 of size 256
2019-10-01 15:30:11.037527: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d945d00 next 928 of size 256
2019-10-01 15:30:11.037532: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d945e00 next 929 of size 256
2019-10-01 15:30:11.037537: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d945f00 next 930 of size 256
2019-10-01 15:30:11.037542: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d946000 next 931 of size 256
2019-10-01 15:30:11.037547: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d946100 next 932 of size 256
2019-10-01 15:30:11.037552: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d946200 next 933 of size 256
2019-10-01 15:30:11.037557: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d946300 next 934 of size 256
2019-10-01 15:30:11.037562: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d946400 next 935 of size 256
2019-10-01 15:30:11.037567: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d946500 next 936 of size 256
2019-10-01 15:30:11.037572: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d946600 next 937 of size 256
2019-10-01 15:30:11.037577: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d946700 next 938 of size 256
2019-10-01 15:30:11.037582: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d946800 next 939 of size 256
2019-10-01 15:30:11.037587: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d946900 next 940 of size 256
2019-10-01 15:30:11.037592: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d946a00 next 941 of size 256
2019-10-01 15:30:11.037597: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d946b00 next 942 of size 256
2019-10-01 15:30:11.037602: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d946c00 next 943 of size 256
2019-10-01 15:30:11.037607: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d946d00 next 944 of size 256
2019-10-01 15:30:11.037613: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d946e00 next 945 of size 256
2019-10-01 15:30:11.037621: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d946f00 next 946 of size 256
2019-10-01 15:30:11.037627: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d947000 next 947 of size 256
2019-10-01 15:30:11.037632: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d947100 next 948 of size 256
2019-10-01 15:30:11.037637: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d947200 next 949 of size 256
2019-10-01 15:30:11.037642: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d947300 next 950 of size 256
2019-10-01 15:30:11.037647: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d947400 next 951 of size 256
2019-10-01 15:30:11.037652: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d947500 next 952 of size 256
2019-10-01 15:30:11.037657: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d947600 next 953 of size 256
2019-10-01 15:30:11.037662: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d947700 next 954 of size 256
2019-10-01 15:30:11.037667: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d947800 next 955 of size 256
2019-10-01 15:30:11.037674: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d947900 next 956 of size 256
2019-10-01 15:30:11.037679: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d947a00 next 957 of size 256
2019-10-01 15:30:11.037684: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d947b00 next 958 of size 256
2019-10-01 15:30:11.037689: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d947c00 next 959 of size 256
2019-10-01 15:30:11.037694: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d947d00 next 960 of size 256
2019-10-01 15:30:11.037699: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d947e00 next 961 of size 256
2019-10-01 15:30:11.037704: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d947f00 next 962 of size 256
2019-10-01 15:30:11.037709: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d948000 next 963 of size 256
2019-10-01 15:30:11.037714: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d948100 next 964 of size 256
2019-10-01 15:30:11.037719: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d948200 next 965 of size 256
2019-10-01 15:30:11.037724: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d948300 next 966 of size 256
2019-10-01 15:30:11.037729: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d948400 next 967 of size 256
2019-10-01 15:30:11.037734: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d948500 next 968 of size 256
2019-10-01 15:30:11.037739: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d948600 next 969 of size 256
2019-10-01 15:30:11.037744: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d948700 next 970 of size 256
2019-10-01 15:30:11.037749: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d948800 next 971 of size 256
2019-10-01 15:30:11.037754: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d948900 next 972 of size 256
2019-10-01 15:30:11.037759: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d948a00 next 973 of size 256
2019-10-01 15:30:11.037764: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d948b00 next 974 of size 256
2019-10-01 15:30:11.037769: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d948c00 next 975 of size 256
2019-10-01 15:30:11.037774: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d948d00 next 976 of size 256
2019-10-01 15:30:11.037779: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d948e00 next 977 of size 256
2019-10-01 15:30:11.037788: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d948f00 next 978 of size 256
2019-10-01 15:30:11.037793: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d949000 next 979 of size 256
2019-10-01 15:30:11.037798: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d949100 next 980 of size 256
2019-10-01 15:30:11.037803: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d949200 next 981 of size 256
2019-10-01 15:30:11.037809: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d949300 next 982 of size 256
2019-10-01 15:30:11.037814: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d949400 next 983 of size 256
2019-10-01 15:30:11.037819: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d949500 next 984 of size 256
2019-10-01 15:30:11.037824: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d949600 next 985 of size 256
2019-10-01 15:30:11.037829: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d949700 next 986 of size 256
2019-10-01 15:30:11.037834: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d949800 next 987 of size 256
2019-10-01 15:30:11.037840: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d949900 next 988 of size 256
2019-10-01 15:30:11.037845: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d949a00 next 989 of size 256
2019-10-01 15:30:11.037850: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d949b00 next 990 of size 256
2019-10-01 15:30:11.037856: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d949c00 next 991 of size 256
2019-10-01 15:30:11.037861: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d949d00 next 992 of size 256
2019-10-01 15:30:11.037866: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d949e00 next 993 of size 256
2019-10-01 15:30:11.037871: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d949f00 next 994 of size 256
2019-10-01 15:30:11.037876: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94a000 next 995 of size 256
2019-10-01 15:30:11.037881: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94a100 next 996 of size 256
2019-10-01 15:30:11.037886: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94a200 next 997 of size 256
2019-10-01 15:30:11.037891: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94a300 next 998 of size 256
2019-10-01 15:30:11.037896: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94a400 next 999 of size 256
2019-10-01 15:30:11.037901: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94a500 next 1000 of size 256
2019-10-01 15:30:11.037906: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94a600 next 1001 of size 256
2019-10-01 15:30:11.037911: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94a700 next 1002 of size 256
2019-10-01 15:30:11.037916: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94a800 next 1003 of size 256
2019-10-01 15:30:11.037921: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94a900 next 1004 of size 256
2019-10-01 15:30:11.037926: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94aa00 next 1005 of size 256
2019-10-01 15:30:11.037931: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94ab00 next 1006 of size 256
2019-10-01 15:30:11.037936: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94ac00 next 1007 of size 256
2019-10-01 15:30:11.037941: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94ad00 next 1008 of size 256
2019-10-01 15:30:11.037947: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94ae00 next 1009 of size 256
2019-10-01 15:30:11.037952: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94af00 next 1010 of size 256
2019-10-01 15:30:11.037959: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94b000 next 1011 of size 256
2019-10-01 15:30:11.037965: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94b100 next 1012 of size 256
2019-10-01 15:30:11.037970: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94b200 next 1013 of size 256
2019-10-01 15:30:11.037975: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94b300 next 1014 of size 256
2019-10-01 15:30:11.037980: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94b400 next 1015 of size 256
2019-10-01 15:30:11.037985: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94b500 next 1016 of size 256
2019-10-01 15:30:11.037990: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94b600 next 1017 of size 256
2019-10-01 15:30:11.037995: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94b700 next 1018 of size 256
2019-10-01 15:30:11.038000: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94b800 next 1019 of size 256
2019-10-01 15:30:11.038007: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94b900 next 1020 of size 256
2019-10-01 15:30:11.038012: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94ba00 next 1021 of size 256
2019-10-01 15:30:11.038017: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94bb00 next 1022 of size 256
2019-10-01 15:30:11.038022: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94bc00 next 1023 of size 256
2019-10-01 15:30:11.038027: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94bd00 next 1024 of size 256
2019-10-01 15:30:11.038032: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94be00 next 1025 of size 256
2019-10-01 15:30:11.038037: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94bf00 next 1026 of size 256
2019-10-01 15:30:11.038042: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94c000 next 1027 of size 256
2019-10-01 15:30:11.038047: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94c100 next 1028 of size 256
2019-10-01 15:30:11.038052: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94c200 next 1029 of size 256
2019-10-01 15:30:11.038057: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94c300 next 1030 of size 256
2019-10-01 15:30:11.038062: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94c400 next 1031 of size 256
2019-10-01 15:30:11.038067: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94c500 next 1032 of size 256
2019-10-01 15:30:11.038073: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94c600 next 1033 of size 256
2019-10-01 15:30:11.038078: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94c700 next 1034 of size 256
2019-10-01 15:30:11.038083: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94c800 next 1035 of size 256
2019-10-01 15:30:11.038088: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94c900 next 1036 of size 256
2019-10-01 15:30:11.038093: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94ca00 next 1037 of size 256
2019-10-01 15:30:11.038098: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94cb00 next 1038 of size 256
2019-10-01 15:30:11.038103: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94cc00 next 1039 of size 256
2019-10-01 15:30:11.038108: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94cd00 next 1040 of size 256
2019-10-01 15:30:11.038113: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94ce00 next 1041 of size 256
2019-10-01 15:30:11.038118: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94cf00 next 1042 of size 256
2019-10-01 15:30:11.038126: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94d000 next 1043 of size 256
2019-10-01 15:30:11.038131: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94d100 next 1044 of size 256
2019-10-01 15:30:11.038136: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94d200 next 1045 of size 256
2019-10-01 15:30:11.038141: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94d300 next 1046 of size 256
2019-10-01 15:30:11.038146: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94d400 next 1047 of size 256
2019-10-01 15:30:11.038151: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94d500 next 1048 of size 256
2019-10-01 15:30:11.038157: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94d600 next 1049 of size 256
2019-10-01 15:30:11.038162: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94d700 next 1050 of size 256
2019-10-01 15:30:11.038167: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94d800 next 1051 of size 256
2019-10-01 15:30:11.038173: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94d900 next 1052 of size 256
2019-10-01 15:30:11.038179: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94da00 next 1053 of size 67108864
2019-10-01 15:30:11.038184: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x138194da00 next 1054 of size 8192
2019-10-01 15:30:11.038189: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x138194fa00 next 1055 of size 256
2019-10-01 15:30:11.038194: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x138194fb00 next 1056 of size 8192
2019-10-01 15:30:11.038199: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1381951b00 next 1057 of size 8192
2019-10-01 15:30:11.038204: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1381953b00 next 1058 of size 256
2019-10-01 15:30:11.038209: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1381953c00 next 1059 of size 8192
2019-10-01 15:30:11.038214: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1381955c00 next 1060 of size 256
2019-10-01 15:30:11.038219: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1381955d00 next 1061 of size 16777216
2019-10-01 15:30:11.038225: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1382955d00 next 1062 of size 256
2019-10-01 15:30:11.038230: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1382955e00 next 1063 of size 256
2019-10-01 15:30:11.038235: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1382955f00 next 1064 of size 256
2019-10-01 15:30:11.038240: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1382956000 next 1065 of size 16777216
2019-10-01 15:30:11.038245: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1383956000 next 1066 of size 8192
2019-10-01 15:30:11.038250: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1383958000 next 1067 of size 256
2019-10-01 15:30:11.038255: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1383958100 next 1068 of size 8192
2019-10-01 15:30:11.038261: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x138395a100 next 1069 of size 16777216
2019-10-01 15:30:11.038266: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x138495a100 next 1070 of size 256
2019-10-01 15:30:11.038271: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x138495a200 next 1071 of size 256
2019-10-01 15:30:11.038276: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x138495a300 next 1072 of size 256
2019-10-01 15:30:11.038281: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x138495a400 next 1073 of size 16777216
2019-10-01 15:30:11.038286: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x138595a400 next 1074 of size 67108864
2019-10-01 15:30:11.038293: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x138995a400 next 1075 of size 256
2019-10-01 15:30:11.038299: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x138995a500 next 1076 of size 256
2019-10-01 15:30:11.038304: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x138995a600 next 1077 of size 8192
2019-10-01 15:30:11.038309: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x138995c600 next 1078 of size 256
2019-10-01 15:30:11.038314: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x138995c700 next 1079 of size 8192
2019-10-01 15:30:11.038319: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x138995e700 next 1080 of size 256
2019-10-01 15:30:11.038324: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x138995e800 next 1081 of size 67108864
2019-10-01 15:30:11.038329: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x138d95e800 next 1082 of size 8192
2019-10-01 15:30:11.038334: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x138d960800 next 1083 of size 256
2019-10-01 15:30:11.038341: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x138d960900 next 1084 of size 8192
2019-10-01 15:30:11.038346: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x138d962900 next 1085 of size 8192
2019-10-01 15:30:11.038351: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x138d964900 next 1086 of size 256
2019-10-01 15:30:11.038356: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x138d964a00 next 1087 of size 8192
2019-10-01 15:30:11.038361: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x138d966a00 next 1088 of size 8192
2019-10-01 15:30:11.038366: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x138d968a00 next 1089 of size 256
2019-10-01 15:30:11.038371: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x138d968b00 next 1090 of size 8192
2019-10-01 15:30:11.038376: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x138d96ab00 next 1091 of size 8192
2019-10-01 15:30:11.038381: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x138d96cb00 next 1092 of size 256
2019-10-01 15:30:11.038386: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x138d96cc00 next 1093 of size 8192
2019-10-01 15:30:11.038391: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x138d96ec00 next 1094 of size 16777216
2019-10-01 15:30:11.038396: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x138e96ec00 next 1095 of size 256
2019-10-01 15:30:11.038401: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x138e96ed00 next 1096 of size 256
2019-10-01 15:30:11.038407: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x138e96ee00 next 1097 of size 256
2019-10-01 15:30:11.038412: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x138e96ef00 next 1098 of size 256
2019-10-01 15:30:11.038417: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x138e96f000 next 1099 of size 256
2019-10-01 15:30:11.038422: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x138e96f100 next 1100 of size 256
2019-10-01 15:30:11.038427: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x138e96f200 next 1101 of size 256
2019-10-01 15:30:11.038432: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x138e96f300 next 1102 of size 16777216
2019-10-01 15:30:11.038437: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x138f96f300 next 1103 of size 16777216
2019-10-01 15:30:11.038442: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x139096f300 next 1104 of size 256
2019-10-01 15:30:11.038447: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x139096f400 next 1105 of size 256
2019-10-01 15:30:11.038452: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x139096f500 next 1106 of size 16777216
2019-10-01 15:30:11.038460: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x139196f500 next 1107 of size 16777216
2019-10-01 15:30:11.038465: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x139296f500 next 1108 of size 256
2019-10-01 15:30:11.038470: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x139296f600 next 1109 of size 256
2019-10-01 15:30:11.038475: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x139296f700 next 1110 of size 16777216
2019-10-01 15:30:11.038480: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x139396f700 next 1111 of size 8192
2019-10-01 15:30:11.038485: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1393971700 next 1112 of size 256
2019-10-01 15:30:11.038490: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1393971800 next 1113 of size 8192
2019-10-01 15:30:11.038495: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1393973800 next 1114 of size 8192
2019-10-01 15:30:11.038500: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1393975800 next 1115 of size 256
2019-10-01 15:30:11.038507: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1393975900 next 1116 of size 8192
2019-10-01 15:30:11.038512: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1393977900 next 1117 of size 8192
2019-10-01 15:30:11.038517: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1393979900 next 1118 of size 256
2019-10-01 15:30:11.038522: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1393979a00 next 1119 of size 8192
2019-10-01 15:30:11.038527: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x139397ba00 next 1120 of size 8192
2019-10-01 15:30:11.038532: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x139397da00 next 1121 of size 256
2019-10-01 15:30:11.038559: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x139397db00 next 1122 of size 8192
2019-10-01 15:30:11.038569: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x139397fb00 next 1123 of size 8192
2019-10-01 15:30:11.038577: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1393981b00 next 1124 of size 256
2019-10-01 15:30:11.038585: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1393981c00 next 1125 of size 8192
2019-10-01 15:30:11.038593: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1393983c00 next 1126 of size 8192
2019-10-01 15:30:11.038602: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1393985c00 next 1127 of size 256
2019-10-01 15:30:11.038609: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1393985d00 next 1128 of size 8192
2019-10-01 15:30:11.038617: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1393987d00 next 1129 of size 8192
2019-10-01 15:30:11.038625: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1393989d00 next 1130 of size 256
2019-10-01 15:30:11.038633: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1393989e00 next 1131 of size 8192
2019-10-01 15:30:11.038640: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x139398be00 next 1132 of size 8192
2019-10-01 15:30:11.038648: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x139398de00 next 1133 of size 256
2019-10-01 15:30:11.038655: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x139398df00 next 1134 of size 8192
2019-10-01 15:30:11.038663: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x139398ff00 next 1135 of size 512
2019-10-01 15:30:11.038671: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1393990100 next 1136 of size 256
2019-10-01 15:30:11.038678: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1393990200 next 1137 of size 512
2019-10-01 15:30:11.038686: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1393990400 next 1138 of size 67108864
2019-10-01 15:30:11.038700: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1397990400 next 1139 of size 256
2019-10-01 15:30:11.038709: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1397990500 next 1140 of size 256
2019-10-01 15:30:11.038718: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1397990600 next 1141 of size 16777216
2019-10-01 15:30:11.038726: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1398990600 next 1142 of size 256
2019-10-01 15:30:11.038734: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1398990700 next 1143 of size 256
2019-10-01 15:30:11.038739: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1398990800 next 1144 of size 16777216
2019-10-01 15:30:11.038744: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1399990800 next 1145 of size 256
2019-10-01 15:30:11.038750: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1399990900 next 1146 of size 67108864
2019-10-01 15:30:11.038755: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x139d990900 next 1147 of size 16777216
2019-10-01 15:30:11.038762: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x139e990900 next 1148 of size 256
2019-10-01 15:30:11.038767: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x139e990a00 next 1149 of size 256
2019-10-01 15:30:11.038772: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x139e990b00 next 1150 of size 256
2019-10-01 15:30:11.038777: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x139e990c00 next 1151 of size 256
2019-10-01 15:30:11.038782: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x139e990d00 next 1152 of size 256
2019-10-01 15:30:11.038788: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x139e990e00 next 1153 of size 256
2019-10-01 15:30:11.038793: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x139e990f00 next 1154 of size 256
2019-10-01 15:30:11.038798: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x139e991000 next 1155 of size 8192
2019-10-01 15:30:11.038803: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x139e993000 next 1156 of size 256
2019-10-01 15:30:11.038808: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x139e993100 next 1157 of size 8192
2019-10-01 15:30:11.038813: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x139e995100 next 1158 of size 256
2019-10-01 15:30:11.038818: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x139e995200 next 1159 of size 16777216
2019-10-01 15:30:11.038823: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x139f995200 next 1160 of size 256
2019-10-01 15:30:11.038829: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x139f995300 next 1161 of size 256
2019-10-01 15:30:11.038834: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x139f995400 next 1162 of size 256
2019-10-01 15:30:11.038839: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x139f995500 next 1163 of size 16777216
2019-10-01 15:30:11.038845: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13a0995500 next 1164 of size 256
2019-10-01 15:30:11.038850: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13a0995600 next 1165 of size 8192
2019-10-01 15:30:11.038855: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13a0997600 next 1166 of size 256
2019-10-01 15:30:11.038860: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13a0997700 next 1167 of size 8192
2019-10-01 15:30:11.038865: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13a0999700 next 1168 of size 16777216
2019-10-01 15:30:11.038870: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13a1999700 next 1169 of size 256
2019-10-01 15:30:11.038875: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13a1999800 next 1170 of size 16777216
2019-10-01 15:30:11.038883: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13a2999800 next 1171 of size 16777216
2019-10-01 15:30:11.038889: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13a3999800 next 1172 of size 32768
2019-10-01 15:30:11.038894: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13a39a1800 next 1173 of size 256
2019-10-01 15:30:11.038900: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13a39a1900 next 1174 of size 32768
2019-10-01 15:30:11.038905: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13a39a9900 next 1175 of size 16777216
2019-10-01 15:30:11.038910: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13a49a9900 next 1176 of size 256
2019-10-01 15:30:11.038915: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13a49a9a00 next 1177 of size 256
2019-10-01 15:30:11.038920: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13a49a9b00 next 1178 of size 256
2019-10-01 15:30:11.038926: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13a49a9c00 next 1179 of size 8192
2019-10-01 15:30:11.038932: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13a49abc00 next 1180 of size 256
2019-10-01 15:30:11.038937: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13a49abd00 next 1181 of size 8192
2019-10-01 15:30:11.038942: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13a49add00 next 1182 of size 8192
2019-10-01 15:30:11.038947: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13a49afd00 next 1183 of size 256
2019-10-01 15:30:11.038953: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13a49afe00 next 1184 of size 8192
2019-10-01 15:30:11.038958: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13a49b1e00 next 1185 of size 16777216
2019-10-01 15:30:11.038963: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13a59b1e00 next 1186 of size 256
2019-10-01 15:30:11.038968: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13a59b1f00 next 1187 of size 16777216
2019-10-01 15:30:11.038974: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13a69b1f00 next 1188 of size 256
2019-10-01 15:30:11.038979: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13a69b2000 next 1189 of size 16777216
2019-10-01 15:30:11.038984: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13a79b2000 next 1190 of size 8192
2019-10-01 15:30:11.038989: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13a79b4000 next 1191 of size 256
2019-10-01 15:30:11.038995: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13a79b4100 next 1192 of size 8192
2019-10-01 15:30:11.039000: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13a79b6100 next 1193 of size 16777216
2019-10-01 15:30:11.039005: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13a89b6100 next 1194 of size 256
2019-10-01 15:30:11.039010: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13a89b6200 next 1195 of size 256
2019-10-01 15:30:11.039015: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13a89b6300 next 1196 of size 8192
2019-10-01 15:30:11.039021: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13a89b8300 next 1197 of size 256
2019-10-01 15:30:11.039026: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13a89b8400 next 1198 of size 8192
2019-10-01 15:30:11.039031: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13a89ba400 next 1199 of size 8192
2019-10-01 15:30:11.039036: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13a89bc400 next 1200 of size 256
2019-10-01 15:30:11.039041: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13a89bc500 next 1201 of size 8192
2019-10-01 15:30:11.039046: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13a89be500 next 1202 of size 67108864
2019-10-01 15:30:11.039055: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13ac9be500 next 1203 of size 256
2019-10-01 15:30:11.039060: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13ac9be600 next 1204 of size 16777216
2019-10-01 15:30:11.039065: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13ad9be600 next 1205 of size 256
2019-10-01 15:30:11.039070: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13ad9be700 next 1206 of size 67108864
2019-10-01 15:30:11.039075: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b19be700 next 1207 of size 8192
2019-10-01 15:30:11.039081: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b19c0700 next 1208 of size 256
2019-10-01 15:30:11.039086: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b19c0800 next 1209 of size 8192
2019-10-01 15:30:11.039091: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b19c2800 next 1210 of size 512
2019-10-01 15:30:11.039096: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b19c2a00 next 1211 of size 256
2019-10-01 15:30:11.039103: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b19c2b00 next 1212 of size 512
2019-10-01 15:30:11.039108: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b19c2d00 next 1213 of size 8192
2019-10-01 15:30:11.039113: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b19c4d00 next 1214 of size 256
2019-10-01 15:30:11.039118: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b19c4e00 next 1215 of size 8192
2019-10-01 15:30:11.039123: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b19c6e00 next 1216 of size 8192
2019-10-01 15:30:11.039129: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b19c8e00 next 1217 of size 256
2019-10-01 15:30:11.039134: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b19c8f00 next 1218 of size 8192
2019-10-01 15:30:11.039139: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b19caf00 next 1219 of size 16777216
2019-10-01 15:30:11.039144: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b29caf00 next 1220 of size 256
2019-10-01 15:30:11.039149: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b29cb000 next 1221 of size 256
2019-10-01 15:30:11.039154: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b29cb100 next 1222 of size 256
2019-10-01 15:30:11.039159: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b29cb200 next 1223 of size 256
2019-10-01 15:30:11.039164: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b29cb300 next 1224 of size 256
2019-10-01 15:30:11.039169: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b29cb400 next 1225 of size 256
2019-10-01 15:30:11.039174: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b29cb500 next 1226 of size 256
2019-10-01 15:30:11.039179: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b29cb600 next 1227 of size 16777216
2019-10-01 15:30:11.039185: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b39cb600 next 1228 of size 16777216
2019-10-01 15:30:11.039190: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b49cb600 next 1229 of size 256
2019-10-01 15:30:11.039195: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b49cb700 next 1230 of size 256
2019-10-01 15:30:11.039200: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b49cb800 next 1231 of size 256
2019-10-01 15:30:11.039205: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b49cb900 next 1232 of size 256
2019-10-01 15:30:11.039211: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b49cba00 next 1233 of size 16777216
2019-10-01 15:30:11.039216: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b59cba00 next 1234 of size 8192
2019-10-01 15:30:11.039224: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b59cda00 next 1235 of size 256
2019-10-01 15:30:11.039229: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b59cdb00 next 1236 of size 8192
2019-10-01 15:30:11.039234: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b59cfb00 next 1237 of size 8192
2019-10-01 15:30:11.039239: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b59d1b00 next 1238 of size 256
2019-10-01 15:30:11.039245: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b59d1c00 next 1239 of size 8192
2019-10-01 15:30:11.039250: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b59d3c00 next 1240 of size 16777216
2019-10-01 15:30:11.039255: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b69d3c00 next 1241 of size 256
2019-10-01 15:30:11.039260: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b69d3d00 next 1242 of size 256
2019-10-01 15:30:11.039265: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b69d3e00 next 1243 of size 256
2019-10-01 15:30:11.039271: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b69d3f00 next 1244 of size 256
2019-10-01 15:30:11.039277: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b69d4000 next 1245 of size 16777216
2019-10-01 15:30:11.039282: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b79d4000 next 1246 of size 8192
2019-10-01 15:30:11.039287: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b79d6000 next 1247 of size 256
2019-10-01 15:30:11.039292: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b79d6100 next 1248 of size 8192
2019-10-01 15:30:11.039297: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b79d8100 next 1249 of size 16777216
2019-10-01 15:30:11.039302: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b89d8100 next 1250 of size 256
2019-10-01 15:30:11.039307: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b89d8200 next 1251 of size 256
2019-10-01 15:30:11.039312: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b89d8300 next 1252 of size 256
2019-10-01 15:30:11.039317: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b89d8400 next 1253 of size 256
2019-10-01 15:30:11.039322: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b89d8500 next 1254 of size 16777216
2019-10-01 15:30:11.039328: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b99d8500 next 1255 of size 8192
2019-10-01 15:30:11.039333: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b99da500 next 1256 of size 256
2019-10-01 15:30:11.039338: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b99da600 next 1257 of size 8192
2019-10-01 15:30:11.039343: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b99dc600 next 1258 of size 16777216
2019-10-01 15:30:11.039348: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13ba9dc600 next 1259 of size 256
2019-10-01 15:30:11.039353: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13ba9dc700 next 1260 of size 256
2019-10-01 15:30:11.039358: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13ba9dc800 next 1261 of size 256
2019-10-01 15:30:11.039363: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13ba9dc900 next 1262 of size 16777216
2019-10-01 15:30:11.039368: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13bb9dc900 next 1263 of size 67108864
2019-10-01 15:30:11.039373: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13bf9dc900 next 1264 of size 256
2019-10-01 15:30:11.039378: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13bf9dca00 next 1265 of size 256
2019-10-01 15:30:11.039383: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13bf9dcb00 next 1266 of size 16777216
2019-10-01 15:30:11.039391: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13c09dcb00 next 1267 of size 256
2019-10-01 15:30:11.039396: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13c09dcc00 next 1268 of size 256
2019-10-01 15:30:11.039401: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13c09dcd00 next 1269 of size 256
2019-10-01 15:30:11.039406: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13c09dce00 next 1270 of size 256
2019-10-01 15:30:11.039412: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13c09dcf00 next 1271 of size 16777216
2019-10-01 15:30:11.039417: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13c19dcf00 next 1272 of size 256
2019-10-01 15:30:11.039422: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13c19dd000 next 1273 of size 67108864
2019-10-01 15:30:11.039427: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13c59dd000 next 1274 of size 512
2019-10-01 15:30:11.039432: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13c59dd200 next 1275 of size 256
2019-10-01 15:30:11.039438: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13c59dd300 next 1276 of size 512
2019-10-01 15:30:11.039444: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13c59dd500 next 1277 of size 16777216
2019-10-01 15:30:11.039449: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13c69dd500 next 1278 of size 256
2019-10-01 15:30:11.039454: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13c69dd600 next 1279 of size 256
2019-10-01 15:30:11.039459: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13c69dd700 next 1280 of size 16777216
2019-10-01 15:30:11.039464: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13c79dd700 next 1281 of size 8192
2019-10-01 15:30:11.039469: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13c79df700 next 1282 of size 256
2019-10-01 15:30:11.039474: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13c79df800 next 1283 of size 8192
2019-10-01 15:30:11.039479: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13c79e1800 next 1284 of size 8192
2019-10-01 15:30:11.039484: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13c79e3800 next 1285 of size 256
2019-10-01 15:30:11.039489: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13c79e3900 next 1286 of size 8192
2019-10-01 15:30:11.039494: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13c79e5900 next 1287 of size 512
2019-10-01 15:30:11.039499: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13c79e5b00 next 1288 of size 256
2019-10-01 15:30:11.039504: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13c79e5c00 next 1289 of size 512
2019-10-01 15:30:11.039509: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13c79e5e00 next 1290 of size 16777216
2019-10-01 15:30:11.039514: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13c89e5e00 next 1291 of size 256
2019-10-01 15:30:11.039520: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13c89e5f00 next 1292 of size 256
2019-10-01 15:30:11.039525: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13c89e6000 next 1293 of size 256
2019-10-01 15:30:11.039530: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13c89e6100 next 1294 of size 256
2019-10-01 15:30:11.039535: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13c89e6200 next 1295 of size 256
2019-10-01 15:30:11.039541: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13c89e6300 next 1296 of size 16777216
2019-10-01 15:30:11.039546: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13c99e6300 next 1297 of size 67108864
2019-10-01 15:30:11.039551: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13cd9e6300 next 1298 of size 256
2019-10-01 15:30:11.039558: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13cd9e6400 next 1299 of size 256
2019-10-01 15:30:11.039564: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13cd9e6500 next 1300 of size 256
2019-10-01 15:30:11.039569: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13cd9e6600 next 1301 of size 256
2019-10-01 15:30:11.039574: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13cd9e6700 next 1302 of size 67108864
2019-10-01 15:30:11.039579: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d19e6700 next 1303 of size 16777216
2019-10-01 15:30:11.039584: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d29e6700 next 1304 of size 256
2019-10-01 15:30:11.039589: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d29e6800 next 1305 of size 256
2019-10-01 15:30:11.039595: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d29e6900 next 1306 of size 256
2019-10-01 15:30:11.039600: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d29e6a00 next 1307 of size 16777216
2019-10-01 15:30:11.039606: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d39e6a00 next 1308 of size 8192
2019-10-01 15:30:11.039611: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d39e8a00 next 1309 of size 256
2019-10-01 15:30:11.039616: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d39e8b00 next 1310 of size 8192
2019-10-01 15:30:11.039621: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d39eab00 next 1311 of size 8192
2019-10-01 15:30:11.039626: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d39ecb00 next 1312 of size 256
2019-10-01 15:30:11.039631: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d39ecc00 next 1313 of size 8192
2019-10-01 15:30:11.039636: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d39eec00 next 1314 of size 8192
2019-10-01 15:30:11.039641: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d39f0c00 next 1315 of size 256
2019-10-01 15:30:11.039646: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d39f0d00 next 1316 of size 8192
2019-10-01 15:30:11.039651: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d39f2d00 next 1317 of size 16777216
2019-10-01 15:30:11.039657: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d49f2d00 next 1318 of size 256
2019-10-01 15:30:11.039662: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d49f2e00 next 1319 of size 256
2019-10-01 15:30:11.039667: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d49f2f00 next 1320 of size 256
2019-10-01 15:30:11.039672: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d49f3000 next 1321 of size 256
2019-10-01 15:30:11.039677: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d49f3100 next 1322 of size 256
2019-10-01 15:30:11.039682: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d49f3200 next 1323 of size 16777216
2019-10-01 15:30:11.039687: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d59f3200 next 1324 of size 8192
2019-10-01 15:30:11.039692: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d59f5200 next 1325 of size 256
2019-10-01 15:30:11.039697: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d59f5300 next 1326 of size 256
2019-10-01 15:30:11.039702: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d59f5400 next 1327 of size 8192
2019-10-01 15:30:11.039708: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d59f7400 next 1328 of size 8192
2019-10-01 15:30:11.039713: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d59f9400 next 1329 of size 256
2019-10-01 15:30:11.039718: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d59f9500 next 1330 of size 8192
2019-10-01 15:30:11.039725: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d59fb500 next 1331 of size 512
2019-10-01 15:30:11.039730: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d59fb700 next 1332 of size 256
2019-10-01 15:30:11.039735: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d59fb800 next 1333 of size 512
2019-10-01 15:30:11.039741: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d59fba00 next 1334 of size 16777216
2019-10-01 15:30:11.039746: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d69fba00 next 1335 of size 256
2019-10-01 15:30:11.039751: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d69fbb00 next 1336 of size 256
2019-10-01 15:30:11.039756: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d69fbc00 next 1337 of size 256
2019-10-01 15:30:11.039761: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d69fbd00 next 1338 of size 256
2019-10-01 15:30:11.039766: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d69fbe00 next 1339 of size 256
2019-10-01 15:30:11.039772: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d69fbf00 next 1340 of size 8192
2019-10-01 15:30:11.039777: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d69fdf00 next 1341 of size 8192
2019-10-01 15:30:11.039782: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d69fff00 next 1342 of size 8192
2019-10-01 15:30:11.039788: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d6a01f00 next 1343 of size 16384000
2019-10-01 15:30:11.039793: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d79a1f00 next 1344 of size 256
2019-10-01 15:30:11.039798: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d79a2000 next 1345 of size 16384000
2019-10-01 15:30:11.039803: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d8942000 next 1346 of size 16777216
2019-10-01 15:30:11.039808: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d9942000 next 1347 of size 8192
2019-10-01 15:30:11.039813: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d9944000 next 1348 of size 8192
2019-10-01 15:30:11.039818: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d9946000 next 1349 of size 8192
2019-10-01 15:30:11.039823: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d9948000 next 1350 of size 8192
2019-10-01 15:30:11.039828: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d994a000 next 1351 of size 8192
2019-10-01 15:30:11.039833: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d994c000 next 1352 of size 8192
2019-10-01 15:30:11.039838: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d994e000 next 1353 of size 8192
2019-10-01 15:30:11.039843: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d9950000 next 1354 of size 8192
2019-10-01 15:30:11.039848: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d9952000 next 1355 of size 8192
2019-10-01 15:30:11.039853: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d9954000 next 1356 of size 256
2019-10-01 15:30:11.039858: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d9954100 next 1357 of size 8192
2019-10-01 15:30:11.039864: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d9956100 next 1358 of size 8192
2019-10-01 15:30:11.039869: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d9958100 next 1359 of size 256
2019-10-01 15:30:11.039874: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d9958200 next 1360 of size 8192
2019-10-01 15:30:11.039879: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d995a200 next 1361 of size 8192
2019-10-01 15:30:11.039887: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d995c200 next 1362 of size 256
2019-10-01 15:30:11.039892: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d995c300 next 1363 of size 8192
2019-10-01 15:30:11.039897: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d995e300 next 1364 of size 8192
2019-10-01 15:30:11.039902: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d9960300 next 1365 of size 256
2019-10-01 15:30:11.039907: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d9960400 next 1366 of size 8192
2019-10-01 15:30:11.039912: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d9962400 next 1367 of size 8192
2019-10-01 15:30:11.039917: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d9964400 next 1368 of size 256
2019-10-01 15:30:11.039922: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d9964500 next 1369 of size 8192
2019-10-01 15:30:11.039927: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d9966500 next 1370 of size 8192
2019-10-01 15:30:11.039932: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d9968500 next 1371 of size 256
2019-10-01 15:30:11.039938: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d9968600 next 1372 of size 8192
2019-10-01 15:30:11.039944: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d996a600 next 1373 of size 8192
2019-10-01 15:30:11.039949: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d996c600 next 1374 of size 256
2019-10-01 15:30:11.039954: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d996c700 next 1375 of size 8192
2019-10-01 15:30:11.039959: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d996e700 next 1376 of size 67108864
2019-10-01 15:30:11.039964: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13dd96e700 next 1377 of size 67108864
2019-10-01 15:30:11.039969: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13e196e700 next 1378 of size 256
2019-10-01 15:30:11.039974: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13e196e800 next 1379 of size 512
2019-10-01 15:30:11.039979: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13e196ea00 next 1380 of size 16384000
2019-10-01 15:30:11.039984: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13e290ea00 next 1381 of size 16777216
2019-10-01 15:30:11.039989: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13e390ea00 next 1382 of size 256
2019-10-01 15:30:11.039994: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13e390eb00 next 1383 of size 16384000
2019-10-01 15:30:11.039999: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13e48aeb00 next 1384 of size 16777216
2019-10-01 15:30:11.040004: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13e58aeb00 next 1385 of size 16777216
2019-10-01 15:30:11.040009: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13e68aeb00 next 1386 of size 16384000
2019-10-01 15:30:11.040014: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13e784eb00 next 1387 of size 67108864
2019-10-01 15:30:11.040020: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13eb84eb00 next 1388 of size 8192
2019-10-01 15:30:11.040025: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13eb850b00 next 1389 of size 256
2019-10-01 15:30:11.040030: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13eb850c00 next 1390 of size 8192
2019-10-01 15:30:11.040035: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13eb852c00 next 1391 of size 8192
2019-10-01 15:30:11.040040: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13eb854c00 next 1392 of size 256
2019-10-01 15:30:11.040045: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13eb854d00 next 1393 of size 8192
2019-10-01 15:30:11.040052: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13eb856d00 next 1394 of size 512
2019-10-01 15:30:11.040058: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13eb856f00 next 1395 of size 256
2019-10-01 15:30:11.040063: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13eb857000 next 1396 of size 512
2019-10-01 15:30:11.040068: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13eb857200 next 1397 of size 16777216
2019-10-01 15:30:11.040073: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13ec857200 next 1398 of size 16384000
2019-10-01 15:30:11.040078: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13ed7f7200 next 1399 of size 16384000
2019-10-01 15:30:11.040083: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13ee797200 next 1400 of size 16384000
2019-10-01 15:30:11.040088: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13ef737200 next 1401 of size 256
2019-10-01 15:30:11.040094: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13ef737300 next 1402 of size 16777216
2019-10-01 15:30:11.040099: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13f0737300 next 1403 of size 16777216
2019-10-01 15:30:11.040105: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13f1737300 next 1404 of size 67108864
2019-10-01 15:30:11.040110: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13f5737300 next 1405 of size 67108864
2019-10-01 15:30:11.040115: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13f9737300 next 1406 of size 8192
2019-10-01 15:30:11.040120: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13f9739300 next 1407 of size 8192
2019-10-01 15:30:11.040125: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13f973b300 next 1408 of size 8192
2019-10-01 15:30:11.040130: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13f973d300 next 1409 of size 16777216
2019-10-01 15:30:11.040135: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13fa73d300 next 1410 of size 8192
2019-10-01 15:30:11.040140: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13fa73f300 next 1411 of size 8192
2019-10-01 15:30:11.040145: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13fa741300 next 1412 of size 8192
2019-10-01 15:30:11.040150: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13fa743300 next 1413 of size 16384000
2019-10-01 15:30:11.040155: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13fb6e3300 next 1414 of size 16384000
2019-10-01 15:30:11.040160: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13fc683300 next 1415 of size 256
2019-10-01 15:30:11.040165: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13fc683400 next 1416 of size 67108864
2019-10-01 15:30:11.040170: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1400683400 next 1417 of size 16384000
2019-10-01 15:30:11.040175: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1401623400 next 1418 of size 8192
2019-10-01 15:30:11.040180: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1401625400 next 1419 of size 8192
2019-10-01 15:30:11.040185: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1401627400 next 1420 of size 8192
2019-10-01 15:30:11.040190: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1401629400 next 1421 of size 8192
2019-10-01 15:30:11.040195: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x140162b400 next 1422 of size 67108864
2019-10-01 15:30:11.040200: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x140562b400 next 1423 of size 67108864
2019-10-01 15:30:11.040205: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x140962b400 next 1424 of size 16777216
2019-10-01 15:30:11.040210: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x140a62b400 next 1425 of size 256
2019-10-01 15:30:11.040219: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x140a62b500 next 1426 of size 16777216
2019-10-01 15:30:11.040224: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x140b62b500 next 1427 of size 67108864
2019-10-01 15:30:11.040229: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x140f62b500 next 1428 of size 16777216
2019-10-01 15:30:11.040234: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x141062b500 next 1429 of size 67108864
2019-10-01 15:30:11.040240: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x141462b500 next 1430 of size 16777216
2019-10-01 15:30:11.040245: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x141562b500 next 1431 of size 8192
2019-10-01 15:30:11.040250: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x141562d500 next 1432 of size 32768
2019-10-01 15:30:11.040255: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1415635500 next 1433 of size 512
2019-10-01 15:30:11.040260: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1415635700 next 1434 of size 8192
2019-10-01 15:30:11.040265: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1415637700 next 1435 of size 8192
2019-10-01 15:30:11.040272: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1415639700 next 1436 of size 32768
2019-10-01 15:30:11.040277: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1415641700 next 1437 of size 8192
2019-10-01 15:30:11.040282: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1415643700 next 1438 of size 8192
2019-10-01 15:30:11.040287: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1415645700 next 1439 of size 8192
2019-10-01 15:30:11.040292: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1415647700 next 1440 of size 8192
2019-10-01 15:30:11.040297: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1415649700 next 1441 of size 8192
2019-10-01 15:30:11.040302: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x141564b700 next 1442 of size 8192
2019-10-01 15:30:11.040307: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x141564d700 next 1443 of size 256
2019-10-01 15:30:11.040312: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x141564d800 next 1444 of size 67108864
2019-10-01 15:30:11.040317: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x141964d800 next 1445 of size 8192
2019-10-01 15:30:11.040322: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x141964f800 next 1446 of size 8192
2019-10-01 15:30:11.040327: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1419651800 next 1447 of size 16777216
2019-10-01 15:30:11.040332: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x141a651800 next 1448 of size 32768
2019-10-01 15:30:11.040338: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x141a659800 next 1449 of size 256
2019-10-01 15:30:11.040343: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x141a659900 next 1450 of size 32768
2019-10-01 15:30:11.040348: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x141a661900 next 1451 of size 16777216
2019-10-01 15:30:11.040353: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x141b661900 next 1452 of size 67108864
2019-10-01 15:30:11.040358: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x141f661900 next 1453 of size 512
2019-10-01 15:30:11.040363: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x141f661b00 next 1454 of size 16777216
2019-10-01 15:30:11.040368: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1420661b00 next 1455 of size 16777216
2019-10-01 15:30:11.040374: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1421661b00 next 1456 of size 8192
2019-10-01 15:30:11.040381: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1421663b00 next 1457 of size 8192
2019-10-01 15:30:11.040386: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1421665b00 next 1458 of size 256
2019-10-01 15:30:11.040391: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1421665c00 next 1459 of size 67108864
2019-10-01 15:30:11.040396: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1425665c00 next 1460 of size 8192
2019-10-01 15:30:11.040401: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1425667c00 next 1461 of size 256
2019-10-01 15:30:11.040407: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1425667d00 next 1462 of size 8192
2019-10-01 15:30:11.040412: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1425669d00 next 1463 of size 256
2019-10-01 15:30:11.040417: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1425669e00 next 1464 of size 16777216
2019-10-01 15:30:11.040422: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1426669e00 next 1465 of size 8192
2019-10-01 15:30:11.040427: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x142666be00 next 1466 of size 16777216
2019-10-01 15:30:11.040432: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x142766be00 next 1467 of size 8192
2019-10-01 15:30:11.040438: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x142766de00 next 1468 of size 67108864
2019-10-01 15:30:11.040443: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x142b66de00 next 1469 of size 512
2019-10-01 15:30:11.040449: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x142b66e000 next 1470 of size 8192
2019-10-01 15:30:11.040458: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x142b670000 next 1471 of size 256
2019-10-01 15:30:11.040466: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x142b670100 next 1472 of size 8192
2019-10-01 15:30:11.040475: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x142b672100 next 1473 of size 16777216
2019-10-01 15:30:11.040484: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x142c672100 next 1474 of size 8192
2019-10-01 15:30:11.040491: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x142c674100 next 1475 of size 256
2019-10-01 15:30:11.040496: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x142c674200 next 1476 of size 8192
2019-10-01 15:30:11.040501: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x142c676200 next 1477 of size 8192
2019-10-01 15:30:11.040507: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x142c678200 next 1478 of size 256
2019-10-01 15:30:11.040512: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x142c678300 next 1479 of size 8192
2019-10-01 15:30:11.040517: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x142c67a300 next 1480 of size 8192
2019-10-01 15:30:11.040522: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x142c67c300 next 1481 of size 256
2019-10-01 15:30:11.040527: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x142c67c400 next 1482 of size 8192
2019-10-01 15:30:11.040532: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x142c67e400 next 1483 of size 8192
2019-10-01 15:30:11.040537: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x142c680400 next 1484 of size 16777216
2019-10-01 15:30:11.040542: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x142d680400 next 1485 of size 256
2019-10-01 15:30:11.040547: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x142d680500 next 1486 of size 8192
2019-10-01 15:30:11.040553: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x142d682500 next 1487 of size 32768
2019-10-01 15:30:11.040558: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x142d68a500 next 1488 of size 256
2019-10-01 15:30:11.040566: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x142d68a600 next 1489 of size 32768
2019-10-01 15:30:11.040572: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x142d692600 next 1490 of size 16777216
2019-10-01 15:30:11.040577: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x142e692600 next 1491 of size 16777216
2019-10-01 15:30:11.040582: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x142f692600 next 1492 of size 256
2019-10-01 15:30:11.040587: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x142f692700 next 1493 of size 8192
2019-10-01 15:30:11.040592: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x142f694700 next 1494 of size 8192
2019-10-01 15:30:11.040597: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x142f696700 next 1495 of size 256
2019-10-01 15:30:11.040602: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x142f696800 next 1496 of size 8192
2019-10-01 15:30:11.040607: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x142f698800 next 1497 of size 8192
2019-10-01 15:30:11.040613: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x142f69a800 next 1498 of size 256
2019-10-01 15:30:11.040618: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x142f69a900 next 1499 of size 8192
2019-10-01 15:30:11.040624: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x142f69c900 next 1500 of size 16777216
2019-10-01 15:30:11.040629: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x143069c900 next 1501 of size 16777216
2019-10-01 15:30:11.040634: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x143169c900 next 1502 of size 8192
2019-10-01 15:30:11.040639: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x143169e900 next 1503 of size 256
2019-10-01 15:30:11.040645: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x143169ea00 next 1504 of size 16777216
2019-10-01 15:30:11.040650: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x143269ea00 next 1505 of size 8192
2019-10-01 15:30:11.040655: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14326a0a00 next 1506 of size 256
2019-10-01 15:30:11.040660: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14326a0b00 next 1507 of size 8192
2019-10-01 15:30:11.040665: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14326a2b00 next 1508 of size 8192
2019-10-01 15:30:11.040670: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14326a4b00 next 1509 of size 256
2019-10-01 15:30:11.040675: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14326a4c00 next 1510 of size 256
2019-10-01 15:30:11.040680: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14326a4d00 next 1511 of size 8192
2019-10-01 15:30:11.040685: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14326a6d00 next 1512 of size 16777216
2019-10-01 15:30:11.040690: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14336a6d00 next 1513 of size 16777216
2019-10-01 15:30:11.040695: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14346a6d00 next 1514 of size 67108864
2019-10-01 15:30:11.040700: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14386a6d00 next 1515 of size 8192
2019-10-01 15:30:11.040705: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14386a8d00 next 1516 of size 8192
2019-10-01 15:30:11.040710: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14386aad00 next 1517 of size 16777216
2019-10-01 15:30:11.040716: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14396aad00 next 1518 of size 67108864
2019-10-01 15:30:11.040721: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x143d6aad00 next 1519 of size 67108864
2019-10-01 15:30:11.040726: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14416aad00 next 1520 of size 256
2019-10-01 15:30:11.040733: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14416aae00 next 1521 of size 512
2019-10-01 15:30:11.040739: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14416ab000 next 1522 of size 32768
2019-10-01 15:30:11.040744: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14416b3000 next 1523 of size 8192
2019-10-01 15:30:11.040750: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14416b5000 next 1524 of size 512
2019-10-01 15:30:11.040755: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14416b5200 next 1525 of size 8192
2019-10-01 15:30:11.040760: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14416b7200 next 1526 of size 8192
2019-10-01 15:30:11.040765: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14416b9200 next 1527 of size 8192
2019-10-01 15:30:11.040770: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14416bb200 next 1528 of size 8192
2019-10-01 15:30:11.040775: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14416bd200 next 1529 of size 8192
2019-10-01 15:30:11.040779: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14416bf200 next 1530 of size 8192
2019-10-01 15:30:11.040785: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14416c1200 next 1531 of size 32768
2019-10-01 15:30:11.040791: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14416c9200 next 1532 of size 512
2019-10-01 15:30:11.040796: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14416c9400 next 1533 of size 8192
2019-10-01 15:30:11.040801: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14416cb400 next 1534 of size 16777216
2019-10-01 15:30:11.040806: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14426cb400 next 1535 of size 8192
2019-10-01 15:30:11.040811: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14426cd400 next 1536 of size 67108864
2019-10-01 15:30:11.040816: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14466cd400 next 1537 of size 8192
2019-10-01 15:30:11.040822: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14466cf400 next 1538 of size 256
2019-10-01 15:30:11.040827: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14466cf500 next 1539 of size 8192
2019-10-01 15:30:11.040832: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14466d1500 next 1540 of size 8192
2019-10-01 15:30:11.040837: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14466d3500 next 1541 of size 256
2019-10-01 15:30:11.040842: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14466d3600 next 1542 of size 8192
2019-10-01 15:30:11.040847: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14466d5600 next 1543 of size 8192
2019-10-01 15:30:11.040852: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14466d7600 next 1544 of size 256
2019-10-01 15:30:11.040857: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14466d7700 next 1545 of size 8192
2019-10-01 15:30:11.040862: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14466d9700 next 1546 of size 8192
2019-10-01 15:30:11.040867: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14466db700 next 1547 of size 256
2019-10-01 15:30:11.040872: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14466db800 next 1548 of size 8192
2019-10-01 15:30:11.040877: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14466dd800 next 1549 of size 8192
2019-10-01 15:30:11.040883: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14466df800 next 1550 of size 256
2019-10-01 15:30:11.040888: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14466df900 next 1551 of size 8192
2019-10-01 15:30:11.040893: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14466e1900 next 1552 of size 16777216
2019-10-01 15:30:11.040900: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14476e1900 next 1553 of size 16777216
2019-10-01 15:30:11.040906: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14486e1900 next 1554 of size 8192
2019-10-01 15:30:11.040911: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14486e3900 next 1555 of size 256
2019-10-01 15:30:11.040916: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14486e3a00 next 1556 of size 16777216
2019-10-01 15:30:11.040921: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14496e3a00 next 1557 of size 67108864
2019-10-01 15:30:11.040926: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x144d6e3a00 next 1558 of size 67108864
2019-10-01 15:30:11.040931: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14516e3a00 next 1559 of size 256
2019-10-01 15:30:11.040936: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14516e3b00 next 1560 of size 67108864
2019-10-01 15:30:11.040941: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14556e3b00 next 1561 of size 8192
2019-10-01 15:30:11.040946: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14556e5b00 next 1562 of size 67108864
2019-10-01 15:30:11.040952: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14596e5b00 next 1563 of size 8192
2019-10-01 15:30:11.040958: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14596e7b00 next 1564 of size 256
2019-10-01 15:30:11.040963: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14596e7c00 next 1565 of size 8192
2019-10-01 15:30:11.040968: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14596e9c00 next 1566 of size 16777216
2019-10-01 15:30:11.040973: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x145a6e9c00 next 1567 of size 16777216
2019-10-01 15:30:11.040978: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x145b6e9c00 next 1568 of size 16777216
2019-10-01 15:30:11.040983: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x145c6e9c00 next 1569 of size 8192
2019-10-01 15:30:11.040988: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x145c6ebc00 next 1570 of size 8192
2019-10-01 15:30:11.040993: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x145c6edc00 next 1571 of size 8192
2019-10-01 15:30:11.040998: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x145c6efc00 next 1572 of size 8192
2019-10-01 15:30:11.041004: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x145c6f1c00 next 1573 of size 256
2019-10-01 15:30:11.041009: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x145c6f1d00 next 1574 of size 67108864
2019-10-01 15:30:11.041014: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14606f1d00 next 1575 of size 16777216
2019-10-01 15:30:11.041019: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14616f1d00 next 1576 of size 8192
2019-10-01 15:30:11.041024: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14616f3d00 next 1577 of size 256
2019-10-01 15:30:11.041029: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14616f3e00 next 1578 of size 8192
2019-10-01 15:30:11.041034: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14616f5e00 next 1579 of size 8192
2019-10-01 15:30:11.041040: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14616f7e00 next 1580 of size 256
2019-10-01 15:30:11.041045: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14616f7f00 next 1581 of size 8192
2019-10-01 15:30:11.041050: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14616f9f00 next 1582 of size 8192
2019-10-01 15:30:11.041055: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14616fbf00 next 1583 of size 256
2019-10-01 15:30:11.041060: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14616fc000 next 1584 of size 8192
2019-10-01 15:30:11.041068: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14616fe000 next 1585 of size 16777216
2019-10-01 15:30:11.041073: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14626fe000 next 1586 of size 67108864
2019-10-01 15:30:11.041078: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14666fe000 next 1587 of size 8192
2019-10-01 15:30:11.041083: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1466700000 next 1588 of size 8192
2019-10-01 15:30:11.041088: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1466702000 next 1589 of size 16777216
2019-10-01 15:30:11.041094: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1467702000 next 1590 of size 256
2019-10-01 15:30:11.041099: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1467702100 next 1591 of size 8192
2019-10-01 15:30:11.041104: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1467704100 next 1592 of size 8192
2019-10-01 15:30:11.041109: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1467706100 next 1593 of size 256
2019-10-01 15:30:11.041114: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1467706200 next 1594 of size 8192
2019-10-01 15:30:11.041119: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1467708200 next 1595 of size 8192
2019-10-01 15:30:11.041125: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146770a200 next 1596 of size 256
2019-10-01 15:30:11.041130: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146770a300 next 1597 of size 8192
2019-10-01 15:30:11.041135: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146770c300 next 1598 of size 32768
2019-10-01 15:30:11.041141: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1467714300 next 1599 of size 256
2019-10-01 15:30:11.041146: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1467714400 next 1600 of size 32768
2019-10-01 15:30:11.041151: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146771c400 next 1601 of size 8192
2019-10-01 15:30:11.041156: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146771e400 next 1602 of size 256
2019-10-01 15:30:11.041161: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146771e500 next 1603 of size 8192
2019-10-01 15:30:11.041166: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1467720500 next 1604 of size 8192
2019-10-01 15:30:11.041171: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1467722500 next 1605 of size 256
2019-10-01 15:30:11.041176: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1467722600 next 1606 of size 8192
2019-10-01 15:30:11.041181: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1467724600 next 1607 of size 16777216
2019-10-01 15:30:11.041186: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1468724600 next 1608 of size 8192
2019-10-01 15:30:11.041191: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1468726600 next 1609 of size 16777216
2019-10-01 15:30:11.041196: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1469726600 next 1610 of size 256
2019-10-01 15:30:11.041201: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1469726700 next 1611 of size 256
2019-10-01 15:30:11.041206: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1469726800 next 1612 of size 256
2019-10-01 15:30:11.041212: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1469726900 next 1613 of size 16777216
2019-10-01 15:30:11.041217: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146a726900 next 1614 of size 16777216
2019-10-01 15:30:11.041222: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146b726900 next 1615 of size 16777216
2019-10-01 15:30:11.041227: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146c726900 next 1616 of size 512
2019-10-01 15:30:11.041235: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146c726b00 next 1617 of size 512
2019-10-01 15:30:11.041240: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146c726d00 next 1618 of size 8192
2019-10-01 15:30:11.041245: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146c728d00 next 1619 of size 256
2019-10-01 15:30:11.041250: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146c728e00 next 1620 of size 8192
2019-10-01 15:30:11.041255: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146c72ae00 next 1621 of size 32768
2019-10-01 15:30:11.041260: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146c732e00 next 1622 of size 256
2019-10-01 15:30:11.041265: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146c732f00 next 1623 of size 32768
2019-10-01 15:30:11.041270: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146c73af00 next 1624 of size 8192
2019-10-01 15:30:11.041276: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146c73cf00 next 1625 of size 256
2019-10-01 15:30:11.041281: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146c73d000 next 1626 of size 8192
2019-10-01 15:30:11.041286: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146c73f000 next 1627 of size 8192
2019-10-01 15:30:11.041292: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146c741000 next 1628 of size 256
2019-10-01 15:30:11.041297: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146c741100 next 1629 of size 8192
2019-10-01 15:30:11.041302: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146c743100 next 1630 of size 8192
2019-10-01 15:30:11.041307: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146c745100 next 1631 of size 256
2019-10-01 15:30:11.041312: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146c745200 next 1632 of size 8192
2019-10-01 15:30:11.041317: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146c747200 next 1633 of size 8192
2019-10-01 15:30:11.041322: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146c749200 next 1634 of size 256
2019-10-01 15:30:11.041327: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146c749300 next 1635 of size 8192
2019-10-01 15:30:11.041332: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146c74b300 next 1636 of size 512
2019-10-01 15:30:11.041337: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146c74b500 next 1637 of size 16777216
2019-10-01 15:30:11.041342: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146d74b500 next 1638 of size 256
2019-10-01 15:30:11.041347: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146d74b600 next 1639 of size 8192
2019-10-01 15:30:11.041353: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146d74d600 next 1640 of size 512
2019-10-01 15:30:11.041358: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146d74d800 next 1641 of size 8192
2019-10-01 15:30:11.041362: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146d74f800 next 1642 of size 8192
2019-10-01 15:30:11.041368: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146d751800 next 1643 of size 32768
2019-10-01 15:30:11.041373: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146d759800 next 1644 of size 256
2019-10-01 15:30:11.041378: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146d759900 next 1645 of size 8192
2019-10-01 15:30:11.041383: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146d75b900 next 1646 of size 8192
2019-10-01 15:30:11.041388: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146d75d900 next 1647 of size 8192
2019-10-01 15:30:11.041393: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146d75f900 next 1648 of size 8192
2019-10-01 15:30:11.041401: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146d761900 next 1649 of size 8192
2019-10-01 15:30:11.041406: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146d763900 next 1650 of size 8192
2019-10-01 15:30:11.041411: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146d765900 next 1651 of size 256
2019-10-01 15:30:11.041416: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146d765a00 next 1652 of size 8192
2019-10-01 15:30:11.041421: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146d767a00 next 1653 of size 8192
2019-10-01 15:30:11.041426: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146d769a00 next 1654 of size 256
2019-10-01 15:30:11.041432: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146d769b00 next 1655 of size 8192
2019-10-01 15:30:11.041437: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146d76bb00 next 1656 of size 16777216
2019-10-01 15:30:11.041442: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146e76bb00 next 1657 of size 256
2019-10-01 15:30:11.041447: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146e76bc00 next 1658 of size 67108864
2019-10-01 15:30:11.041452: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x147276bc00 next 1659 of size 67108864
2019-10-01 15:30:11.041458: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x147676bc00 next 1660 of size 8192
2019-10-01 15:30:11.041463: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x147676dc00 next 1661 of size 256
2019-10-01 15:30:11.041469: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x147676dd00 next 1662 of size 512
2019-10-01 15:30:11.041474: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x147676df00 next 1663 of size 16777216
2019-10-01 15:30:11.041479: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x147776df00 next 1664 of size 16777216
2019-10-01 15:30:11.041484: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x147876df00 next 1665 of size 67108864
2019-10-01 15:30:11.041489: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x147c76df00 next 1666 of size 8192
2019-10-01 15:30:11.041494: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x147c76ff00 next 1667 of size 256
2019-10-01 15:30:11.041499: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x147c770000 next 1668 of size 8192
2019-10-01 15:30:11.041504: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x147c772000 next 1669 of size 512
2019-10-01 15:30:11.041509: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x147c772200 next 1670 of size 256
2019-10-01 15:30:11.041514: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x147c772300 next 1671 of size 512
2019-10-01 15:30:11.041519: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x147c772500 next 1672 of size 8192
2019-10-01 15:30:11.041527: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x147c774500 next 1673 of size 256
2019-10-01 15:30:11.041532: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x147c774600 next 1674 of size 8192
2019-10-01 15:30:11.041537: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x147c776600 next 1675 of size 512
2019-10-01 15:30:11.041543: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x147c776800 next 1676 of size 256
2019-10-01 15:30:11.041548: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x147c776900 next 1677 of size 512
2019-10-01 15:30:11.041553: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x147c776b00 next 1678 of size 8192
2019-10-01 15:30:11.041558: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x147c778b00 next 1679 of size 256
2019-10-01 15:30:11.041563: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x147c778c00 next 1680 of size 16777216
2019-10-01 15:30:11.041570: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x147d778c00 next 1681 of size 8192
2019-10-01 15:30:11.041576: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x147d77ac00 next 1682 of size 8192
2019-10-01 15:30:11.041581: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x147d77cc00 next 1683 of size 32768
2019-10-01 15:30:11.041586: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x147d784c00 next 1684 of size 256
2019-10-01 15:30:11.041591: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x147d784d00 next 1685 of size 8192
2019-10-01 15:30:11.041596: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x147d786d00 next 1686 of size 8192
2019-10-01 15:30:11.041601: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x147d788d00 next 1687 of size 8192
2019-10-01 15:30:11.041606: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x147d78ad00 next 1688 of size 8192
2019-10-01 15:30:11.041611: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x147d78cd00 next 1689 of size 8192
2019-10-01 15:30:11.041616: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x147d78ed00 next 1690 of size 256
2019-10-01 15:30:11.041621: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x147d78ee00 next 1691 of size 8192
2019-10-01 15:30:11.041627: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x147d790e00 next 1692 of size 16777216
2019-10-01 15:30:11.041632: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x147e790e00 next 1693 of size 16777216
2019-10-01 15:30:11.041637: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x147f790e00 next 1694 of size 8192
2019-10-01 15:30:11.041642: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x147f792e00 next 1695 of size 8192
2019-10-01 15:30:11.041647: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x147f794e00 next 1696 of size 8192
2019-10-01 15:30:11.041652: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x147f796e00 next 1697 of size 8192
2019-10-01 15:30:11.041657: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x147f798e00 next 1698 of size 8192
2019-10-01 15:30:11.041662: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x147f79ae00 next 1699 of size 8192
2019-10-01 15:30:11.041667: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x147f79ce00 next 1700 of size 16777216
2019-10-01 15:30:11.041672: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x148079ce00 next 1701 of size 8192
2019-10-01 15:30:11.041677: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x148079ee00 next 1702 of size 256
2019-10-01 15:30:11.041683: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x148079ef00 next 1703 of size 16777216
2019-10-01 15:30:11.041688: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x148179ef00 next 1704 of size 256
2019-10-01 15:30:11.041693: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x148179f000 next 1705 of size 256
2019-10-01 15:30:11.041698: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x148179f100 next 1706 of size 67108864
2019-10-01 15:30:11.041703: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x148579f100 next 1707 of size 16777216
2019-10-01 15:30:11.041708: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x148679f100 next 1708 of size 67108864
2019-10-01 15:30:11.041713: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x148a79f100 next 1709 of size 8192
2019-10-01 15:30:11.041718: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x148a7a1100 next 1710 of size 16777216
2019-10-01 15:30:11.041723: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x148b7a1100 next 1711 of size 8192
2019-10-01 15:30:11.041736: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x148b7a3100 next 1712 of size 67108864
2019-10-01 15:30:11.041745: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x148f7a3100 next 1713 of size 16777216
2019-10-01 15:30:11.041754: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14907a3100 next 1714 of size 16777216
2019-10-01 15:30:11.041762: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14917a3100 next 1715 of size 16777216
2019-10-01 15:30:11.041768: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14927a3100 next 1716 of size 8192
2019-10-01 15:30:11.041773: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14927a5100 next 1717 of size 8192
2019-10-01 15:30:11.041778: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14927a7100 next 1718 of size 16777216
2019-10-01 15:30:11.041784: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14937a7100 next 1719 of size 512
2019-10-01 15:30:11.041789: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14937a7300 next 1720 of size 16777216
2019-10-01 15:30:11.041794: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14947a7300 next 1721 of size 8192
2019-10-01 15:30:11.041799: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14947a9300 next 1722 of size 256
2019-10-01 15:30:11.041804: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14947a9400 next 1723 of size 16777216
2019-10-01 15:30:11.041810: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14957a9400 next 1724 of size 8192
2019-10-01 15:30:11.041815: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14957ab400 next 1725 of size 512
2019-10-01 15:30:11.041820: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14957ab600 next 1726 of size 16777216
2019-10-01 15:30:11.041825: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14967ab600 next 1727 of size 8192
2019-10-01 15:30:11.041830: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14967ad600 next 1728 of size 256
2019-10-01 15:30:11.041835: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14967ad700 next 1729 of size 8192
2019-10-01 15:30:11.041840: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14967af700 next 1730 of size 16777216
2019-10-01 15:30:11.041846: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14977af700 next 1731 of size 8192
2019-10-01 15:30:11.041851: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14977b1700 next 1732 of size 67108864
2019-10-01 15:30:11.041856: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x149b7b1700 next 1733 of size 16777216
2019-10-01 15:30:11.041861: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x149c7b1700 next 1734 of size 8192
2019-10-01 15:30:11.041866: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x149c7b3700 next 1735 of size 256
2019-10-01 15:30:11.041871: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x149c7b3800 next 1736 of size 256
2019-10-01 15:30:11.041876: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x149c7b3900 next 1737 of size 256
2019-10-01 15:30:11.041881: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x149c7b3a00 next 1738 of size 512
2019-10-01 15:30:11.041886: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x149c7b3c00 next 1739 of size 256
2019-10-01 15:30:11.041891: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x149c7b3d00 next 1740 of size 512
2019-10-01 15:30:11.041896: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x149c7b3f00 next 1741 of size 8192
2019-10-01 15:30:11.041901: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x149c7b5f00 next 1742 of size 256
2019-10-01 15:30:11.041906: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x149c7b6000 next 1743 of size 8192
2019-10-01 15:30:11.041916: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x149c7b8000 next 1744 of size 8192
2019-10-01 15:30:11.041922: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x149c7ba000 next 1745 of size 256
2019-10-01 15:30:11.041927: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x149c7ba100 next 1746 of size 8192
2019-10-01 15:30:11.041932: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x149c7bc100 next 1747 of size 67108864
2019-10-01 15:30:11.041939: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14a07bc100 next 18446744073709551615 of size 109975552
2019-10-01 15:30:11.041944: I tensorflow/core/common_runtime/bfc_allocator.cc:914]      Summary of in-use Chunks by size: 
2019-10-01 15:30:11.041953: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 769 Chunks of size 256 totalling 192.2KiB
2019-10-01 15:30:11.041959: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 55 Chunks of size 512 totalling 27.5KiB
2019-10-01 15:30:11.041965: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 1280 totalling 1.2KiB
2019-10-01 15:30:11.041970: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 4096 totalling 4.0KiB
2019-10-01 15:30:11.041976: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 515 Chunks of size 8192 totalling 4.02MiB
2019-10-01 15:30:11.041981: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 10752 totalling 10.5KiB
2019-10-01 15:30:11.041988: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 12800 totalling 12.5KiB
2019-10-01 15:30:11.041994: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 51 Chunks of size 32768 totalling 1.59MiB
2019-10-01 15:30:11.042000: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 3 Chunks of size 131072 totalling 384.0KiB
2019-10-01 15:30:11.042006: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 131328 totalling 128.2KiB
2019-10-01 15:30:11.042011: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 8388608 totalling 8.00MiB
2019-10-01 15:30:11.042017: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 32 Chunks of size 16384000 totalling 500.00MiB
2019-10-01 15:30:11.042023: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 213 Chunks of size 16777216 totalling 3.33GiB
2019-10-01 15:30:11.042028: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 102 Chunks of size 67108864 totalling 6.38GiB
2019-10-01 15:30:11.042034: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 109975552 totalling 104.88MiB
2019-10-01 15:30:11.042040: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 262144000 totalling 250.00MiB
2019-10-01 15:30:11.042046: I tensorflow/core/common_runtime/bfc_allocator.cc:921] Sum Total of in-use chunks: 10.55GiB
2019-10-01 15:30:11.042051: I tensorflow/core/common_runtime/bfc_allocator.cc:923] total_region_allocated_bytes_: 11330115840 memory_limit_: 11330115994 available bytes: 154 curr_region_allocation_bytes_: 22660232192
2019-10-01 15:30:11.042061: I tensorflow/core/common_runtime/bfc_allocator.cc:929] Stats: 
Limit:                 11330115994
InUse:                 11330115840
MaxInUse:              11330115840
NumAllocs:                    2072
MaxAllocSize:            262144000

2019-10-01 15:30:11.042142: W tensorflow/core/common_runtime/bfc_allocator.cc:424] ****************************************************************************************************
2019-10-01 15:30:11.055438: W tensorflow/core/common_runtime/bfc_allocator.cc:419] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4B (rounded to 256).  Current allocation summary follows.
2019-10-01 15:30:11.055590: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (256): 	Total Chunks: 769, Chunks in use: 769. 192.2KiB allocated for chunks. 192.2KiB in use in bin. 3.3KiB client-requested in use in bin.
2019-10-01 15:30:11.055616: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (512): 	Total Chunks: 55, Chunks in use: 55. 27.5KiB allocated for chunks. 27.5KiB in use in bin. 27.5KiB client-requested in use in bin.
2019-10-01 15:30:11.055647: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (1024): 	Total Chunks: 1, Chunks in use: 1. 1.2KiB allocated for chunks. 1.2KiB in use in bin. 1.0KiB client-requested in use in bin.
2019-10-01 15:30:11.055667: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (2048): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-10-01 15:30:11.055687: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (4096): 	Total Chunks: 1, Chunks in use: 1. 4.0KiB allocated for chunks. 4.0KiB in use in bin. 4.0KiB client-requested in use in bin.
2019-10-01 15:30:11.055706: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (8192): 	Total Chunks: 517, Chunks in use: 517. 4.05MiB allocated for chunks. 4.05MiB in use in bin. 4.03MiB client-requested in use in bin.
2019-10-01 15:30:11.055724: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (16384): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-10-01 15:30:11.055742: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (32768): 	Total Chunks: 51, Chunks in use: 51. 1.59MiB allocated for chunks. 1.59MiB in use in bin. 1.59MiB client-requested in use in bin.
2019-10-01 15:30:11.055761: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (65536): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-10-01 15:30:11.055781: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (131072): 	Total Chunks: 4, Chunks in use: 4. 512.2KiB allocated for chunks. 512.2KiB in use in bin. 512.0KiB client-requested in use in bin.
2019-10-01 15:30:11.055800: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (262144): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-10-01 15:30:11.055817: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (524288): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-10-01 15:30:11.055833: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (1048576): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-10-01 15:30:11.055850: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (2097152): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-10-01 15:30:11.055867: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (4194304): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-10-01 15:30:11.055886: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (8388608): 	Total Chunks: 33, Chunks in use: 33. 508.00MiB allocated for chunks. 508.00MiB in use in bin. 508.00MiB client-requested in use in bin.
2019-10-01 15:30:11.055903: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (16777216): 	Total Chunks: 213, Chunks in use: 213. 3.33GiB allocated for chunks. 3.33GiB in use in bin. 3.33GiB client-requested in use in bin.
2019-10-01 15:30:11.055921: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (33554432): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-10-01 15:30:11.055940: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (67108864): 	Total Chunks: 103, Chunks in use: 103. 6.48GiB allocated for chunks. 6.48GiB in use in bin. 6.44GiB client-requested in use in bin.
2019-10-01 15:30:11.055960: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (134217728): 	Total Chunks: 1, Chunks in use: 1. 250.00MiB allocated for chunks. 250.00MiB in use in bin. 250.00MiB client-requested in use in bin.
2019-10-01 15:30:11.055984: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (268435456): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-10-01 15:30:11.056003: I tensorflow/core/common_runtime/bfc_allocator.cc:885] Bin for 256B was 256B, Chunk State: 
2019-10-01 15:30:11.056019: I tensorflow/core/common_runtime/bfc_allocator.cc:898] Next region of size 11330115840
2019-10-01 15:30:11.056038: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b60000 next 1 of size 1280
2019-10-01 15:30:11.056055: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b60500 next 2 of size 256
2019-10-01 15:30:11.056073: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b60600 next 3 of size 256
2019-10-01 15:30:11.056089: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b60700 next 4 of size 256
2019-10-01 15:30:11.056105: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b60800 next 5 of size 256
2019-10-01 15:30:11.056120: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b60900 next 6 of size 256
2019-10-01 15:30:11.056135: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b60a00 next 7 of size 256
2019-10-01 15:30:11.056150: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b60b00 next 8 of size 256
2019-10-01 15:30:11.056165: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b60c00 next 9 of size 256
2019-10-01 15:30:11.056180: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b60d00 next 10 of size 256
2019-10-01 15:30:11.056196: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b60e00 next 11 of size 256
2019-10-01 15:30:11.056212: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b60f00 next 12 of size 256
2019-10-01 15:30:11.056227: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b61000 next 13 of size 256
2019-10-01 15:30:11.056242: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b61100 next 14 of size 256
2019-10-01 15:30:11.056257: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b61200 next 15 of size 256
2019-10-01 15:30:11.056271: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b61300 next 16 of size 256
2019-10-01 15:30:11.056286: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b61400 next 17 of size 256
2019-10-01 15:30:11.056302: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b61500 next 18 of size 256
2019-10-01 15:30:11.056317: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b61600 next 19 of size 256
2019-10-01 15:30:11.056332: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b61700 next 20 of size 256
2019-10-01 15:30:11.056347: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b61800 next 21 of size 256
2019-10-01 15:30:11.056361: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b61900 next 22 of size 256
2019-10-01 15:30:11.056377: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b61a00 next 23 of size 256
2019-10-01 15:30:11.056393: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b61b00 next 24 of size 256
2019-10-01 15:30:11.056407: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b61c00 next 25 of size 256
2019-10-01 15:30:11.056422: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b61d00 next 26 of size 256
2019-10-01 15:30:11.056437: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b61e00 next 27 of size 256
2019-10-01 15:30:11.056452: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b61f00 next 28 of size 256
2019-10-01 15:30:11.056467: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b62000 next 29 of size 256
2019-10-01 15:30:11.056489: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b62100 next 30 of size 256
2019-10-01 15:30:11.056505: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b62200 next 31 of size 256
2019-10-01 15:30:11.056520: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b62300 next 32 of size 256
2019-10-01 15:30:11.056536: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b62400 next 33 of size 256
2019-10-01 15:30:11.056552: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b62500 next 34 of size 256
2019-10-01 15:30:11.056568: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b62600 next 35 of size 256
2019-10-01 15:30:11.056582: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b62700 next 36 of size 256
2019-10-01 15:30:11.056597: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b62800 next 37 of size 256
2019-10-01 15:30:11.056612: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b62900 next 38 of size 256
2019-10-01 15:30:11.056628: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b62a00 next 39 of size 256
2019-10-01 15:30:11.056643: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b62b00 next 40 of size 256
2019-10-01 15:30:11.056657: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b62c00 next 41 of size 256
2019-10-01 15:30:11.056673: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b62d00 next 42 of size 256
2019-10-01 15:30:11.056688: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b62e00 next 43 of size 256
2019-10-01 15:30:11.056704: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1203b62f00 next 44 of size 67108864
2019-10-01 15:30:11.056719: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1207b62f00 next 45 of size 8192
2019-10-01 15:30:11.056735: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1207b64f00 next 46 of size 8192
2019-10-01 15:30:11.056750: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1207b66f00 next 47 of size 16384000
2019-10-01 15:30:11.056765: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1208b06f00 next 48 of size 8192
2019-10-01 15:30:11.056781: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1208b08f00 next 49 of size 16777216
2019-10-01 15:30:11.056799: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1209b08f00 next 50 of size 512
2019-10-01 15:30:11.056816: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1209b09100 next 51 of size 8192
2019-10-01 15:30:11.056832: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1209b0b100 next 52 of size 8192
2019-10-01 15:30:11.056848: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1209b0d100 next 53 of size 32768
2019-10-01 15:30:11.056863: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1209b15100 next 54 of size 8192
2019-10-01 15:30:11.056880: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1209b17100 next 55 of size 8192
2019-10-01 15:30:11.056896: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1209b19100 next 56 of size 8192
2019-10-01 15:30:11.056912: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1209b1b100 next 57 of size 8192
2019-10-01 15:30:11.056928: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1209b1d100 next 58 of size 8192
2019-10-01 15:30:11.056944: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1209b1f100 next 59 of size 16777216
2019-10-01 15:30:11.056958: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x120ab1f100 next 60 of size 67108864
2019-10-01 15:30:11.056973: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x120eb1f100 next 61 of size 16777216
2019-10-01 15:30:11.056987: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x120fb1f100 next 62 of size 8192
2019-10-01 15:30:11.057010: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x120fb21100 next 63 of size 8192
2019-10-01 15:30:11.057028: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x120fb23100 next 64 of size 16777216
2019-10-01 15:30:11.057044: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1210b23100 next 65 of size 8192
2019-10-01 15:30:11.057058: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1210b25100 next 66 of size 8192
2019-10-01 15:30:11.057072: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1210b27100 next 67 of size 67108864
2019-10-01 15:30:11.057089: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1214b27100 next 68 of size 16777216
2019-10-01 15:30:11.057105: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1215b27100 next 69 of size 8192
2019-10-01 15:30:11.057120: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1215b29100 next 70 of size 8192
2019-10-01 15:30:11.057137: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1215b2b100 next 71 of size 8192
2019-10-01 15:30:11.057152: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1215b2d100 next 72 of size 8192
2019-10-01 15:30:11.057166: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1215b2f100 next 73 of size 16777216
2019-10-01 15:30:11.057182: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1216b2f100 next 74 of size 512
2019-10-01 15:30:11.057196: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1216b2f300 next 75 of size 512
2019-10-01 15:30:11.057211: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1216b2f500 next 76 of size 8192
2019-10-01 15:30:11.057226: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1216b31500 next 77 of size 32768
2019-10-01 15:30:11.057241: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1216b39500 next 78 of size 67108864
2019-10-01 15:30:11.057257: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x121ab39500 next 79 of size 67108864
2019-10-01 15:30:11.057273: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x121eb39500 next 80 of size 16384000
2019-10-01 15:30:11.057288: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x121fad9500 next 81 of size 8192
2019-10-01 15:30:11.057304: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x121fadb500 next 82 of size 8192
2019-10-01 15:30:11.057319: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x121fadd500 next 83 of size 262144000
2019-10-01 15:30:11.057334: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x122f4dd500 next 84 of size 8192
2019-10-01 15:30:11.057349: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x122f4df500 next 85 of size 16777216
2019-10-01 15:30:11.057363: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12304df500 next 86 of size 67108864
2019-10-01 15:30:11.057378: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12344df500 next 87 of size 67108864
2019-10-01 15:30:11.057393: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12384df500 next 88 of size 8192
2019-10-01 15:30:11.057407: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12384e1500 next 89 of size 16777216
2019-10-01 15:30:11.057421: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12394e1500 next 90 of size 8192
2019-10-01 15:30:11.057437: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12394e3500 next 91 of size 512
2019-10-01 15:30:11.057451: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12394e3700 next 92 of size 512
2019-10-01 15:30:11.057467: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12394e3900 next 93 of size 8192
2019-10-01 15:30:11.057482: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12394e5900 next 94 of size 32768
2019-10-01 15:30:11.057505: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12394ed900 next 95 of size 8192
2019-10-01 15:30:11.057522: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12394ef900 next 96 of size 16777216
2019-10-01 15:30:11.057537: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x123a4ef900 next 97 of size 67108864
2019-10-01 15:30:11.057552: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x123e4ef900 next 98 of size 32768
2019-10-01 15:30:11.057566: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x123e4f7900 next 99 of size 8192
2019-10-01 15:30:11.057582: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x123e4f9900 next 100 of size 16777216
2019-10-01 15:30:11.057599: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x123f4f9900 next 101 of size 8192
2019-10-01 15:30:11.057615: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x123f4fb900 next 102 of size 16777216
2019-10-01 15:30:11.057629: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12404fb900 next 103 of size 16777216
2019-10-01 15:30:11.057645: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12414fb900 next 104 of size 32768
2019-10-01 15:30:11.057660: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1241503900 next 105 of size 8192
2019-10-01 15:30:11.057676: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1241505900 next 106 of size 16777216
2019-10-01 15:30:11.057691: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1242505900 next 107 of size 16777216
2019-10-01 15:30:11.057707: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1243505900 next 108 of size 67108864
2019-10-01 15:30:11.057723: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1247505900 next 109 of size 8192
2019-10-01 15:30:11.057738: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1247507900 next 110 of size 8192
2019-10-01 15:30:11.057752: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1247509900 next 111 of size 8192
2019-10-01 15:30:11.057767: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x124750b900 next 112 of size 8192
2019-10-01 15:30:11.057782: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x124750d900 next 113 of size 16777216
2019-10-01 15:30:11.057797: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x124850d900 next 114 of size 8192
2019-10-01 15:30:11.057812: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x124850f900 next 115 of size 16384000
2019-10-01 15:30:11.057827: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12494af900 next 116 of size 16384000
2019-10-01 15:30:11.057843: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x124a44f900 next 117 of size 8192
2019-10-01 15:30:11.057858: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x124a451900 next 118 of size 256
2019-10-01 15:30:11.057874: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x124a451a00 next 119 of size 8192
2019-10-01 15:30:11.057889: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x124a453a00 next 120 of size 256
2019-10-01 15:30:11.057904: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x124a453b00 next 121 of size 8192
2019-10-01 15:30:11.057918: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x124a455b00 next 122 of size 16777216
2019-10-01 15:30:11.057933: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x124b455b00 next 123 of size 256
2019-10-01 15:30:11.057948: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x124b455c00 next 124 of size 67108864
2019-10-01 15:30:11.057963: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x124f455c00 next 125 of size 8192
2019-10-01 15:30:11.057978: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x124f457c00 next 126 of size 8192
2019-10-01 15:30:11.058001: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x124f459c00 next 127 of size 16777216
2019-10-01 15:30:11.058019: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1250459c00 next 128 of size 8192
2019-10-01 15:30:11.058034: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x125045bc00 next 129 of size 16777216
2019-10-01 15:30:11.058047: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x125145bc00 next 130 of size 16777216
2019-10-01 15:30:11.058062: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x125245bc00 next 131 of size 16777216
2019-10-01 15:30:11.058079: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x125345bc00 next 160 of size 256
2019-10-01 15:30:11.058094: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x125345bd00 next 169 of size 512
2019-10-01 15:30:11.058108: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x125345bf00 next 186 of size 512
2019-10-01 15:30:11.058123: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x125345c100 next 193 of size 512
2019-10-01 15:30:11.058139: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x125345c300 next 195 of size 512
2019-10-01 15:30:11.058155: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x125345c500 next 232 of size 512
2019-10-01 15:30:11.058170: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x125345c700 next 288 of size 512
2019-10-01 15:30:11.058185: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x125345c900 next 293 of size 512
2019-10-01 15:30:11.058200: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x125345cb00 next 295 of size 512
2019-10-01 15:30:11.058214: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x125345cd00 next 326 of size 512
2019-10-01 15:30:11.058229: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x125345cf00 next 336 of size 512
2019-10-01 15:30:11.058244: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x125345d100 next 342 of size 512
2019-10-01 15:30:11.058259: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x125345d300 next 361 of size 512
2019-10-01 15:30:11.058274: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x125345d500 next 364 of size 512
2019-10-01 15:30:11.058291: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x125345d700 next 372 of size 512
2019-10-01 15:30:11.058307: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x125345d900 next 162 of size 512
2019-10-01 15:30:11.058322: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x125345db00 next 163 of size 8192
2019-10-01 15:30:11.058338: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x125345fb00 next 164 of size 8192
2019-10-01 15:30:11.058353: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1253461b00 next 165 of size 16777216
2019-10-01 15:30:11.058369: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1254461b00 next 166 of size 67108864
2019-10-01 15:30:11.058386: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1258461b00 next 161 of size 8192
2019-10-01 15:30:11.058401: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1258463b00 next 157 of size 67108864
2019-10-01 15:30:11.058416: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x125c463b00 next 158 of size 8192
2019-10-01 15:30:11.058431: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x125c465b00 next 152 of size 8192
2019-10-01 15:30:11.058448: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x125c467b00 next 153 of size 8192
2019-10-01 15:30:11.058464: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x125c469b00 next 159 of size 8192
2019-10-01 15:30:11.058478: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x125c46bb00 next 156 of size 67108864
2019-10-01 15:30:11.058498: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x126046bb00 next 151 of size 8192
2019-10-01 15:30:11.058515: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x126046db00 next 146 of size 8192
2019-10-01 15:30:11.058531: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x126046fb00 next 147 of size 8192
2019-10-01 15:30:11.058578: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1260471b00 next 155 of size 32768
2019-10-01 15:30:11.058595: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1260479b00 next 150 of size 16777216
2019-10-01 15:30:11.058610: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1261479b00 next 145 of size 8192
2019-10-01 15:30:11.058626: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x126147bb00 next 141 of size 16777216
2019-10-01 15:30:11.058640: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x126247bb00 next 142 of size 16777216
2019-10-01 15:30:11.058654: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x126347bb00 next 154 of size 8192
2019-10-01 15:30:11.058669: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x126347db00 next 149 of size 16384000
2019-10-01 15:30:11.058685: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x126441db00 next 144 of size 16384000
2019-10-01 15:30:11.058701: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12653bdb00 next 148 of size 8192
2019-10-01 15:30:11.058715: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12653bfb00 next 143 of size 8192
2019-10-01 15:30:11.058730: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12653c1b00 next 139 of size 8192
2019-10-01 15:30:11.058745: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12653c3b00 next 140 of size 32768
2019-10-01 15:30:11.058760: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12653cbb00 next 138 of size 16777216
2019-10-01 15:30:11.058774: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12663cbb00 next 135 of size 8192
2019-10-01 15:30:11.058788: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12663cdb00 next 136 of size 16777216
2019-10-01 15:30:11.058803: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12673cdb00 next 137 of size 16777216
2019-10-01 15:30:11.058819: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12683cdb00 next 134 of size 8192
2019-10-01 15:30:11.058834: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12683cfb00 next 132 of size 8192
2019-10-01 15:30:11.058849: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12683d1b00 next 133 of size 8192
2019-10-01 15:30:11.058865: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12683d3b00 next 167 of size 8192
2019-10-01 15:30:11.058879: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12683d5b00 next 168 of size 8192
2019-10-01 15:30:11.058894: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12683d7b00 next 170 of size 67108864
2019-10-01 15:30:11.058909: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x126c3d7b00 next 171 of size 8192
2019-10-01 15:30:11.058923: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x126c3d9b00 next 172 of size 8192
2019-10-01 15:30:11.058939: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x126c3dbb00 next 173 of size 16777216
2019-10-01 15:30:11.058954: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x126d3dbb00 next 174 of size 32768
2019-10-01 15:30:11.058969: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x126d3e3b00 next 175 of size 8192
2019-10-01 15:30:11.058985: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x126d3e5b00 next 176 of size 67108864
2019-10-01 15:30:11.059001: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12713e5b00 next 177 of size 67108864
2019-10-01 15:30:11.059028: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12753e5b00 next 178 of size 16777216
2019-10-01 15:30:11.059044: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12763e5b00 next 179 of size 16777216
2019-10-01 15:30:11.059059: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12773e5b00 next 180 of size 67108864
2019-10-01 15:30:11.059073: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x127b3e5b00 next 181 of size 67108864
2019-10-01 15:30:11.059089: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x127f3e5b00 next 182 of size 8192
2019-10-01 15:30:11.059105: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x127f3e7b00 next 183 of size 67108864
2019-10-01 15:30:11.059120: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12833e7b00 next 184 of size 8192
2019-10-01 15:30:11.059133: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12833e9b00 next 185 of size 8192
2019-10-01 15:30:11.059147: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12833ebb00 next 187 of size 16384000
2019-10-01 15:30:11.059163: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x128438bb00 next 188 of size 16384000
2019-10-01 15:30:11.059178: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x128532bb00 next 189 of size 8192
2019-10-01 15:30:11.059193: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x128532db00 next 190 of size 8192
2019-10-01 15:30:11.059208: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x128532fb00 next 191 of size 8192
2019-10-01 15:30:11.059224: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1285331b00 next 192 of size 8192
2019-10-01 15:30:11.059239: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1285333b00 next 194 of size 8192
2019-10-01 15:30:11.059254: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1285335b00 next 196 of size 8192
2019-10-01 15:30:11.059269: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1285337b00 next 197 of size 32768
2019-10-01 15:30:11.059285: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x128533fb00 next 198 of size 67108864
2019-10-01 15:30:11.059301: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x128933fb00 next 199 of size 8192
2019-10-01 15:30:11.059317: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1289341b00 next 200 of size 8192
2019-10-01 15:30:11.059332: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1289343b00 next 201 of size 8192
2019-10-01 15:30:11.059348: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1289345b00 next 202 of size 67108864
2019-10-01 15:30:11.059363: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x128d345b00 next 203 of size 16777216
2019-10-01 15:30:11.059380: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x128e345b00 next 204 of size 32768
2019-10-01 15:30:11.059396: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x128e34db00 next 205 of size 16777216
2019-10-01 15:30:11.059411: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x128f34db00 next 206 of size 8192
2019-10-01 15:30:11.059426: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x128f34fb00 next 207 of size 8192
2019-10-01 15:30:11.059441: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x128f351b00 next 208 of size 67108864
2019-10-01 15:30:11.059455: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1293351b00 next 209 of size 67108864
2019-10-01 15:30:11.059470: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1297351b00 next 210 of size 16777216
2019-10-01 15:30:11.059487: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1298351b00 next 211 of size 8192
2019-10-01 15:30:11.059510: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1298353b00 next 212 of size 8192
2019-10-01 15:30:11.059528: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1298355b00 next 213 of size 32768
2019-10-01 15:30:11.059544: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x129835db00 next 214 of size 8192
2019-10-01 15:30:11.059558: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x129835fb00 next 215 of size 32768
2019-10-01 15:30:11.059573: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1298367b00 next 216 of size 16777216
2019-10-01 15:30:11.059589: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1299367b00 next 217 of size 8192
2019-10-01 15:30:11.059604: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1299369b00 next 218 of size 16777216
2019-10-01 15:30:11.059620: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x129a369b00 next 219 of size 8192
2019-10-01 15:30:11.059635: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x129a36bb00 next 220 of size 16777216
2019-10-01 15:30:11.059651: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x129b36bb00 next 221 of size 16384000
2019-10-01 15:30:11.059666: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x129c30bb00 next 222 of size 8192
2019-10-01 15:30:11.059680: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x129c30db00 next 223 of size 8192
2019-10-01 15:30:11.059696: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x129c30fb00 next 224 of size 16777216
2019-10-01 15:30:11.059711: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x129d30fb00 next 225 of size 8192
2019-10-01 15:30:11.059727: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x129d311b00 next 226 of size 8192
2019-10-01 15:30:11.059743: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x129d313b00 next 227 of size 8192
2019-10-01 15:30:11.059758: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x129d315b00 next 228 of size 8192
2019-10-01 15:30:11.059772: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x129d317b00 next 229 of size 8192
2019-10-01 15:30:11.059787: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x129d319b00 next 230 of size 8192
2019-10-01 15:30:11.059801: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x129d31bb00 next 231 of size 16777216
2019-10-01 15:30:11.059816: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x129e31bb00 next 233 of size 8192
2019-10-01 15:30:11.059830: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x129e31db00 next 234 of size 16777216
2019-10-01 15:30:11.059846: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x129f31db00 next 235 of size 67108864
2019-10-01 15:30:11.059861: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12a331db00 next 236 of size 8192
2019-10-01 15:30:11.059875: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12a331fb00 next 237 of size 8192
2019-10-01 15:30:11.059890: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12a3321b00 next 238 of size 16777216
2019-10-01 15:30:11.059905: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12a4321b00 next 239 of size 8192
2019-10-01 15:30:11.059921: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12a4323b00 next 240 of size 8192
2019-10-01 15:30:11.059936: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12a4325b00 next 241 of size 16777216
2019-10-01 15:30:11.059950: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12a5325b00 next 242 of size 8192
2019-10-01 15:30:11.059966: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12a5327b00 next 243 of size 67108864
2019-10-01 15:30:11.059981: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12a9327b00 next 244 of size 16777216
2019-10-01 15:30:11.060003: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12aa327b00 next 245 of size 8192
2019-10-01 15:30:11.060021: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12aa329b00 next 246 of size 67108864
2019-10-01 15:30:11.060036: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12ae329b00 next 247 of size 8192
2019-10-01 15:30:11.060050: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12ae32bb00 next 248 of size 8192
2019-10-01 15:30:11.060065: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12ae32db00 next 249 of size 8192
2019-10-01 15:30:11.060080: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12ae32fb00 next 250 of size 16384000
2019-10-01 15:30:11.060095: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12af2cfb00 next 251 of size 16384000
2019-10-01 15:30:11.060110: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12b026fb00 next 252 of size 8192
2019-10-01 15:30:11.060124: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12b0271b00 next 253 of size 8192
2019-10-01 15:30:11.060142: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12b0273b00 next 254 of size 32768
2019-10-01 15:30:11.060158: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12b027bb00 next 255 of size 16777216
2019-10-01 15:30:11.060172: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12b127bb00 next 256 of size 8192
2019-10-01 15:30:11.060186: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12b127db00 next 257 of size 16777216
2019-10-01 15:30:11.060201: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12b227db00 next 258 of size 8192
2019-10-01 15:30:11.060215: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12b227fb00 next 259 of size 8192
2019-10-01 15:30:11.060230: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12b2281b00 next 260 of size 8192
2019-10-01 15:30:11.060245: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12b2283b00 next 261 of size 67108864
2019-10-01 15:30:11.060259: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12b6283b00 next 262 of size 67108864
2019-10-01 15:30:11.060274: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12ba283b00 next 263 of size 8192
2019-10-01 15:30:11.060290: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12ba285b00 next 264 of size 8192
2019-10-01 15:30:11.060305: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12ba287b00 next 265 of size 8192
2019-10-01 15:30:11.060321: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12ba289b00 next 266 of size 8192
2019-10-01 15:30:11.060336: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12ba28bb00 next 267 of size 8192
2019-10-01 15:30:11.060352: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12ba28db00 next 268 of size 8192
2019-10-01 15:30:11.060368: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12ba28fb00 next 269 of size 16777216
2019-10-01 15:30:11.060383: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12bb28fb00 next 270 of size 8192
2019-10-01 15:30:11.060399: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12bb291b00 next 271 of size 8192
2019-10-01 15:30:11.060414: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12bb293b00 next 272 of size 32768
2019-10-01 15:30:11.060430: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12bb29bb00 next 273 of size 32768
2019-10-01 15:30:11.060444: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12bb2a3b00 next 274 of size 8192
2019-10-01 15:30:11.060461: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12bb2a5b00 next 275 of size 8192
2019-10-01 15:30:11.060477: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12bb2a7b00 next 276 of size 8192
2019-10-01 15:30:11.060499: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12bb2a9b00 next 277 of size 16777216
2019-10-01 15:30:11.060515: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12bc2a9b00 next 278 of size 67108864
2019-10-01 15:30:11.060529: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12c02a9b00 next 279 of size 67108864
2019-10-01 15:30:11.060544: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12c42a9b00 next 280 of size 16384000
2019-10-01 15:30:11.060560: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12c5249b00 next 281 of size 16384000
2019-10-01 15:30:11.060577: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12c61e9b00 next 282 of size 67108864
2019-10-01 15:30:11.060593: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12ca1e9b00 next 283 of size 8192
2019-10-01 15:30:11.060609: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12ca1ebb00 next 284 of size 16777216
2019-10-01 15:30:11.060624: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12cb1ebb00 next 285 of size 8192
2019-10-01 15:30:11.060640: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12cb1edb00 next 286 of size 8192
2019-10-01 15:30:11.060655: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12cb1efb00 next 287 of size 8192
2019-10-01 15:30:11.060671: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12cb1f1b00 next 289 of size 16777216
2019-10-01 15:30:11.060685: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12cc1f1b00 next 290 of size 16777216
2019-10-01 15:30:11.060700: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12cd1f1b00 next 291 of size 16777216
2019-10-01 15:30:11.060713: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12ce1f1b00 next 292 of size 8192
2019-10-01 15:30:11.060728: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12ce1f3b00 next 294 of size 8192
2019-10-01 15:30:11.060743: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12ce1f5b00 next 296 of size 8192
2019-10-01 15:30:11.060759: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12ce1f7b00 next 297 of size 8192
2019-10-01 15:30:11.060774: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12ce1f9b00 next 298 of size 8192
2019-10-01 15:30:11.060789: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12ce1fbb00 next 299 of size 8192
2019-10-01 15:30:11.060804: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12ce1fdb00 next 300 of size 67108864
2019-10-01 15:30:11.060819: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12d21fdb00 next 301 of size 16777216
2019-10-01 15:30:11.060834: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12d31fdb00 next 302 of size 8192
2019-10-01 15:30:11.060850: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12d31ffb00 next 303 of size 16777216
2019-10-01 15:30:11.060865: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12d41ffb00 next 304 of size 67108864
2019-10-01 15:30:11.060881: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12d81ffb00 next 305 of size 8192
2019-10-01 15:30:11.060895: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12d8201b00 next 306 of size 32768
2019-10-01 15:30:11.060909: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12d8209b00 next 307 of size 16384000
2019-10-01 15:30:11.060925: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12d91a9b00 next 308 of size 16777216
2019-10-01 15:30:11.060940: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12da1a9b00 next 309 of size 67108864
2019-10-01 15:30:11.060955: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12de1a9b00 next 310 of size 16777216
2019-10-01 15:30:11.060969: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12df1a9b00 next 311 of size 67108864
2019-10-01 15:30:11.060991: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12e31a9b00 next 312 of size 16777216
2019-10-01 15:30:11.061008: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12e41a9b00 next 313 of size 8192
2019-10-01 15:30:11.061022: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12e41abb00 next 314 of size 8192
2019-10-01 15:30:11.061037: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12e41adb00 next 315 of size 8192
2019-10-01 15:30:11.061053: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12e41afb00 next 316 of size 8192
2019-10-01 15:30:11.061068: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12e41b1b00 next 317 of size 8192
2019-10-01 15:30:11.061082: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12e41b3b00 next 318 of size 32768
2019-10-01 15:30:11.061096: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12e41bbb00 next 319 of size 8192
2019-10-01 15:30:11.061113: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12e41bdb00 next 320 of size 8192
2019-10-01 15:30:11.061129: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12e41bfb00 next 321 of size 16777216
2019-10-01 15:30:11.061145: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12e51bfb00 next 322 of size 8192
2019-10-01 15:30:11.061160: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12e51c1b00 next 323 of size 8192
2019-10-01 15:30:11.061176: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12e51c3b00 next 324 of size 8192
2019-10-01 15:30:11.061190: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12e51c5b00 next 325 of size 16777216
2019-10-01 15:30:11.061204: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12e61c5b00 next 327 of size 8192
2019-10-01 15:30:11.061220: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12e61c7b00 next 328 of size 8192
2019-10-01 15:30:11.061234: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12e61c9b00 next 329 of size 16777216
2019-10-01 15:30:11.061249: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12e71c9b00 next 330 of size 67108864
2019-10-01 15:30:11.061265: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12eb1c9b00 next 331 of size 16384000
2019-10-01 15:30:11.061280: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12ec169b00 next 332 of size 32768
2019-10-01 15:30:11.061294: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12ec171b00 next 333 of size 8192
2019-10-01 15:30:11.061309: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12ec173b00 next 334 of size 16777216
2019-10-01 15:30:11.061325: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12ed173b00 next 335 of size 8192
2019-10-01 15:30:11.061341: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12ed175b00 next 337 of size 8192
2019-10-01 15:30:11.061356: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12ed177b00 next 338 of size 16777216
2019-10-01 15:30:11.061370: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12ee177b00 next 339 of size 67108864
2019-10-01 15:30:11.061385: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12f2177b00 next 340 of size 8192
2019-10-01 15:30:11.061400: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12f2179b00 next 341 of size 8192
2019-10-01 15:30:11.061415: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12f217bb00 next 343 of size 16777216
2019-10-01 15:30:11.061429: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12f317bb00 next 344 of size 16777216
2019-10-01 15:30:11.061444: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12f417bb00 next 345 of size 8192
2019-10-01 15:30:11.061460: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12f417db00 next 346 of size 16777216
2019-10-01 15:30:11.061482: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12f517db00 next 347 of size 8192
2019-10-01 15:30:11.061500: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12f517fb00 next 348 of size 8192
2019-10-01 15:30:11.061514: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12f5181b00 next 349 of size 16777216
2019-10-01 15:30:11.061534: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12f6181b00 next 350 of size 67108864
2019-10-01 15:30:11.061550: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12fa181b00 next 351 of size 16777216
2019-10-01 15:30:11.061565: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12fb181b00 next 352 of size 8192
2019-10-01 15:30:11.061579: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12fb183b00 next 353 of size 8192
2019-10-01 15:30:11.061593: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12fb185b00 next 354 of size 16777216
2019-10-01 15:30:11.061608: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12fc185b00 next 355 of size 8192
2019-10-01 15:30:11.061621: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12fc187b00 next 356 of size 16777216
2019-10-01 15:30:11.061636: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12fd187b00 next 357 of size 8192
2019-10-01 15:30:11.061651: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12fd189b00 next 358 of size 8192
2019-10-01 15:30:11.061667: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12fd18bb00 next 359 of size 8192
2019-10-01 15:30:11.061682: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12fd18db00 next 360 of size 8192
2019-10-01 15:30:11.061697: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12fd18fb00 next 362 of size 8192
2019-10-01 15:30:11.061711: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12fd191b00 next 363 of size 8192
2019-10-01 15:30:11.061726: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12fd193b00 next 365 of size 32768
2019-10-01 15:30:11.061740: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12fd19bb00 next 366 of size 8192
2019-10-01 15:30:11.061754: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12fd19db00 next 367 of size 8192
2019-10-01 15:30:11.061770: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12fd19fb00 next 368 of size 8192
2019-10-01 15:30:11.061785: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x12fd1a1b00 next 369 of size 67108864
2019-10-01 15:30:11.061800: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13011a1b00 next 370 of size 8192
2019-10-01 15:30:11.061815: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13011a3b00 next 371 of size 16777216
2019-10-01 15:30:11.061831: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13021a3b00 next 373 of size 8192
2019-10-01 15:30:11.061847: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13021a5b00 next 374 of size 16777216
2019-10-01 15:30:11.061862: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13031a5b00 next 375 of size 8192
2019-10-01 15:30:11.061877: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13031a7b00 next 376 of size 16777216
2019-10-01 15:30:11.061891: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13041a7b00 next 377 of size 67108864
2019-10-01 15:30:11.061911: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13081a7b00 next 378 of size 16777216
2019-10-01 15:30:11.061928: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13091a7b00 next 379 of size 8192
2019-10-01 15:30:11.061943: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13091a9b00 next 380 of size 16777216
2019-10-01 15:30:11.061958: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x130a1a9b00 next 381 of size 67108864
2019-10-01 15:30:11.061979: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x130e1a9b00 next 382 of size 8192
2019-10-01 15:30:11.061996: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x130e1abb00 next 383 of size 8192
2019-10-01 15:30:11.062011: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x130e1adb00 next 384 of size 8192
2019-10-01 15:30:11.062026: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x130e1afb00 next 385 of size 8192
2019-10-01 15:30:11.062041: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x130e1b1b00 next 386 of size 8192
2019-10-01 15:30:11.062056: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x130e1b3b00 next 387 of size 8192
2019-10-01 15:30:11.062071: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x130e1b5b00 next 388 of size 8192
2019-10-01 15:30:11.062086: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x130e1b7b00 next 389 of size 8192
2019-10-01 15:30:11.062101: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x130e1b9b00 next 390 of size 67108864
2019-10-01 15:30:11.062117: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13121b9b00 next 391 of size 16777216
2019-10-01 15:30:11.062132: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13131b9b00 next 392 of size 32768
2019-10-01 15:30:11.062149: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13131c1b00 next 393 of size 8192
2019-10-01 15:30:11.062164: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13131c3b00 next 394 of size 16777216
2019-10-01 15:30:11.062179: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13141c3b00 next 395 of size 8192
2019-10-01 15:30:11.062194: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13141c5b00 next 396 of size 67108864
2019-10-01 15:30:11.062208: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181c5b00 next 439 of size 256
2019-10-01 15:30:11.062223: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181c5c00 next 437 of size 8192
2019-10-01 15:30:11.062238: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181c7c00 next 436 of size 8192
2019-10-01 15:30:11.062253: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181c9c00 next 399 of size 256
2019-10-01 15:30:11.062269: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181c9d00 next 434 of size 256
2019-10-01 15:30:11.062283: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181c9e00 next 413 of size 256
2019-10-01 15:30:11.062294: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181c9f00 next 432 of size 256
2019-10-01 15:30:11.062302: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181ca000 next 431 of size 8192
2019-10-01 15:30:11.062310: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181cc000 next 430 of size 8192
2019-10-01 15:30:11.062319: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181ce000 next 429 of size 8192
2019-10-01 15:30:11.062327: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181d0000 next 428 of size 8192
2019-10-01 15:30:11.062335: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181d2000 next 425 of size 512
2019-10-01 15:30:11.062343: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181d2200 next 397 of size 8192
2019-10-01 15:30:11.062353: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181d4200 next 433 of size 8192
2019-10-01 15:30:11.062361: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181d6200 next 423 of size 8192
2019-10-01 15:30:11.062369: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181d8200 next 422 of size 8192
2019-10-01 15:30:11.062377: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181da200 next 421 of size 8192
2019-10-01 15:30:11.062394: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181dc200 next 420 of size 8192
2019-10-01 15:30:11.062404: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181de200 next 419 of size 32768
2019-10-01 15:30:11.062412: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181e6200 next 418 of size 32768
2019-10-01 15:30:11.062418: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181ee200 next 411 of size 256
2019-10-01 15:30:11.062424: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181ee300 next 410 of size 256
2019-10-01 15:30:11.062429: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181ee400 next 409 of size 256
2019-10-01 15:30:11.062434: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181ee500 next 440 of size 256
2019-10-01 15:30:11.062439: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181ee600 next 441 of size 256
2019-10-01 15:30:11.062444: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181ee700 next 442 of size 256
2019-10-01 15:30:11.062449: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181ee800 next 445 of size 256
2019-10-01 15:30:11.062455: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181ee900 next 447 of size 256
2019-10-01 15:30:11.062460: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181eea00 next 448 of size 256
2019-10-01 15:30:11.062465: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181eeb00 next 449 of size 256
2019-10-01 15:30:11.062470: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181eec00 next 450 of size 256
2019-10-01 15:30:11.062475: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181eed00 next 451 of size 8192
2019-10-01 15:30:11.062481: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f0d00 next 452 of size 8192
2019-10-01 15:30:11.062486: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f2d00 next 454 of size 256
2019-10-01 15:30:11.062491: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f2e00 next 456 of size 4096
2019-10-01 15:30:11.062497: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f3e00 next 457 of size 256
2019-10-01 15:30:11.062502: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f3f00 next 458 of size 256
2019-10-01 15:30:11.062507: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f4000 next 459 of size 256
2019-10-01 15:30:11.062512: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f4100 next 460 of size 256
2019-10-01 15:30:11.062517: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f4200 next 461 of size 256
2019-10-01 15:30:11.062522: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f4300 next 462 of size 256
2019-10-01 15:30:11.062527: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f4400 next 463 of size 256
2019-10-01 15:30:11.062532: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f4500 next 464 of size 256
2019-10-01 15:30:11.062555: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f4600 next 465 of size 256
2019-10-01 15:30:11.062561: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f4700 next 466 of size 256
2019-10-01 15:30:11.062567: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f4800 next 467 of size 256
2019-10-01 15:30:11.062573: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f4900 next 468 of size 256
2019-10-01 15:30:11.062578: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f4a00 next 469 of size 256
2019-10-01 15:30:11.062583: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f4b00 next 470 of size 256
2019-10-01 15:30:11.062591: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f4c00 next 471 of size 256
2019-10-01 15:30:11.062596: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f4d00 next 472 of size 256
2019-10-01 15:30:11.062601: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f4e00 next 473 of size 256
2019-10-01 15:30:11.062607: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f4f00 next 474 of size 256
2019-10-01 15:30:11.062612: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f5000 next 475 of size 256
2019-10-01 15:30:11.062617: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f5100 next 476 of size 256
2019-10-01 15:30:11.062622: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f5200 next 477 of size 256
2019-10-01 15:30:11.062627: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f5300 next 478 of size 256
2019-10-01 15:30:11.062632: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f5400 next 479 of size 256
2019-10-01 15:30:11.062637: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f5500 next 480 of size 256
2019-10-01 15:30:11.062642: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f5600 next 481 of size 256
2019-10-01 15:30:11.062647: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f5700 next 482 of size 256
2019-10-01 15:30:11.062653: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f5800 next 483 of size 256
2019-10-01 15:30:11.062658: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f5900 next 484 of size 256
2019-10-01 15:30:11.062663: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f5a00 next 485 of size 256
2019-10-01 15:30:11.062668: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f5b00 next 486 of size 256
2019-10-01 15:30:11.062674: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f5c00 next 487 of size 256
2019-10-01 15:30:11.062679: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f5d00 next 488 of size 256
2019-10-01 15:30:11.062684: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f5e00 next 489 of size 256
2019-10-01 15:30:11.062689: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f5f00 next 490 of size 256
2019-10-01 15:30:11.062694: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f6000 next 491 of size 256
2019-10-01 15:30:11.062699: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f6100 next 492 of size 256
2019-10-01 15:30:11.062704: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f6200 next 493 of size 256
2019-10-01 15:30:11.062709: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f6300 next 494 of size 256
2019-10-01 15:30:11.062714: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f6400 next 495 of size 256
2019-10-01 15:30:11.062719: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f6500 next 496 of size 256
2019-10-01 15:30:11.062724: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f6600 next 497 of size 256
2019-10-01 15:30:11.062729: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f6700 next 498 of size 256
2019-10-01 15:30:11.062736: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f6800 next 499 of size 256
2019-10-01 15:30:11.062741: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f6900 next 500 of size 256
2019-10-01 15:30:11.062746: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f6a00 next 501 of size 256
2019-10-01 15:30:11.062751: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f6b00 next 502 of size 256
2019-10-01 15:30:11.062756: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f6c00 next 503 of size 256
2019-10-01 15:30:11.062764: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f6d00 next 504 of size 256
2019-10-01 15:30:11.062770: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f6e00 next 505 of size 256
2019-10-01 15:30:11.062775: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f6f00 next 506 of size 256
2019-10-01 15:30:11.062780: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f7000 next 507 of size 256
2019-10-01 15:30:11.062785: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f7100 next 508 of size 256
2019-10-01 15:30:11.062790: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f7200 next 509 of size 256
2019-10-01 15:30:11.062795: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f7300 next 510 of size 256
2019-10-01 15:30:11.062800: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f7400 next 511 of size 256
2019-10-01 15:30:11.062805: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f7500 next 512 of size 256
2019-10-01 15:30:11.062810: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f7600 next 513 of size 256
2019-10-01 15:30:11.062815: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f7700 next 514 of size 256
2019-10-01 15:30:11.062821: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f7800 next 515 of size 256
2019-10-01 15:30:11.062826: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f7900 next 516 of size 256
2019-10-01 15:30:11.062831: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f7a00 next 517 of size 256
2019-10-01 15:30:11.062836: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f7b00 next 518 of size 256
2019-10-01 15:30:11.062841: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f7c00 next 519 of size 256
2019-10-01 15:30:11.062847: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f7d00 next 520 of size 256
2019-10-01 15:30:11.062852: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f7e00 next 521 of size 256
2019-10-01 15:30:11.062857: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f7f00 next 522 of size 256
2019-10-01 15:30:11.062862: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f8000 next 523 of size 256
2019-10-01 15:30:11.062867: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f8100 next 524 of size 256
2019-10-01 15:30:11.062873: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f8200 next 525 of size 256
2019-10-01 15:30:11.062878: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f8300 next 526 of size 256
2019-10-01 15:30:11.062886: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f8400 next 527 of size 256
2019-10-01 15:30:11.062895: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f8500 next 528 of size 256
2019-10-01 15:30:11.062904: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f8600 next 529 of size 256
2019-10-01 15:30:11.062915: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f8700 next 530 of size 256
2019-10-01 15:30:11.062933: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f8800 next 531 of size 256
2019-10-01 15:30:11.062948: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f8900 next 532 of size 256
2019-10-01 15:30:11.062962: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f8a00 next 533 of size 256
2019-10-01 15:30:11.062978: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f8b00 next 534 of size 256
2019-10-01 15:30:11.062992: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f8c00 next 535 of size 256
2019-10-01 15:30:11.063014: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f8d00 next 536 of size 256
2019-10-01 15:30:11.063030: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f8e00 next 537 of size 256
2019-10-01 15:30:11.063044: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f8f00 next 538 of size 256
2019-10-01 15:30:11.063058: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f9000 next 539 of size 256
2019-10-01 15:30:11.063073: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f9100 next 540 of size 256
2019-10-01 15:30:11.063087: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f9200 next 541 of size 256
2019-10-01 15:30:11.063101: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f9300 next 542 of size 256
2019-10-01 15:30:11.063116: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f9400 next 543 of size 256
2019-10-01 15:30:11.063131: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f9500 next 544 of size 256
2019-10-01 15:30:11.063144: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f9600 next 545 of size 256
2019-10-01 15:30:11.063159: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f9700 next 546 of size 256
2019-10-01 15:30:11.063175: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f9800 next 547 of size 256
2019-10-01 15:30:11.063190: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f9900 next 548 of size 256
2019-10-01 15:30:11.063204: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f9a00 next 549 of size 256
2019-10-01 15:30:11.063220: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f9b00 next 550 of size 256
2019-10-01 15:30:11.063235: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f9c00 next 551 of size 256
2019-10-01 15:30:11.063251: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f9d00 next 552 of size 256
2019-10-01 15:30:11.063266: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f9e00 next 553 of size 256
2019-10-01 15:30:11.063280: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181f9f00 next 554 of size 256
2019-10-01 15:30:11.063295: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181fa000 next 555 of size 256
2019-10-01 15:30:11.063310: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181fa100 next 556 of size 256
2019-10-01 15:30:11.063326: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181fa200 next 557 of size 256
2019-10-01 15:30:11.063341: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181fa300 next 558 of size 256
2019-10-01 15:30:11.063355: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181fa400 next 559 of size 256
2019-10-01 15:30:11.063370: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181fa500 next 560 of size 256
2019-10-01 15:30:11.063385: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181fa600 next 561 of size 256
2019-10-01 15:30:11.063400: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181fa700 next 562 of size 256
2019-10-01 15:30:11.063417: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181fa800 next 563 of size 256
2019-10-01 15:30:11.063432: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181fa900 next 564 of size 256
2019-10-01 15:30:11.063448: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181faa00 next 565 of size 256
2019-10-01 15:30:11.063463: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181fab00 next 566 of size 256
2019-10-01 15:30:11.063477: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181fac00 next 567 of size 256
2019-10-01 15:30:11.063492: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181fad00 next 568 of size 256
2019-10-01 15:30:11.063512: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181fae00 next 569 of size 256
2019-10-01 15:30:11.063529: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181faf00 next 570 of size 256
2019-10-01 15:30:11.063543: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181fb000 next 571 of size 256
2019-10-01 15:30:11.063557: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181fb100 next 572 of size 256
2019-10-01 15:30:11.063572: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181fb200 next 573 of size 256
2019-10-01 15:30:11.063587: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181fb300 next 574 of size 256
2019-10-01 15:30:11.063602: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181fb400 next 575 of size 256
2019-10-01 15:30:11.063617: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181fb500 next 576 of size 256
2019-10-01 15:30:11.063631: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181fb600 next 577 of size 256
2019-10-01 15:30:11.063644: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181fb700 next 578 of size 256
2019-10-01 15:30:11.063659: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181fb800 next 579 of size 256
2019-10-01 15:30:11.063673: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181fb900 next 580 of size 256
2019-10-01 15:30:11.063688: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181fba00 next 581 of size 256
2019-10-01 15:30:11.063703: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181fbb00 next 582 of size 256
2019-10-01 15:30:11.063718: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181fbc00 next 583 of size 256
2019-10-01 15:30:11.063732: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181fbd00 next 584 of size 256
2019-10-01 15:30:11.063747: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181fbe00 next 585 of size 256
2019-10-01 15:30:11.063761: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181fbf00 next 586 of size 256
2019-10-01 15:30:11.063775: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181fc000 next 587 of size 256
2019-10-01 15:30:11.063790: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181fc100 next 588 of size 256
2019-10-01 15:30:11.063806: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181fc200 next 589 of size 256
2019-10-01 15:30:11.063821: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181fc300 next 590 of size 256
2019-10-01 15:30:11.063837: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181fc400 next 591 of size 8192
2019-10-01 15:30:11.063852: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13181fe400 next 592 of size 8192
2019-10-01 15:30:11.063866: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1318200400 next 594 of size 256
2019-10-01 15:30:11.063880: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1318200500 next 595 of size 256
2019-10-01 15:30:11.063898: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1318200600 next 597 of size 256
2019-10-01 15:30:11.063914: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1318200700 next 598 of size 256
2019-10-01 15:30:11.063929: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1318200800 next 599 of size 256
2019-10-01 15:30:11.063943: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1318200900 next 600 of size 8192
2019-10-01 15:30:11.063959: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1318202900 next 398 of size 12800
2019-10-01 15:30:11.063977: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1318205b00 next 400 of size 131328
2019-10-01 15:30:11.064000: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1318225c00 next 401 of size 256
2019-10-01 15:30:11.064017: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1318225d00 next 402 of size 131072
2019-10-01 15:30:11.064033: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1318245d00 next 403 of size 16777216
2019-10-01 15:30:11.064048: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1319245d00 next 404 of size 8192
2019-10-01 15:30:11.064063: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1319247d00 next 405 of size 32768
2019-10-01 15:30:11.064078: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x131924fd00 next 406 of size 16384000
2019-10-01 15:30:11.064093: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x131a1efd00 next 407 of size 16384000
2019-10-01 15:30:11.064107: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x131b18fd00 next 408 of size 16384000
2019-10-01 15:30:11.064122: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x131c12fd00 next 438 of size 131072
2019-10-01 15:30:11.064138: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x131c14fd00 next 435 of size 131072
2019-10-01 15:30:11.064153: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x131c16fd00 next 427 of size 16777216
2019-10-01 15:30:11.064167: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x131d16fd00 next 426 of size 16384000
2019-10-01 15:30:11.064181: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x131e10fd00 next 424 of size 16777216
2019-10-01 15:30:11.064197: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x131f10fd00 next 417 of size 16777216
2019-10-01 15:30:11.064211: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x132010fd00 next 416 of size 67108864
2019-10-01 15:30:11.064226: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x132410fd00 next 601 of size 8192
2019-10-01 15:30:11.064240: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1324111d00 next 602 of size 8192
2019-10-01 15:30:11.064256: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1324113d00 next 605 of size 256
2019-10-01 15:30:11.064270: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1324113e00 next 606 of size 256
2019-10-01 15:30:11.064286: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1324113f00 next 607 of size 256
2019-10-01 15:30:11.064301: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1324114000 next 608 of size 256
2019-10-01 15:30:11.064315: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1324114100 next 609 of size 256
2019-10-01 15:30:11.064330: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1324114200 next 610 of size 256
2019-10-01 15:30:11.064345: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1324114300 next 611 of size 256
2019-10-01 15:30:11.064360: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1324114400 next 612 of size 256
2019-10-01 15:30:11.064374: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1324114500 next 614 of size 256
2019-10-01 15:30:11.064393: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1324114600 next 615 of size 256
2019-10-01 15:30:11.064409: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1324114700 next 616 of size 8192
2019-10-01 15:30:11.064425: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1324116700 next 617 of size 8192
2019-10-01 15:30:11.064439: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1324118700 next 619 of size 256
2019-10-01 15:30:11.064453: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1324118800 next 621 of size 256
2019-10-01 15:30:11.064467: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1324118900 next 622 of size 256
2019-10-01 15:30:11.064488: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1324118a00 next 623 of size 8192
2019-10-01 15:30:11.064504: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x132411aa00 next 624 of size 8192
2019-10-01 15:30:11.064521: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x132411ca00 next 625 of size 8192
2019-10-01 15:30:11.064536: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x132411ea00 next 626 of size 8192
2019-10-01 15:30:11.064551: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1324120a00 next 627 of size 8192
2019-10-01 15:30:11.064566: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1324122a00 next 628 of size 8192
2019-10-01 15:30:11.064582: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1324124a00 next 631 of size 512
2019-10-01 15:30:11.064597: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1324124c00 next 632 of size 512
2019-10-01 15:30:11.064612: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1324124e00 next 634 of size 256
2019-10-01 15:30:11.064628: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1324124f00 next 635 of size 256
2019-10-01 15:30:11.064644: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1324125000 next 637 of size 8192
2019-10-01 15:30:11.064660: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1324127000 next 638 of size 8192
2019-10-01 15:30:11.064675: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1324129000 next 639 of size 8192
2019-10-01 15:30:11.064689: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x132412b000 next 640 of size 8192
2019-10-01 15:30:11.064705: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x132412d000 next 642 of size 256
2019-10-01 15:30:11.064720: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x132412d100 next 643 of size 256
2019-10-01 15:30:11.064734: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x132412d200 next 644 of size 256
2019-10-01 15:30:11.064750: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x132412d300 next 415 of size 10752
2019-10-01 15:30:11.064765: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x132412fd00 next 414 of size 67108864
2019-10-01 15:30:11.064780: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x132812fd00 next 412 of size 16777216
2019-10-01 15:30:11.064795: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x132912fd00 next 443 of size 67108864
2019-10-01 15:30:11.064811: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x132d12fd00 next 444 of size 16777216
2019-10-01 15:30:11.064826: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x132e12fd00 next 446 of size 16777216
2019-10-01 15:30:11.064842: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x132f12fd00 next 453 of size 67108864
2019-10-01 15:30:11.064857: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x133312fd00 next 455 of size 8388608
2019-10-01 15:30:11.064872: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x133392fd00 next 593 of size 16777216
2019-10-01 15:30:11.064890: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x133492fd00 next 596 of size 16777216
2019-10-01 15:30:11.064904: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x133592fd00 next 603 of size 16777216
2019-10-01 15:30:11.064920: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x133692fd00 next 604 of size 16777216
2019-10-01 15:30:11.064935: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x133792fd00 next 613 of size 67108864
2019-10-01 15:30:11.064950: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x133b92fd00 next 618 of size 16777216
2019-10-01 15:30:11.064964: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x133c92fd00 next 620 of size 16777216
2019-10-01 15:30:11.064988: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x133d92fd00 next 629 of size 16777216
2019-10-01 15:30:11.065006: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x133e92fd00 next 630 of size 16777216
2019-10-01 15:30:11.065020: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x133f92fd00 next 633 of size 16777216
2019-10-01 15:30:11.065035: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x134092fd00 next 636 of size 16777216
2019-10-01 15:30:11.065049: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x134192fd00 next 641 of size 67108864
2019-10-01 15:30:11.065063: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x134592fd00 next 645 of size 67108864
2019-10-01 15:30:11.065078: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x134992fd00 next 646 of size 8192
2019-10-01 15:30:11.065093: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1349931d00 next 647 of size 8192
2019-10-01 15:30:11.065107: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1349933d00 next 648 of size 8192
2019-10-01 15:30:11.065122: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1349935d00 next 649 of size 8192
2019-10-01 15:30:11.065137: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1349937d00 next 650 of size 8192
2019-10-01 15:30:11.065152: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1349939d00 next 651 of size 8192
2019-10-01 15:30:11.065168: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x134993bd00 next 652 of size 8192
2019-10-01 15:30:11.065184: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x134993dd00 next 653 of size 32768
2019-10-01 15:30:11.065199: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1349945d00 next 654 of size 32768
2019-10-01 15:30:11.065214: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x134994dd00 next 655 of size 16777216
2019-10-01 15:30:11.065229: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x134a94dd00 next 656 of size 256
2019-10-01 15:30:11.065243: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x134a94de00 next 657 of size 256
2019-10-01 15:30:11.065258: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x134a94df00 next 658 of size 256
2019-10-01 15:30:11.065273: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x134a94e000 next 659 of size 16777216
2019-10-01 15:30:11.065289: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x134b94e000 next 660 of size 8192
2019-10-01 15:30:11.065305: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x134b950000 next 661 of size 8192
2019-10-01 15:30:11.065321: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x134b952000 next 662 of size 16777216
2019-10-01 15:30:11.065336: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x134c952000 next 663 of size 256
2019-10-01 15:30:11.065351: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x134c952100 next 664 of size 256
2019-10-01 15:30:11.065366: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x134c952200 next 665 of size 256
2019-10-01 15:30:11.065384: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x134c952300 next 666 of size 16777216
2019-10-01 15:30:11.065399: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x134d952300 next 667 of size 8192
2019-10-01 15:30:11.065413: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x134d954300 next 668 of size 8192
2019-10-01 15:30:11.065429: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x134d956300 next 669 of size 67108864
2019-10-01 15:30:11.065444: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1351956300 next 670 of size 256
2019-10-01 15:30:11.065459: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1351956400 next 671 of size 8192
2019-10-01 15:30:11.065482: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1351958400 next 672 of size 8192
2019-10-01 15:30:11.065498: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x135195a400 next 673 of size 256
2019-10-01 15:30:11.065513: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x135195a500 next 674 of size 67108864
2019-10-01 15:30:11.065528: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x135595a500 next 675 of size 8192
2019-10-01 15:30:11.065544: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x135595c500 next 676 of size 8192
2019-10-01 15:30:11.065559: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x135595e500 next 677 of size 8192
2019-10-01 15:30:11.065574: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1355960500 next 678 of size 8192
2019-10-01 15:30:11.065589: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1355962500 next 679 of size 8192
2019-10-01 15:30:11.065605: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1355964500 next 680 of size 8192
2019-10-01 15:30:11.065620: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1355966500 next 681 of size 8192
2019-10-01 15:30:11.065635: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1355968500 next 682 of size 8192
2019-10-01 15:30:11.065651: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x135596a500 next 683 of size 67108864
2019-10-01 15:30:11.065666: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x135996a500 next 684 of size 256
2019-10-01 15:30:11.065679: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x135996a600 next 685 of size 256
2019-10-01 15:30:11.065693: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x135996a700 next 686 of size 256
2019-10-01 15:30:11.065708: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x135996a800 next 687 of size 256
2019-10-01 15:30:11.065723: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x135996a900 next 688 of size 256
2019-10-01 15:30:11.065738: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x135996aa00 next 689 of size 8192
2019-10-01 15:30:11.065753: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x135996ca00 next 690 of size 8192
2019-10-01 15:30:11.065768: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x135996ea00 next 691 of size 32768
2019-10-01 15:30:11.065783: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1359976a00 next 692 of size 32768
2019-10-01 15:30:11.065797: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x135997ea00 next 693 of size 256
2019-10-01 15:30:11.065812: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x135997eb00 next 694 of size 67108864
2019-10-01 15:30:11.065827: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x135d97eb00 next 695 of size 16777216
2019-10-01 15:30:11.065842: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x135e97eb00 next 696 of size 256
2019-10-01 15:30:11.065856: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x135e97ec00 next 697 of size 256
2019-10-01 15:30:11.065873: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x135e97ed00 next 698 of size 256
2019-10-01 15:30:11.065888: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x135e97ee00 next 699 of size 16777216
2019-10-01 15:30:11.065904: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x135f97ee00 next 700 of size 8192
2019-10-01 15:30:11.065918: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x135f980e00 next 701 of size 8192
2019-10-01 15:30:11.065934: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x135f982e00 next 702 of size 8192
2019-10-01 15:30:11.065948: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x135f984e00 next 703 of size 8192
2019-10-01 15:30:11.065971: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x135f986e00 next 704 of size 16777216
2019-10-01 15:30:11.065989: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1360986e00 next 705 of size 256
2019-10-01 15:30:11.066003: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1360986f00 next 706 of size 256
2019-10-01 15:30:11.066017: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1360987000 next 707 of size 256
2019-10-01 15:30:11.066031: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1360987100 next 708 of size 256
2019-10-01 15:30:11.066046: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1360987200 next 709 of size 256
2019-10-01 15:30:11.066061: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1360987300 next 710 of size 16777216
2019-10-01 15:30:11.066076: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1361987300 next 711 of size 67108864
2019-10-01 15:30:11.066089: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1365987300 next 712 of size 256
2019-10-01 15:30:11.066103: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1365987400 next 713 of size 256
2019-10-01 15:30:11.066120: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1365987500 next 714 of size 256
2019-10-01 15:30:11.066134: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1365987600 next 715 of size 67108864
2019-10-01 15:30:11.066149: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1369987600 next 716 of size 8192
2019-10-01 15:30:11.066164: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1369989600 next 717 of size 8192
2019-10-01 15:30:11.066179: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x136998b600 next 718 of size 32768
2019-10-01 15:30:11.066193: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1369993600 next 719 of size 32768
2019-10-01 15:30:11.066207: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x136999b600 next 720 of size 8192
2019-10-01 15:30:11.066223: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x136999d600 next 721 of size 8192
2019-10-01 15:30:11.066237: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x136999f600 next 722 of size 32768
2019-10-01 15:30:11.066252: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13699a7600 next 723 of size 32768
2019-10-01 15:30:11.066267: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13699af600 next 724 of size 67108864
2019-10-01 15:30:11.066281: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x136d9af600 next 725 of size 256
2019-10-01 15:30:11.066295: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x136d9af700 next 726 of size 256
2019-10-01 15:30:11.066310: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x136d9af800 next 727 of size 256
2019-10-01 15:30:11.066325: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x136d9af900 next 728 of size 256
2019-10-01 15:30:11.066339: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x136d9afa00 next 729 of size 256
2019-10-01 15:30:11.066358: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x136d9afb00 next 730 of size 256
2019-10-01 15:30:11.066372: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x136d9afc00 next 731 of size 256
2019-10-01 15:30:11.066386: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x136d9afd00 next 732 of size 256
2019-10-01 15:30:11.066400: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x136d9afe00 next 733 of size 256
2019-10-01 15:30:11.066414: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x136d9aff00 next 734 of size 256
2019-10-01 15:30:11.066430: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x136d9b0000 next 735 of size 256
2019-10-01 15:30:11.066451: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x136d9b0100 next 736 of size 256
2019-10-01 15:30:11.066468: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x136d9b0200 next 737 of size 256
2019-10-01 15:30:11.066484: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x136d9b0300 next 738 of size 256
2019-10-01 15:30:11.066499: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x136d9b0400 next 739 of size 8192
2019-10-01 15:30:11.066514: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x136d9b2400 next 740 of size 8192
2019-10-01 15:30:11.066530: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x136d9b4400 next 741 of size 67108864
2019-10-01 15:30:11.066570: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13719b4400 next 742 of size 16777216
2019-10-01 15:30:11.066586: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13729b4400 next 743 of size 256
2019-10-01 15:30:11.066602: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13729b4500 next 744 of size 256
2019-10-01 15:30:11.066619: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13729b4600 next 745 of size 256
2019-10-01 15:30:11.066634: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13729b4700 next 746 of size 16777216
2019-10-01 15:30:11.066649: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13739b4700 next 747 of size 8192
2019-10-01 15:30:11.066664: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13739b6700 next 748 of size 8192
2019-10-01 15:30:11.066679: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13739b8700 next 749 of size 8192
2019-10-01 15:30:11.066695: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13739ba700 next 750 of size 8192
2019-10-01 15:30:11.066709: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13739bc700 next 751 of size 8192
2019-10-01 15:30:11.066724: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13739be700 next 752 of size 8192
2019-10-01 15:30:11.066739: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13739c0700 next 753 of size 32768
2019-10-01 15:30:11.066754: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13739c8700 next 754 of size 32768
2019-10-01 15:30:11.066767: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13739d0700 next 755 of size 8192
2019-10-01 15:30:11.066782: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13739d2700 next 756 of size 8192
2019-10-01 15:30:11.066797: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13739d4700 next 757 of size 32768
2019-10-01 15:30:11.066812: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13739dc700 next 758 of size 32768
2019-10-01 15:30:11.066827: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13739e4700 next 759 of size 16777216
2019-10-01 15:30:11.066842: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13749e4700 next 760 of size 256
2019-10-01 15:30:11.066857: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13749e4800 next 761 of size 256
2019-10-01 15:30:11.066875: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13749e4900 next 762 of size 256
2019-10-01 15:30:11.066892: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13749e4a00 next 763 of size 256
2019-10-01 15:30:11.066907: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13749e4b00 next 764 of size 256
2019-10-01 15:30:11.066923: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13749e4c00 next 765 of size 8192
2019-10-01 15:30:11.066939: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13749e6c00 next 766 of size 8192
2019-10-01 15:30:11.066954: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13749e8c00 next 767 of size 16384000
2019-10-01 15:30:11.066977: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1375988c00 next 768 of size 16777216
2019-10-01 15:30:11.066995: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1376988c00 next 769 of size 16384000
2019-10-01 15:30:11.067010: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1377928c00 next 770 of size 8192
2019-10-01 15:30:11.067024: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137792ac00 next 771 of size 8192
2019-10-01 15:30:11.067038: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137792cc00 next 772 of size 8192
2019-10-01 15:30:11.067053: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137792ec00 next 773 of size 8192
2019-10-01 15:30:11.067068: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1377930c00 next 774 of size 8192
2019-10-01 15:30:11.067081: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1377932c00 next 775 of size 8192
2019-10-01 15:30:11.067096: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1377934c00 next 776 of size 8192
2019-10-01 15:30:11.067111: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1377936c00 next 777 of size 8192
2019-10-01 15:30:11.067127: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1377938c00 next 778 of size 8192
2019-10-01 15:30:11.067142: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137793ac00 next 779 of size 8192
2019-10-01 15:30:11.067157: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137793cc00 next 780 of size 16777216
2019-10-01 15:30:11.067172: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137893cc00 next 781 of size 256
2019-10-01 15:30:11.067188: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137893cd00 next 782 of size 16777216
2019-10-01 15:30:11.067203: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137993cd00 next 783 of size 67108864
2019-10-01 15:30:11.067220: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93cd00 next 784 of size 256
2019-10-01 15:30:11.067235: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93ce00 next 785 of size 256
2019-10-01 15:30:11.067250: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93cf00 next 786 of size 256
2019-10-01 15:30:11.067264: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93d000 next 787 of size 256
2019-10-01 15:30:11.067279: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93d100 next 788 of size 256
2019-10-01 15:30:11.067295: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93d200 next 789 of size 256
2019-10-01 15:30:11.067310: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93d300 next 790 of size 256
2019-10-01 15:30:11.067325: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93d400 next 791 of size 256
2019-10-01 15:30:11.067340: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93d500 next 792 of size 256
2019-10-01 15:30:11.067354: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93d600 next 793 of size 256
2019-10-01 15:30:11.067372: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93d700 next 794 of size 256
2019-10-01 15:30:11.067387: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93d800 next 795 of size 256
2019-10-01 15:30:11.067400: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93d900 next 796 of size 256
2019-10-01 15:30:11.067414: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93da00 next 797 of size 256
2019-10-01 15:30:11.067428: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93db00 next 798 of size 256
2019-10-01 15:30:11.067444: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93dc00 next 799 of size 256
2019-10-01 15:30:11.067459: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93dd00 next 800 of size 256
2019-10-01 15:30:11.067481: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93de00 next 801 of size 256
2019-10-01 15:30:11.067498: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93df00 next 802 of size 256
2019-10-01 15:30:11.067513: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93e000 next 803 of size 256
2019-10-01 15:30:11.067528: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93e100 next 804 of size 256
2019-10-01 15:30:11.067543: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93e200 next 805 of size 256
2019-10-01 15:30:11.067557: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93e300 next 806 of size 256
2019-10-01 15:30:11.067571: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93e400 next 807 of size 256
2019-10-01 15:30:11.067586: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93e500 next 808 of size 256
2019-10-01 15:30:11.067601: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93e600 next 809 of size 256
2019-10-01 15:30:11.067615: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93e700 next 810 of size 256
2019-10-01 15:30:11.067630: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93e800 next 811 of size 256
2019-10-01 15:30:11.067645: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93e900 next 812 of size 256
2019-10-01 15:30:11.067662: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93ea00 next 813 of size 256
2019-10-01 15:30:11.067677: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93eb00 next 814 of size 256
2019-10-01 15:30:11.067692: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93ec00 next 815 of size 256
2019-10-01 15:30:11.067707: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93ed00 next 816 of size 256
2019-10-01 15:30:11.067722: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93ee00 next 817 of size 256
2019-10-01 15:30:11.067737: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93ef00 next 818 of size 256
2019-10-01 15:30:11.067753: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93f000 next 819 of size 256
2019-10-01 15:30:11.067767: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93f100 next 820 of size 256
2019-10-01 15:30:11.067783: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93f200 next 821 of size 256
2019-10-01 15:30:11.067798: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93f300 next 822 of size 256
2019-10-01 15:30:11.067813: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93f400 next 823 of size 256
2019-10-01 15:30:11.067828: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93f500 next 824 of size 256
2019-10-01 15:30:11.067843: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93f600 next 825 of size 256
2019-10-01 15:30:11.067861: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93f700 next 826 of size 256
2019-10-01 15:30:11.067876: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93f800 next 827 of size 256
2019-10-01 15:30:11.067891: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93f900 next 828 of size 256
2019-10-01 15:30:11.067906: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93fa00 next 829 of size 256
2019-10-01 15:30:11.067921: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93fb00 next 830 of size 256
2019-10-01 15:30:11.067935: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93fc00 next 831 of size 256
2019-10-01 15:30:11.067948: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93fd00 next 832 of size 256
2019-10-01 15:30:11.067971: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93fe00 next 833 of size 256
2019-10-01 15:30:11.067987: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d93ff00 next 834 of size 256
2019-10-01 15:30:11.068001: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d940000 next 835 of size 256
2019-10-01 15:30:11.068015: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d940100 next 836 of size 256
2019-10-01 15:30:11.068030: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d940200 next 837 of size 256
2019-10-01 15:30:11.068045: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d940300 next 838 of size 256
2019-10-01 15:30:11.068060: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d940400 next 839 of size 256
2019-10-01 15:30:11.068075: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d940500 next 840 of size 256
2019-10-01 15:30:11.068090: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d940600 next 841 of size 256
2019-10-01 15:30:11.068105: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d940700 next 842 of size 256
2019-10-01 15:30:11.068121: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d940800 next 843 of size 256
2019-10-01 15:30:11.068135: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d940900 next 844 of size 256
2019-10-01 15:30:11.068149: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d940a00 next 845 of size 256
2019-10-01 15:30:11.068164: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d940b00 next 846 of size 256
2019-10-01 15:30:11.068179: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d940c00 next 847 of size 256
2019-10-01 15:30:11.068195: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d940d00 next 848 of size 256
2019-10-01 15:30:11.068209: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d940e00 next 849 of size 256
2019-10-01 15:30:11.068224: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d940f00 next 850 of size 256
2019-10-01 15:30:11.068238: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d941000 next 851 of size 256
2019-10-01 15:30:11.068254: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d941100 next 852 of size 256
2019-10-01 15:30:11.068267: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d941200 next 853 of size 256
2019-10-01 15:30:11.068284: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d941300 next 854 of size 256
2019-10-01 15:30:11.068299: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d941400 next 855 of size 256
2019-10-01 15:30:11.068314: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d941500 next 856 of size 256
2019-10-01 15:30:11.068329: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d941600 next 857 of size 256
2019-10-01 15:30:11.068348: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d941700 next 858 of size 256
2019-10-01 15:30:11.068362: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d941800 next 859 of size 256
2019-10-01 15:30:11.068376: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d941900 next 860 of size 256
2019-10-01 15:30:11.068391: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d941a00 next 861 of size 256
2019-10-01 15:30:11.068406: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d941b00 next 862 of size 256
2019-10-01 15:30:11.068421: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d941c00 next 863 of size 256
2019-10-01 15:30:11.068436: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d941d00 next 864 of size 256
2019-10-01 15:30:11.068452: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d941e00 next 865 of size 256
2019-10-01 15:30:11.068473: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d941f00 next 866 of size 256
2019-10-01 15:30:11.068489: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d942000 next 867 of size 256
2019-10-01 15:30:11.068504: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d942100 next 868 of size 256
2019-10-01 15:30:11.068519: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d942200 next 869 of size 256
2019-10-01 15:30:11.068533: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d942300 next 870 of size 256
2019-10-01 15:30:11.068546: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d942400 next 871 of size 256
2019-10-01 15:30:11.068561: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d942500 next 872 of size 256
2019-10-01 15:30:11.068577: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d942600 next 873 of size 256
2019-10-01 15:30:11.068591: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d942700 next 874 of size 256
2019-10-01 15:30:11.068606: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d942800 next 875 of size 256
2019-10-01 15:30:11.068621: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d942900 next 876 of size 256
2019-10-01 15:30:11.068636: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d942a00 next 877 of size 256
2019-10-01 15:30:11.068651: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d942b00 next 878 of size 256
2019-10-01 15:30:11.068666: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d942c00 next 879 of size 256
2019-10-01 15:30:11.068682: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d942d00 next 880 of size 256
2019-10-01 15:30:11.068696: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d942e00 next 881 of size 256
2019-10-01 15:30:11.068710: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d942f00 next 882 of size 256
2019-10-01 15:30:11.068725: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d943000 next 883 of size 256
2019-10-01 15:30:11.068741: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d943100 next 884 of size 256
2019-10-01 15:30:11.068755: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d943200 next 885 of size 256
2019-10-01 15:30:11.068770: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d943300 next 886 of size 256
2019-10-01 15:30:11.068785: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d943400 next 887 of size 256
2019-10-01 15:30:11.068799: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d943500 next 888 of size 256
2019-10-01 15:30:11.068813: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d943600 next 889 of size 256
2019-10-01 15:30:11.068832: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d943700 next 890 of size 256
2019-10-01 15:30:11.068847: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d943800 next 891 of size 256
2019-10-01 15:30:11.068862: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d943900 next 892 of size 256
2019-10-01 15:30:11.068877: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d943a00 next 893 of size 256
2019-10-01 15:30:11.068891: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d943b00 next 894 of size 256
2019-10-01 15:30:11.068906: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d943c00 next 895 of size 256
2019-10-01 15:30:11.068922: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d943d00 next 896 of size 256
2019-10-01 15:30:11.068936: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d943e00 next 897 of size 256
2019-10-01 15:30:11.068957: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d943f00 next 898 of size 256
2019-10-01 15:30:11.068974: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d944000 next 899 of size 256
2019-10-01 15:30:11.068989: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d944100 next 900 of size 256
2019-10-01 15:30:11.069006: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d944200 next 901 of size 256
2019-10-01 15:30:11.069021: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d944300 next 902 of size 256
2019-10-01 15:30:11.069035: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d944400 next 903 of size 256
2019-10-01 15:30:11.069051: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d944500 next 904 of size 256
2019-10-01 15:30:11.069067: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d944600 next 905 of size 256
2019-10-01 15:30:11.069082: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d944700 next 906 of size 256
2019-10-01 15:30:11.069096: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d944800 next 907 of size 256
2019-10-01 15:30:11.069111: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d944900 next 908 of size 256
2019-10-01 15:30:11.069126: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d944a00 next 909 of size 256
2019-10-01 15:30:11.069141: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d944b00 next 910 of size 256
2019-10-01 15:30:11.069156: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d944c00 next 911 of size 256
2019-10-01 15:30:11.069171: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d944d00 next 912 of size 256
2019-10-01 15:30:11.069186: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d944e00 next 913 of size 256
2019-10-01 15:30:11.069201: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d944f00 next 914 of size 256
2019-10-01 15:30:11.069214: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d945000 next 915 of size 256
2019-10-01 15:30:11.069229: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d945100 next 916 of size 256
2019-10-01 15:30:11.069244: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d945200 next 917 of size 256
2019-10-01 15:30:11.069257: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d945300 next 918 of size 256
2019-10-01 15:30:11.069272: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d945400 next 919 of size 256
2019-10-01 15:30:11.069286: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d945500 next 920 of size 256
2019-10-01 15:30:11.069300: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d945600 next 921 of size 256
2019-10-01 15:30:11.069316: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d945700 next 922 of size 256
2019-10-01 15:30:11.069332: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d945800 next 923 of size 256
2019-10-01 15:30:11.069347: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d945900 next 924 of size 256
2019-10-01 15:30:11.069362: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d945a00 next 925 of size 256
2019-10-01 15:30:11.069378: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d945b00 next 926 of size 256
2019-10-01 15:30:11.069393: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d945c00 next 927 of size 256
2019-10-01 15:30:11.069408: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d945d00 next 928 of size 256
2019-10-01 15:30:11.069422: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d945e00 next 929 of size 256
2019-10-01 15:30:11.069436: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d945f00 next 930 of size 256
2019-10-01 15:30:11.069459: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d946000 next 931 of size 256
2019-10-01 15:30:11.069474: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d946100 next 932 of size 256
2019-10-01 15:30:11.069489: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d946200 next 933 of size 256
2019-10-01 15:30:11.069503: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d946300 next 934 of size 256
2019-10-01 15:30:11.069518: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d946400 next 935 of size 256
2019-10-01 15:30:11.069532: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d946500 next 936 of size 256
2019-10-01 15:30:11.069547: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d946600 next 937 of size 256
2019-10-01 15:30:11.069562: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d946700 next 938 of size 256
2019-10-01 15:30:11.069577: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d946800 next 939 of size 256
2019-10-01 15:30:11.069593: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d946900 next 940 of size 256
2019-10-01 15:30:11.069608: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d946a00 next 941 of size 256
2019-10-01 15:30:11.069623: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d946b00 next 942 of size 256
2019-10-01 15:30:11.069639: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d946c00 next 943 of size 256
2019-10-01 15:30:11.069653: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d946d00 next 944 of size 256
2019-10-01 15:30:11.069669: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d946e00 next 945 of size 256
2019-10-01 15:30:11.069685: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d946f00 next 946 of size 256
2019-10-01 15:30:11.069699: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d947000 next 947 of size 256
2019-10-01 15:30:11.069713: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d947100 next 948 of size 256
2019-10-01 15:30:11.069728: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d947200 next 949 of size 256
2019-10-01 15:30:11.069743: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d947300 next 950 of size 256
2019-10-01 15:30:11.069757: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d947400 next 951 of size 256
2019-10-01 15:30:11.069771: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d947500 next 952 of size 256
2019-10-01 15:30:11.069785: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d947600 next 953 of size 256
2019-10-01 15:30:11.069803: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d947700 next 954 of size 256
2019-10-01 15:30:11.069819: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d947800 next 955 of size 256
2019-10-01 15:30:11.069834: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d947900 next 956 of size 256
2019-10-01 15:30:11.069850: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d947a00 next 957 of size 256
2019-10-01 15:30:11.069866: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d947b00 next 958 of size 256
2019-10-01 15:30:11.069881: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d947c00 next 959 of size 256
2019-10-01 15:30:11.069895: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d947d00 next 960 of size 256
2019-10-01 15:30:11.069910: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d947e00 next 961 of size 256
2019-10-01 15:30:11.069925: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d947f00 next 962 of size 256
2019-10-01 15:30:11.069951: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d948000 next 963 of size 256
2019-10-01 15:30:11.069967: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d948100 next 964 of size 256
2019-10-01 15:30:11.069981: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d948200 next 965 of size 256
2019-10-01 15:30:11.069996: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d948300 next 966 of size 256
2019-10-01 15:30:11.070012: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d948400 next 967 of size 256
2019-10-01 15:30:11.070027: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d948500 next 968 of size 256
2019-10-01 15:30:11.070042: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d948600 next 969 of size 256
2019-10-01 15:30:11.070056: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d948700 next 970 of size 256
2019-10-01 15:30:11.070071: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d948800 next 971 of size 256
2019-10-01 15:30:11.070086: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d948900 next 972 of size 256
2019-10-01 15:30:11.070101: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d948a00 next 973 of size 256
2019-10-01 15:30:11.070117: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d948b00 next 974 of size 256
2019-10-01 15:30:11.070133: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d948c00 next 975 of size 256
2019-10-01 15:30:11.070148: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d948d00 next 976 of size 256
2019-10-01 15:30:11.070164: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d948e00 next 977 of size 256
2019-10-01 15:30:11.070179: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d948f00 next 978 of size 256
2019-10-01 15:30:11.070196: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d949000 next 979 of size 256
2019-10-01 15:30:11.070211: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d949100 next 980 of size 256
2019-10-01 15:30:11.070225: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d949200 next 981 of size 256
2019-10-01 15:30:11.070239: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d949300 next 982 of size 256
2019-10-01 15:30:11.070248: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d949400 next 983 of size 256
2019-10-01 15:30:11.070256: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d949500 next 984 of size 256
2019-10-01 15:30:11.070265: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d949600 next 985 of size 256
2019-10-01 15:30:11.070275: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d949700 next 986 of size 256
2019-10-01 15:30:11.070283: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d949800 next 987 of size 256
2019-10-01 15:30:11.070291: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d949900 next 988 of size 256
2019-10-01 15:30:11.070299: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d949a00 next 989 of size 256
2019-10-01 15:30:11.070308: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d949b00 next 990 of size 256
2019-10-01 15:30:11.070316: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d949c00 next 991 of size 256
2019-10-01 15:30:11.070324: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d949d00 next 992 of size 256
2019-10-01 15:30:11.070331: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d949e00 next 993 of size 256
2019-10-01 15:30:11.070339: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d949f00 next 994 of size 256
2019-10-01 15:30:11.070347: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94a000 next 995 of size 256
2019-10-01 15:30:11.070359: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94a100 next 996 of size 256
2019-10-01 15:30:11.070366: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94a200 next 997 of size 256
2019-10-01 15:30:11.070371: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94a300 next 998 of size 256
2019-10-01 15:30:11.070376: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94a400 next 999 of size 256
2019-10-01 15:30:11.070382: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94a500 next 1000 of size 256
2019-10-01 15:30:11.070387: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94a600 next 1001 of size 256
2019-10-01 15:30:11.070392: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94a700 next 1002 of size 256
2019-10-01 15:30:11.070397: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94a800 next 1003 of size 256
2019-10-01 15:30:11.070402: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94a900 next 1004 of size 256
2019-10-01 15:30:11.070408: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94aa00 next 1005 of size 256
2019-10-01 15:30:11.070413: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94ab00 next 1006 of size 256
2019-10-01 15:30:11.070418: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94ac00 next 1007 of size 256
2019-10-01 15:30:11.070423: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94ad00 next 1008 of size 256
2019-10-01 15:30:11.070428: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94ae00 next 1009 of size 256
2019-10-01 15:30:11.070433: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94af00 next 1010 of size 256
2019-10-01 15:30:11.070438: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94b000 next 1011 of size 256
2019-10-01 15:30:11.070443: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94b100 next 1012 of size 256
2019-10-01 15:30:11.070449: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94b200 next 1013 of size 256
2019-10-01 15:30:11.070454: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94b300 next 1014 of size 256
2019-10-01 15:30:11.070459: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94b400 next 1015 of size 256
2019-10-01 15:30:11.070464: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94b500 next 1016 of size 256
2019-10-01 15:30:11.070469: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94b600 next 1017 of size 256
2019-10-01 15:30:11.070475: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94b700 next 1018 of size 256
2019-10-01 15:30:11.070481: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94b800 next 1019 of size 256
2019-10-01 15:30:11.070486: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94b900 next 1020 of size 256
2019-10-01 15:30:11.070491: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94ba00 next 1021 of size 256
2019-10-01 15:30:11.070496: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94bb00 next 1022 of size 256
2019-10-01 15:30:11.070501: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94bc00 next 1023 of size 256
2019-10-01 15:30:11.070506: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94bd00 next 1024 of size 256
2019-10-01 15:30:11.070511: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94be00 next 1025 of size 256
2019-10-01 15:30:11.070516: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94bf00 next 1026 of size 256
2019-10-01 15:30:11.070521: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94c000 next 1027 of size 256
2019-10-01 15:30:11.070529: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94c100 next 1028 of size 256
2019-10-01 15:30:11.070560: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94c200 next 1029 of size 256
2019-10-01 15:30:11.070569: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94c300 next 1030 of size 256
2019-10-01 15:30:11.070574: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94c400 next 1031 of size 256
2019-10-01 15:30:11.070579: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94c500 next 1032 of size 256
2019-10-01 15:30:11.070585: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94c600 next 1033 of size 256
2019-10-01 15:30:11.070590: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94c700 next 1034 of size 256
2019-10-01 15:30:11.070595: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94c800 next 1035 of size 256
2019-10-01 15:30:11.070600: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94c900 next 1036 of size 256
2019-10-01 15:30:11.070605: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94ca00 next 1037 of size 256
2019-10-01 15:30:11.070611: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94cb00 next 1038 of size 256
2019-10-01 15:30:11.070616: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94cc00 next 1039 of size 256
2019-10-01 15:30:11.070621: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94cd00 next 1040 of size 256
2019-10-01 15:30:11.070626: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94ce00 next 1041 of size 256
2019-10-01 15:30:11.070631: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94cf00 next 1042 of size 256
2019-10-01 15:30:11.070636: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94d000 next 1043 of size 256
2019-10-01 15:30:11.070642: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94d100 next 1044 of size 256
2019-10-01 15:30:11.070647: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94d200 next 1045 of size 256
2019-10-01 15:30:11.070652: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94d300 next 1046 of size 256
2019-10-01 15:30:11.070657: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94d400 next 1047 of size 256
2019-10-01 15:30:11.070662: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94d500 next 1048 of size 256
2019-10-01 15:30:11.070667: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94d600 next 1049 of size 256
2019-10-01 15:30:11.070674: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94d700 next 1050 of size 256
2019-10-01 15:30:11.070679: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94d800 next 1051 of size 256
2019-10-01 15:30:11.070685: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94d900 next 1052 of size 256
2019-10-01 15:30:11.070691: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x137d94da00 next 1053 of size 67108864
2019-10-01 15:30:11.070697: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x138194da00 next 1054 of size 8192
2019-10-01 15:30:11.070702: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x138194fa00 next 1055 of size 256
2019-10-01 15:30:11.070707: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x138194fb00 next 1056 of size 8192
2019-10-01 15:30:11.070712: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1381951b00 next 1057 of size 8192
2019-10-01 15:30:11.070718: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1381953b00 next 1058 of size 256
2019-10-01 15:30:11.070723: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1381953c00 next 1059 of size 8192
2019-10-01 15:30:11.070731: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1381955c00 next 1060 of size 256
2019-10-01 15:30:11.070737: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1381955d00 next 1061 of size 16777216
2019-10-01 15:30:11.070742: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1382955d00 next 1062 of size 256
2019-10-01 15:30:11.070747: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1382955e00 next 1063 of size 256
2019-10-01 15:30:11.070753: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1382955f00 next 1064 of size 256
2019-10-01 15:30:11.070758: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1382956000 next 1065 of size 16777216
2019-10-01 15:30:11.070763: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1383956000 next 1066 of size 8192
2019-10-01 15:30:11.070768: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1383958000 next 1067 of size 256
2019-10-01 15:30:11.070774: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1383958100 next 1068 of size 8192
2019-10-01 15:30:11.070779: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x138395a100 next 1069 of size 16777216
2019-10-01 15:30:11.070785: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x138495a100 next 1070 of size 256
2019-10-01 15:30:11.070790: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x138495a200 next 1071 of size 256
2019-10-01 15:30:11.070795: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x138495a300 next 1072 of size 256
2019-10-01 15:30:11.070801: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x138495a400 next 1073 of size 16777216
2019-10-01 15:30:11.070806: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x138595a400 next 1074 of size 67108864
2019-10-01 15:30:11.070811: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x138995a400 next 1075 of size 256
2019-10-01 15:30:11.070816: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x138995a500 next 1076 of size 256
2019-10-01 15:30:11.070821: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x138995a600 next 1077 of size 8192
2019-10-01 15:30:11.070827: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x138995c600 next 1078 of size 256
2019-10-01 15:30:11.070832: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x138995c700 next 1079 of size 8192
2019-10-01 15:30:11.070837: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x138995e700 next 1080 of size 256
2019-10-01 15:30:11.070842: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x138995e800 next 1081 of size 67108864
2019-10-01 15:30:11.070849: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x138d95e800 next 1082 of size 8192
2019-10-01 15:30:11.070854: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x138d960800 next 1083 of size 256
2019-10-01 15:30:11.070860: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x138d960900 next 1084 of size 8192
2019-10-01 15:30:11.070865: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x138d962900 next 1085 of size 8192
2019-10-01 15:30:11.070870: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x138d964900 next 1086 of size 256
2019-10-01 15:30:11.070876: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x138d964a00 next 1087 of size 8192
2019-10-01 15:30:11.070881: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x138d966a00 next 1088 of size 8192
2019-10-01 15:30:11.070886: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x138d968a00 next 1089 of size 256
2019-10-01 15:30:11.070892: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x138d968b00 next 1090 of size 8192
2019-10-01 15:30:11.070897: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x138d96ab00 next 1091 of size 8192
2019-10-01 15:30:11.070906: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x138d96cb00 next 1092 of size 256
2019-10-01 15:30:11.070912: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x138d96cc00 next 1093 of size 8192
2019-10-01 15:30:11.070917: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x138d96ec00 next 1094 of size 16777216
2019-10-01 15:30:11.070922: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x138e96ec00 next 1095 of size 256
2019-10-01 15:30:11.070927: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x138e96ed00 next 1096 of size 256
2019-10-01 15:30:11.070932: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x138e96ee00 next 1097 of size 256
2019-10-01 15:30:11.070938: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x138e96ef00 next 1098 of size 256
2019-10-01 15:30:11.070943: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x138e96f000 next 1099 of size 256
2019-10-01 15:30:11.070948: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x138e96f100 next 1100 of size 256
2019-10-01 15:30:11.070954: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x138e96f200 next 1101 of size 256
2019-10-01 15:30:11.070959: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x138e96f300 next 1102 of size 16777216
2019-10-01 15:30:11.070964: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x138f96f300 next 1103 of size 16777216
2019-10-01 15:30:11.070970: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x139096f300 next 1104 of size 256
2019-10-01 15:30:11.070975: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x139096f400 next 1105 of size 256
2019-10-01 15:30:11.070980: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x139096f500 next 1106 of size 16777216
2019-10-01 15:30:11.070985: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x139196f500 next 1107 of size 16777216
2019-10-01 15:30:11.070990: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x139296f500 next 1108 of size 256
2019-10-01 15:30:11.070995: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x139296f600 next 1109 of size 256
2019-10-01 15:30:11.071001: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x139296f700 next 1110 of size 16777216
2019-10-01 15:30:11.071006: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x139396f700 next 1111 of size 8192
2019-10-01 15:30:11.071011: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1393971700 next 1112 of size 256
2019-10-01 15:30:11.071016: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1393971800 next 1113 of size 8192
2019-10-01 15:30:11.071023: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1393973800 next 1114 of size 8192
2019-10-01 15:30:11.071028: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1393975800 next 1115 of size 256
2019-10-01 15:30:11.071033: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1393975900 next 1116 of size 8192
2019-10-01 15:30:11.071039: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1393977900 next 1117 of size 8192
2019-10-01 15:30:11.071044: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1393979900 next 1118 of size 256
2019-10-01 15:30:11.071049: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1393979a00 next 1119 of size 8192
2019-10-01 15:30:11.071054: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x139397ba00 next 1120 of size 8192
2019-10-01 15:30:11.071059: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x139397da00 next 1121 of size 256
2019-10-01 15:30:11.071064: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x139397db00 next 1122 of size 8192
2019-10-01 15:30:11.071070: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x139397fb00 next 1123 of size 8192
2019-10-01 15:30:11.071078: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1393981b00 next 1124 of size 256
2019-10-01 15:30:11.071083: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1393981c00 next 1125 of size 8192
2019-10-01 15:30:11.071089: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1393983c00 next 1126 of size 8192
2019-10-01 15:30:11.071094: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1393985c00 next 1127 of size 256
2019-10-01 15:30:11.071099: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1393985d00 next 1128 of size 8192
2019-10-01 15:30:11.071104: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1393987d00 next 1129 of size 8192
2019-10-01 15:30:11.071109: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1393989d00 next 1130 of size 256
2019-10-01 15:30:11.071115: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1393989e00 next 1131 of size 8192
2019-10-01 15:30:11.071120: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x139398be00 next 1132 of size 8192
2019-10-01 15:30:11.071125: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x139398de00 next 1133 of size 256
2019-10-01 15:30:11.071130: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x139398df00 next 1134 of size 8192
2019-10-01 15:30:11.071136: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x139398ff00 next 1135 of size 512
2019-10-01 15:30:11.071141: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1393990100 next 1136 of size 256
2019-10-01 15:30:11.071146: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1393990200 next 1137 of size 512
2019-10-01 15:30:11.071151: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1393990400 next 1138 of size 67108864
2019-10-01 15:30:11.071157: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1397990400 next 1139 of size 256
2019-10-01 15:30:11.071162: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1397990500 next 1140 of size 256
2019-10-01 15:30:11.071167: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1397990600 next 1141 of size 16777216
2019-10-01 15:30:11.071173: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1398990600 next 1142 of size 256
2019-10-01 15:30:11.071178: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1398990700 next 1143 of size 256
2019-10-01 15:30:11.071183: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1398990800 next 1144 of size 16777216
2019-10-01 15:30:11.071188: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1399990800 next 1145 of size 256
2019-10-01 15:30:11.071195: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1399990900 next 1146 of size 67108864
2019-10-01 15:30:11.071200: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x139d990900 next 1147 of size 16777216
2019-10-01 15:30:11.071205: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x139e990900 next 1148 of size 256
2019-10-01 15:30:11.071210: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x139e990a00 next 1149 of size 256
2019-10-01 15:30:11.071215: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x139e990b00 next 1150 of size 256
2019-10-01 15:30:11.071220: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x139e990c00 next 1151 of size 256
2019-10-01 15:30:11.071226: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x139e990d00 next 1152 of size 256
2019-10-01 15:30:11.071231: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x139e990e00 next 1153 of size 256
2019-10-01 15:30:11.071236: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x139e990f00 next 1154 of size 256
2019-10-01 15:30:11.071241: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x139e991000 next 1155 of size 8192
2019-10-01 15:30:11.071249: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x139e993000 next 1156 of size 256
2019-10-01 15:30:11.071254: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x139e993100 next 1157 of size 8192
2019-10-01 15:30:11.071259: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x139e995100 next 1158 of size 256
2019-10-01 15:30:11.071264: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x139e995200 next 1159 of size 16777216
2019-10-01 15:30:11.071269: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x139f995200 next 1160 of size 256
2019-10-01 15:30:11.071274: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x139f995300 next 1161 of size 256
2019-10-01 15:30:11.071279: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x139f995400 next 1162 of size 256
2019-10-01 15:30:11.071285: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x139f995500 next 1163 of size 16777216
2019-10-01 15:30:11.071290: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13a0995500 next 1164 of size 256
2019-10-01 15:30:11.071295: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13a0995600 next 1165 of size 8192
2019-10-01 15:30:11.071300: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13a0997600 next 1166 of size 256
2019-10-01 15:30:11.071305: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13a0997700 next 1167 of size 8192
2019-10-01 15:30:11.071310: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13a0999700 next 1168 of size 16777216
2019-10-01 15:30:11.071315: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13a1999700 next 1169 of size 256
2019-10-01 15:30:11.071320: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13a1999800 next 1170 of size 16777216
2019-10-01 15:30:11.071325: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13a2999800 next 1171 of size 16777216
2019-10-01 15:30:11.071331: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13a3999800 next 1172 of size 32768
2019-10-01 15:30:11.071336: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13a39a1800 next 1173 of size 256
2019-10-01 15:30:11.071341: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13a39a1900 next 1174 of size 32768
2019-10-01 15:30:11.071346: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13a39a9900 next 1175 of size 16777216
2019-10-01 15:30:11.071351: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13a49a9900 next 1176 of size 256
2019-10-01 15:30:11.071356: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13a49a9a00 next 1177 of size 256
2019-10-01 15:30:11.071363: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13a49a9b00 next 1178 of size 256
2019-10-01 15:30:11.071368: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13a49a9c00 next 1179 of size 8192
2019-10-01 15:30:11.071373: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13a49abc00 next 1180 of size 256
2019-10-01 15:30:11.071378: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13a49abd00 next 1181 of size 8192
2019-10-01 15:30:11.071383: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13a49add00 next 1182 of size 8192
2019-10-01 15:30:11.071388: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13a49afd00 next 1183 of size 256
2019-10-01 15:30:11.071394: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13a49afe00 next 1184 of size 8192
2019-10-01 15:30:11.071399: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13a49b1e00 next 1185 of size 16777216
2019-10-01 15:30:11.071404: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13a59b1e00 next 1186 of size 256
2019-10-01 15:30:11.071409: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13a59b1f00 next 1187 of size 16777216
2019-10-01 15:30:11.071417: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13a69b1f00 next 1188 of size 256
2019-10-01 15:30:11.071422: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13a69b2000 next 1189 of size 16777216
2019-10-01 15:30:11.071427: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13a79b2000 next 1190 of size 8192
2019-10-01 15:30:11.071432: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13a79b4000 next 1191 of size 256
2019-10-01 15:30:11.071438: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13a79b4100 next 1192 of size 8192
2019-10-01 15:30:11.071447: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13a79b6100 next 1193 of size 16777216
2019-10-01 15:30:11.071456: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13a89b6100 next 1194 of size 256
2019-10-01 15:30:11.071466: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13a89b6200 next 1195 of size 256
2019-10-01 15:30:11.071481: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13a89b6300 next 1196 of size 8192
2019-10-01 15:30:11.071495: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13a89b8300 next 1197 of size 256
2019-10-01 15:30:11.071510: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13a89b8400 next 1198 of size 8192
2019-10-01 15:30:11.071524: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13a89ba400 next 1199 of size 8192
2019-10-01 15:30:11.071537: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13a89bc400 next 1200 of size 256
2019-10-01 15:30:11.071552: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13a89bc500 next 1201 of size 8192
2019-10-01 15:30:11.071567: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13a89be500 next 1202 of size 67108864
2019-10-01 15:30:11.071588: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13ac9be500 next 1203 of size 256
2019-10-01 15:30:11.071602: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13ac9be600 next 1204 of size 16777216
2019-10-01 15:30:11.071616: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13ad9be600 next 1205 of size 256
2019-10-01 15:30:11.071631: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13ad9be700 next 1206 of size 67108864
2019-10-01 15:30:11.071646: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b19be700 next 1207 of size 8192
2019-10-01 15:30:11.071660: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b19c0700 next 1208 of size 256
2019-10-01 15:30:11.071675: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b19c0800 next 1209 of size 8192
2019-10-01 15:30:11.071692: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b19c2800 next 1210 of size 512
2019-10-01 15:30:11.071706: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b19c2a00 next 1211 of size 256
2019-10-01 15:30:11.071722: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b19c2b00 next 1212 of size 512
2019-10-01 15:30:11.071737: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b19c2d00 next 1213 of size 8192
2019-10-01 15:30:11.071751: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b19c4d00 next 1214 of size 256
2019-10-01 15:30:11.071767: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b19c4e00 next 1215 of size 8192
2019-10-01 15:30:11.071782: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b19c6e00 next 1216 of size 8192
2019-10-01 15:30:11.071797: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b19c8e00 next 1217 of size 256
2019-10-01 15:30:11.071812: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b19c8f00 next 1218 of size 8192
2019-10-01 15:30:11.071826: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b19caf00 next 1219 of size 16777216
2019-10-01 15:30:11.071849: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b29caf00 next 1220 of size 256
2019-10-01 15:30:11.071867: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b29cb000 next 1221 of size 256
2019-10-01 15:30:11.071881: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b29cb100 next 1222 of size 256
2019-10-01 15:30:11.071896: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b29cb200 next 1223 of size 256
2019-10-01 15:30:11.071911: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b29cb300 next 1224 of size 256
2019-10-01 15:30:11.071927: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b29cb400 next 1225 of size 256
2019-10-01 15:30:11.071942: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b29cb500 next 1226 of size 256
2019-10-01 15:30:11.071957: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b29cb600 next 1227 of size 16777216
2019-10-01 15:30:11.071972: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b39cb600 next 1228 of size 16777216
2019-10-01 15:30:11.071986: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b49cb600 next 1229 of size 256
2019-10-01 15:30:11.071999: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b49cb700 next 1230 of size 256
2019-10-01 15:30:11.072014: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b49cb800 next 1231 of size 256
2019-10-01 15:30:11.072030: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b49cb900 next 1232 of size 256
2019-10-01 15:30:11.072045: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b49cba00 next 1233 of size 16777216
2019-10-01 15:30:11.072060: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b59cba00 next 1234 of size 8192
2019-10-01 15:30:11.072076: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b59cda00 next 1235 of size 256
2019-10-01 15:30:11.072091: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b59cdb00 next 1236 of size 8192
2019-10-01 15:30:11.072105: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b59cfb00 next 1237 of size 8192
2019-10-01 15:30:11.072120: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b59d1b00 next 1238 of size 256
2019-10-01 15:30:11.072135: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b59d1c00 next 1239 of size 8192
2019-10-01 15:30:11.072151: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b59d3c00 next 1240 of size 16777216
2019-10-01 15:30:11.072166: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b69d3c00 next 1241 of size 256
2019-10-01 15:30:11.072183: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b69d3d00 next 1242 of size 256
2019-10-01 15:30:11.072199: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b69d3e00 next 1243 of size 256
2019-10-01 15:30:11.072214: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b69d3f00 next 1244 of size 256
2019-10-01 15:30:11.072229: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b69d4000 next 1245 of size 16777216
2019-10-01 15:30:11.072244: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b79d4000 next 1246 of size 8192
2019-10-01 15:30:11.072259: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b79d6000 next 1247 of size 256
2019-10-01 15:30:11.072272: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b79d6100 next 1248 of size 8192
2019-10-01 15:30:11.072288: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b79d8100 next 1249 of size 16777216
2019-10-01 15:30:11.072302: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b89d8100 next 1250 of size 256
2019-10-01 15:30:11.072317: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b89d8200 next 1251 of size 256
2019-10-01 15:30:11.072338: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b89d8300 next 1252 of size 256
2019-10-01 15:30:11.072355: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b89d8400 next 1253 of size 256
2019-10-01 15:30:11.072371: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b89d8500 next 1254 of size 16777216
2019-10-01 15:30:11.072386: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b99d8500 next 1255 of size 8192
2019-10-01 15:30:11.072401: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b99da500 next 1256 of size 256
2019-10-01 15:30:11.072416: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b99da600 next 1257 of size 8192
2019-10-01 15:30:11.072433: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13b99dc600 next 1258 of size 16777216
2019-10-01 15:30:11.072447: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13ba9dc600 next 1259 of size 256
2019-10-01 15:30:11.072461: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13ba9dc700 next 1260 of size 256
2019-10-01 15:30:11.072476: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13ba9dc800 next 1261 of size 256
2019-10-01 15:30:11.072492: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13ba9dc900 next 1262 of size 16777216
2019-10-01 15:30:11.072509: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13bb9dc900 next 1263 of size 67108864
2019-10-01 15:30:11.072522: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13bf9dc900 next 1264 of size 256
2019-10-01 15:30:11.072537: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13bf9dca00 next 1265 of size 256
2019-10-01 15:30:11.072551: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13bf9dcb00 next 1266 of size 16777216
2019-10-01 15:30:11.072566: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13c09dcb00 next 1267 of size 256
2019-10-01 15:30:11.072581: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13c09dcc00 next 1268 of size 256
2019-10-01 15:30:11.072595: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13c09dcd00 next 1269 of size 256
2019-10-01 15:30:11.072610: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13c09dce00 next 1270 of size 256
2019-10-01 15:30:11.072625: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13c09dcf00 next 1271 of size 16777216
2019-10-01 15:30:11.072640: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13c19dcf00 next 1272 of size 256
2019-10-01 15:30:11.072654: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13c19dd000 next 1273 of size 67108864
2019-10-01 15:30:11.072672: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13c59dd000 next 1274 of size 512
2019-10-01 15:30:11.072687: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13c59dd200 next 1275 of size 256
2019-10-01 15:30:11.072702: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13c59dd300 next 1276 of size 512
2019-10-01 15:30:11.072717: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13c59dd500 next 1277 of size 16777216
2019-10-01 15:30:11.072732: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13c69dd500 next 1278 of size 256
2019-10-01 15:30:11.072748: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13c69dd600 next 1279 of size 256
2019-10-01 15:30:11.072762: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13c69dd700 next 1280 of size 16777216
2019-10-01 15:30:11.072776: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13c79dd700 next 1281 of size 8192
2019-10-01 15:30:11.072790: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13c79df700 next 1282 of size 256
2019-10-01 15:30:11.072805: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13c79df800 next 1283 of size 8192
2019-10-01 15:30:11.072827: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13c79e1800 next 1284 of size 8192
2019-10-01 15:30:11.072843: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13c79e3800 next 1285 of size 256
2019-10-01 15:30:11.072858: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13c79e3900 next 1286 of size 8192
2019-10-01 15:30:11.072873: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13c79e5900 next 1287 of size 512
2019-10-01 15:30:11.072888: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13c79e5b00 next 1288 of size 256
2019-10-01 15:30:11.072904: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13c79e5c00 next 1289 of size 512
2019-10-01 15:30:11.072920: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13c79e5e00 next 1290 of size 16777216
2019-10-01 15:30:11.072934: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13c89e5e00 next 1291 of size 256
2019-10-01 15:30:11.072949: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13c89e5f00 next 1292 of size 256
2019-10-01 15:30:11.072964: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13c89e6000 next 1293 of size 256
2019-10-01 15:30:11.072979: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13c89e6100 next 1294 of size 256
2019-10-01 15:30:11.072993: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13c89e6200 next 1295 of size 256
2019-10-01 15:30:11.073007: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13c89e6300 next 1296 of size 16777216
2019-10-01 15:30:11.073021: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13c99e6300 next 1297 of size 67108864
2019-10-01 15:30:11.073036: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13cd9e6300 next 1298 of size 256
2019-10-01 15:30:11.073052: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13cd9e6400 next 1299 of size 256
2019-10-01 15:30:11.073067: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13cd9e6500 next 1300 of size 256
2019-10-01 15:30:11.073082: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13cd9e6600 next 1301 of size 256
2019-10-01 15:30:11.073096: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13cd9e6700 next 1302 of size 67108864
2019-10-01 15:30:11.073112: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d19e6700 next 1303 of size 16777216
2019-10-01 15:30:11.073126: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d29e6700 next 1304 of size 256
2019-10-01 15:30:11.073143: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d29e6800 next 1305 of size 256
2019-10-01 15:30:11.073160: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d29e6900 next 1306 of size 256
2019-10-01 15:30:11.073176: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d29e6a00 next 1307 of size 16777216
2019-10-01 15:30:11.073191: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d39e6a00 next 1308 of size 8192
2019-10-01 15:30:11.073205: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d39e8a00 next 1309 of size 256
2019-10-01 15:30:11.073220: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d39e8b00 next 1310 of size 8192
2019-10-01 15:30:11.073234: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d39eab00 next 1311 of size 8192
2019-10-01 15:30:11.073251: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d39ecb00 next 1312 of size 256
2019-10-01 15:30:11.073266: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d39ecc00 next 1313 of size 8192
2019-10-01 15:30:11.073281: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d39eec00 next 1314 of size 8192
2019-10-01 15:30:11.073296: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d39f0c00 next 1315 of size 256
2019-10-01 15:30:11.073322: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d39f0d00 next 1316 of size 8192
2019-10-01 15:30:11.073337: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d39f2d00 next 1317 of size 16777216
2019-10-01 15:30:11.073352: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d49f2d00 next 1318 of size 256
2019-10-01 15:30:11.073366: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d49f2e00 next 1319 of size 256
2019-10-01 15:30:11.073383: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d49f2f00 next 1320 of size 256
2019-10-01 15:30:11.073397: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d49f3000 next 1321 of size 256
2019-10-01 15:30:11.073412: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d49f3100 next 1322 of size 256
2019-10-01 15:30:11.073427: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d49f3200 next 1323 of size 16777216
2019-10-01 15:30:11.073442: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d59f3200 next 1324 of size 8192
2019-10-01 15:30:11.073457: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d59f5200 next 1325 of size 256
2019-10-01 15:30:11.073472: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d59f5300 next 1326 of size 256
2019-10-01 15:30:11.073488: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d59f5400 next 1327 of size 8192
2019-10-01 15:30:11.073502: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d59f7400 next 1328 of size 8192
2019-10-01 15:30:11.073516: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d59f9400 next 1329 of size 256
2019-10-01 15:30:11.073530: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d59f9500 next 1330 of size 8192
2019-10-01 15:30:11.073546: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d59fb500 next 1331 of size 512
2019-10-01 15:30:11.073561: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d59fb700 next 1332 of size 256
2019-10-01 15:30:11.073576: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d59fb800 next 1333 of size 512
2019-10-01 15:30:11.073592: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d59fba00 next 1334 of size 16777216
2019-10-01 15:30:11.073606: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d69fba00 next 1335 of size 256
2019-10-01 15:30:11.073622: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d69fbb00 next 1336 of size 256
2019-10-01 15:30:11.073637: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d69fbc00 next 1337 of size 256
2019-10-01 15:30:11.073654: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d69fbd00 next 1338 of size 256
2019-10-01 15:30:11.073669: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d69fbe00 next 1339 of size 256
2019-10-01 15:30:11.073684: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d69fbf00 next 1340 of size 8192
2019-10-01 15:30:11.073699: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d69fdf00 next 1341 of size 8192
2019-10-01 15:30:11.073713: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d69fff00 next 1342 of size 8192
2019-10-01 15:30:11.073729: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d6a01f00 next 1343 of size 16384000
2019-10-01 15:30:11.073744: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d79a1f00 next 1344 of size 256
2019-10-01 15:30:11.073760: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d79a2000 next 1345 of size 16384000
2019-10-01 15:30:11.073774: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d8942000 next 1346 of size 16777216
2019-10-01 15:30:11.073789: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d9942000 next 1347 of size 8192
2019-10-01 15:30:11.073811: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d9944000 next 1348 of size 8192
2019-10-01 15:30:11.073828: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d9946000 next 1349 of size 8192
2019-10-01 15:30:11.073845: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d9948000 next 1350 of size 8192
2019-10-01 15:30:11.073860: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d994a000 next 1351 of size 8192
2019-10-01 15:30:11.073873: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d994c000 next 1352 of size 8192
2019-10-01 15:30:11.073887: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d994e000 next 1353 of size 8192
2019-10-01 15:30:11.073903: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d9950000 next 1354 of size 8192
2019-10-01 15:30:11.073919: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d9952000 next 1355 of size 8192
2019-10-01 15:30:11.073933: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d9954000 next 1356 of size 256
2019-10-01 15:30:11.073949: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d9954100 next 1357 of size 8192
2019-10-01 15:30:11.073963: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d9956100 next 1358 of size 8192
2019-10-01 15:30:11.073979: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d9958100 next 1359 of size 256
2019-10-01 15:30:11.073993: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d9958200 next 1360 of size 8192
2019-10-01 15:30:11.074008: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d995a200 next 1361 of size 8192
2019-10-01 15:30:11.074024: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d995c200 next 1362 of size 256
2019-10-01 15:30:11.074038: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d995c300 next 1363 of size 8192
2019-10-01 15:30:11.074052: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d995e300 next 1364 of size 8192
2019-10-01 15:30:11.074067: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d9960300 next 1365 of size 256
2019-10-01 15:30:11.074082: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d9960400 next 1366 of size 8192
2019-10-01 15:30:11.074097: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d9962400 next 1367 of size 8192
2019-10-01 15:30:11.074112: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d9964400 next 1368 of size 256
2019-10-01 15:30:11.074126: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d9964500 next 1369 of size 8192
2019-10-01 15:30:11.074142: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d9966500 next 1370 of size 8192
2019-10-01 15:30:11.074151: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d9968500 next 1371 of size 256
2019-10-01 15:30:11.074159: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d9968600 next 1372 of size 8192
2019-10-01 15:30:11.074167: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d996a600 next 1373 of size 8192
2019-10-01 15:30:11.074175: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d996c600 next 1374 of size 256
2019-10-01 15:30:11.074184: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d996c700 next 1375 of size 8192
2019-10-01 15:30:11.074192: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13d996e700 next 1376 of size 67108864
2019-10-01 15:30:11.074200: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13dd96e700 next 1377 of size 67108864
2019-10-01 15:30:11.074208: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13e196e700 next 1378 of size 256
2019-10-01 15:30:11.074217: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13e196e800 next 1379 of size 512
2019-10-01 15:30:11.074229: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13e196ea00 next 1380 of size 16384000
2019-10-01 15:30:11.074238: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13e290ea00 next 1381 of size 16777216
2019-10-01 15:30:11.074247: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13e390ea00 next 1382 of size 256
2019-10-01 15:30:11.074255: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13e390eb00 next 1383 of size 16384000
2019-10-01 15:30:11.074263: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13e48aeb00 next 1384 of size 16777216
2019-10-01 15:30:11.074272: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13e58aeb00 next 1385 of size 16777216
2019-10-01 15:30:11.074280: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13e68aeb00 next 1386 of size 16384000
2019-10-01 15:30:11.074288: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13e784eb00 next 1387 of size 67108864
2019-10-01 15:30:11.074297: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13eb84eb00 next 1388 of size 8192
2019-10-01 15:30:11.074305: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13eb850b00 next 1389 of size 256
2019-10-01 15:30:11.074313: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13eb850c00 next 1390 of size 8192
2019-10-01 15:30:11.074321: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13eb852c00 next 1391 of size 8192
2019-10-01 15:30:11.074330: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13eb854c00 next 1392 of size 256
2019-10-01 15:30:11.074338: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13eb854d00 next 1393 of size 8192
2019-10-01 15:30:11.074346: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13eb856d00 next 1394 of size 512
2019-10-01 15:30:11.074354: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13eb856f00 next 1395 of size 256
2019-10-01 15:30:11.074363: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13eb857000 next 1396 of size 512
2019-10-01 15:30:11.074371: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13eb857200 next 1397 of size 16777216
2019-10-01 15:30:11.074379: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13ec857200 next 1398 of size 16384000
2019-10-01 15:30:11.074388: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13ed7f7200 next 1399 of size 16384000
2019-10-01 15:30:11.074396: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13ee797200 next 1400 of size 16384000
2019-10-01 15:30:11.074404: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13ef737200 next 1401 of size 256
2019-10-01 15:30:11.074414: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13ef737300 next 1402 of size 16777216
2019-10-01 15:30:11.074422: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13f0737300 next 1403 of size 16777216
2019-10-01 15:30:11.074431: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13f1737300 next 1404 of size 67108864
2019-10-01 15:30:11.074440: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13f5737300 next 1405 of size 67108864
2019-10-01 15:30:11.074445: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13f9737300 next 1406 of size 8192
2019-10-01 15:30:11.074451: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13f9739300 next 1407 of size 8192
2019-10-01 15:30:11.074456: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13f973b300 next 1408 of size 8192
2019-10-01 15:30:11.074461: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13f973d300 next 1409 of size 16777216
2019-10-01 15:30:11.074467: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13fa73d300 next 1410 of size 8192
2019-10-01 15:30:11.074478: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13fa73f300 next 1411 of size 8192
2019-10-01 15:30:11.074483: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13fa741300 next 1412 of size 8192
2019-10-01 15:30:11.074489: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13fa743300 next 1413 of size 16384000
2019-10-01 15:30:11.074494: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13fb6e3300 next 1414 of size 16384000
2019-10-01 15:30:11.074499: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13fc683300 next 1415 of size 256
2019-10-01 15:30:11.074504: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x13fc683400 next 1416 of size 67108864
2019-10-01 15:30:11.074510: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1400683400 next 1417 of size 16384000
2019-10-01 15:30:11.074515: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1401623400 next 1418 of size 8192
2019-10-01 15:30:11.074520: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1401625400 next 1419 of size 8192
2019-10-01 15:30:11.074526: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1401627400 next 1420 of size 8192
2019-10-01 15:30:11.074531: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1401629400 next 1421 of size 8192
2019-10-01 15:30:11.074598: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x140162b400 next 1422 of size 67108864
2019-10-01 15:30:11.074606: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x140562b400 next 1423 of size 67108864
2019-10-01 15:30:11.074611: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x140962b400 next 1424 of size 16777216
2019-10-01 15:30:11.074616: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x140a62b400 next 1425 of size 256
2019-10-01 15:30:11.074623: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x140a62b500 next 1426 of size 16777216
2019-10-01 15:30:11.074633: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x140b62b500 next 1427 of size 67108864
2019-10-01 15:30:11.074642: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x140f62b500 next 1428 of size 16777216
2019-10-01 15:30:11.074653: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x141062b500 next 1429 of size 67108864
2019-10-01 15:30:11.074666: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x141462b500 next 1430 of size 16777216
2019-10-01 15:30:11.074680: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x141562b500 next 1431 of size 8192
2019-10-01 15:30:11.074695: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x141562d500 next 1432 of size 32768
2019-10-01 15:30:11.074711: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1415635500 next 1433 of size 512
2019-10-01 15:30:11.074726: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1415635700 next 1434 of size 8192
2019-10-01 15:30:11.074741: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1415637700 next 1435 of size 8192
2019-10-01 15:30:11.074755: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1415639700 next 1436 of size 32768
2019-10-01 15:30:11.074769: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1415641700 next 1437 of size 8192
2019-10-01 15:30:11.074783: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1415643700 next 1438 of size 8192
2019-10-01 15:30:11.074798: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1415645700 next 1439 of size 8192
2019-10-01 15:30:11.074813: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1415647700 next 1440 of size 8192
2019-10-01 15:30:11.074828: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1415649700 next 1441 of size 8192
2019-10-01 15:30:11.074842: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x141564b700 next 1442 of size 8192
2019-10-01 15:30:11.074865: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x141564d700 next 1443 of size 256
2019-10-01 15:30:11.074883: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x141564d800 next 1444 of size 67108864
2019-10-01 15:30:11.074897: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x141964d800 next 1445 of size 8192
2019-10-01 15:30:11.074911: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x141964f800 next 1446 of size 8192
2019-10-01 15:30:11.074926: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1419651800 next 1447 of size 16777216
2019-10-01 15:30:11.074942: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x141a651800 next 1448 of size 32768
2019-10-01 15:30:11.074958: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x141a659800 next 1449 of size 256
2019-10-01 15:30:11.074974: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x141a659900 next 1450 of size 32768
2019-10-01 15:30:11.074989: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x141a661900 next 1451 of size 16777216
2019-10-01 15:30:11.075004: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x141b661900 next 1452 of size 67108864
2019-10-01 15:30:11.075020: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x141f661900 next 1453 of size 512
2019-10-01 15:30:11.075035: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x141f661b00 next 1454 of size 16777216
2019-10-01 15:30:11.075050: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1420661b00 next 1455 of size 16777216
2019-10-01 15:30:11.075064: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1421661b00 next 1456 of size 8192
2019-10-01 15:30:11.075079: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1421663b00 next 1457 of size 8192
2019-10-01 15:30:11.075095: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1421665b00 next 1458 of size 256
2019-10-01 15:30:11.075110: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1421665c00 next 1459 of size 67108864
2019-10-01 15:30:11.075125: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1425665c00 next 1460 of size 8192
2019-10-01 15:30:11.075139: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1425667c00 next 1461 of size 256
2019-10-01 15:30:11.075153: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1425667d00 next 1462 of size 8192
2019-10-01 15:30:11.075168: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1425669d00 next 1463 of size 256
2019-10-01 15:30:11.075183: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1425669e00 next 1464 of size 16777216
2019-10-01 15:30:11.075197: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1426669e00 next 1465 of size 8192
2019-10-01 15:30:11.075214: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x142666be00 next 1466 of size 16777216
2019-10-01 15:30:11.075228: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x142766be00 next 1467 of size 8192
2019-10-01 15:30:11.075244: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x142766de00 next 1468 of size 67108864
2019-10-01 15:30:11.075260: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x142b66de00 next 1469 of size 512
2019-10-01 15:30:11.075276: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x142b66e000 next 1470 of size 8192
2019-10-01 15:30:11.075291: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x142b670000 next 1471 of size 256
2019-10-01 15:30:11.075306: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x142b670100 next 1472 of size 8192
2019-10-01 15:30:11.075320: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x142b672100 next 1473 of size 16777216
2019-10-01 15:30:11.075340: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x142c672100 next 1474 of size 8192
2019-10-01 15:30:11.075357: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x142c674100 next 1475 of size 256
2019-10-01 15:30:11.075371: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x142c674200 next 1476 of size 8192
2019-10-01 15:30:11.075386: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x142c676200 next 1477 of size 8192
2019-10-01 15:30:11.075401: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x142c678200 next 1478 of size 256
2019-10-01 15:30:11.075416: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x142c678300 next 1479 of size 8192
2019-10-01 15:30:11.075431: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x142c67a300 next 1480 of size 8192
2019-10-01 15:30:11.075447: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x142c67c300 next 1481 of size 256
2019-10-01 15:30:11.075463: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x142c67c400 next 1482 of size 8192
2019-10-01 15:30:11.075478: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x142c67e400 next 1483 of size 8192
2019-10-01 15:30:11.075492: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x142c680400 next 1484 of size 16777216
2019-10-01 15:30:11.075507: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x142d680400 next 1485 of size 256
2019-10-01 15:30:11.075521: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x142d680500 next 1486 of size 8192
2019-10-01 15:30:11.075535: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x142d682500 next 1487 of size 32768
2019-10-01 15:30:11.075550: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x142d68a500 next 1488 of size 256
2019-10-01 15:30:11.075565: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x142d68a600 next 1489 of size 32768
2019-10-01 15:30:11.075580: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x142d692600 next 1490 of size 16777216
2019-10-01 15:30:11.075596: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x142e692600 next 1491 of size 16777216
2019-10-01 15:30:11.075611: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x142f692600 next 1492 of size 256
2019-10-01 15:30:11.075626: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x142f692700 next 1493 of size 8192
2019-10-01 15:30:11.075641: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x142f694700 next 1494 of size 8192
2019-10-01 15:30:11.075657: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x142f696700 next 1495 of size 256
2019-10-01 15:30:11.075672: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x142f696800 next 1496 of size 8192
2019-10-01 15:30:11.075687: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x142f698800 next 1497 of size 8192
2019-10-01 15:30:11.075703: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x142f69a800 next 1498 of size 256
2019-10-01 15:30:11.075719: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x142f69a900 next 1499 of size 8192
2019-10-01 15:30:11.075735: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x142f69c900 next 1500 of size 16777216
2019-10-01 15:30:11.075751: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x143069c900 next 1501 of size 16777216
2019-10-01 15:30:11.075767: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x143169c900 next 1502 of size 8192
2019-10-01 15:30:11.075783: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x143169e900 next 1503 of size 256
2019-10-01 15:30:11.075798: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x143169ea00 next 1504 of size 16777216
2019-10-01 15:30:11.075814: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x143269ea00 next 1505 of size 8192
2019-10-01 15:30:11.075836: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14326a0a00 next 1506 of size 256
2019-10-01 15:30:11.075852: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14326a0b00 next 1507 of size 8192
2019-10-01 15:30:11.075866: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14326a2b00 next 1508 of size 8192
2019-10-01 15:30:11.075880: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14326a4b00 next 1509 of size 256
2019-10-01 15:30:11.075895: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14326a4c00 next 1510 of size 256
2019-10-01 15:30:11.075909: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14326a4d00 next 1511 of size 8192
2019-10-01 15:30:11.075926: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14326a6d00 next 1512 of size 16777216
2019-10-01 15:30:11.075942: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14336a6d00 next 1513 of size 16777216
2019-10-01 15:30:11.075957: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14346a6d00 next 1514 of size 67108864
2019-10-01 15:30:11.075972: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14386a6d00 next 1515 of size 8192
2019-10-01 15:30:11.075986: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14386a8d00 next 1516 of size 8192
2019-10-01 15:30:11.076000: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14386aad00 next 1517 of size 16777216
2019-10-01 15:30:11.076015: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14396aad00 next 1518 of size 67108864
2019-10-01 15:30:11.076031: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x143d6aad00 next 1519 of size 67108864
2019-10-01 15:30:11.076047: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14416aad00 next 1520 of size 256
2019-10-01 15:30:11.076062: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14416aae00 next 1521 of size 512
2019-10-01 15:30:11.076077: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14416ab000 next 1522 of size 32768
2019-10-01 15:30:11.076092: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14416b3000 next 1523 of size 8192
2019-10-01 15:30:11.076106: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14416b5000 next 1524 of size 512
2019-10-01 15:30:11.076120: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14416b5200 next 1525 of size 8192
2019-10-01 15:30:11.076133: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14416b7200 next 1526 of size 8192
2019-10-01 15:30:11.076147: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14416b9200 next 1527 of size 8192
2019-10-01 15:30:11.076163: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14416bb200 next 1528 of size 8192
2019-10-01 15:30:11.076177: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14416bd200 next 1529 of size 8192
2019-10-01 15:30:11.076191: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14416bf200 next 1530 of size 8192
2019-10-01 15:30:11.076207: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14416c1200 next 1531 of size 32768
2019-10-01 15:30:11.076223: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14416c9200 next 1532 of size 512
2019-10-01 15:30:11.076238: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14416c9400 next 1533 of size 8192
2019-10-01 15:30:11.076254: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14416cb400 next 1534 of size 16777216
2019-10-01 15:30:11.076268: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14426cb400 next 1535 of size 8192
2019-10-01 15:30:11.076283: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14426cd400 next 1536 of size 67108864
2019-10-01 15:30:11.076298: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14466cd400 next 1537 of size 8192
2019-10-01 15:30:11.076323: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14466cf400 next 1538 of size 256
2019-10-01 15:30:11.076341: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14466cf500 next 1539 of size 8192
2019-10-01 15:30:11.076354: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14466d1500 next 1540 of size 8192
2019-10-01 15:30:11.076370: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14466d3500 next 1541 of size 256
2019-10-01 15:30:11.076387: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14466d3600 next 1542 of size 8192
2019-10-01 15:30:11.076402: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14466d5600 next 1543 of size 8192
2019-10-01 15:30:11.076417: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14466d7600 next 1544 of size 256
2019-10-01 15:30:11.076431: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14466d7700 next 1545 of size 8192
2019-10-01 15:30:11.076446: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14466d9700 next 1546 of size 8192
2019-10-01 15:30:11.076461: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14466db700 next 1547 of size 256
2019-10-01 15:30:11.076477: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14466db800 next 1548 of size 8192
2019-10-01 15:30:11.076492: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14466dd800 next 1549 of size 8192
2019-10-01 15:30:11.076507: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14466df800 next 1550 of size 256
2019-10-01 15:30:11.076523: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14466df900 next 1551 of size 8192
2019-10-01 15:30:11.076538: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14466e1900 next 1552 of size 16777216
2019-10-01 15:30:11.076553: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14476e1900 next 1553 of size 16777216
2019-10-01 15:30:11.076569: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14486e1900 next 1554 of size 8192
2019-10-01 15:30:11.076583: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14486e3900 next 1555 of size 256
2019-10-01 15:30:11.076592: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14486e3a00 next 1556 of size 16777216
2019-10-01 15:30:11.076600: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14496e3a00 next 1557 of size 67108864
2019-10-01 15:30:11.076609: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x144d6e3a00 next 1558 of size 67108864
2019-10-01 15:30:11.076617: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14516e3a00 next 1559 of size 256
2019-10-01 15:30:11.076625: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14516e3b00 next 1560 of size 67108864
2019-10-01 15:30:11.076634: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14556e3b00 next 1561 of size 8192
2019-10-01 15:30:11.076642: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14556e5b00 next 1562 of size 67108864
2019-10-01 15:30:11.076651: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14596e5b00 next 1563 of size 8192
2019-10-01 15:30:11.076659: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14596e7b00 next 1564 of size 256
2019-10-01 15:30:11.076667: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14596e7c00 next 1565 of size 8192
2019-10-01 15:30:11.076676: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14596e9c00 next 1566 of size 16777216
2019-10-01 15:30:11.076685: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x145a6e9c00 next 1567 of size 16777216
2019-10-01 15:30:11.076693: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x145b6e9c00 next 1568 of size 16777216
2019-10-01 15:30:11.076701: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x145c6e9c00 next 1569 of size 8192
2019-10-01 15:30:11.076713: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x145c6ebc00 next 1570 of size 8192
2019-10-01 15:30:11.076722: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x145c6edc00 next 1571 of size 8192
2019-10-01 15:30:11.076731: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x145c6efc00 next 1572 of size 8192
2019-10-01 15:30:11.076739: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x145c6f1c00 next 1573 of size 256
2019-10-01 15:30:11.076748: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x145c6f1d00 next 1574 of size 67108864
2019-10-01 15:30:11.076756: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14606f1d00 next 1575 of size 16777216
2019-10-01 15:30:11.076764: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14616f1d00 next 1576 of size 8192
2019-10-01 15:30:11.076773: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14616f3d00 next 1577 of size 256
2019-10-01 15:30:11.076782: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14616f3e00 next 1578 of size 8192
2019-10-01 15:30:11.076790: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14616f5e00 next 1579 of size 8192
2019-10-01 15:30:11.076799: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14616f7e00 next 1580 of size 256
2019-10-01 15:30:11.076807: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14616f7f00 next 1581 of size 8192
2019-10-01 15:30:11.076816: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14616f9f00 next 1582 of size 8192
2019-10-01 15:30:11.076824: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14616fbf00 next 1583 of size 256
2019-10-01 15:30:11.076833: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14616fc000 next 1584 of size 8192
2019-10-01 15:30:11.076842: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14616fe000 next 1585 of size 16777216
2019-10-01 15:30:11.076850: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14626fe000 next 1586 of size 67108864
2019-10-01 15:30:11.076858: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14666fe000 next 1587 of size 8192
2019-10-01 15:30:11.076867: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1466700000 next 1588 of size 8192
2019-10-01 15:30:11.076875: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1466702000 next 1589 of size 16777216
2019-10-01 15:30:11.076881: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1467702000 next 1590 of size 256
2019-10-01 15:30:11.076887: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1467702100 next 1591 of size 8192
2019-10-01 15:30:11.076892: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1467704100 next 1592 of size 8192
2019-10-01 15:30:11.076897: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1467706100 next 1593 of size 256
2019-10-01 15:30:11.076906: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1467706200 next 1594 of size 8192
2019-10-01 15:30:11.076915: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1467708200 next 1595 of size 8192
2019-10-01 15:30:11.076925: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146770a200 next 1596 of size 256
2019-10-01 15:30:11.076939: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146770a300 next 1597 of size 8192
2019-10-01 15:30:11.076954: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146770c300 next 1598 of size 32768
2019-10-01 15:30:11.076969: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1467714300 next 1599 of size 256
2019-10-01 15:30:11.076984: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1467714400 next 1600 of size 32768
2019-10-01 15:30:11.076999: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146771c400 next 1601 of size 8192
2019-10-01 15:30:11.077022: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146771e400 next 1602 of size 256
2019-10-01 15:30:11.077038: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146771e500 next 1603 of size 8192
2019-10-01 15:30:11.077053: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1467720500 next 1604 of size 8192
2019-10-01 15:30:11.077067: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1467722500 next 1605 of size 256
2019-10-01 15:30:11.077082: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1467722600 next 1606 of size 8192
2019-10-01 15:30:11.077097: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1467724600 next 1607 of size 16777216
2019-10-01 15:30:11.077111: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1468724600 next 1608 of size 8192
2019-10-01 15:30:11.077126: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1468726600 next 1609 of size 16777216
2019-10-01 15:30:11.077140: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1469726600 next 1610 of size 256
2019-10-01 15:30:11.077155: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1469726700 next 1611 of size 256
2019-10-01 15:30:11.077169: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1469726800 next 1612 of size 256
2019-10-01 15:30:11.077185: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x1469726900 next 1613 of size 16777216
2019-10-01 15:30:11.077199: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146a726900 next 1614 of size 16777216
2019-10-01 15:30:11.077215: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146b726900 next 1615 of size 16777216
2019-10-01 15:30:11.077231: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146c726900 next 1616 of size 512
2019-10-01 15:30:11.077247: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146c726b00 next 1617 of size 512
2019-10-01 15:30:11.077267: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146c726d00 next 1618 of size 8192
2019-10-01 15:30:11.077282: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146c728d00 next 1619 of size 256
2019-10-01 15:30:11.077297: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146c728e00 next 1620 of size 8192
2019-10-01 15:30:11.077312: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146c72ae00 next 1621 of size 32768
2019-10-01 15:30:11.077326: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146c732e00 next 1622 of size 256
2019-10-01 15:30:11.077342: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146c732f00 next 1623 of size 32768
2019-10-01 15:30:11.077356: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146c73af00 next 1624 of size 8192
2019-10-01 15:30:11.077371: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146c73cf00 next 1625 of size 256
2019-10-01 15:30:11.077386: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146c73d000 next 1626 of size 8192
2019-10-01 15:30:11.077401: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146c73f000 next 1627 of size 8192
2019-10-01 15:30:11.077416: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146c741000 next 1628 of size 256
2019-10-01 15:30:11.077431: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146c741100 next 1629 of size 8192
2019-10-01 15:30:11.077445: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146c743100 next 1630 of size 8192
2019-10-01 15:30:11.077461: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146c745100 next 1631 of size 256
2019-10-01 15:30:11.077476: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146c745200 next 1632 of size 8192
2019-10-01 15:30:11.077491: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146c747200 next 1633 of size 8192
2019-10-01 15:30:11.077513: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146c749200 next 1634 of size 256
2019-10-01 15:30:11.077530: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146c749300 next 1635 of size 8192
2019-10-01 15:30:11.077546: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146c74b300 next 1636 of size 512
2019-10-01 15:30:11.077561: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146c74b500 next 1637 of size 16777216
2019-10-01 15:30:11.077575: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146d74b500 next 1638 of size 256
2019-10-01 15:30:11.077590: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146d74b600 next 1639 of size 8192
2019-10-01 15:30:11.077604: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146d74d600 next 1640 of size 512
2019-10-01 15:30:11.077619: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146d74d800 next 1641 of size 8192
2019-10-01 15:30:11.077634: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146d74f800 next 1642 of size 8192
2019-10-01 15:30:11.077650: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146d751800 next 1643 of size 32768
2019-10-01 15:30:11.077665: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146d759800 next 1644 of size 256
2019-10-01 15:30:11.077680: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146d759900 next 1645 of size 8192
2019-10-01 15:30:11.077695: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146d75b900 next 1646 of size 8192
2019-10-01 15:30:11.077709: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146d75d900 next 1647 of size 8192
2019-10-01 15:30:11.077724: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146d75f900 next 1648 of size 8192
2019-10-01 15:30:11.077737: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146d761900 next 1649 of size 8192
2019-10-01 15:30:11.077753: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146d763900 next 1650 of size 8192
2019-10-01 15:30:11.077767: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146d765900 next 1651 of size 256
2019-10-01 15:30:11.077782: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146d765a00 next 1652 of size 8192
2019-10-01 15:30:11.077796: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146d767a00 next 1653 of size 8192
2019-10-01 15:30:11.077810: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146d769a00 next 1654 of size 256
2019-10-01 15:30:11.077825: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146d769b00 next 1655 of size 8192
2019-10-01 15:30:11.077840: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146d76bb00 next 1656 of size 16777216
2019-10-01 15:30:11.077855: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146e76bb00 next 1657 of size 256
2019-10-01 15:30:11.077869: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x146e76bc00 next 1658 of size 67108864
2019-10-01 15:30:11.077884: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x147276bc00 next 1659 of size 67108864
2019-10-01 15:30:11.077898: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x147676bc00 next 1660 of size 8192
2019-10-01 15:30:11.077913: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x147676dc00 next 1661 of size 256
2019-10-01 15:30:11.077928: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x147676dd00 next 1662 of size 512
2019-10-01 15:30:11.077944: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x147676df00 next 1663 of size 16777216
2019-10-01 15:30:11.077958: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x147776df00 next 1664 of size 16777216
2019-10-01 15:30:11.077973: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x147876df00 next 1665 of size 67108864
2019-10-01 15:30:11.077994: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x147c76df00 next 1666 of size 8192
2019-10-01 15:30:11.078009: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x147c76ff00 next 1667 of size 256
2019-10-01 15:30:11.078024: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x147c770000 next 1668 of size 8192
2019-10-01 15:30:11.078038: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x147c772000 next 1669 of size 512
2019-10-01 15:30:11.078053: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x147c772200 next 1670 of size 256
2019-10-01 15:30:11.078067: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x147c772300 next 1671 of size 512
2019-10-01 15:30:11.078081: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x147c772500 next 1672 of size 8192
2019-10-01 15:30:11.078097: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x147c774500 next 1673 of size 256
2019-10-01 15:30:11.078112: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x147c774600 next 1674 of size 8192
2019-10-01 15:30:11.078126: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x147c776600 next 1675 of size 512
2019-10-01 15:30:11.078141: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x147c776800 next 1676 of size 256
2019-10-01 15:30:11.078157: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x147c776900 next 1677 of size 512
2019-10-01 15:30:11.078171: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x147c776b00 next 1678 of size 8192
2019-10-01 15:30:11.078186: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x147c778b00 next 1679 of size 256
2019-10-01 15:30:11.078201: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x147c778c00 next 1680 of size 16777216
2019-10-01 15:30:11.078215: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x147d778c00 next 1681 of size 8192
2019-10-01 15:30:11.078232: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x147d77ac00 next 1682 of size 8192
2019-10-01 15:30:11.078246: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x147d77cc00 next 1683 of size 32768
2019-10-01 15:30:11.078261: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x147d784c00 next 1684 of size 256
2019-10-01 15:30:11.078276: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x147d784d00 next 1685 of size 8192
2019-10-01 15:30:11.078291: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x147d786d00 next 1686 of size 8192
2019-10-01 15:30:11.078306: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x147d788d00 next 1687 of size 8192
2019-10-01 15:30:11.078321: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x147d78ad00 next 1688 of size 8192
2019-10-01 15:30:11.078336: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x147d78cd00 next 1689 of size 8192
2019-10-01 15:30:11.078351: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x147d78ed00 next 1690 of size 256
2019-10-01 15:30:11.078366: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x147d78ee00 next 1691 of size 8192
2019-10-01 15:30:11.078379: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x147d790e00 next 1692 of size 16777216
2019-10-01 15:30:11.078395: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x147e790e00 next 1693 of size 16777216
2019-10-01 15:30:11.078410: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x147f790e00 next 1694 of size 8192
2019-10-01 15:30:11.078425: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x147f792e00 next 1695 of size 8192
2019-10-01 15:30:11.078440: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x147f794e00 next 1696 of size 8192
2019-10-01 15:30:11.078454: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x147f796e00 next 1697 of size 8192
2019-10-01 15:30:11.078475: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x147f798e00 next 1698 of size 8192
2019-10-01 15:30:11.078491: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x147f79ae00 next 1699 of size 8192
2019-10-01 15:30:11.078506: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x147f79ce00 next 1700 of size 16777216
2019-10-01 15:30:11.078520: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x148079ce00 next 1701 of size 8192
2019-10-01 15:30:11.078552: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x148079ee00 next 1702 of size 256
2019-10-01 15:30:11.078568: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x148079ef00 next 1703 of size 16777216
2019-10-01 15:30:11.078582: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x148179ef00 next 1704 of size 256
2019-10-01 15:30:11.078596: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x148179f000 next 1705 of size 256
2019-10-01 15:30:11.078611: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x148179f100 next 1706 of size 67108864
2019-10-01 15:30:11.078628: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x148579f100 next 1707 of size 16777216
2019-10-01 15:30:11.078642: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x148679f100 next 1708 of size 67108864
2019-10-01 15:30:11.078656: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x148a79f100 next 1709 of size 8192
2019-10-01 15:30:11.078671: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x148a7a1100 next 1710 of size 16777216
2019-10-01 15:30:11.078686: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x148b7a1100 next 1711 of size 8192
2019-10-01 15:30:11.078699: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x148b7a3100 next 1712 of size 67108864
2019-10-01 15:30:11.078710: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x148f7a3100 next 1713 of size 16777216
2019-10-01 15:30:11.078720: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14907a3100 next 1714 of size 16777216
2019-10-01 15:30:11.078729: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14917a3100 next 1715 of size 16777216
2019-10-01 15:30:11.078738: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14927a3100 next 1716 of size 8192
2019-10-01 15:30:11.078746: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14927a5100 next 1717 of size 8192
2019-10-01 15:30:11.078754: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14927a7100 next 1718 of size 16777216
2019-10-01 15:30:11.078762: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14937a7100 next 1719 of size 512
2019-10-01 15:30:11.078773: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14937a7300 next 1720 of size 16777216
2019-10-01 15:30:11.078787: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14947a7300 next 1721 of size 8192
2019-10-01 15:30:11.078800: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14947a9300 next 1722 of size 256
2019-10-01 15:30:11.078815: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14947a9400 next 1723 of size 16777216
2019-10-01 15:30:11.078828: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14957a9400 next 1724 of size 8192
2019-10-01 15:30:11.078842: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14957ab400 next 1725 of size 512
2019-10-01 15:30:11.078857: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14957ab600 next 1726 of size 16777216
2019-10-01 15:30:11.078871: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14967ab600 next 1727 of size 8192
2019-10-01 15:30:11.078884: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14967ad600 next 1728 of size 256
2019-10-01 15:30:11.078906: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14967ad700 next 1729 of size 8192
2019-10-01 15:30:11.078921: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14967af700 next 1730 of size 16777216
2019-10-01 15:30:11.078936: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14977af700 next 1731 of size 8192
2019-10-01 15:30:11.078950: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14977b1700 next 1732 of size 67108864
2019-10-01 15:30:11.078963: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x149b7b1700 next 1733 of size 16777216
2019-10-01 15:30:11.078976: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x149c7b1700 next 1734 of size 8192
2019-10-01 15:30:11.078989: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x149c7b3700 next 1735 of size 256
2019-10-01 15:30:11.079003: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x149c7b3800 next 1736 of size 256
2019-10-01 15:30:11.079016: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x149c7b3900 next 1737 of size 256
2019-10-01 15:30:11.079031: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x149c7b3a00 next 1738 of size 512
2019-10-01 15:30:11.079046: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x149c7b3c00 next 1739 of size 256
2019-10-01 15:30:11.079060: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x149c7b3d00 next 1740 of size 512
2019-10-01 15:30:11.079075: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x149c7b3f00 next 1741 of size 8192
2019-10-01 15:30:11.079089: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x149c7b5f00 next 1742 of size 256
2019-10-01 15:30:11.079103: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x149c7b6000 next 1743 of size 8192
2019-10-01 15:30:11.079117: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x149c7b8000 next 1744 of size 8192
2019-10-01 15:30:11.079130: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x149c7ba000 next 1745 of size 256
2019-10-01 15:30:11.079146: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x149c7ba100 next 1746 of size 8192
2019-10-01 15:30:11.079161: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x149c7bc100 next 1747 of size 67108864
2019-10-01 15:30:11.079176: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x14a07bc100 next 18446744073709551615 of size 109975552
2019-10-01 15:30:11.079189: I tensorflow/core/common_runtime/bfc_allocator.cc:914]      Summary of in-use Chunks by size: 
2019-10-01 15:30:11.079215: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 769 Chunks of size 256 totalling 192.2KiB
2019-10-01 15:30:11.079234: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 55 Chunks of size 512 totalling 27.5KiB
2019-10-01 15:30:11.079250: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 1280 totalling 1.2KiB
2019-10-01 15:30:11.079266: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 4096 totalling 4.0KiB
2019-10-01 15:30:11.079283: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 515 Chunks of size 8192 totalling 4.02MiB
2019-10-01 15:30:11.079300: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 10752 totalling 10.5KiB
2019-10-01 15:30:11.079317: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 12800 totalling 12.5KiB
2019-10-01 15:30:11.079332: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 51 Chunks of size 32768 totalling 1.59MiB
2019-10-01 15:30:11.079349: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 3 Chunks of size 131072 totalling 384.0KiB
2019-10-01 15:30:11.079365: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 131328 totalling 128.2KiB
2019-10-01 15:30:11.079382: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 8388608 totalling 8.00MiB
2019-10-01 15:30:11.079404: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 32 Chunks of size 16384000 totalling 500.00MiB
2019-10-01 15:30:11.079421: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 213 Chunks of size 16777216 totalling 3.33GiB
2019-10-01 15:30:11.079437: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 102 Chunks of size 67108864 totalling 6.38GiB
2019-10-01 15:30:11.079452: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 109975552 totalling 104.88MiB
2019-10-01 15:30:11.079468: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 262144000 totalling 250.00MiB
2019-10-01 15:30:11.079485: I tensorflow/core/common_runtime/bfc_allocator.cc:921] Sum Total of in-use chunks: 10.55GiB
2019-10-01 15:30:11.079500: I tensorflow/core/common_runtime/bfc_allocator.cc:923] total_region_allocated_bytes_: 11330115840 memory_limit_: 11330115994 available bytes: 154 curr_region_allocation_bytes_: 22660232192
2019-10-01 15:30:11.079520: I tensorflow/core/common_runtime/bfc_allocator.cc:929] Stats: 
Limit:                 11330115994
InUse:                 11330115840
MaxInUse:              11330115840
NumAllocs:                    2072
MaxAllocSize:            262144000

2019-10-01 15:30:11.079654: W tensorflow/core/common_runtime/bfc_allocator.cc:424] ****************************************************************************************************
2019-10-01 15:30:11.103157: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at cwise_ops_common.cc:70 : Resource exhausted: OOM when allocating tensor with shape[] and type float on /job:local/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
2019-10-01 15:30:13.133584: I lingvo/core/ops/record_yielder.cc:353] 0x7f1ad499c830Basic record yielder exit
E1001 15:30:17.710438 139811655153408 base_runner.py:212] trainer done (fatal error): <class 'tensorflow.python.framework.errors_impl.InternalError'>
I1001 15:30:17.712625 139811655153408 base_runner.py:106] trainer exception: From /job:local/replica:0/task:0:
Dst tensor is not initialized.
	 [[node mul_271 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1748) ]]

Original stack trace for 'mul_271':
  File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 1837, in <module>
    tf.app.run(main)
  File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File "usr/local/lib/python3.6/dist-packages/absl/app.py", line 299, in run
    _run_main(main, args)
  File "usr/local/lib/python3.6/dist-packages/absl/app.py", line 250, in _run_main
    sys.exit(main(argv))
  File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 1833, in main
    RunnerManager(FLAGS.model).Start()
  File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 1825, in Start
    self.StartRunners(self.CreateRunners(FLAGS.job.split(','), FLAGS.logdir))
  File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 1573, in CreateRunners
    trial)
  File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 1532, in _CreateRunner
    return self.Trainer(cfg, *common_args)
  File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 395, in __init__
    self._model.ConstructFPropBPropGraph()
  File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/base_model.py", line 1221, in ConstructFPropBPropGraph
    self._task.BProp()
  File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/base_model.py", line 563, in BProp
    self._BPropForVariables(self.vars)
  File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/base_model.py", line 594, in _BPropForVariables
    gradient_adjuster=self.AdjustGradients)
  File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/learner.py", line 172, in Apply
    var_grads, p.l2_regularizer_weight, p=2.0)
  File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/py_utils.py", line 2035, in AdjustGradientsWithLpLoss
    return lp_loss, var_grads.Transform(LpGrad)
  File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/py_utils.py", line 636, in Transform
    return Transform(self, fn)
  File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/py_utils.py", line 499, in Transform
    values = [Transform(v[k], fn) for k in keys]
  File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/py_utils.py", line 499, in <listcomp>
    values = [Transform(v[k], fn) for k in keys]
  File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/py_utils.py", line 499, in Transform
    values = [Transform(v[k], fn) for k in keys]
  File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/py_utils.py", line 499, in <listcomp>
    values = [Transform(v[k], fn) for k in keys]
  File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/py_utils.py", line 499, in Transform
    values = [Transform(v[k], fn) for k in keys]
  File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/py_utils.py", line 499, in <listcomp>
    values = [Transform(v[k], fn) for k in keys]
  File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/py_utils.py", line 499, in Transform
    values = [Transform(v[k], fn) for k in keys]
  File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/py_utils.py", line 499, in <listcomp>
    values = [Transform(v[k], fn) for k in keys]
  File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/py_utils.py", line 499, in Transform
    values = [Transform(v[k], fn) for k in keys]
  File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/py_utils.py", line 499, in <listcomp>
    values = [Transform(v[k], fn) for k in keys]
  File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/py_utils.py", line 499, in Transform
    values = [Transform(v[k], fn) for k in keys]
  File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/py_utils.py", line 499, in <listcomp>
    values = [Transform(v[k], fn) for k in keys]
  File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/py_utils.py", line 499, in Transform
    values = [Transform(v[k], fn) for k in keys]
  File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/py_utils.py", line 499, in <listcomp>
    values = [Transform(v[k], fn) for k in keys]
  File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/py_utils.py", line 502, in Transform
    return fn(v)
  File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/py_utils.py", line 2030, in LpGrad
    delta = lp_regularizer_weight * grad_v
  File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/variables.py", line 1078, in _run_op
    return tensor_oper(a.value(), *args, **kwargs)
  File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_ops.py", line 925, in r_binary_op_wrapper
    return func(x, y, name=name)
  File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_ops.py", line 1206, in _mul_dispatch
    return gen_math_ops.mul(x, y, name=name)
  File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/gen_math_ops.py", line 6704, in mul
    "Mul", x=x, y=y, name=name)
  File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/op_def_library.py", line 793, in _apply_op_helper
    op_def=op_def)
  File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py", line 507, in new_func
    return func(*args, **kwargs)
  File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py", line 3357, in create_op
    attrs, op_def, compute_device)
  File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py", line 3426, in _create_op_internal
    op_def=op_def)
  File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py", line 1748, in __init__
    self._traceback = tf_stack.extract_stack()


E1001 15:30:17.782090 139811655153408 base_runner.py:219] Traceback (most recent call last):
E1001 15:30:17.782230 139811655153408 base_runner.py:219]   File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py", line 1369, in _do_call
E1001 15:30:17.782293 139811655153408 base_runner.py:219]     return fn(*args)
E1001 15:30:17.782347 139811655153408 base_runner.py:219]   File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py", line 1352, in _run_fn
E1001 15:30:17.782397 139811655153408 base_runner.py:219]     target_list, run_metadata)
E1001 15:30:17.782445 139811655153408 base_runner.py:219]   File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py", line 1447, in _call_tf_sessionrun
E1001 15:30:17.782493 139811655153408 base_runner.py:219]     run_metadata)
E1001 15:30:17.782582 139811655153408 base_runner.py:219] tensorflow.python.framework.errors_impl.InternalError: From /job:local/replica:0/task:0:
E1001 15:30:17.782682 139811655153408 base_runner.py:219] Dst tensor is not initialized.
E1001 15:30:17.782772 139811655153408 base_runner.py:219] 	 [[{{node mul_271}}]]
E1001 15:30:17.782862 139811655153408 base_runner.py:219] 
E1001 15:30:17.782953 139811655153408 base_runner.py:219] During handling of the above exception, another exception occurred:
E1001 15:30:17.783053 139811655153408 base_runner.py:219] 
E1001 15:30:17.783165 139811655153408 base_runner.py:219] Traceback (most recent call last):
E1001 15:30:17.783278 139811655153408 base_runner.py:219]   File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/base_runner.py", line 168, in _RunLoop
E1001 15:30:17.783387 139811655153408 base_runner.py:219]     loop_func(*loop_args)
E1001 15:30:17.783497 139811655153408 base_runner.py:219]   File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 536, in _Loop
E1001 15:30:17.783607 139811655153408 base_runner.py:219]     model_task.per_example_tensors,
E1001 15:30:17.783717 139811655153408 base_runner.py:219]   File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py", line 956, in run
E1001 15:30:17.783828 139811655153408 base_runner.py:219]     run_metadata_ptr)
E1001 15:30:17.783937 139811655153408 base_runner.py:219]   File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py", line 1180, in _run
E1001 15:30:17.784046 139811655153408 base_runner.py:219]     feed_dict_tensor, options, run_metadata)
E1001 15:30:17.784149 139811655153408 base_runner.py:219]   File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py", line 1361, in _do_run
E1001 15:30:17.784257 139811655153408 base_runner.py:219]     run_metadata)
E1001 15:30:17.784365 139811655153408 base_runner.py:219]   File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py", line 1388, in _do_call
E1001 15:30:17.784465 139811655153408 base_runner.py:219]     raise type(e)(node_def, op, message)
E1001 15:30:17.784549 139811655153408 base_runner.py:219] tensorflow.python.framework.errors_impl.InternalError: From /job:local/replica:0/task:0:
E1001 15:30:17.784634 139811655153408 base_runner.py:219] Dst tensor is not initialized.
E1001 15:30:17.784715 139811655153408 base_runner.py:219] 	 [[node mul_271 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1748) ]]
E1001 15:30:17.784797 139811655153408 base_runner.py:219] 
E1001 15:30:17.784879 139811655153408 base_runner.py:219] Original stack trace for 'mul_271':
E1001 15:30:17.784961 139811655153408 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 1837, in <module>
E1001 15:30:17.785043 139811655153408 base_runner.py:219]     tf.app.run(main)
E1001 15:30:17.785132 139811655153408 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py", line 40, in run
E1001 15:30:17.785215 139811655153408 base_runner.py:219]     _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
E1001 15:30:17.785299 139811655153408 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/absl/app.py", line 299, in run
E1001 15:30:17.785381 139811655153408 base_runner.py:219]     _run_main(main, args)
E1001 15:30:17.785463 139811655153408 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/absl/app.py", line 250, in _run_main
E1001 15:30:17.785547 139811655153408 base_runner.py:219]     sys.exit(main(argv))
E1001 15:30:17.785630 139811655153408 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 1833, in main
E1001 15:30:17.785718 139811655153408 base_runner.py:219]     RunnerManager(FLAGS.model).Start()
E1001 15:30:17.785806 139811655153408 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 1825, in Start
E1001 15:30:17.785895 139811655153408 base_runner.py:219]     self.StartRunners(self.CreateRunners(FLAGS.job.split(','), FLAGS.logdir))
E1001 15:30:17.785984 139811655153408 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 1573, in CreateRunners
E1001 15:30:17.786071 139811655153408 base_runner.py:219]     trial)
E1001 15:30:17.786159 139811655153408 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 1532, in _CreateRunner
E1001 15:30:17.786247 139811655153408 base_runner.py:219]     return self.Trainer(cfg, *common_args)
E1001 15:30:17.786332 139811655153408 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 395, in __init__
E1001 15:30:17.786419 139811655153408 base_runner.py:219]     self._model.ConstructFPropBPropGraph()
E1001 15:30:17.786507 139811655153408 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/base_model.py", line 1221, in ConstructFPropBPropGraph
E1001 15:30:17.786616 139811655153408 base_runner.py:219]     self._task.BProp()
E1001 15:30:17.786706 139811655153408 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/base_model.py", line 563, in BProp
E1001 15:30:17.786792 139811655153408 base_runner.py:219]     self._BPropForVariables(self.vars)
E1001 15:30:17.786880 139811655153408 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/base_model.py", line 594, in _BPropForVariables
E1001 15:30:17.786967 139811655153408 base_runner.py:219]     gradient_adjuster=self.AdjustGradients)
E1001 15:30:17.787055 139811655153408 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/learner.py", line 172, in Apply
E1001 15:30:17.787135 139811655153408 base_runner.py:219]     var_grads, p.l2_regularizer_weight, p=2.0)
E1001 15:30:17.787209 139811655153408 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/py_utils.py", line 2035, in AdjustGradientsWithLpLoss
E1001 15:30:17.787286 139811655153408 base_runner.py:219]     return lp_loss, var_grads.Transform(LpGrad)
E1001 15:30:17.787377 139811655153408 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/py_utils.py", line 636, in Transform
E1001 15:30:17.787467 139811655153408 base_runner.py:219]     return Transform(self, fn)
E1001 15:30:17.787555 139811655153408 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/py_utils.py", line 499, in Transform
E1001 15:30:17.787644 139811655153408 base_runner.py:219]     values = [Transform(v[k], fn) for k in keys]
E1001 15:30:17.787736 139811655153408 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/py_utils.py", line 499, in <listcomp>
E1001 15:30:17.787823 139811655153408 base_runner.py:219]     values = [Transform(v[k], fn) for k in keys]
E1001 15:30:17.787911 139811655153408 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/py_utils.py", line 499, in Transform
E1001 15:30:17.787999 139811655153408 base_runner.py:219]     values = [Transform(v[k], fn) for k in keys]
E1001 15:30:17.788086 139811655153408 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/py_utils.py", line 499, in <listcomp>
E1001 15:30:17.788175 139811655153408 base_runner.py:219]     values = [Transform(v[k], fn) for k in keys]
E1001 15:30:17.788264 139811655153408 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/py_utils.py", line 499, in Transform
E1001 15:30:17.788350 139811655153408 base_runner.py:219]     values = [Transform(v[k], fn) for k in keys]
E1001 15:30:17.788439 139811655153408 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/py_utils.py", line 499, in <listcomp>
E1001 15:30:17.788517 139811655153408 base_runner.py:219]     values = [Transform(v[k], fn) for k in keys]
E1001 15:30:17.788568 139811655153408 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/py_utils.py", line 499, in Transform
E1001 15:30:17.788614 139811655153408 base_runner.py:219]     values = [Transform(v[k], fn) for k in keys]
E1001 15:30:17.788661 139811655153408 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/py_utils.py", line 499, in <listcomp>
E1001 15:30:17.788707 139811655153408 base_runner.py:219]     values = [Transform(v[k], fn) for k in keys]
E1001 15:30:17.788772 139811655153408 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/py_utils.py", line 499, in Transform
E1001 15:30:17.788823 139811655153408 base_runner.py:219]     values = [Transform(v[k], fn) for k in keys]
E1001 15:30:17.788870 139811655153408 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/py_utils.py", line 499, in <listcomp>
E1001 15:30:17.788916 139811655153408 base_runner.py:219]     values = [Transform(v[k], fn) for k in keys]
E1001 15:30:17.788962 139811655153408 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/py_utils.py", line 499, in Transform
E1001 15:30:17.789008 139811655153408 base_runner.py:219]     values = [Transform(v[k], fn) for k in keys]
E1001 15:30:17.789053 139811655153408 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/py_utils.py", line 499, in <listcomp>
E1001 15:30:17.789098 139811655153408 base_runner.py:219]     values = [Transform(v[k], fn) for k in keys]
E1001 15:30:17.789144 139811655153408 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/py_utils.py", line 499, in Transform
E1001 15:30:17.789190 139811655153408 base_runner.py:219]     values = [Transform(v[k], fn) for k in keys]
E1001 15:30:17.789236 139811655153408 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/py_utils.py", line 499, in <listcomp>
E1001 15:30:17.789282 139811655153408 base_runner.py:219]     values = [Transform(v[k], fn) for k in keys]
E1001 15:30:17.789328 139811655153408 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/py_utils.py", line 502, in Transform
E1001 15:30:17.789373 139811655153408 base_runner.py:219]     return fn(v)
E1001 15:30:17.789418 139811655153408 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/py_utils.py", line 2030, in LpGrad
E1001 15:30:17.789464 139811655153408 base_runner.py:219]     delta = lp_regularizer_weight * grad_v
E1001 15:30:17.789515 139811655153408 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/variables.py", line 1078, in _run_op
E1001 15:30:17.789562 139811655153408 base_runner.py:219]     return tensor_oper(a.value(), *args, **kwargs)
E1001 15:30:17.789608 139811655153408 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_ops.py", line 925, in r_binary_op_wrapper
E1001 15:30:17.789654 139811655153408 base_runner.py:219]     return func(x, y, name=name)
E1001 15:30:17.789700 139811655153408 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_ops.py", line 1206, in _mul_dispatch
E1001 15:30:17.789745 139811655153408 base_runner.py:219]     return gen_math_ops.mul(x, y, name=name)
E1001 15:30:17.789791 139811655153408 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/gen_math_ops.py", line 6704, in mul
E1001 15:30:17.789837 139811655153408 base_runner.py:219]     "Mul", x=x, y=y, name=name)
E1001 15:30:17.789901 139811655153408 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/op_def_library.py", line 793, in _apply_op_helper
E1001 15:30:17.789951 139811655153408 base_runner.py:219]     op_def=op_def)
E1001 15:30:17.789997 139811655153408 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py", line 507, in new_func
E1001 15:30:17.790043 139811655153408 base_runner.py:219]     return func(*args, **kwargs)
E1001 15:30:17.790090 139811655153408 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py", line 3357, in create_op
E1001 15:30:17.790136 139811655153408 base_runner.py:219]     attrs, op_def, compute_device)
E1001 15:30:17.790181 139811655153408 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py", line 3426, in _create_op_internal
E1001 15:30:17.790227 139811655153408 base_runner.py:219]     op_def=op_def)
E1001 15:30:17.790273 139811655153408 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py", line 1748, in __init__
E1001 15:30:17.790318 139811655153408 base_runner.py:219]     self._traceback = tf_stack.extract_stack()
E1001 15:30:17.790364 139811655153408 base_runner.py:219] 
E1001 15:30:17.790410 139811655153408 base_runner.py:219] 
WARNING:tensorflow:Issue encountered when serializing __model_split_id_stack.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
W1001 15:30:23.509442 139811663546112 meta_graph.py:448] Issue encountered when serializing __model_split_id_stack.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing __batch_norm_update_dict.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'dict' object has no attribute 'name'
W1001 15:30:23.513944 139811663546112 meta_graph.py:448] Issue encountered when serializing __batch_norm_update_dict.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'dict' object has no attribute 'name'
I1001 15:30:25.466457 139811663546112 checkpointer.py:113] Save checkpoint done: /tmp/mnist/log/train/ckpt-00000000
I1001 15:30:25.473957 139811663546112 trainer.py:380] Steps/second: 0.000000, Examples/second: 0.000000
